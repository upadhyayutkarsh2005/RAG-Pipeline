{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6300b3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hii\n"
     ]
    }
   ],
   "source": [
    "print(\"hii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea5f86d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce40ce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b81d0d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=Document(\n",
    "    page_content=\"this is the main text content I am using to create RAG\",\n",
    "    metadata={\n",
    "        \"source\":\"exmaple.txt\",\n",
    "        \"pages\":1,\n",
    "        \"author\":\"Utkarsh Upadhyay\",\n",
    "        \"date_created\":\"2025-12-16\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "815f4f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'exmaple.txt', 'pages': 1, 'author': 'Utkarsh Upadhyay', 'date_created': '2025-12-16'}, page_content='this is the main text content I am using to create RAG')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5acd44d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##create a simple text file\n",
    "\n",
    "import os\n",
    "os.makedirs(\"../data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b210d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sample text files created!\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"../data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
    "    \n",
    "    \"../data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
    "from experience without being explicitly programmed. It focuses on developing computer programs\n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"✅ Sample text files created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0d9728e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/ragenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "##text loader\n",
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3f8f5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
     ]
    }
   ],
   "source": [
    "loader=TextLoader(\"../data/text_files/python_intro.txt\",encoding=\"utf-8\")\n",
    "document=loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "009577c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Directory Loader\n",
    "from langchain_community.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21290407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.'),\n",
       " Document(metadata={'source': '../data/text_files/machine_learning.txt'}, page_content='Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    ')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load all the text files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/text_files\",\n",
    "    glob=\"**/*.txt\", ## Pattern to match files  \n",
    "    loader_cls= TextLoader, ##loader class to use\n",
    "    loader_kwargs={'encoding': 'utf-8'},\n",
    "    show_progress=False\n",
    "\n",
    ")\n",
    "\n",
    "documents=dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4734d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 0}, page_content='M A N N I N G\\nSebastian Raschka\\nFROM\\nSCRATCH\\nBUILD A'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 1}, page_content='1) Data\\npreparation\\n& sampling\\n2) Attention\\nmechanism\\nBuilding an LLM\\nSTAGE 1\\nFoundation model\\nSTAGE 2\\nSTAGE 3\\nClassiﬁer\\nPersonal assistant\\nDataset with class labels\\nInstruction dataset\\nImplements the data sampling and\\nunderstand the basic mechanism\\nPretrains the LLM on unlabeled\\ndata to obtain a foundation\\nmodel for further ﬁne-tuning\\nFine-tunes the pretrained\\nLLM to create a\\nclassiﬁcation model\\nFine-tunes the pretrained\\nLLM to create a personal\\nassistant or chat model\\n4) Pretraining\\n9) Fine-tuning\\n5) Training\\nloop\\n6) Model\\nevaluation\\n7) Load\\npretrained\\nweights\\n8) Fine-tuning\\n3) LLM\\narchitecture\\nThe three main stages of coding a large language model (LLM) are implementing the LLM architecture and data \\npreparation process (stage 1), pretraining an LLM to create a foundation model (stage 2), and fine-tuning the \\nfoundation model to become a personal assistant or text classifier (stage 3). Each of these stages is explored \\nand implemented in this book.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 2}, page_content='Build a Large Language Model (From Scratch)'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 3}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 4}, page_content='Build a Large\\nLanguage Model\\n(From Scratch)\\nSEBASTIAN RASCHKA\\nM A N N I N G\\nSHELTER ISLAND'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 5}, page_content='For online information and ordering of this and other Manning books, please visit\\nwww.manning.com. The publisher offers discounts on this book when ordered in quantity. \\nFor more information, please contact\\nSpecial Sales Department\\nManning Publications Co.\\n20 Baldwin Road\\nPO Box 761\\nShelter Island, NY 11964\\nEmail: orders@manning.com\\n©2025 by Manning Publications Co. All rights reserved.\\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in \\nany form or by means electronic, mechanical, photocopying, or otherwise, without prior written \\npermission of the publisher.\\nMany of the designations used by manufacturers and sellers to distinguish their products are \\nclaimed as trademarks. Where those designations appear in the book, and Manning Publications \\nwas aware of a trademark claim, the designations have been printed in initial caps or all caps.\\nRecognizing the importance of preserving what has been written, it is Manning’s policy to have \\nthe books we publish printed on acid-free paper, and we exert our best efforts to that end. \\nRecognizing also our responsibility to conserve the resources of our planet, Manning books\\nare printed on paper that is at least 15 percent recycled and processed without the use of \\nelemental chlorine.\\nThe authors and publisher have made every effort to ensure that the information in this book \\nwas correct at press time. The authors and publisher do not assume and hereby disclaim any \\nliability to any party for any loss, damage, or disruption caused by errors or omissions, whether \\nsuch errors or omissions result from negligence, accident, or any other cause, or from any usage \\nof the information herein.\\nManning Publications Co.\\nDevelopment editor: Dustin Archibald\\n20 Baldwin Road\\nTechnical editor: David Caswell\\nPO Box 761\\nReview editor: Kishor Rit\\nShelter Island, NY 11964\\nProduction editor: Aleksandar Dragosavljevic´\\nCopy editors: Kari Lucke and Alisa Larson\\nProofreader: Mike Beady\\nTechnical proofreader: Jerry Kuch\\nTypesetter: Dennis Dalinnik\\nCover designer: Marija Tudor\\nISBN: 9781633437166\\nPrinted in the United States of America'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 6}, page_content='v\\nbrief contents\\n1\\n■\\nUnderstanding large language models\\n1\\n2\\n■\\nWorking with text data\\n17\\n3\\n■\\nCoding attention mechanisms\\n50\\n4\\n■\\nImplementing a GPT model from scratch to \\ngenerate text\\n92\\n5\\n■\\nPretraining on unlabeled data\\n128\\n6\\n■\\nFine-tuning for classification\\n169\\n7\\n■\\nFine-tuning to follow instructions\\n204\\nA\\n■\\nIntroduction to PyTorch\\n251\\nB\\n■\\nReferences and further reading\\n289\\nC\\n■\\nExercise solutions\\n300\\nD\\n■\\nAdding bells and whistles to the training loop\\n313\\nE\\n■\\nParameter-efficient fine-tuning with LoRA\\n322'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 7}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 8}, page_content='vii\\ncontents\\npreface\\nxi\\nacknowledgments\\nxiii\\nabout this book\\nxv\\nabout the author\\nxix\\nabout the cover illustration\\nxx\\n1 Understanding large language models\\n1\\n1.1\\nWhat is an LLM?\\n2\\n1.2\\nApplications of LLMs\\n4\\n1.3\\nStages of building and using LLMs\\n5\\n1.4\\nIntroducing the transformer architecture\\n7\\n1.5\\nUtilizing large datasets\\n10\\n1.6\\nA closer look at the GPT architecture\\n12\\n1.7\\nBuilding a large language model\\n14\\n2 Working with text data\\n17\\n2.1\\nUnderstanding word embeddings\\n18\\n2.2\\nTokenizing text\\n21\\n2.3\\nConverting tokens into token IDs\\n24\\n2.4\\nAdding special context tokens\\n29'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 9}, page_content='CONTENTS\\nviii\\n2.5\\nByte pair encoding\\n33\\n2.6\\nData sampling with a sliding window\\n35\\n2.7\\nCreating token embeddings\\n41\\n2.8\\nEncoding word positions\\n43\\n3 Coding attention mechanisms\\n50\\n3.1\\nThe problem with modeling long sequences\\n52\\n3.2\\nCapturing data dependencies with attention \\nmechanisms\\n54\\n3.3\\nAttending to different parts of the input with \\nself-attention\\n55\\nA simple self-attention mechanism without trainable weights\\n56\\nComputing attention weights for all input tokens\\n61\\n3.4\\nImplementing self-attention with trainable weights\\n64\\nComputing the attention weights step by step\\n65\\n■Implementing a \\ncompact self-attention Python class\\n70\\n3.5\\nHiding future words with causal attention\\n74\\nApplying a causal attention mask\\n75\\n■Masking additional \\nattention weights with dropout\\n78\\n■Implementing a compact \\ncausal attention class\\n80\\n3.6\\nExtending single-head attention to multi-head \\nattention\\n82\\nStacking multiple single-head attention layers\\n82\\n■Implementing \\nmulti-head attention with weight splits\\n86\\n4 Implementing a GPT model from scratch to generate text\\n92\\n4.1\\nCoding an LLM architecture\\n93\\n4.2\\nNormalizing activations with layer normalization\\n99\\n4.3\\nImplementing a feed forward network with GELU \\nactivations\\n105\\n4.4\\nAdding shortcut connections\\n109\\n4.5\\nConnecting attention and linear layers in a transformer \\nblock\\n113\\n4.6\\nCoding the GPT model\\n117\\n4.7\\nGenerating text\\n122'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 10}, page_content='CONTENTS\\nix\\n5 Pretraining on unlabeled data\\n128\\n5.1\\nEvaluating generative text models\\n129\\nUsing GPT to generate text\\n130\\n■Calculating the text \\ngeneration loss\\n132\\n■Calculating the training and validation \\nset losses\\n140\\n5.2\\nTraining an LLM\\n146\\n5.3\\nDecoding strategies to control randomness\\n151\\nTemperature scaling\\n152\\n■Top-k sampling\\n155\\nModifying the text generation function\\n157\\n5.4\\nLoading and saving model weights in PyTorch\\n159\\n5.5\\nLoading pretrained weights from OpenAI\\n160\\n6 Fine-tuning for classification\\n169\\n6.1\\nDifferent categories of fine-tuning\\n170\\n6.2\\nPreparing the dataset\\n172\\n6.3\\nCreating data loaders\\n175\\n6.4\\nInitializing a model with pretrained weights\\n181\\n6.5\\nAdding a classification head\\n183\\n6.6\\nCalculating the classification loss and accuracy\\n190\\n6.7\\nFine-tuning the model on supervised data\\n195\\n6.8\\nUsing the LLM as a spam classifier\\n200\\n7 Fine-tuning to follow instructions\\n204\\n7.1\\nIntroduction to instruction fine-tuning\\n205\\n7.2\\nPreparing a dataset for supervised instruction \\nfine-tuning\\n207\\n7.3\\nOrganizing data into training batches\\n211\\n7.4\\nCreating data loaders for an instruction dataset\\n223\\n7.5\\nLoading a pretrained LLM\\n226\\n7.6\\nFine-tuning the LLM on instruction data\\n229\\n7.7\\nExtracting and saving responses\\n233\\n7.8\\nEvaluating the fine-tuned LLM\\n238\\n7.9\\nConclusions\\n247\\nWhat’s next?\\n247\\n■Staying up to date in a fast-moving \\nfield\\n248\\n■Final words\\n248'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 11}, page_content='CONTENTS\\nx\\nappendix A\\nIntroduction to PyTorch\\n251\\nappendix B\\nReferences and further reading\\n289\\nappendix C\\nExercise solutions\\n300\\nappendix D\\nAdding bells and whistles to the training loop\\n313\\nappendix E\\nParameter-efficient fine-tuning with LoRA\\n322\\nindex\\n337'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 12}, page_content='xi\\npreface\\nI’ve always been fascinated with language models. More than a decade ago, my jour-\\nney into AI began with a statistical pattern classification class, which led to my first\\nindependent project: developing a model and web application to detect the mood of\\na song based on its lyrics.\\n Fast forward to 2022, with the release of ChatGPT, large language models (LLMs)\\nhave taken the world by storm and have revolutionized how many of us work. These\\nmodels are incredibly versatile, aiding in tasks such as checking grammar, composing\\nemails, summarizing lengthy documents, and much more. This is owed to their ability\\nto parse and generate human-like text, which is important in various fields, from cus-\\ntomer service to content creation, and even in more technical domains like coding\\nand data analysis. \\n As their name implies, a hallmark of LLMs is that they are “large”—very large—\\nencompassing millions to billions of parameters. (For comparison, using more tradi-\\ntional machine learning or statistical methods, the Iris flower dataset can be classified\\nwith more than 90% accuracy using a small model with only two parameters.) How-\\never, despite the large size of LLMs compared to more traditional methods, LLMs\\ndon’t have to be a black box. \\n In this book, you will learn how to build an LLM one step at a time. By the end,\\nyou will have a solid understanding of how an LLM, like the ones used in ChatGPT,\\nworks on a fundamental level. I believe that developing confidence with each part of\\nthe fundamental concepts and underlying code is crucial for success. This not only'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 13}, page_content='PREFACE\\nxii\\nhelps in fixing bugs and improving performance but also enables experimentation\\nwith new ideas.\\n Several years ago, when I started working with LLMs, I had to learn how to imple-\\nment them the hard way, sifting through many research papers and incomplete code\\nrepositories to develop a general understanding. With this book, I hope to make\\nLLMs more accessible by developing and sharing a step-by-step implementation tuto-\\nrial detailing all the major components and development phases of an LLM.\\n I strongly believe that the best way to understand LLMs is to code one from\\nscratch—and you’ll see that this can be fun too! \\n Happy reading and coding!'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 14}, page_content='xiii\\nacknowledgments\\nWriting a book is a significant undertaking, and I would like to express my sincere\\ngratitude to my wife, Liza, for her patience and support throughout this process. Her\\nunconditional love and constant encouragement have been absolutely essential.\\n I am incredibly grateful to Daniel Kleine, whose invaluable feedback on the in-\\nprogress chapters and code went above and beyond. With his keen eye for detail and\\ninsightful suggestions, Daniel’s contributions have undoubtedly made this book a\\nsmoother and more enjoyable reading experience.\\n I would also like to thank the wonderful staff at Manning Publications, including\\nMichael Stephens, for the many productive discussions that helped shape the direc-\\ntion of this book, and Dustin Archibald, whose constructive feedback and guidance in\\nadhering to the Manning guidelines have been crucial. I also appreciate your flexibil-\\nity in accommodating the unique requirements of this unconventional from-scratch\\napproach. A special thanks to Aleksandar Dragosavljevic´, Kari Lucke, and Mike Beady\\nfor their work on the professional layouts and to Susan Honeywell and her team for\\nrefining and polishing the graphics.\\n I want to express my heartfelt gratitude to Robin Campbell and her outstanding\\nmarketing team for their invaluable support throughout the writing process.\\n Finally, I extend my thanks to the reviewers: Anandaganesh Balakrishnan, Anto\\nAravinth, Ayush Bihani, Bassam Ismail, Benjamin Muskalla, Bruno Sonnino, Christian\\nProkopp, Daniel Kleine, David Curran, Dibyendu Roy Chowdhury, Gary Pass, Georg\\nSommer, Giovanni Alzetta, Guillermo Alcántara, Jonathan Reeves, Kunal Ghosh,\\nNicolas Modrzyk, Paul Silisteanu, Raul Ciotescu, Scott Ling, Sriram Macharla, Sumit'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 15}, page_content='ACKNOWLEDGMENTS\\nxiv\\nPal, Vahid Mirjalili, Vaijanath Rao, and Walter Reade for their thorough feedback on\\nthe drafts. Your keen eyes and insightful comments have been essential in improving\\nthe quality of this book.\\n To everyone who has contributed to this journey, I am sincerely grateful. Your sup-\\nport, expertise, and dedication have been instrumental in bringing this book to frui-\\ntion. Thank you!'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 16}, page_content='xv\\nabout this book\\nBuild a Large Language Model (From Scratch) was written to help you understand and\\ncreate your own GPT-like large language models (LLMs) from the ground up. It\\nbegins by focusing on the fundamentals of working with text data and coding atten-\\ntion mechanisms and then guides you through implementing a complete GPT\\nmodel from scratch. The book then covers the pretraining mechanism as well as\\nfine-tuning for specific tasks such as text classification and following instructions. By\\nthe end of this book, you’ll have a deep understanding of how LLMs work and the\\nskills to build your own models. While the models you’ll create are smaller in scale\\ncompared to the large foundational models, they use the same concepts and serve\\nas powerful educational tools to grasp the core mechanisms and techniques used in\\nbuilding state-of-the-art LLMs.\\nWho should read this book\\nBuild a Large Language Model (From Scratch) is for machine learning enthusiasts, engi-\\nneers, researchers, students, and practitioners who want to gain a deep understand-\\ning of how LLMs work and learn to build their own models from scratch. Both\\nbeginners and experienced developers will be able to use their existing skills and\\nknowledge to grasp the concepts and techniques used in creating LLMs.\\n What sets this book apart is its comprehensive coverage of the entire process of\\nbuilding LLMs, from working with datasets to implementing the model architecture,\\npretraining on unlabeled data, and fine-tuning for specific tasks. As of this writing, no'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 17}, page_content='ABOUT THIS BOOK\\nxvi\\nother resource provides such a complete and hands-on approach to building LLMs\\nfrom the ground up.\\n To understand the code examples in this book, you should have a solid grasp of\\nPython programming. While some familiarity with machine learning, deep learning,\\nand artificial intelligence can be beneficial, an extensive background in these areas is\\nnot required. LLMs are a unique subset of AI, so even if you’re relatively new to the\\nfield, you’ll be able to follow along.\\n If you have some experience with deep neural networks, you may find certain con-\\ncepts more familiar, as LLMs are built upon these architectures. However, proficiency\\nin PyTorch is not a prerequisite. Appendix A provides a concise introduction to\\nPyTorch, equipping you with the necessary skills to comprehend the code examples\\nthroughout the book.\\n A high school–level understanding of mathematics, particularly working with vec-\\ntors and matrices, can be helpful as we explore the inner workings of LLMs. However,\\nadvanced mathematical knowledge is not necessary to grasp the key concepts and\\nideas presented in this book.\\n The most important prerequisite is a strong foundation in Python programming.\\nWith this knowledge, you’ll be well prepared to explore the fascinating world of LLMs\\nand understand the concepts and code examples presented in this book.\\nHow this book is organized: A roadmap\\nThis book is designed to be read sequentially, as each chapter builds upon the con-\\ncepts and techniques introduced in the previous ones. The book is divided into seven\\nchapters that cover the essential aspects of LLMs and their implementation.\\n Chapter 1 provides a high-level introduction to the fundamental concepts behind\\nLLMs. It explores the transformer architecture, which forms the basis for LLMs such\\nas those used on the ChatGPT platform.\\n Chapter 2 lays out a plan for building an LLM from scratch. It covers the process of\\npreparing text for LLM training, including splitting text into word and subword\\ntokens, using byte pair encoding for advanced tokenization, sampling training exam-\\nples with a sliding window approach, and converting tokens into vectors that feed into\\nthe LLM.\\n Chapter 3 focuses on the attention mechanisms used in LLMs. It introduces a basic\\nself-attention framework and progresses to an enhanced self-attention mechanism.\\nThe chapter also covers the implementation of a causal attention module that enables\\nLLMs to generate one token at a time, masking randomly selected attention weights\\nwith dropout to reduce overfitting and stacking multiple causal attention modules\\ninto a multihead attention module.\\n Chapter 4 focuses on coding a GPT-like LLM that can be trained to generate\\nhuman-like text. It covers techniques such as normalizing layer activations to stabilize\\nneural network training, adding shortcut connections in deep neural networks to\\ntrain models more effectively, implementing transformer blocks to create GPT models'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 18}, page_content='ABOUT THIS BOOK\\nxvii\\nof various sizes, and computing the number of parameters and storage requirements\\nof GPT models.\\n Chapter 5 implements the pretraining process of LLMs. It covers computing the\\ntraining and validation set losses to assess the quality of LLM-generated text, imple-\\nmenting a training function and pretraining the LLM, saving and loading model\\nweights to continue training an LLM, and loading pretrained weights from OpenAI.\\n Chapter 6 introduces different LLM fine-tuning approaches. It covers preparing a\\ndataset for text classification, modifying a pretrained LLM for fine-tuning, fine-tuning\\nan LLM to identify spam messages, and evaluating the accuracy of a fine-tuned LLM\\nclassifier.\\n Chapter 7 explores the instruction fine-tuning process of LLMs. It covers prepar-\\ning a dataset for supervised instruction fine-tuning, organizing instruction data in\\ntraining batches, loading a pretrained LLM and fine-tuning it to follow human\\ninstructions, extracting LLM-generated instruction responses for evaluation, and eval-\\nuating an instruction-fine-tuned LLM.\\nAbout the code\\nTo make it as easy as possible to follow along, all code examples in this book are con-\\nveniently available on the Manning website at https://www.manning.com/books/\\nbuild-a-large-language-model-from-scratch, as well as in Jupyter notebook format on\\nGitHub at https://github.com/rasbt/LLMs-from-scratch. And don’t worry about get-\\nting stuck—solutions to all the code exercises can be found in appendix C.\\n This book contains many examples of source code both in numbered listings and\\nin line with normal text. In both cases, source code is formatted in a fixed-width\\nfont like this to separate it from ordinary text.\\n In many cases, the original source code has been reformatted; we’ve added line\\nbreaks and reworked indentation to accommodate the available page space in the\\nbook. In rare cases, even this was not enough, and listings include line-continuation\\nmarkers (➥). Additionally, comments in the source code have often been removed\\nfrom the listings when the code is described in the text. Code annotations accompany\\nmany of the listings, highlighting important concepts.\\n One of the key goals of this book is accessibility, so the code examples have been\\ncarefully designed to run efficiently on a regular laptop, without the need for any spe-\\ncial hardware. But if you do have access to a GPU, certain sections provide helpful tips\\non scaling up the datasets and models to take advantage of that extra power.\\n Throughout the book, we’ll be using PyTorch as our go-to tensor and a deep learn-\\ning library to implement LLMs from the ground up. If PyTorch is new to you, I recom-\\nmend you start with appendix A, which provides an in-depth introduction, complete\\nwith setup recommendations.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 19}, page_content='ABOUT THIS BOOK\\nxviii\\nliveBook discussion forum\\nPurchase of Build a Large Language Model (From Scratch) includes free access to live-\\nBook, Manning’s online reading platform. Using liveBook’s exclusive discussion fea-\\ntures, you can attach comments to the book globally or to specific sections or\\nparagraphs. It’s a snap to make notes for yourself, ask and answer technical questions,\\nand receive help from the author and other users. To access the forum, go to https://\\nlivebook.manning.com/book/build-a-large-language-model-from-scratch/discussion.\\nYou can also learn more about Manning’s forums and the rules of conduct at https://\\nlivebook.manning.com/discussion.\\n Manning’s commitment to readers is to provide a venue where a meaningful dia-\\nlogue between individual readers and between readers and the author can take place.\\nIt is not a commitment to any specific amount of participation on the part of the\\nauthor, whose contribution to the forum remains voluntary (and unpaid). We suggest\\nyou try asking the author some challenging questions lest his interest stray! The forum\\nand the archives of previous discussions will be accessible from the publisher’s website\\nas long as the book is in print.\\nOther online resources\\nInterested in the latest AI and LLM research trends?\\n\\uf0a1Check out my blog at https://magazine.sebastianraschka.com, where I regularly\\ndiscusses the latest AI research with a focus on LLMs.\\nNeed help getting up to speed with deep learning and PyTorch?\\n\\uf0a1I offer several free courses on my website at https://sebastianraschka.com/\\nteaching. These resources can help you quickly get up to speed with the latest\\ntechniques.\\nLooking for bonus materials related to the book?\\n\\uf0a1Visit the book’s GitHub repository at https://github.com/rasbt/LLMs-from\\n-scratch to find additional resources and examples to supplement your learning.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 20}, page_content='xix\\nabout the author\\nSEBASTIAN RASCHKA, PhD, has been working in machine learn-\\ning and AI for more than a decade. In addition to being a\\nresearcher, Sebastian has a strong passion for education. He is\\nknown for his bestselling books on machine learning with\\nPython and his contributions to open source.\\n      Sebastian is a staff research engineer at Lightning AI, focus-\\ning on implementing and training LLMs. Before his industry\\nexperience, Sebastian was an assistant professor in the Depart-\\nment of Statistics at the University of Wisconsin-Madison, where\\nhe focused on deep learning research. You can learn more about Sebastian at https://\\nsebastianraschka.com.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 21}, page_content='xx\\nabout the cover illustration\\nThe figure on the cover of Build a Large Language Model (From Scratch), titled “Le duch-\\nesse,” or “The duchess,” is taken from a book by Louis Curmer published in 1841.\\nEach illustration is finely drawn and colored by hand. \\n In those days, it was easy to identify where people lived and what their trade or sta-\\ntion in life was just by their dress. Manning celebrates the inventiveness and initiative\\nof the computer business with book covers based on the rich diversity of regional cul-\\nture centuries ago, brought back to life by pictures from collections such as this one.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 22}, page_content='1\\nUnderstanding large\\nlanguage models\\nLarge language models (LLMs), such as those offered in OpenAI’s ChatGPT, are\\ndeep neural network models that have been developed over the past few years.\\nThey ushered in a new era for natural language processing (NLP). Before the\\nadvent of LLMs, traditional methods excelled at categorization tasks such as email\\nspam classification and straightforward pattern recognition that could be captured\\nwith handcrafted rules or simpler models. However, they typically underperformed\\nin language tasks that demanded complex understanding and generation abilities,\\nsuch as parsing detailed instructions, conducting contextual analysis, and creating\\ncoherent and contextually appropriate original text. For example, previous genera-\\ntions of language models could not write an email from a list of keywords—a task\\nthat is trivial for contemporary LLMs.\\nThis chapter covers\\n\\uf0a1High-level explanations of the fundamental \\nconcepts behind large language models (LLMs)\\n\\uf0a1Insights into the transformer architecture from \\nwhich LLMs are derived\\n\\uf0a1A plan for building an LLM from scratch'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 23}, page_content='2\\nCHAPTER 1\\nUnderstanding large language models\\n LLMs have remarkable capabilities to understand, generate, and interpret human\\nlanguage. However, it’s important to clarify that when we say language models “under-\\nstand,” we mean that they can process and generate text in ways that appear coher-\\nent and contextually relevant, not that they possess human-like consciousness or\\ncomprehension. \\n Enabled by advancements in deep learning, which is a subset of machine learn-\\ning and artificial intelligence (AI) focused on neural networks, LLMs are trained on\\nvast quantities of text data. This large-scale training allows LLMs to capture deeper\\ncontextual information and subtleties of human language compared to previous\\napproaches. As a result, LLMs have significantly improved performance in a wide\\nrange of NLP tasks, including text translation, sentiment analysis, question answer-\\ning, and many more. \\n Another important distinction between contemporary LLMs and earlier NLP mod-\\nels is that earlier NLP models were typically designed for specific tasks, such as text\\ncategorization, language translation, etc. While those earlier NLP models excelled in\\ntheir narrow applications, LLMs demonstrate a broader proficiency across a wide\\nrange of NLP tasks. \\n The success behind LLMs can be attributed to the transformer architecture that\\nunderpins many LLMs and the vast amounts of data on which LLMs are trained,\\nallowing them to capture a wide variety of linguistic nuances, contexts, and patterns\\nthat would be challenging to encode manually. \\n This shift toward implementing models based on the transformer architecture and\\nusing large training datasets to train LLMs has fundamentally transformed NLP, pro-\\nviding more capable tools for understanding and interacting with human language. \\n The following discussion sets a foundation to accomplish the primary objective of\\nthis book: understanding LLMs by implementing a ChatGPT-like LLM based on the\\ntransformer architecture step by step in code.\\n1.1\\nWhat is an LLM?\\nAn LLM is a neural network designed to understand, generate, and respond to human-\\nlike text. These models are deep neural networks trained on massive amounts of text\\ndata, sometimes encompassing large portions of the entire publicly available text on\\nthe internet.\\n The “large” in “large language model” refers to both the model’s size in terms of\\nparameters and the immense dataset on which it’s trained. Models like this often have\\ntens or even hundreds of billions of parameters, which are the adjustable weights in\\nthe network that are optimized during training to predict the next word in a sequence.\\nNext-word prediction is sensible because it harnesses the inherent sequential nature\\nof language to train models on understanding context, structure, and relationships\\nwithin text. Yet, it is a very simple task, and so it is surprising to many researchers that\\nit can produce such capable models. In later chapters, we will discuss and implement\\nthe next-word training procedure step by step.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 24}, page_content='3\\n1.1\\nWhat is an LLM?\\n LLMs utilize an architecture called the transformer, which allows them to pay selec-\\ntive attention to different parts of the input when making predictions, making them\\nespecially adept at handling the nuances and complexities of human language. \\n Since LLMs are capable of generating text, LLMs are also often referred to as a form\\nof generative artificial intelligence, often abbreviated as generative AI or GenAI. As illus-\\ntrated in figure 1.1, AI encompasses the broader field of creating machines that can\\nperform tasks requiring human-like intelligence, including understanding lan-\\nguage, recognizing patterns, and making decisions, and includes subfields like machine\\nlearning and deep learning. \\nThe algorithms used to implement AI are the focus of the field of machine learning.\\nSpecifically, machine learning involves the development of algorithms that can learn\\nfrom and make predictions or decisions based on data without being explicitly pro-\\ngrammed. To illustrate this, imagine a spam filter as a practical application of\\nmachine learning. Instead of manually writing rules to identify spam emails, a\\nmachine learning algorithm is fed examples of emails labeled as spam and legitimate\\nemails. By minimizing the error in its predictions on a training dataset, the model\\nthen learns to recognize patterns and characteristics indicative of spam, enabling it to\\nclassify new emails as either spam or not spam.\\n As illustrated in figure 1.1, deep learning is a subset of machine learning that focuses\\non utilizing neural networks with three or more layers (also called deep neural net-\\nworks) to model complex patterns and abstractions in data. In contrast to deep learn-\\ning, traditional machine learning requires manual feature extraction. This means that\\nhuman experts need to identify and select the most relevant features for the model.\\nArtiﬁcial intelligence\\nMachine learning\\nDeep learning\\nLarge language models\\nSystems with\\nhuman-like intelligence\\nAlgorithms that learn rules\\nautomatically from data\\nMachine learning with\\nneural networks consisting\\nof many layers\\nDeep neural network for\\nparsing and generating\\nhuman-like text\\nGenAI\\nGenAI involves the use of\\ndeep neural networks to\\ncreate new content, such\\nas text, images, or various\\nforms of media\\nFigure 1.1\\nAs this hierarchical depiction of the relationship between the different fields suggests, LLMs \\nrepresent a specific application of deep learning techniques, using their ability to process and generate human-\\nlike text. Deep learning is a specialized branch of machine learning that focuses on using multilayer neural \\nnetworks. Machine learning and deep learning are fields aimed at implementing algorithms that enable computers \\nto learn from data and perform tasks that typically require human intelligence.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 25}, page_content='4\\nCHAPTER 1\\nUnderstanding large language models\\n While the field of AI is now dominated by machine learning and deep learning, it\\nalso includes other approaches—for example, using rule-based systems, genetic algo-\\nrithms, expert systems, fuzzy logic, or symbolic reasoning.\\n Returning to the spam classification example, in traditional machine learning,\\nhuman experts might manually extract features from email text such as the fre-\\nquency of certain trigger words (for example, “prize,” “win,” “free”), the number of\\nexclamation marks, use of all uppercase words, or the presence of suspicious links.\\nThis dataset, created based on these expert-defined features, would then be used to\\ntrain the model. In contrast to traditional machine learning, deep learning does not\\nrequire manual feature extraction. This means that human experts do not need to\\nidentify and select the most relevant features for a deep learning model. (However,\\nboth traditional machine learning and deep learning for spam classification still\\nrequire the collection of labels, such as spam or non-spam, which need to be gath-\\nered either by an expert or users.)\\n Let’s look at some of the problems LLMs can solve today, the challenges that LLMs\\naddress, and the general LLM architecture we will implement later.\\n1.2\\nApplications of LLMs\\nOwing to their advanced capabilities to parse and understand unstructured text data,\\nLLMs have a broad range of applications across various domains. Today, LLMs are\\nemployed for machine translation, generation of novel texts (see figure 1.2), senti-\\nment analysis, text summarization, and many other tasks. LLMs have recently been\\nused for content creation, such as writing fiction, articles, and even computer code. \\n LLMs can also power sophisticated chatbots and virtual assistants, such as OpenAI’s\\nChatGPT or Google’s Gemini (formerly called Bard), which can answer user queries\\nand augment traditional search engines such as Google Search or Microsoft Bing.\\n Moreover, LLMs may be used for effective knowledge retrieval from vast volumes\\nof text in specialized areas such as medicine or law. This includes sifting through doc-\\numents, summarizing lengthy passages, and answering technical questions.\\n In short, LLMs are invaluable for automating almost any task that involves parsing\\nand generating text. Their applications are virtually endless, and as we continue to\\ninnovate and explore new ways to use these models, it’s clear that LLMs have the\\npotential to redefine our relationship with technology, making it more conversational,\\nintuitive, and accessible.\\n We will focus on understanding how LLMs work from the ground up, coding an\\nLLM that can generate texts. You will also learn about techniques that allow LLMs to\\ncarry out queries, ranging from answering questions to summarizing text, translating\\ntext into different languages, and more. In other words, you will learn how complex\\nLLM assistants such as ChatGPT work by building one step by step.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 26}, page_content='5\\n1.3\\nStages of building and using LLMs\\n1.3\\nStages of building and using LLMs\\nWhy should we build our own LLMs? Coding an LLM from the ground up is an excel-\\nlent exercise to understand its mechanics and limitations. Also, it equips us with the\\nrequired knowledge for pretraining or fine-tuning existing open source LLM architec-\\ntures to our own domain-specific datasets or tasks. \\nNOTE\\nMost LLMs today are implemented using the PyTorch deep learning\\nlibrary, which is what we will use. Readers can find a comprehensive introduc-\\ntion to PyTorch in appendix A.\\nResearch has shown that when it comes to modeling performance, custom-built\\nLLMs—those tailored for specific tasks or domains—can outperform general-purpose\\nLLMs, such as those provided by ChatGPT, which are designed for a wide array of\\napplications. Examples of these include BloombergGPT (specialized for finance) and\\nLLMs tailored for medical question answering (see appendix B for more details).\\n Using custom-built LLMs offers several advantages, particularly regarding data pri-\\nvacy. For instance, companies may prefer not to share sensitive data with third-party\\nLLM providers like OpenAI due to confidentiality concerns. Additionally, developing\\nsmaller custom LLMs enables deployment directly on customer devices, such as laptops\\nand smartphones, which is something companies like Apple are currently exploring.\\nUser input\\n(instructions)\\nModel output\\nFigure 1.2\\nLLM interfaces enable natural language communication between users and AI systems. This \\nscreenshot shows ChatGPT writing a poem according to a user’s specifications.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 27}, page_content='6\\nCHAPTER 1\\nUnderstanding large language models\\nThis local implementation can significantly decrease latency and reduce server-related\\ncosts. Furthermore, custom LLMs grant developers complete autonomy, allowing\\nthem to control updates and modifications to the model as needed.\\n The general process of creating an LLM includes pretraining and fine-tuning. The\\n“pre” in “pretraining” refers to the initial phase where a model like an LLM is trained\\non a large, diverse dataset to develop a broad understanding of language. This pre-\\ntrained model then serves as a foundational resource that can be further refined\\nthrough fine-tuning, a process where the model is specifically trained on a narrower\\ndataset that is more specific to particular tasks or domains. This two-stage training\\napproach consisting of pretraining and fine-tuning is depicted in figure 1.3.\\nThe first step in creating an LLM is to train it on a large corpus of text data, sometimes\\nreferred to as raw text. Here, “raw” refers to the fact that this data is just regular text\\nwithout any labeling information. (Filtering may be applied, such as removing format-\\nting characters or documents in unknown languages.)\\nNOTE\\nReaders with a background in machine learning may note that label-\\ning information is typically required for traditional machine learning models\\nand deep neural networks trained via the conventional supervised learning\\nparadigm. However, this is not the case for the pretraining stage of LLMs. In\\nthis phase, LLMs use self-supervised learning, where the model generates its\\nown labels from the input data.\\n• Internet texts\\n• Books\\n• Wikipedia\\n• Research articles\\nTrain\\nPretrained LLM\\n(foundation model)\\nRaw, unlabeled text\\n(trillions of words)\\nTrain\\nLabeled dataset\\nFine-tuned LLM\\n• Text completion\\n• Few-shot capabilities\\n• Classiﬁcation\\n• Summarization\\n• Translation\\n• Personal assistant\\n• …\\nAn LLM is pretrained\\non unlabeled text data.\\nThe LLM has a few\\nbasic capabilities\\nafter pretraining.\\nA pretrained LLM can be\\nfurther trained on a labeled\\ndataset to obtain a ﬁne-tuned\\nLLM for speciﬁc tasks.\\nFigure 1.3\\nPretraining an LLM involves next-word prediction on large text datasets. A pretrained LLM \\ncan then be fine-tuned using a smaller labeled dataset.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 28}, page_content='7\\n1.4\\nIntroducing the transformer architecture\\nThis first training stage of an LLM is also known as pretraining, creating an initial pre-\\ntrained LLM, often called a base or foundation model. A typical example of such a model\\nis the GPT-3 model (the precursor of the original model offered in ChatGPT). This\\nmodel is capable of text completion—that is, finishing a half-written sentence pro-\\nvided by a user. It also has limited few-shot capabilities, which means it can learn to\\nperform new tasks based on only a few examples instead of needing extensive train-\\ning data.\\n After obtaining a pretrained LLM from training on large text datasets, where the\\nLLM is trained to predict the next word in the text, we can further train the LLM on\\nlabeled data, also known as fine-tuning.\\n The two most popular categories of fine-tuning LLMs are instruction fine-tuning and\\nclassification fine-tuning. In instruction fine-tuning, the labeled dataset consists of\\ninstruction and answer pairs, such as a query to translate a text accompanied by the\\ncorrectly translated text. In classification fine-tuning, the labeled dataset consists of\\ntexts and associated class labels—for example, emails associated with “spam” and “not\\nspam” labels.\\n We will cover code implementations for pretraining and fine-tuning an LLM, and\\nwe will delve deeper into the specifics of both instruction and classification fine-tuning\\nafter pretraining a base LLM.\\n1.4\\nIntroducing the transformer architecture\\nMost modern LLMs rely on the transformer architecture, which is a deep neural net-\\nwork architecture introduced in the 2017 paper “Attention Is All You Need” (https://\\narxiv.org/abs/1706.03762). To understand LLMs, we must understand the original\\ntransformer, which was developed for machine translation, translating English texts to\\nGerman and French. A simplified version of the transformer architecture is depicted\\nin figure 1.4. \\n The transformer architecture consists of two submodules: an encoder and a\\ndecoder. The encoder module processes the input text and encodes it into a series of\\nnumerical representations or vectors that capture the contextual information of the\\ninput. Then, the decoder module takes these encoded vectors and generates the out-\\nput text. In a translation task, for example, the encoder would encode the text from\\nthe source language into vectors, and the decoder would decode these vectors to gen-\\nerate text in the target language. Both the encoder and decoder consist of many layers\\nconnected by a so-called self-attention mechanism. You may have many questions\\nregarding how the inputs are preprocessed and encoded. These will be addressed in a\\nstep-by-step implementation in subsequent chapters.\\n A key component of transformers and LLMs is the self-attention mechanism (not\\nshown), which allows the model to weigh the importance of different words or tokens\\nin a sequence relative to each other. This mechanism enables the model to capture\\nlong-range dependencies and contextual relationships within the input data, enhanc-\\ning its ability to generate coherent and contextually relevant output. However, due to'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 29}, page_content='8\\nCHAPTER 1\\nUnderstanding large language models\\nits complexity, we will defer further explanation to chapter 3, where we will discuss\\nand implement it step by step.\\n Later variants of the transformer architecture, such as BERT (short for bidirectional\\nencoder representations from transformers) and the various GPT models (short for genera-\\ntive pretrained transformers), built on this concept to adapt this architecture for different\\ntasks. If interested, refer to appendix B for further reading suggestions.\\n BERT, which is built upon the original transformer’s encoder submodule, differs\\nin its training approach from GPT. While GPT is designed for generative tasks, BERT\\nand its variants specialize in masked word prediction, where the model predicts masked\\nInput text\\nOutput layers\\nEncoder\\nEmbeddings\\nDecoder\\n1. The input text to\\nbe translated.\\n8. The complete output\\n(translation)\\n5. A partial output\\ntext: the model\\ncompletes the\\ntranslation one\\nword at a time.\\n3. The encoder has\\naccess to the\\ncomplete input\\ntext to produce\\ntext encodings\\nused by the\\ndecoder.\\n7. The decoder\\ngenerates the\\ntranslated text\\none word at a\\ntime.\\nPreprocessing steps\\nInput text\\nPreprocessing steps\\n2. The input text is\\nprepared for the\\nencoder.\\n4. The encoder returns\\nembedding vectors as\\ninput to the decoder.\\n6. The input text is\\nprepared for the\\ndecoder.\\nFigure 1.4\\nA simplified depiction of the original transformer architecture, which is a deep learning model for \\nlanguage translation. The transformer consists of two parts: (a) an encoder that processes the input text and \\nproduces an embedding representation (a numerical representation that captures many different factors in \\ndifferent dimensions) of the text that the (b) decoder can use to generate the translated text one word at a time. \\nThis figure shows the final stage of the translation process where the decoder has to generate only the final word \\n(“Beispiel”), given the original input text (“This is an example”) and a partially translated sentence (“Das ist \\nein”), to complete the translation.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 30}, page_content='9\\n1.4\\nIntroducing the transformer architecture\\nor hidden words in a given sentence, as shown in figure 1.5. This unique training strategy\\nequips BERT with strengths in text classification tasks, including sentiment prediction\\nand document categorization. As an application of its capabilities, as of this writing, X\\n(formerly Twitter) uses BERT to detect toxic content.\\nGPT, on the other hand, focuses on the decoder portion of the original transformer\\narchitecture and is designed for tasks that require generating texts. This includes\\nmachine translation, text summarization, fiction writing, writing computer code,\\nand more. \\n GPT models, primarily designed and trained to perform text completion tasks,\\nalso show remarkable versatility in their capabilities. These models are adept at exe-\\ncuting both zero-shot and few-shot learning tasks. Zero-shot learning refers to the abil-\\nity to generalize to completely unseen tasks without any prior specific examples. On\\nthe other hand, few-shot learning involves learning from a minimal number of exam-\\nples the user provides as input, as shown in figure 1.6.\\nInput text\\nEncoder\\nDecoder\\nPreprocessing steps\\nInput text\\nPreprocessing steps\\nBERT\\nGPT\\nReceives inputs where words\\nare randomly masked during\\ntraining\\nLearns to\\ngenerate one\\nword at a\\ntime\\nThis is an __ of how concise I __ be\\nThis is an example of how concise I can be\\nThis is an example of how concise I can be\\nThis is an example of how concise I can\\nFills in the\\nmissing\\nwords to\\ngenerate\\nthe original\\nsentence\\nReceives incomplete texts\\nFigure 1.5\\nA visual representation of the transformer’s encoder and decoder submodules. On the left, the \\nencoder segment exemplifies BERT-like LLMs, which focus on masked word prediction and are primarily used for \\ntasks like text classification. On the right, the decoder segment showcases GPT-like LLMs, designed for \\ngenerative tasks and producing coherent text sequences.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 31}, page_content='10\\nCHAPTER 1\\nUnderstanding large language models\\n1.5\\nUtilizing large datasets\\nThe large training datasets for popular GPT- and BERT-like models represent diverse\\nand comprehensive text corpora encompassing billions of words, which include a vast\\narray of topics and natural and computer languages. To provide a concrete example,\\ntable 1.1 summarizes the dataset used for pretraining GPT-3, which served as the base\\nmodel for the first version of ChatGPT.\\n \\nTransformers vs. LLMs\\nToday’s LLMs are based on the transformer architecture. Hence, transformers and\\nLLMs are terms that are often used synonymously in the literature. However, note\\nthat not all transformers are LLMs since transformers can also be used for com-\\nputer vision. Also, not all LLMs are transformers, as there are LLMs based on recur-\\nrent and convolutional architectures. The main motivation behind these alternative\\napproaches is to improve the computational efficiency of LLMs. Whether these alter-\\nnative LLM architectures can compete with the capabilities of transformer-based\\nLLMs and whether they are going to be adopted in practice remains to be seen. For\\nsimplicity, I use the term “LLM” to refer to transformer-based LLMs similar to GPT.\\n(Interested readers can find literature references describing these architectures in\\nappendix B.) \\nBreakfast is the\\nInput\\nOutput\\nmost important meal of the day.\\nTEXT COMPLETION\\nZERO-SHOT\\nFEW-SHOT\\nTranslate English to German:\\nbreakfast =>\\ngaot => goat\\nsheo => shoe\\npohne =>\\nphone\\nCreates plausible text\\ngiven a partial input text\\nCompletes a task given a\\nfew examples of the task\\nCompletes a\\ntask without an\\nexplicit example\\nFigure 1.6\\nIn addition to text completion, GPT-like LLMs can solve various tasks based on their inputs without \\nneeding retraining, fine-tuning, or task-specific model architecture changes. Sometimes it is helpful to provide \\nexamples of the target within the input, which is known as a few-shot setting. However, GPT-like LLMs are also \\ncapable of carrying out tasks without a specific example, which is called zero-shot setting.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 32}, page_content='11\\n1.5\\nUtilizing large datasets\\nTable 1.1 reports the number of tokens, where a token is a unit of text that a model\\nreads and the number of tokens in a dataset is roughly equivalent to the number of\\nwords and punctuation characters in the text. Chapter 2 addresses tokenization, the\\nprocess of converting text into tokens.\\n The main takeaway is that the scale and diversity of this training dataset allow these\\nmodels to perform well on diverse tasks, including language syntax, semantics, and\\ncontext—even some requiring general knowledge. \\nThe pretrained nature of these models makes them incredibly versatile for further\\nfine-tuning on downstream tasks, which is why they are also known as base or founda-\\ntion models. Pretraining LLMs requires access to significant resources and is very\\nexpensive. For example, the GPT-3 pretraining cost is estimated to be $4.6 million in\\nterms of cloud computing credits (https://mng.bz/VxEW). \\nTable 1.1\\nThe pretraining dataset of the popular GPT-3 LLM\\nDataset name\\nDataset description\\nNumber of tokens\\nProportion \\nin training data\\nCommonCrawl (filtered)\\nWeb crawl data\\n410 billion\\n60%\\nWebText2\\nWeb crawl data\\n19 billion\\n22%\\nBooks1\\nInternet-based book corpus\\n12 billion\\n8%\\nBooks2\\nInternet-based book corpus\\n55 billion\\n8%\\nWikipedia\\nHigh-quality text\\n3 billion\\n3%\\nGPT-3 dataset details\\nTable 1.1 displays the dataset used for GPT-3. The proportions column in the table\\nsums up to 100% of the sampled data, adjusted for rounding errors. Although the\\nsubsets in the Number of Tokens column total 499 billion, the model was trained on\\nonly 300 billion tokens. The authors of the GPT-3 paper did not specify why the model\\nwas not trained on all 499 billion tokens.\\nFor context, consider the size of the CommonCrawl dataset, which alone consists of\\n410 billion tokens and requires about 570 GB of storage. In comparison, later itera-\\ntions of models like GPT-3, such as Meta’s LLaMA, have expanded their training\\nscope to include additional data sources like Arxiv research papers (92 GB) and\\nStackExchange’s code-related Q&As (78 GB). \\nThe authors of the GPT-3 paper did not share the training dataset, but a comparable\\ndataset that is publicly available is Dolma: An Open Corpus of Three Trillion Tokens for\\nLLM Pretraining Research by Soldaini et al. 2024 (https://arxiv.org/abs/2402.00159).\\nHowever, the collection may contain copyrighted works, and the exact usage terms\\nmay depend on the intended use case and country.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 33}, page_content='12\\nCHAPTER 1\\nUnderstanding large language models\\n The good news is that many pretrained LLMs, available as open source models,\\ncan be used as general-purpose tools to write, extract, and edit texts that were not\\npart of the training data. Also, LLMs can be fine-tuned on specific tasks with rela-\\ntively smaller datasets, reducing the computational resources needed and improving\\nperformance.\\n We will implement the code for pretraining and use it to pretrain an LLM for educa-\\ntional purposes. All computations are executable on consumer hardware. After imple-\\nmenting the pretraining code, we will learn how to reuse openly available model weights\\nand load them into the architecture we will implement, allowing us to skip the expen-\\nsive pretraining stage when we fine-tune our LLM.\\n1.6\\nA closer look at the GPT architecture\\nGPT was originally introduced in the paper “Improving Language Understanding by\\nGenerative Pre-Training” (https://mng.bz/x2qg) by Radford et al. from OpenAI.\\nGPT-3 is a scaled-up version of this model that has more parameters and was trained\\non a larger dataset. In addition, the original model offered in ChatGPT was created by\\nfine-tuning GPT-3 on a large instruction dataset using a method from OpenAI’s\\nInstructGPT paper (https://arxiv.org/abs/2203.02155). As figure 1.6 shows, these\\nmodels are competent text completion models and can carry out other tasks such as\\nspelling correction, classification, or language translation. This is actually very remark-\\nable given that GPT models are pretrained on a relatively simple next-word prediction\\ntask, as depicted in figure 1.7.\\nThe next-word prediction task is a form of self-supervised learning, which is a form of\\nself-labeling. This means that we don’t need to collect labels for the training data\\nexplicitly but can use the structure of the data itself: we can use the next word in a sen-\\ntence or document as the label that the model is supposed to predict. Since this next-\\nword prediction task allows us to create labels “on the fly,” it is possible to use massive\\nunlabeled text datasets to train LLMs.\\n Compared to the original transformer architecture we covered in section 1.4, the\\ngeneral GPT architecture is relatively simple. Essentially, it’s just the decoder part\\nwithout the encoder (figure 1.8). Since decoder-style models like GPT generate text\\nby predicting text one word at a time, they are considered a type of autoregressive\\nmodel. Autoregressive models incorporate their previous outputs as inputs for future\\nThe model is simply trained to\\npredict the next   word\\nFigure 1.7\\nIn the next-word prediction pretraining task for GPT \\nmodels, the system learns to predict the upcoming word in a \\nsentence by looking at the words that have come before it. This \\napproach helps the model understand how words and phrases \\ntypically fit together in language, forming a foundation that can \\nbe applied to various other tasks.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 34}, page_content='13\\n1.6\\nA closer look at the GPT architecture\\npredictions. Consequently, in GPT, each new word is chosen based on the sequence\\nthat precedes it, which improves the coherence of the resulting text.\\n Architectures such as GPT-3 are also significantly larger than the original transformer\\nmodel. For instance, the original transformer repeated the encoder and decoder blocks\\nsix times. GPT-3 has 96 transformer layers and 175 billion parameters in total.\\nGPT-3 was introduced in 2020, which, by the standards of deep learning and large lan-\\nguage model development, is considered a long time ago. However, more recent archi-\\ntectures, such as Meta’s Llama models, are still based on the same underlying concepts,\\nintroducing only minor modifications. Hence, understanding GPT remains as relevant\\nas ever, so I focus on implementing the prominent architecture behind GPT while pro-\\nviding pointers to specific tweaks employed by alternative LLMs.\\n Although the original transformer model, consisting of encoder and decoder blocks,\\nwas explicitly designed for language translation, GPT models—despite their larger yet\\nOutput layers\\nDecoder\\nInput text\\nPreprocessing steps\\nOutput layers\\nDecoder\\nInput text\\nPreprocessing steps\\nOutput layers\\nDecoder\\nInput text\\nPreprocessing steps\\nIteration 1\\nIteration 2\\nIteration 3\\nCreates the next\\nword based on\\nthe input text\\nThe output of the\\nprevious round\\nserves as input to\\nthe next round.\\nFigure 1.8\\nThe GPT architecture employs only the decoder portion of the original transformer. It is designed for \\nunidirectional, left-to-right processing, making it well suited for text generation and next-word prediction tasks to \\ngenerate text in an iterative fashion, one word at a time.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 35}, page_content='14\\nCHAPTER 1\\nUnderstanding large language models\\nsimpler decoder-only architecture aimed at next-word prediction—are also capable of\\nperforming translation tasks. This capability was initially unexpected to researchers, as\\nit emerged from a model primarily trained on a next-word prediction task, which is a\\ntask that did not specifically target translation.\\n The ability to perform tasks that the model wasn’t explicitly trained to perform is\\ncalled an emergent behavior. This capability isn’t explicitly taught during training but\\nemerges as a natural consequence of the model’s exposure to vast quantities of multi-\\nlingual data in diverse contexts. The fact that GPT models can “learn” the translation\\npatterns between languages and perform translation tasks even though they weren’t\\nspecifically trained for it demonstrates the benefits and capabilities of these large-\\nscale, generative language models. We can perform diverse tasks without using diverse\\nmodels for each.\\n1.7\\nBuilding a large language model\\nNow that we’ve laid the groundwork for understanding LLMs, let’s code one from\\nscratch. We will take the fundamental idea behind GPT as a blueprint and tackle this\\nin three stages, as outlined in figure 1.9.\\n1) Data\\npreparation\\n& sampling\\n2) Attention\\nmechanism\\nBuilding an LLM\\nSTAGE 1\\nFoundation model\\nSTAGE 2\\nSTAGE 3\\nClassiﬁer\\nPersonal assistant\\nDataset with class labels\\nInstruction dataset\\nImplements the data sampling and\\nunderstand the basic mechanism\\nPretrains the LLM on unlabeled\\ndata to obtain a foundation\\nmodel for further ﬁne-tuning\\nFine-tunes the pretrained\\nLLM to create a\\nclassiﬁcation model\\nFine-tunes the pretrained\\nLLM to create a personal\\nassistant or chat model\\n4) Pretraining\\n9) Fine-tuning\\n5) Training\\nloop\\n6) Model\\nevaluation\\n7) Load\\npretrained\\nweights\\n8) Fine-tuning\\n3) LLM\\narchitecture\\nFigure 1.9\\nThe three main stages of coding an LLM are implementing the LLM architecture and data preparation \\nprocess (stage 1), pretraining an LLM to create a foundation model (stage 2), and fine-tuning the foundation \\nmodel to become a personal assistant or text classifier (stage 3).'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 36}, page_content='15\\nSummary\\nIn stage 1, we will learn about the fundamental data preprocessing steps and code the\\nattention mechanism at the heart of every LLM. Next, in stage 2, we will learn how to\\ncode and pretrain a GPT-like LLM capable of generating new texts. We will also go\\nover the fundamentals of evaluating LLMs, which is essential for developing capable\\nNLP systems. \\n Pretraining an LLM from scratch is a significant endeavor, demanding thousands\\nto millions of dollars in computing costs for GPT-like models. Therefore, the focus of\\nstage 2 is on implementing training for educational purposes using a small dataset. In\\naddition, I also provide code examples for loading openly available model weights.\\n Finally, in stage 3, we will take a pretrained LLM and fine-tune it to follow instruc-\\ntions such as answering queries or classifying texts—the most common tasks in many\\nreal-world applications and research.\\n I hope you are looking forward to embarking on this exciting journey!\\nSummary\\n\\uf0a1LLMs have transformed the field of natural language processing, which previ-\\nously mostly relied on explicit rule-based systems and simpler statistical meth-\\nods. The advent of LLMs introduced new deep learning-driven approaches\\nthat led to advancements in understanding, generating, and translating human\\nlanguage.\\n\\uf0a1Modern LLMs are trained in two main steps: \\n– First, they are pretrained on a large corpus of unlabeled text by using the\\nprediction of the next word in a sentence as a label.\\n– Then, they are fine-tuned on a smaller, labeled target dataset to follow\\ninstructions or perform classification tasks.\\n\\uf0a1LLMs are based on the transformer architecture. The key idea of the trans-\\nformer architecture is an attention mechanism that gives the LLM selective\\naccess to the whole input sequence when generating the output one word at\\na time.\\n\\uf0a1The original transformer architecture consists of an encoder for parsing text\\nand a decoder for generating text. \\n\\uf0a1LLMs for generating text and following instructions, such as GPT-3 and\\nChatGPT, only implement decoder modules, simplifying the architecture.\\n\\uf0a1Large datasets consisting of billions of words are essential for pretraining\\nLLMs.\\n\\uf0a1While the general pretraining task for GPT-like models is to predict the next\\nword in a sentence, these LLMs exhibit emergent properties, such as capabili-\\nties to classify, translate, or summarize texts.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 37}, page_content='16\\nCHAPTER 1\\nUnderstanding large language models\\n\\uf0a1Once an LLM is pretrained, the resulting foundation model can be fine-tuned\\nmore efficiently for various downstream tasks. \\n\\uf0a1LLMs fine-tuned on custom datasets can outperform general LLMs on specific\\ntasks.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 38}, page_content='17\\nWorking with text data\\nSo far, we’ve covered the general structure of large language models (LLMs) and\\nlearned that they are pretrained on vast amounts of text. Specifically, our focus was on\\ndecoder-only LLMs based on the transformer architecture, which underlies the mod-\\nels used in ChatGPT and other popular GPT-like LLMs.\\n During the pretraining stage, LLMs process text one word at a time. Training\\nLLMs with millions to billions of parameters using a next-word prediction task\\nyields models with impressive capabilities. These models can then be further fine-\\ntuned to follow general instructions or perform specific target tasks. But before we\\ncan implement and train LLMs, we need to prepare the training dataset, as illus-\\ntrated in figure 2.1.\\nThis chapter covers\\n\\uf0a1Preparing text for large language model training\\n\\uf0a1Splitting text into word and subword tokens\\n\\uf0a1Byte pair encoding as a more advanced way of \\ntokenizing text\\n\\uf0a1Sampling training examples with a sliding window \\napproach\\n\\uf0a1Converting tokens into vectors that feed into a \\nlarge language model'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 39}, page_content='18\\nCHAPTER 2\\nWorking with text data\\nYou’ll learn how to prepare input text for training LLMs. This involves splitting text\\ninto individual word and subword tokens, which can then be encoded into vector rep-\\nresentations for the LLM. You’ll also learn about advanced tokenization schemes like\\nbyte pair encoding, which is utilized in popular LLMs like GPT. Lastly, we’ll imple-\\nment a sampling and data-loading strategy to produce the input-output pairs neces-\\nsary for training LLMs.\\n2.1\\nUnderstanding word embeddings\\nDeep neural network models, including LLMs, cannot process raw text directly. Since\\ntext is categorical, it isn’t compatible with the mathematical operations used to imple-\\nment and train neural networks. Therefore, we need a way to represent words as\\ncontinuous-valued vectors. \\nNOTE\\nReaders unfamiliar with vectors and tensors in a computational con-\\ntext can learn more in appendix A, section A.2.2.\\nThe concept of converting data into a vector format is often referred to as embedding.\\nUsing a specific neural network layer or another pretrained neural network model, we\\ncan embed different data types—for example, video, audio, and text, as illustrated in\\nfigure 2.2. However, it’s important to note that different data formats require distinct\\nembedding models. For example, an embedding model designed for text would not\\nbe suitable for embedding audio or video data.\\nImplements the data\\nsampling pipeline\\n1) Data\\npreparation\\n& sampling\\n2) Attention\\nmechanism\\nBuilding an LLM\\nSTAGE 1\\nFoundation model\\nSTAGE 2\\nSTAGE 3\\nClassiﬁer\\nPersonal assistant\\nDataset with class labels\\nInstruction dataset\\nImplements the data sampling and\\nunderstand the basic mechanism\\nPretrains the LLM on unlabeled\\ndata to obtain a foundation\\nmodel for further ﬁnetuning\\nFine-tunes the pretrained\\nLLM to create a\\nclassiﬁcation model\\nFine-tunes the pretrained\\nLLM to create a personal\\nassistant or chat model\\n4) Pretraining\\n9) Fine-tuning\\n5) Training\\nloop\\n6) Model\\nevaluation\\n7) Load\\npretrained\\nweights\\n8) Fine-tuning\\n3) LLM\\narchitecture\\nFigure 2.1\\nThe three main stages of coding an LLM. This chapter focuses on step 1 of stage 1: implementing the \\ndata sample pipeline.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 40}, page_content='19\\n2.1\\nUnderstanding word embeddings\\nAt its core, an embedding is a mapping from discrete objects, such as words, images,\\nor even entire documents, to points in a continuous vector space—the primary pur-\\npose of embeddings is to convert nonnumeric data into a format that neural networks\\ncan process.\\n While word embeddings are the most common form of text embedding, there are\\nalso embeddings for sentences, paragraphs, or whole documents. Sentence or para-\\ngraph embeddings are popular choices for retrieval-augmented generation. Retrieval-\\naugmented generation combines generation (like producing text) with retrieval (like\\nsearching an external knowledge base) to pull relevant information when generating\\ntext, which is a technique that is beyond the scope of this book. Since our goal is to\\ntrain GPT-like LLMs, which learn to generate text one word at a time, we will focus on\\nword embeddings.\\n Several algorithms and frameworks have been developed to generate word embed-\\ndings. One of the earlier and most popular examples is the Word2Vec approach.\\nWord2Vec trained neural network architecture to generate word embeddings by pre-\\ndicting the context of a word given the target word or vice versa. The main idea\\nbehind Word2Vec is that words that appear in similar contexts tend to have similar\\nmeanings. Consequently, when projected into two-dimensional word embeddings for\\nvisualization purposes, similar terms are clustered together, as shown in figure 2.3.\\n Word embeddings can have varying dimensions, from one to thousands. A higher\\ndimensionality might capture more nuanced relationships but at the cost of computa-\\ntional efficiency.\\nVideo\\nsample\\nAudio\\nsample\\nText\\nsample\\nVideo embedding model\\nAudio embedding model\\nText embedding model\\nVideo embedding vector\\nAudio embedding vector\\nText embedding vector\\n1.23\\n-0.31\\n0.89\\n-0.15\\n0.45\\n2.11\\n1.78\\n0.18\\n-2.10\\nEmbedding model converts raw\\ninput into a vector representation\\nUnlabeled\\ninput data\\nVector representation\\nof the input\\nFigure 2.2\\nDeep learning models cannot process data formats like video, audio, and text in their raw \\nform. Thus, we use an embedding model to transform this raw data into a dense vector representation \\nthat deep learning architectures can easily understand and process. Specifically, this figure illustrates \\nthe process of converting raw data into a three-dimensional numerical vector.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 41}, page_content='20\\nCHAPTER 2\\nWorking with text data\\nWhile we can use pretrained models such as Word2Vec to generate embeddings for\\nmachine learning models, LLMs commonly produce their own embeddings that are\\npart of the input layer and are updated during training. The advantage of optimizing\\nthe embeddings as part of the LLM training instead of using Word2Vec is that the\\nembeddings are optimized to the specific task and data at hand. We will implement\\nsuch embedding layers later in this chapter. (LLMs can also create contextualized out-\\nput embeddings, as we discuss in chapter 3.)\\n Unfortunately, high-dimensional embeddings present a challenge for visualiza-\\ntion because our sensory perception and common graphical representations are\\ninherently limited to three dimensions or fewer, which is why figure 2.3 shows two-\\ndimensional embeddings in a two-dimensional scatterplot. However, when working\\nwith LLMs, we typically use embeddings with a much higher dimensionality. For\\nboth GPT-2 and GPT-3, the embedding size (often referred to as the dimensionality\\nof the model’s hidden states) varies based on the specific model variant and size. It\\nis a tradeoff between performance and efficiency. The smallest GPT-2 models (117M\\nand 125M parameters) use an embedding size of 768 dimensions to provide con-\\ncrete examples. The largest GPT-3 model (175B parameters) uses an embedding\\nsize of 12,288 dimensions. \\n Next, we will walk through the required steps for preparing the embeddings used\\nby an LLM, which include splitting text into words, converting words into tokens, and\\nturning tokens into embedding vectors.\\nFirst dimension\\nSecond\\ndimension\\neagle\\nduck\\ngoose\\nsquirrel\\nlong\\nlonger\\nlongest\\nGermany\\nBerlin\\nEngland\\nLondon\\nVector embedding of\\nthe word squirrel\\nVector embeddings of\\ndifferent types of birds\\nFigure 2.3\\nIf word embeddings are two-dimensional, we can plot them in a two-\\ndimensional scatterplot for visualization purposes as shown here. When using word \\nembedding techniques, such as Word2Vec, words corresponding to similar concepts \\noften appear close to each other in the embedding space. For instance, different types \\nof birds appear closer to each other in the embedding space than in countries and cities.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 42}, page_content='21\\n2.2\\nTokenizing text\\n2.2\\nTokenizing text\\nLet’s discuss how we split input text into individual tokens, a required preprocessing\\nstep for creating embeddings for an LLM. These tokens are either individual words or\\nspecial characters, including punctuation characters, as shown in figure 2.4.\\nThe text we will tokenize for LLM training is “The Verdict,” a short story by Edith\\nWharton, which has been released into the public domain and is thus permitted to be\\nused for LLM training tasks. The text is available on Wikisource at https://en.wikisource\\n.org/wiki/The_Verdict, and you can copy and paste it into a text file, which I copied\\ninto a text file \"the-verdict.txt\".\\n Alternatively, you can find this \"the-verdict.txt\" file in this book’s GitHub\\nrepository at https://mng.bz/Adng. You can download the file with the following\\nPython code:\\n \\n \\nGPT-like\\ndecoder-only\\ntransformer\\nInput text:\\nToken embeddings:\\nThis is an example.\\nTokenized text:\\nThis\\nis\\nan\\nexample\\nOutput text\\nPostprocessing steps\\nToken IDs:\\n40134\\n2052\\n133\\n389\\n.\\n12\\nThis section covers the\\nconcept of splitting\\ntext into tokens\\nFigure 2.4\\nA view of the text processing steps in the context of an LLM. Here, we split an \\ninput text into individual tokens, which are either words or special characters, such as \\npunctuation characters.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 43}, page_content='22\\nCHAPTER 2\\nWorking with text data\\nimport urllib.request\\nurl = (\"https://raw.githubusercontent.com/rasbt/\"\\n       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\\n       \"the-verdict.txt\")\\nfile_path = \"the-verdict.txt\"\\nurllib.request.urlretrieve(url, file_path)\\nNext, we can load the the-verdict.txt file using Python’s standard file reading utilities. \\nwith open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\\n    raw_text = f.read()\\nprint(\"Total number of character:\", len(raw_text))\\nprint(raw_text[:99])\\nThe print command prints the total number of characters followed by the first 100\\ncharacters of this file for illustration purposes:\\nTotal number of character: 20479\\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow \\nenough--so it was no \\nOur goal is to tokenize this 20,479-character short story into individual words and spe-\\ncial characters that we can then turn into embeddings for LLM training.\\nNOTE\\nIt’s common to process millions of articles and hundreds of thousands\\nof books—many gigabytes of text—when working with LLMs. However, for\\neducational purposes, it’s sufficient to work with smaller text samples like a\\nsingle book to illustrate the main ideas behind the text processing steps and\\nto make it possible to run it in a reasonable time on consumer hardware.\\nHow can we best split this text to obtain a list of tokens? For this, we go on a small\\nexcursion and use Python’s regular expression library re for illustration purposes.\\n(You don’t have to learn or memorize any regular expression syntax since we will later\\ntransition to a prebuilt tokenizer.)\\n Using some simple example text, we can use the re.split command with the fol-\\nlowing syntax to split a text on whitespace characters:\\nimport re\\ntext = \"Hello, world. This, is a test.\"\\nresult = re.split(r\\'(\\\\s)\\', text)\\nprint(result)\\nThe result is a list of individual words, whitespaces, and punctuation characters:\\n[\\'Hello,\\', \\' \\', \\'world.\\', \\' \\', \\'This,\\', \\' \\', \\'is\\', \\' \\', \\'a\\', \\' \\', \\'test.\\']\\nListing 2.1\\nReading in a short story as text sample into Python'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 44}, page_content='23\\n2.2\\nTokenizing text\\nThis simple tokenization scheme mostly works for separating the example text into\\nindividual words; however, some words are still connected to punctuation characters\\nthat we want to have as separate list entries. We also refrain from making all text lower-\\ncase because capitalization helps LLMs distinguish between proper nouns and com-\\nmon nouns, understand sentence structure, and learn to generate text with proper\\ncapitalization.\\n Let’s modify the regular expression splits on whitespaces (\\\\s), commas, and peri-\\nods ([,.]):\\nresult = re.split(r\\'([,.]|\\\\s)\\', text)\\nprint(result)\\nWe can see that the words and punctuation characters are now separate list entries just\\nas we wanted:\\n[\\'Hello\\', \\',\\', \\'\\', \\' \\', \\'world\\', \\'.\\', \\'\\', \\' \\', \\'This\\', \\',\\', \\'\\', \\' \\', \\'is\\',\\n\\' \\', \\'a\\', \\' \\', \\'test\\', \\'.\\', \\'\\']\\nA small remaining problem is that the list still includes whitespace characters. Option-\\nally, we can remove these redundant characters safely as follows:\\nresult = [item for item in result if item.strip()]\\nprint(result)\\nThe resulting whitespace-free output looks like as follows:\\n[\\'Hello\\', \\',\\', \\'world\\', \\'.\\', \\'This\\', \\',\\', \\'is\\', \\'a\\', \\'test\\', \\'.\\']\\nNOTE\\nWhen developing a simple tokenizer, whether we should encode\\nwhitespaces as separate characters or just remove them depends on our appli-\\ncation and its requirements. Removing whitespaces reduces the memory and\\ncomputing requirements. However, keeping whitespaces can be useful if we\\ntrain models that are sensitive to the exact structure of the text (for example,\\nPython code, which is sensitive to indentation and spacing). Here, we remove\\nwhitespaces for simplicity and brevity of the tokenized outputs. Later, we will\\nswitch to a tokenization scheme that includes whitespaces.\\nThe tokenization scheme we devised here works well on the simple sample text. Let’s\\nmodify it a bit further so that it can also handle other types of punctuation, such as ques-\\ntion marks, quotation marks, and the double-dashes we have seen earlier in the first 100\\ncharacters of Edith Wharton’s short story, along with additional special characters:\\ntext = \"Hello, world. Is this-- a test?\"\\nresult = re.split(r\\'([,.:;?_!\"()\\\\\\']|--|\\\\s)\\', text)\\nresult = [item.strip() for item in result if item.strip()]\\nprint(result)'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 45}, page_content='24\\nCHAPTER 2\\nWorking with text data\\nThe resulting output is:\\n[\\'Hello\\', \\',\\', \\'world\\', \\'.\\', \\'Is\\', \\'this\\', \\'--\\', \\'a\\', \\'test\\', \\'?\\']\\nAs we can see based on the results summarized in figure 2.5, our tokenization scheme\\ncan now handle the various special characters in the text successfully.\\nNow that we have a basic tokenizer working, let’s apply it to Edith Wharton’s entire\\nshort story:\\npreprocessed = re.split(r\\'([,.:;?_!\"()\\\\\\']|--|\\\\s)\\', raw_text)\\npreprocessed = [item.strip() for item in preprocessed if item.strip()]\\nprint(len(preprocessed))\\nThis print statement outputs 4690, which is the number of tokens in this text (without\\nwhitespaces). Let’s print the first 30 tokens for a quick visual check:\\nprint(preprocessed[:30])\\nThe resulting output shows that our tokenizer appears to be handling the text well\\nsince all words and special characters are neatly separated:\\n[\\'I\\', \\'HAD\\', \\'always\\', \\'thought\\', \\'Jack\\', \\'Gisburn\\', \\'rather\\', \\'a\\',\\n\\'cheap\\', \\'genius\\', \\'--\\', \\'though\\', \\'a\\', \\'good\\', \\'fellow\\', \\'enough\\',\\n\\'--\\', \\'so\\', \\'it\\', \\'was\\', \\'no\\', \\'great\\', \\'surprise\\', \\'to\\', \\'me\\', \\'to\\',\\n\\'hear\\', \\'that\\', \\',\\', \\'in\\']\\n2.3\\nConverting tokens into token IDs\\nNext, let’s convert these tokens from a Python string to an integer representation to\\nproduce the token IDs. This conversion is an intermediate step before converting the\\ntoken IDs into embedding vectors.\\n To map the previously generated tokens into token IDs, we have to build a vocabu-\\nlary first. This vocabulary defines how we map each unique word and special character\\nto a unique integer, as shown in figure 2.6.\\nInput text\\nHello, world. Is this-- a test?\\nHello\\n,\\nworld\\n.\\nIs\\nthis\\n--\\na\\ntest\\n?\\nTokenized text\\nFigure 2.5\\nThe tokenization scheme we implemented so far splits \\ntext into individual words and punctuation characters. In this specific \\nexample, the sample text gets split into 10 individual tokens.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 46}, page_content='25\\n2.3\\nConverting tokens into token IDs\\nNow that we have tokenized Edith Wharton’s short story and assigned it to a Python\\nvariable called preprocessed, let’s create a list of all unique tokens and sort them\\nalphabetically to determine the vocabulary size:\\nall_words = sorted(set(preprocessed))\\nvocab_size = len(all_words)\\nprint(vocab_size)\\nAfter determining that the vocabulary size is 1,130 via this code, we create the vocabu-\\nlary and print its first 51 entries for illustration purposes.\\nvocab = {token:integer for integer,token in enumerate(all_words)}\\nfor i, item in enumerate(vocab.items()):\\n    print(item)\\n    if i >= 50:\\n        break\\nListing 2.2\\nCreating a vocabulary\\nComplete training dataset\\nThe quick brown fox jumps\\nover the lazy dog\\nTokenized training dataset\\nThe\\nquick\\nbrown\\nVocabulary\\nbrown\\ndog\\nfox\\n0\\n1\\n2\\n1. Tokenization breaks\\ndown the input text\\ninto individual tokens.\\n2. Each unique token is\\nadded to the vocabulary\\nin alphabetical order.\\njumps\\nlazy\\nover\\n3\\n4\\n5\\nquick\\n6\\nthe\\n7\\nThe vocabulary\\ncontains all unique\\ntokens in the training\\nset and is usually\\nsorted alphabetically.\\nEach unique token is\\nmapped to a unique\\ninteger called token ID.\\nUnique tokens\\nToken IDs\\nThe training set consists\\nof only one sentence for\\nillustration purposes.\\nInput text\\nFigure 2.6\\nWe build a vocabulary by tokenizing the entire text in a training dataset into individual \\ntokens. These individual tokens are then sorted alphabetically, and duplicate tokens are removed. \\nThe unique tokens are then aggregated into a vocabulary that defines a mapping from each unique \\ntoken to a unique integer value. The depicted vocabulary is purposefully small and contains no \\npunctuation or special characters for simplicity.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 47}, page_content='26\\nCHAPTER 2\\nWorking with text data\\nThe output is\\n(\\'!\\', 0)\\n(\\'\"\\', 1)\\n(\"\\'\", 2)\\n...\\n(\\'Her\\', 49)\\n(\\'Hermia\\', 50)\\nAs we can see, the dictionary contains individual tokens associated with unique inte-\\nger labels. Our next goal is to apply this vocabulary to convert new text into token IDs\\n(figure 2.7).\\nWhen we want to convert the outputs of an LLM from numbers back into text, we need a\\nway to turn token IDs into text. For this, we can create an inverse version of the vocabu-\\nlary that maps token IDs back to the corresponding text tokens. \\nTokenization breaks down the\\ntraining set into individual tokens.\\nNew tokenized sample\\ntext is mapped to\\ntoken IDs using an\\nexisting vocabulary.\\nFigure 2.7\\nStarting with a new text sample, we tokenize the text and use the vocabulary to convert \\nthe text tokens into token IDs. The vocabulary is built from the entire training set and can be applied \\nto the training set itself and any new text samples. The depicted vocabulary contains no punctuation \\nor special characters for simplicity.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 48}, page_content='27\\n2.3\\nConverting tokens into token IDs\\n Let’s implement a complete tokenizer class in Python with an encode method that\\nsplits text into tokens and carries out the string-to-integer mapping to produce token\\nIDs via the vocabulary. In addition, we’ll implement a decode method that carries out\\nthe reverse integer-to-string mapping to convert the token IDs back into text. The fol-\\nlowing listing shows the code for this tokenizer implementation.\\nclass SimpleTokenizerV1:\\n    def __init__(self, vocab):\\n        self.str_to_int = vocab           \\n        self.int_to_str = {i:s for s,i in vocab.items()}       \\n    \\n    def encode(self, text):        \\n        preprocessed = re.split(r\\'([,.?_!\"()\\\\\\']|--|\\\\s)\\', text)\\n        preprocessed = [\\n            item.strip() for item in preprocessed if item.strip()\\n        ]\\n        ids = [self.str_to_int[s] for s in preprocessed]\\n        return ids\\n        \\n    def decode(self, ids):        \\n        text = \" \".join([self.int_to_str[i] for i in ids]) \\n        \\n        text = re.sub(r\\'\\\\s+([,.?!\"()\\\\\\'])\\', r\\'\\\\1\\', text)   \\n        return text\\nUsing the SimpleTokenizerV1 Python class, we can now instantiate new tokenizer\\nobjects via an existing vocabulary, which we can then use to encode and decode text,\\nas illustrated in figure 2.8.\\n Let’s instantiate a new tokenizer object from the SimpleTokenizerV1 class and\\ntokenize a passage from Edith Wharton’s short story to try it out in practice:\\ntokenizer = SimpleTokenizerV1(vocab)\\ntext = \"\"\"\"It\\'s the last he painted, you know,\" \\n       Mrs. Gisburn said with pardonable pride.\"\"\"\\nids = tokenizer.encode(text)\\nprint(ids)\\nThe preceding code prints the following token IDs:\\n[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, \\n754, 793, 7]\\nNext, let’s see whether we can turn these token IDs back into text using the decode\\nmethod:\\nprint(tokenizer.decode(ids))\\nListing 2.3\\nImplementing a simple text tokenizer\\nStores the vocabulary as a class attribute for\\naccess in the encode and decode methods\\nCreates an inverse\\nvocabulary that maps\\ntoken IDs back to the\\noriginal text tokens\\nProcesses \\ninput text \\ninto token \\nIDs\\nConverts token IDs \\nback into text\\nRemoves spaces \\nbefore the specified \\npunctuation'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 49}, page_content='28\\nCHAPTER 2\\nWorking with text data\\nThis outputs:\\n\\'\" It\\\\\\' s the last he painted, you know,\" Mrs. Gisburn said with \\npardonable pride.\\'\\nBased on this output, we can see that the decode method successfully converted the\\ntoken IDs back into the original text.\\n So far, so good. We implemented a tokenizer capable of tokenizing and detokeniz-\\ning text based on a snippet from the training set. Let’s now apply it to a new text sam-\\nple not contained in the training set:\\ntext = \"Hello, do you like tea?\"\\nprint(tokenizer.encode(text))\\nExecuting this code will result in the following error:\\nKeyError: \\'Hello\\'\\nThe problem is that the word “Hello” was not used in the “The Verdict” short story.\\nHence, it is not contained in the vocabulary. This highlights the need to consider\\nlarge and diverse training sets to extend the vocabulary when working on LLMs.\\nThe brown dog\\nplayfully chased\\nthe swift fox\\nSample text\\nTokenized sample text\\nThe\\nbrown\\ndog\\nToken IDs\\n7\\n0\\n1\\nThe brown dog\\nplayfully chased\\nthe swift fox\\nSample text\\nTokenized sample text\\nThe\\nbrown\\ndog\\nToken IDs\\n7\\n0\\n1\\nCalling tokenizer.encode(text) on sample text\\nCalling tokenizer.decode(ids) on token IDs\\nVocabulary\\nInverse\\nvocabulary\\nFigure 2.8\\nTokenizer implementations share two common methods: an encode method and a decode \\nmethod. The encode method takes in the sample text, splits it into individual tokens, and converts the \\ntokens into token IDs via the vocabulary. The decode method takes in token IDs, converts them back \\ninto text tokens, and concatenates the text tokens into natural text.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 50}, page_content='29\\n2.4\\nAdding special context tokens\\n Next, we will test the tokenizer further on text that contains unknown words and\\ndiscuss additional special tokens that can be used to provide further context for an\\nLLM during training.\\n2.4\\nAdding special context tokens\\nWe need to modify the tokenizer to handle unknown words. We also need to address\\nthe usage and addition of special context tokens that can enhance a model’s under-\\nstanding of context or other relevant information in the text. These special tokens\\ncan include markers for unknown words and document boundaries, for example. In\\nparticular, we will modify the vocabulary and tokenizer, SimpleTokenizerV2, to sup-\\nport two new tokens, <|unk|> and <|endoftext|>, as illustrated in figure 2.9.\\nWe can modify the tokenizer to use an <|unk|> token if it encounters a word that is\\nnot part of the vocabulary. Furthermore, we add a token between unrelated texts.\\nFor example, when training GPT-like LLMs on multiple independent documents or\\nbooks, it is common to insert a token before each document or book that follows a\\nprevious text source, as illustrated in figure 2.10. This helps the LLM understand\\nthat although these text sources are concatenated for training, they are, in fact,\\nunrelated.\\nSample text\\nTokenized sample text\\nThe\\nbrown\\ndog\\nToken IDs\\nplayfully\\n7\\n0\\n1\\n783\\nbrown\\ndog\\nfox\\n0\\n1\\n2\\n<|unk|>\\n<|endoftext|>\\n783\\n784\\nExisting vocabulary\\nExtend vocabulary\\nwith additional\\nspecial tokens\\nThe brown dog\\nplayfully chased\\nthe swift fox\\nFigure 2.9\\nWe add special tokens to a vocabulary to deal with certain contexts. For instance, \\nwe add an <|unk|> token to represent new and unknown words that were not part of the training \\ndata and thus not part of the existing vocabulary. Furthermore, we add an <|endoftext|> \\ntoken that we can use to separate two unrelated text sources.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 51}, page_content='30\\nCHAPTER 2\\nWorking with text data\\nLet’s now modify the vocabulary to include these two special tokens, <unk> and\\n<|endoftext|>, by adding them to our list of all unique words:\\nall_tokens = sorted(list(set(preprocessed)))\\nall_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\\nvocab = {token:integer for integer,token in enumerate(all_tokens)}\\nprint(len(vocab.items()))\\nBased on the output of this print statement, the new vocabulary size is 1,132 (the pre-\\nvious vocabulary size was 1,130).\\n As an additional quick check, let’s print the last five entries of the updated vocabulary:\\nfor i, item in enumerate(list(vocab.items())[-5:]):\\n    print(item)\\nThe code prints\\n(\\'younger\\', 1127)\\n(\\'your\\', 1128)\\n(\\'yourself\\', 1129)\\n(\\'<|endoftext|>\\', 1130)\\n(\\'<|unk|>\\', 1131)\\nThe <|endoftext|> tokens are\\nprepended to each subsequent text\\nsource.\\nIndependent text source\\nText concatenated from all\\nindependent sources\\n“… the underdog\\nteam ﬁnally clinched\\nthe championship in\\na thrilling overtime\\nvictory.”\\n“\\n…\\n<|endoftext|>\\nElara and Finn lived\\nwith kindness and\\nwisdom, enjoying\\ntheir days happily\\never after.”\\n“\\n…\\n<|endoftext|>\\nThe Dow Jones\\nIndustrial Average\\nclosed up 250 points\\ntoday, marking its\\nhighest gain in the\\npast three months.”\\n“\\n…\\n<|endoftext|>\\nAmelia smiled,\\nknowing her journey\\nhad forever changed\\nher heart.”\\n“… in a thrilling overtime victory.\\n… days happily ever after.\\n<|endoftext|>\\n<|endoftext|>\\n… marking its highest gain in the past three months.\\n… journey had forever\\n<|endoftext|>\\nchanged her heart.”\\nFigure 2.10\\nWhen working with multiple independent text source, we add <|endoftext|> \\ntokens between these texts. These <|endoftext|> tokens act as markers, signaling the \\nstart or end of a particular segment, allowing for more effective processing and understanding \\nby the LLM.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 52}, page_content='31\\n2.4\\nAdding special context tokens\\nBased on the code output, we can confirm that the two new special tokens were\\nindeed successfully incorporated into the vocabulary. Next, we adjust the tokenizer\\nfrom code listing 2.3 accordingly as shown in the following listing.\\nclass SimpleTokenizerV2:\\n    def __init__(self, vocab):\\n        self.str_to_int = vocab\\n        self.int_to_str = { i:s for s,i in vocab.items()}\\n    \\n    def encode(self, text):\\n        preprocessed = re.split(r\\'([,.:;?_!\"()\\\\\\']|--|\\\\s)\\', text)\\n        preprocessed = [\\n            item.strip() for item in preprocessed if item.strip()\\n        ]\\n        preprocessed = [item if item in self.str_to_int           \\n                        else \"<|unk|>\" for item in preprocessed]\\n        ids = [self.str_to_int[s] for s in preprocessed]\\n        return ids\\n        \\n    def decode(self, ids):\\n        text = \" \".join([self.int_to_str[i] for i in ids])\\n        text = re.sub(r\\'\\\\s+([,.:;?!\"()\\\\\\'])\\', r\\'\\\\1\\', text)   \\n        return text\\nCompared to the SimpleTokenizerV1 we implemented in listing 2.3, the new Simple-\\nTokenizerV2 replaces unknown words with <|unk|> tokens. \\n Let’s now try this new tokenizer out in practice. For this, we will use a simple text\\nsample that we concatenate from two independent and unrelated sentences:\\ntext1 = \"Hello, do you like tea?\"\\ntext2 = \"In the sunlit terraces of the palace.\"\\ntext = \" <|endoftext|> \".join((text1, text2))\\nprint(text)\\nThe output is\\nHello, do you like tea? <|endoftext|> In the sunlit terraces of \\nthe palace.\\nNext, let’s tokenize the sample text using the SimpleTokenizerV2 on the vocab we\\npreviously created in listing 2.2:\\ntokenizer = SimpleTokenizerV2(vocab)\\nprint(tokenizer.encode(text))\\nListing 2.4\\nA simple text tokenizer that handles unknown words\\nReplaces\\nunknown words\\nby <|unk|>\\ntokens\\nReplaces spaces \\nbefore the specified \\npunctuations'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 53}, page_content='32\\nCHAPTER 2\\nWorking with text data\\nThis prints the following token IDs:\\n[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\\nWe can see that the list of token IDs contains 1130 for the <|endoftext|> separator\\ntoken as well as two 1131 tokens, which are used for unknown words. \\n Let’s detokenize the text for a quick sanity check:\\nprint(tokenizer.decode(tokenizer.encode(text)))\\nThe output is\\n<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of \\nthe <|unk|>.\\nBased on comparing this detokenized text with the original input text, we know that\\nthe training dataset, Edith Wharton’s short story “The Verdict,” does not contain the\\nwords “Hello” and “palace.”\\n Depending on the LLM, some researchers also consider additional special tokens\\nsuch as the following:\\n\\uf0a1\\n[BOS] (beginning of sequence)—This token marks the start of a text. It signifies to\\nthe LLM where a piece of content begins.\\n\\uf0a1\\n[EOS] (end of sequence)—This token is positioned at the end of a text and\\nis especially useful when concatenating multiple unrelated texts, similar to\\n<|endoftext|>. For instance, when combining two different Wikipedia arti-\\ncles or books, the [EOS] token indicates where one ends and the next begins.\\n\\uf0a1\\n[PAD] (padding)—When training LLMs with batch sizes larger than one, the\\nbatch might contain texts of varying lengths. To ensure all texts have the same\\nlength, the shorter texts are extended or “padded” using the [PAD] token, up to\\nthe length of the longest text in the batch.\\nThe tokenizer used for GPT models does not need any of these tokens; it only uses an\\n<|endoftext|> token for simplicity. <|endoftext|> is analogous to the [EOS] token.\\n<|endoftext|> is also used for padding. However, as we’ll explore in subsequent\\nchapters, when training on batched inputs, we typically use a mask, meaning we don’t\\nattend to padded tokens. Thus, the specific token chosen for padding becomes incon-\\nsequential.\\n Moreover, the tokenizer used for GPT models also doesn’t use an <|unk|> token\\nfor out-of-vocabulary words. Instead, GPT models use a byte pair encoding tokenizer,\\nwhich breaks words down into subword units, which we will discuss next.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 54}, page_content='33\\n2.5\\nByte pair encoding\\n2.5\\nByte pair encoding\\nLet’s look at a more sophisticated tokenization scheme based on a concept called byte\\npair encoding (BPE). The BPE tokenizer was used to train LLMs such as GPT-2, GPT-3,\\nand the original model used in ChatGPT.\\n Since implementing BPE can be relatively complicated, we will use an existing\\nPython open source library called tiktoken (https://github.com/openai/tiktoken), which\\nimplements the BPE algorithm very efficiently based on source code in Rust. Similar\\nto other Python libraries, we can install the tiktoken library via Python’s pip installer\\nfrom the terminal:\\npip install tiktoken\\nThe code we will use is based on tiktoken 0.7.0. You can use the following code to\\ncheck the version you currently have installed:\\nfrom importlib.metadata import version\\nimport tiktoken\\nprint(\"tiktoken version:\", version(\"tiktoken\"))\\nOnce installed, we can instantiate the BPE tokenizer from tiktoken as follows:\\ntokenizer = tiktoken.get_encoding(\"gpt2\")\\nThe usage of this tokenizer is similar to the SimpleTokenizerV2 we implemented pre-\\nviously via an encode method:\\ntext = (\\n    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\\n     \"of someunknownPlace.\"\\n)\\nintegers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\\nprint(integers)\\nThe code prints the following token IDs: \\n[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250,\\n 8812, 2114, 286, 617, 34680, 27271, 13]\\nWe can then convert the token IDs back into text using the decode method, similar to\\nour SimpleTokenizerV2:\\nstrings = tokenizer.decode(integers)\\nprint(strings)\\nThe code prints\\nHello, do you like tea? <|endoftext|> In the sunlit terraces of\\n someunknownPlace.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 55}, page_content='34\\nCHAPTER 2\\nWorking with text data\\nWe can make two noteworthy observations based on the token IDs and decoded text.\\nFirst, the <|endoftext|> token is assigned a relatively large token ID, namely, 50256.\\nIn fact, the BPE tokenizer, which was used to train models such as GPT-2, GPT-3, and\\nthe original model used in ChatGPT, has a total vocabulary size of 50,257, with\\n<|endoftext|> being assigned the largest token ID.\\n Second, the BPE tokenizer encodes and decodes unknown words, such as\\nsomeunknownPlace, correctly. The BPE tokenizer can handle any unknown word. How\\ndoes it achieve this without using <|unk|> tokens?\\n The algorithm underlying BPE breaks down words that aren’t in its predefined\\nvocabulary into smaller subword units or even individual characters, enabling it to\\nhandle out-of-vocabulary words. So, thanks to the BPE algorithm, if the tokenizer\\nencounters an unfamiliar word during tokenization, it can represent it as a sequence\\nof subword tokens or characters, as illustrated in figure 2.11.\\nThe ability to break down unknown words into individual characters ensures that\\nthe tokenizer and, consequently, the LLM that is trained with it can process any text,\\neven if it contains words that were not present in its training data.\\nA detailed discussion and implementation of BPE is out of the scope of this book, but\\nin short, it builds its vocabulary by iteratively merging frequent characters into sub-\\nwords and frequent subwords into words. For example, BPE starts with adding all indi-\\nvidual single characters to its vocabulary (“a,” “b,” etc.). In the next stage, it merges\\ncharacter combinations that frequently occur together into subwords. For example,\\n“d” and “e” may be merged into the subword “de,” which is common in many English\\nExercise 2.1 Byte pair encoding of unknown words \\nTry the BPE tokenizer from the tiktoken library on the unknown words “Akwirw ier” and\\nprint the individual token IDs. Then, call the decode function on each of the resulting\\nintegers in this list to reproduce the mapping shown in figure 2.11. Lastly, call the\\ndecode method on the token IDs to check whether it can reconstruct the original\\ninput, “Akwirw ier.”\\n\"Akwirw\\nier \"\\n\"Ak\"\\n\"w\"\\n\"ir\"\\n\"w\"\\n\" \"\\n\"ier\"\\n33901\\n86\\n343\\n86\\n220\\n959\\nText sample with\\nunknown words\\nUnknown words are\\ntokenized into individual\\ncharacters or subwords.\\nTokens:\\nToken IDs:\\nFigure 2.11\\nBPE tokenizers \\nbreak down unknown words \\ninto subwords and individual \\ncharacters. This way, a BPE \\ntokenizer can parse any word \\nand doesn’t need to replace \\nunknown words with special \\ntokens, such as <|unk|>.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 56}, page_content='35\\n2.6\\nData sampling with a sliding window\\nwords like “define,” “depend,” “made,” and “hidden.” The merges are determined by\\na frequency cutoff.\\n2.6\\nData sampling with a sliding window\\nThe next step in creating the embeddings for the LLM is to generate the input–target\\npairs required for training an LLM. What do these input–target pairs look like? As we\\nalready learned, LLMs are pretrained by predicting the next word in a text, as depicted\\nin figure 2.12.\\nLet’s implement a data loader that fetches the input–target pairs in figure 2.12 from\\nthe training dataset using a sliding window approach. To get started, we will tokenize\\nthe whole “The Verdict” short story using the BPE tokenizer:\\nwith open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\\n    raw_text = f.read()\\nenc_text = tokenizer.encode(raw_text)\\nprint(len(enc_text))\\nExecuting this code will return 5145, the total number of tokens in the training set,\\nafter applying the BPE tokenizer.\\n Next, we remove the first 50 tokens from the dataset for demonstration purposes,\\nas it results in a slightly more interesting text passage in the next steps:\\nenc_sample = enc_text[50:]\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nInput the\\nLLM receives\\nTarget to\\npredict\\nText\\nsample:\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nThe LLM can’t\\naccess words past\\nthe target.\\nFigure 2.12\\nGiven a text sample, extract input blocks as subsamples that serve as \\ninput to the LLM, and the LLM’s prediction task during training is to predict the next \\nword that follows the input block. During training, we mask out all words that are past \\nthe target. Note that the text shown in this figure must undergo tokenization before \\nthe LLM can process it; however, this figure omits the tokenization step for clarity.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 57}, page_content='36\\nCHAPTER 2\\nWorking with text data\\nOne of the easiest and most intuitive ways to create the input–target pairs for the next-\\nword prediction task is to create two variables, x and y, where x contains the input\\ntokens and y contains the targets, which are the inputs shifted by 1:\\ncontext_size = 4        \\nx = enc_sample[:context_size]\\ny = enc_sample[1:context_size+1]\\nprint(f\"x: {x}\")\\nprint(f\"y:      {y}\")\\nRunning the previous code prints the following output:\\nx: [290, 4920, 2241, 287]\\ny:      [4920, 2241, 287, 257]\\nBy processing the inputs along with the targets, which are the inputs shifted by one\\nposition, we can create the next-word prediction tasks (see figure 2.12), as follows:\\nfor i in range(1, context_size+1):\\n    context = enc_sample[:i]\\n    desired = enc_sample[i]\\n    print(context, \"---->\", desired)\\nThe code prints\\n[290] ----> 4920\\n[290, 4920] ----> 2241\\n[290, 4920, 2241] ----> 287\\n[290, 4920, 2241, 287] ----> 257\\nEverything left of the arrow (---->) refers to the input an LLM would receive, and\\nthe token ID on the right side of the arrow represents the target token ID that the\\nLLM is supposed to predict. Let’s repeat the previous code but convert the token IDs\\ninto text:\\nfor i in range(1, context_size+1):\\n    context = enc_sample[:i]\\n    desired = enc_sample[i]\\n    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))\\nThe following outputs show how the input and outputs look in text format:\\n and ---->  established\\n and established ---->  himself\\n and established himself ---->  in\\n and established himself in ---->  a\\nWe’ve now created the input–target pairs that we can use for LLM training.\\n There’s only one more task before we can turn the tokens into embeddings: imple-\\nmenting an efficient data loader that iterates over the input dataset and returns the\\nThe context size determines \\nhow many tokens are included \\nin the input.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 58}, page_content='37\\n2.6\\nData sampling with a sliding window\\ninputs and targets as PyTorch tensors, which can be thought of as multidimensional\\narrays. In particular, we are interested in returning two tensors: an input tensor con-\\ntaining the text that the LLM sees and a target tensor that includes the targets for the\\nLLM to predict, as depicted in figure 2.13. While the figure shows the tokens in string\\nformat for illustration purposes, the code implementation will operate on token IDs\\ndirectly since the encode method of the BPE tokenizer performs both tokenization\\nand conversion into token IDs as a single step.\\nNOTE\\nFor the efficient data loader implementation, we will use PyTorch’s\\nbuilt-in Dataset and DataLoader classes. For additional information and\\nguidance on installing PyTorch, please see section A.2.1.3 in appendix A.\\nThe code for the dataset class is shown in the following listing.\\nimport torch\\nfrom torch.utils.data import Dataset, DataLoader\\nclass GPTDatasetV1(Dataset):\\n    def __init__(self, txt, tokenizer, max_length, stride):\\n        self.input_ids = []\\n        self.target_ids = []\\n        token_ids = tokenizer.encode(txt)   \\nListing 2.5\\nA dataset for batched inputs and targets\\n\"In the heart of the city stood the old library, a relic from a bygone era. Its\\nstone walls bore the marks of time, and ivy clung tightly to its facade …\"\\nx = tensor([[  \"In\",      \"the\",     \"heart\",  \"of\"   ],\\n[  \"the\" ,    \"city\",    \"stood\",  \"the\"  ],\\n[  \"old\",     \"library\", \",\",      \"a\"    ],\\n[ …\\n]])\\ny = tensor([[  \"the\",     \"heart\",   \"of\",     \"the\"  ],\\n[  \"city\",    \"stood\",   \"the\",    \"old\"  ],\\n[  \"library\", “,\",\\n“a\",\\n“relic\"],\\n[ …\\n]])\\nSample text\\nTensor\\ncontaining\\nthe inputs\\nTensor\\ncontaining\\nthe targets\\nFigure 2.13\\nTo implement efficient data loaders, we collect the inputs in a tensor, x, where each row \\nrepresents one input context. A second tensor, y, contains the corresponding prediction targets (next \\nwords), which are created by shifting the input by one position.\\nTokenizes the \\nentire text'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 59}, page_content='38\\nCHAPTER 2\\nWorking with text data\\n        for i in range(0, len(token_ids) - max_length, stride):    \\n            input_chunk = token_ids[i:i + max_length]\\n            target_chunk = token_ids[i + 1: i + max_length + 1]\\n            self.input_ids.append(torch.tensor(input_chunk))\\n            self.target_ids.append(torch.tensor(target_chunk))\\n    def __len__(self):   \\n        return len(self.input_ids)\\n    def __getitem__(self, idx):        \\n        return self.input_ids[idx], self.target_ids[idx]\\nThe GPTDatasetV1 class is based on the PyTorch Dataset class and defines how indi-\\nvidual rows are fetched from the dataset, where each row consists of a number of\\ntoken IDs (based on a max_length) assigned to an input_chunk tensor. The target_\\nchunk tensor contains the corresponding targets. I recommend reading on to see what\\nthe data returned from this dataset looks like when we combine the dataset with a\\nPyTorch DataLoader—this will bring additional intuition and clarity.\\nNOTE\\nIf you are new to the structure of PyTorch Dataset classes, such as\\nshown in listing 2.5, refer to section A.6 in appendix A, which explains the\\ngeneral structure and usage of PyTorch Dataset and DataLoader classes.\\nThe following code uses the GPTDatasetV1 to load the inputs in batches via a PyTorch\\nDataLoader.\\ndef create_dataloader_v1(txt, batch_size=4, max_length=256,\\n                         stride=128, shuffle=True, drop_last=True,\\n                         num_workers=0):\\n    tokenizer = tiktoken.get_encoding(\"gpt2\")                        \\n    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)  \\n    dataloader = DataLoader(\\n        dataset,\\n        batch_size=batch_size,\\n        shuffle=shuffle,\\n        drop_last=drop_last,    \\n        num_workers=num_workers    \\n    )\\n    return dataloader\\nListing 2.6\\nA data loader to generate batches with input-with pairs\\nUses a sliding window to chunk\\nthe book into overlapping\\nsequences of max_length\\nReturns the total number \\nof rows in the dataset\\nReturns a single row \\nfrom the dataset\\nInitializes the \\ntokenizer\\nCreates \\ndataset\\ndrop_last=True drops the last \\nbatch if it is shorter than the \\nspecified batch_size to prevent \\nloss spikes during training.\\nThe number of CPU processes \\nto use for preprocessing'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 60}, page_content='39\\n2.6\\nData sampling with a sliding window\\nLet’s test the dataloader with a batch size of 1 for an LLM with a context size of 4 to\\ndevelop an intuition of how the GPTDatasetV1 class from listing 2.5 and the create_\\ndataloader_v1 function from listing 2.6 work together:\\nwith open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\\n    raw_text = f.read()\\ndataloader = create_dataloader_v1(\\n    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\\ndata_iter = iter(dataloader)     \\nfirst_batch = next(data_iter)\\nprint(first_batch)\\nExecuting the preceding code prints the following:\\n[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\\nThe first_batch variable contains two tensors: the first tensor stores the input token\\nIDs, and the second tensor stores the target token IDs. Since the max_length is set to\\n4, each of the two tensors contains four token IDs. Note that an input size of 4 is quite\\nsmall and only chosen for simplicity. It is common to train LLMs with input sizes of at\\nleast 256.\\n To understand the meaning of stride=1, let’s fetch another batch from this dataset:\\nsecond_batch = next(data_iter)\\nprint(second_batch)\\nThe second batch has the following contents:\\n[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\\nIf we compare the first and second batches, we can see that the second batch’s token\\nIDs are shifted by one position (for example, the second ID in the first batch’s input is\\n367, which is the first ID of the second batch’s input). The stride setting dictates the\\nnumber of positions the inputs shift across batches, emulating a sliding window\\napproach, as demonstrated in figure 2.14.\\nBatch sizes of 1, such as we have sampled from the data loader so far, are useful for\\nillustration purposes. If you have previous experience with deep learning, you may\\nknow that small batch sizes require less memory during training but lead to more\\nExercise 2.2 Data loaders with different strides and context sizes\\nTo develop more intuition for how the data loader works, try to run it with different\\nsettings such as max_length=2 and stride=2, and max_length=8 and stride=2.\\nConverts dataloader into a Python \\niterator to fetch the next entry via \\nPython’s built-in next() function'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 61}, page_content='40\\nCHAPTER 2\\nWorking with text data\\nnoisy model updates. Just like in regular deep learning, the batch size is a tradeoff and\\na hyperparameter to experiment with when training LLMs.\\n Let’s look briefly at how we can use the data loader to sample with a batch size\\ngreater than 1:\\ndataloader = create_dataloader_v1(\\n    raw_text, batch_size=8, max_length=4, stride=4,\\n    shuffle=False\\n)\\ndata_iter = iter(dataloader)\\ninputs, targets = next(data_iter)\\nprint(\"Inputs:\\\\n\", inputs)\\nprint(\"\\\\nTargets:\\\\n\", targets)\\nThis prints\\nInputs:\\n tensor([[   40,   367,  2885,  1464],\\n        [ 1807,  3619,   402,   271],\\n        [10899,  2138,   257,  7026],\\n        [15632,   438,  2016,   257],\\n        [  922,  5891,  1576,   438],\\n        [  568,   340,   373,   645],\\n\"In the heart of the city stood the old library, a relic from a bygone era. Its\\nstone walls bore the marks of time, and ivy clung tightly to its facade …\"\\nSample text\\nA stride of 1 moves the input ﬁeld by 1 position\\n\"In the heart of the city stood the old library, a relic from a bygone era. Its\\nstone walls bore the marks of time, and ivy clung tightly to its facade …\"\\nA stride of 4 moves the input ﬁeld by 4 positions\\n\"In the heart of\"\\n\"the heart of the\"\\nInputs of batch 1:\\nInputs of batch 2:\\n\"In the heart of\"\\n“the city stood the\"\\nInputs of batch 1:\\nInputs of batch 2:\\nFigure 2.14\\nWhen creating multiple batches from the input dataset, we slide an \\ninput window across the text. If the stride is set to 1, we shift the input window by \\none position when creating the next batch. If we set the stride equal to the input \\nwindow size, we can prevent overlaps between the batches.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 62}, page_content='41\\n2.7\\nCreating token embeddings\\n        [ 1049,  5975,   284,   502],\\n        [  284,  3285,   326,    11]])\\nTargets:\\n tensor([[  367,  2885,  1464,  1807],\\n        [ 3619,   402,   271, 10899],\\n        [ 2138,   257,  7026, 15632],\\n        [  438,  2016,   257,   922],\\n        [ 5891,  1576,   438,   568],\\n        [  340,   373,   645,  1049],\\n        [ 5975,   284,   502,   284],\\n        [ 3285,   326,    11,   287]])\\nNote that we increase the stride to 4 to utilize the data set fully (we don’t skip a single\\nword). This avoids any overlap between the batches since more overlap could lead to\\nincreased overfitting.\\n2.7\\nCreating token embeddings\\nThe last step in preparing the input text for LLM training is to convert the token IDs\\ninto embedding vectors, as shown in figure 2.15. As a preliminary step, we must initialize\\nGPT-like\\ndecoder-only\\ntransformer\\nInput text:\\nToken embeddings:\\nThis is an example.\\nTokenized text:\\nOutput text\\nPostprocessing steps\\nToken IDs:\\nThis\\nis\\nan\\nexample\\n.\\n40134\\n2052\\n133\\n389\\n12\\nCreating input\\ntoken embeddings\\nFigure 2.15\\nPreparation involves tokenizing text, converting text tokens to token IDs, and \\nconverting token IDs into embedding vectors. Here, we consider the previously created token \\nIDs to create the token embedding vectors.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 63}, page_content='42\\nCHAPTER 2\\nWorking with text data\\nthese embedding weights with random values. This initialization serves as the starting\\npoint for the LLM’s learning process. In chapter 5, we will optimize the embedding\\nweights as part of the LLM training.\\n A continuous vector representation, or embedding, is necessary since GPT-like\\nLLMs are deep neural networks trained with the backpropagation algorithm. \\nNOTE\\nIf you are unfamiliar with how neural networks are trained with back-\\npropagation, please read section B.4 in appendix A.\\nLet’s see how the token ID to embedding vector conversion works with a hands-on\\nexample. Suppose we have the following four input tokens with IDs 2, 3, 5, and 1:\\ninput_ids = torch.tensor([2, 3, 5, 1])\\nFor the sake of simplicity, suppose we have a small vocabulary of only 6 words (instead\\nof the 50,257 words in the BPE tokenizer vocabulary), and we want to create embed-\\ndings of size 3 (in GPT-3, the embedding size is 12,288 dimensions):\\nvocab_size = 6\\noutput_dim = 3\\nUsing the vocab_size and output_dim, we can instantiate an embedding layer in\\nPyTorch, setting the random seed to 123 for reproducibility purposes:\\ntorch.manual_seed(123)\\nembedding_layer = torch.nn.Embedding(vocab_size, output_dim)\\nprint(embedding_layer.weight)\\nThe print statement prints the embedding layer’s underlying weight matrix:\\nParameter containing:\\ntensor([[ 0.3374, -0.1778, -0.1690],\\n        [ 0.9178,  1.5810,  1.3010],\\n        [ 1.2753, -0.2010, -0.1606],\\n        [-0.4015,  0.9666, -1.1481],\\n        [-1.1589,  0.3255, -0.6315],\\n        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\\nThe weight matrix of the embedding layer contains small, random values. These val-\\nues are optimized during LLM training as part of the LLM optimization itself. More-\\nover, we can see that the weight matrix has six rows and three columns. There is one row\\nfor each of the six possible tokens in the vocabulary, and there is one column for each of\\nthe three embedding dimensions.\\n Now, let’s apply it to a token ID to obtain the embedding vector: \\nprint(embedding_layer(torch.tensor([3])))'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 64}, page_content='43\\n2.8\\nEncoding word positions\\nThe returned embedding vector is\\ntensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\\nIf we compare the embedding vector for token ID 3 to the previous embedding\\nmatrix, we see that it is identical to the fourth row (Python starts with a zero index, so\\nit’s the row corresponding to index 3). In other words, the embedding layer is essen-\\ntially a lookup operation that retrieves rows from the embedding layer’s weight matrix\\nvia a token ID.\\nNOTE\\nFor those who are familiar with one-hot encoding, the embedding\\nlayer approach described here is essentially just a more efficient way of imple-\\nmenting one-hot encoding followed by matrix multiplication in a fully con-\\nnected layer, which is illustrated in the supplementary code on GitHub at\\nhttps://mng.bz/ZEB5. Because the embedding layer is just a more efficient\\nimplementation equivalent to the one-hot encoding and matrix-multiplica-\\ntion approach, it can be seen as a neural network layer that can be optimized\\nvia backpropagation.\\nWe’ve seen how to convert a single token ID into a three-dimensional embedding vec-\\ntor. Let’s now apply that to all four input IDs (torch.tensor([2, 3, 5, 1])):\\nprint(embedding_layer(input_ids))\\nThe print output reveals that this results in a 4 × 3 matrix:\\ntensor([[ 1.2753, -0.2010, -0.1606],\\n        [-0.4015,  0.9666, -1.1481],\\n        [-2.8400, -0.7849, -1.4096],\\n        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\\nEach row in this output matrix is obtained via a lookup operation from the embed-\\nding weight matrix, as illustrated in figure 2.16.\\n Having now created embedding vectors from token IDs, next we’ll add a small\\nmodification to these embedding vectors to encode positional information about a\\ntoken within a text.\\n2.8\\nEncoding word positions\\nIn principle, token embeddings are a suitable input for an LLM. However, a minor\\nshortcoming of LLMs is that their self-attention mechanism (see chapter 3) doesn’t\\nhave a notion of position or order for the tokens within a sequence. The way the pre-\\nviously introduced embedding layer works is that the same token ID always gets\\nmapped to the same vector representation, regardless of where the token ID is posi-\\ntioned in the input sequence, as shown in figure 2.17.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 65}, page_content='44\\nCHAPTER 2\\nWorking with text data\\n0.3374\\n−0.1778\\n−0.1690\\n0.9178\\n1.5810\\n1.3010\\n1.2753\\n−0.2010\\n−0.1606\\n−0.4015\\n0.9666\\n−1.1481\\n−1.1589\\n0.3255\\n−0.6315\\n−2.8400\\n−0.7849\\n−1.4096\\nWeight matrix of the\\nembedding layer\\n2\\n3\\n5\\n1\\n1.2753\\n−0.2010\\n−0.1606\\n−0.4015\\n0.9666\\n−1.1481\\n−2.8400\\n−0.7849\\n−1.4096\\n0.9178\\n1.5810\\n1.3010\\n2\\n1.2753\\n−0.2010\\n−0.1606\\n2\\n3\\n5\\n1\\n5\\nToken IDs to embed\\nEmbedded token IDs\\nEmbedding vector of\\nthe ﬁrst token ID\\nEmbedding vector of\\nthe third token ID\\n1.2753\\n−0.2010\\n−0.1606\\n−2.8400\\n−0.7849\\n−1.4096\\nfox\\njumps\\nover\\ndog\\nfox\\njumps\\nover\\ndog\\nInput text\\n−2.8400\\n−0.7849\\n−1.4096\\nFigure 2.16\\nEmbedding layers perform a lookup operation, retrieving the embedding \\nvector corresponding to the token ID from the embedding layer’s weight matrix. For \\ninstance, the embedding vector of the token ID 5 is the sixth row of the embedding \\nlayer weight matrix (it is the sixth instead of the fifth row because Python starts \\ncounting at 0). We assume that the token IDs were produced by the small vocabulary \\nfrom section 2.3.\\n0.3374\\n−0.1778 −0.1690\\n0.9178\\n1.5810\\n1.3010\\n1.2753\\n−0.2010 −0.1606\\n−0.4015\\n0.9666\\n−1.1481\\n−1.1589\\n0.3255\\n−0.6315\\n−2.8400 −0.7849 −1.4096\\n2\\n3\\n5\\n2\\n1.2753\\n−0.2010 −0.1606\\n−0.4015\\n0.9666\\n−1.1481\\n−2.8400 −0.7849 −1.4096\\n1.2753\\n−0.2010\\n−0.1606\\n2\\n1.2753\\n−0.2010 −0.1606\\n2\\n3\\n5\\n2\\nfox\\njumps\\nover\\nfox\\nfox\\njumps\\nover\\nfox\\n2\\n1.2753\\n−0.2010\\n−0.1606\\nWeight matrix of the\\nembedding layer\\nToken IDs to embed\\nThe same token IDs\\nresult in the same\\nembedding vectors\\n1.2753\\n−0.2010 −0.1606\\nFigure 2.17\\nThe embedding layer converts a token ID into the same vector \\nrepresentation regardless of where it is located in the input sequence. For \\nexample, the token ID 5, whether it’s in the first or fourth position in the \\ntoken ID input vector, will result in the same embedding vector.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 66}, page_content='45\\n2.8\\nEncoding word positions\\nIn principle, the deterministic, position-independent embedding of the token ID is\\ngood for reproducibility purposes. However, since the self-attention mechanism of\\nLLMs itself is also position-agnostic, it is helpful to inject additional position informa-\\ntion into the LLM.\\n To achieve this, we can use two broad categories of position-aware embeddings: rela-\\ntive positional embeddings and absolute positional embeddings. Absolute positional\\nembeddings are directly associated with specific positions in a sequence. For each posi-\\ntion in the input sequence, a unique embedding is added to the token’s embedding to\\nconvey its exact location. For instance, the first token will have a specific positional\\nembedding, the second token another distinct embedding, and so on, as illustrated in\\nfigure 2.18.\\nInstead of focusing on the absolute position of a token, the emphasis of relative posi-\\ntional embeddings is on the relative position or distance between tokens. This means\\nthe model learns the relationships in terms of “how far apart” rather than “at which\\nexact position.” The advantage here is that the model can generalize better to sequences\\nof varying lengths, even if it hasn’t seen such lengths during training.\\n Both types of positional embeddings aim to augment the capacity of LLMs to\\nunderstand the order and relationships between tokens, ensuring more accurate and\\ncontext-aware predictions. The choice between them often depends on the specific\\napplication and the nature of the data being processed.\\n OpenAI’s GPT models use absolute positional embeddings that are optimized\\nduring the training process rather than being fixed or predefined like the positional\\nencodings in the original transformer model. This optimization process is part of the\\nmodel training itself. For now, let’s create the initial positional embeddings to create the\\nLLM inputs.\\nInput embeddings:\\nPositional embeddings:\\nToken embeddings:\\n+\\n+\\n+\\n+\\n2.1\\n2.2\\n2.3\\n3.1\\n3.2\\n3.3\\n4.1\\n4.2\\n4.3\\n5.1\\n5.2\\n5.3\\n1.1\\n1.2\\n1.3\\n2.1\\n2.2\\n2.3\\n3.1\\n3.2\\n3.3\\n4.1\\n4.2\\n4.3\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\nEmbedding of the ﬁrst token\\nEmbedding of the third token\\nFigure 2.18\\nPositional embeddings are added to the token embedding vector to create the \\ninput embeddings for an LLM. The positional vectors have the same dimension as the original \\ntoken embeddings. The token embeddings are shown with value 1 for simplicity.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 67}, page_content='46\\nCHAPTER 2\\nWorking with text data\\n Previously, we focused on very small embedding sizes for simplicity. Now, let’s con-\\nsider more realistic and useful embedding sizes and encode the input tokens into a\\n256-dimensional vector representation, which is smaller than what the original GPT-3\\nmodel used (in GPT-3, the embedding size is 12,288 dimensions) but still reasonable\\nfor experimentation. Furthermore, we assume that the token IDs were created by the\\nBPE tokenizer we implemented earlier, which has a vocabulary size of 50,257:\\nvocab_size = 50257\\noutput_dim = 256\\ntoken_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\\nUsing the previous token_embedding_layer, if we sample data from the data loader,\\nwe embed each token in each batch into a 256-dimensional vector. If we have a batch\\nsize of 8 with four tokens each, the result will be an 8 × 4 × 256 tensor.\\n Let’s instantiate the data loader (see section 2.6) first:\\nmax_length = 4\\ndataloader = create_dataloader_v1(\\n    raw_text, batch_size=8, max_length=max_length,\\n   stride=max_length, shuffle=False\\n)\\ndata_iter = iter(dataloader)\\ninputs, targets = next(data_iter)\\nprint(\"Token IDs:\\\\n\", inputs)\\nprint(\"\\\\nInputs shape:\\\\n\", inputs.shape)\\nThis code prints\\nToken IDs:\\n tensor([[   40,   367,  2885,  1464],\\n        [ 1807,  3619,   402,   271],\\n        [10899,  2138,   257,  7026],\\n        [15632,   438,  2016,   257],\\n        [  922,  5891,  1576,   438],\\n        [  568,   340,   373,   645],\\n        [ 1049,  5975,   284,   502],\\n        [  284,  3285,   326,    11]])\\nInputs shape:\\n torch.Size([8, 4])\\nAs we can see, the token ID tensor is 8 × 4 dimensional, meaning that the data batch\\nconsists of eight text samples with four tokens each.\\n Let’s now use the embedding layer to embed these token IDs into 256-dimensional\\nvectors:\\ntoken_embeddings = token_embedding_layer(inputs)\\nprint(token_embeddings.shape)'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 68}, page_content='47\\n2.8\\nEncoding word positions\\nThe print function call returns\\ntorch.Size([8, 4, 256])\\nThe 8 × 4 × 256–dimensional tensor output shows that each token ID is now embed-\\nded as a 256-dimensional vector.\\n For a GPT model’s absolute embedding approach, we just need to create another\\nembedding layer that has the same embedding dimension as the token_embedding_\\nlayer:\\ncontext_length = max_length\\npos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\\npos_embeddings = pos_embedding_layer(torch.arange(context_length))\\nprint(pos_embeddings.shape)\\nThe input to the pos_embeddings is usually a placeholder vector torch.arange(con-\\ntext_length), which contains a sequence of numbers 0, 1, ..., up to the maximum\\ninput length –1. The context_length is a variable that represents the supported input\\nsize of the LLM. Here, we choose it similar to the maximum length of the input text.\\nIn practice, input text can be longer than the supported context length, in which case\\nwe have to truncate the text.\\n The output of the print statement is\\ntorch.Size([4, 256])\\nAs we can see, the positional embedding tensor consists of four 256-dimensional vec-\\ntors. We can now add these directly to the token embeddings, where PyTorch will add\\nthe 4 × 256–dimensional pos_embeddings tensor to each 4 × 256–dimensional token\\nembedding tensor in each of the eight batches:\\ninput_embeddings = token_embeddings + pos_embeddings\\nprint(input_embeddings.shape)\\nThe print output is\\ntorch.Size([8, 4, 256])\\nThe input_embeddings we created, as summarized in figure 2.19, are the embedded\\ninput examples that can now be processed by the main LLM modules, which we will\\nbegin implementing in the next chapter.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 69}, page_content='48\\nCHAPTER 2\\nWorking with text data\\nSummary\\n\\uf0a1LLMs require textual data to be converted into numerical vectors, known as\\nembeddings, since they can’t process raw text. Embeddings transform discrete\\ndata (like words or images) into continuous vector spaces, making them com-\\npatible with neural network operations. \\n\\uf0a1As the first step, raw text is broken into tokens, which can be words or characters.\\nThen, the tokens are converted into integer representations, termed token IDs.\\n\\uf0a1Special tokens, such as <|unk|> and <|endoftext|>, can be added to enhance\\nthe model’s understanding and handle various contexts, such as unknown\\nwords or marking the boundary between unrelated texts.\\nGPT-like\\ndecoder-only\\ntransformer\\nOutput text\\nPostprocessing steps\\nInput text:\\nInput embeddings:\\nThis is an example.\\nTokenized text:\\nThis\\nis\\nan\\nexample\\nToken IDs:\\n40134\\n2052\\n133\\n389\\n.\\n12\\nToken embeddings:\\nPositional embeddings:\\n+\\nThe input embedding pipeline\\nFigure 2.19\\nAs part of the input processing pipeline, input text is first broken \\nup into individual tokens. These tokens are then converted into token IDs using a \\nvocabulary. The token IDs are converted into embedding vectors to which positional \\nembeddings of a similar size are added, resulting in input embeddings that are used \\nas input for the main LLM layers.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 70}, page_content='49\\nSummary\\n\\uf0a1The byte pair encoding (BPE) tokenizer used for LLMs like GPT-2 and GPT-3\\ncan efficiently handle unknown words by breaking them down into subword\\nunits or individual characters.\\n\\uf0a1We use a sliding window approach on tokenized data to generate input–target\\npairs for LLM training.\\n\\uf0a1Embedding layers in PyTorch function as a lookup operation, retrieving vectors\\ncorresponding to token IDs. The resulting embedding vectors provide continu-\\nous representations of tokens, which is crucial for training deep learning mod-\\nels like LLMs. \\n\\uf0a1While token embeddings provide consistent vector representations for each\\ntoken, they lack a sense of the token’s position in a sequence. To rectify this,\\ntwo main types of positional embeddings exist: absolute and relative. OpenAI’s\\nGPT models utilize absolute positional embeddings, which are added to the token\\nembedding vectors and are optimized during the model training.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 71}, page_content='50\\nCoding attention\\nmechanisms\\nAt this point, you know how to prepare the input text for training LLMs by splitting\\ntext into individual word and subword tokens, which can be encoded into vector rep-\\nresentations, embeddings, for the LLM. \\n Now, we will look at an integral part of the LLM architecture itself, attention\\nmechanisms, as illustrated in figure 3.1. We will largely look at attention mechanisms\\nin isolation and focus on them at a mechanistic level. Then we will code the remaining\\nThis chapter covers\\n\\uf0a1The reasons for using attention mechanisms in \\nneural networks\\n\\uf0a1A basic self-attention framework, progressing to \\nan enhanced self-attention mechanism \\n\\uf0a1A causal attention module that allows LLMs to \\ngenerate one token at a time\\n\\uf0a1Masking randomly selected attention weights with \\ndropout to reduce overfitting\\n\\uf0a1Stacking multiple causal attention modules into a \\nmulti-head attention module'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 72}, page_content='51\\nparts of the LLM surrounding the self-attention mechanism to see it in action and to\\ncreate a model to generate text.\\n We will implement four different variants of attention mechanisms, as illustrated in\\nfigure 3.2. These different attention variants build on each other, and the goal is to\\nThis chapter implements the\\nattention mechanism, an important\\nbuilding block of GPT-like LLMs\\n1) Data\\npreparation\\n& sampling\\n2) Attention\\nmechanism\\nBuilding an LLM\\nSTAGE 1\\nFoundation model\\nSTAGE 2\\nSTAGE 3\\nClassiﬁer\\nPersonal assistant\\nDataset with class labels\\nInstruction dataset\\n4) Pretraining\\n9) Fine-tuning\\n5) Training\\nloop\\n6) Model\\nevaluation\\n7) Load\\npretrained\\nweights\\n8) Fine-tuning\\n3) LLM\\narchitecture\\n2) Attention\\nmechanism\\nFigure 3.1\\nThe three main stages of coding an LLM. This chapter focuses on step 2 of stage 1: implementing \\nattention mechanisms, which are an integral part of the LLM architecture.\\n1) Simpliﬁed\\nself-attention\\n2) Self-attention\\n3) Causal attention\\n4) Multi-head\\nattention\\nA simpliﬁed self-attention\\ntechnique to introduce the\\nbroader idea\\nSelf-attention with trainable\\nweights that forms the basis of\\nthe mechanism used in LLMs\\nA type of self-attention used in LLMs\\nthat allows a model to consider only\\nprevious and current inputs in a\\nsequence, ensuring temporal order\\nduring the text generation\\nAn extension of self-attention and\\ncausal attention that enables the\\nmodel to simultaneously attend\\nto information from different\\nrepresentation subspaces\\nFigure 3.2\\nThe figure depicts different attention mechanisms we will code in this chapter, starting \\nwith a simplified version of self-attention before adding the trainable weights. The causal attention \\nmechanism adds a mask to self-attention that allows the LLM to generate one word at a time. Finally, \\nmulti-head attention organizes the attention mechanism into multiple heads, allowing the model to \\ncapture various aspects of the input data in parallel.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 73}, page_content='52\\nCHAPTER 3\\nCoding attention mechanisms\\narrive at a compact and efficient implementation of multi-head attention that we can\\nthen plug into the LLM architecture we will code in the next chapter.\\n3.1\\nThe problem with modeling long sequences\\nBefore we dive into the self-attention mechanism at the heart of LLMs, let’s consider\\nthe problem with pre-LLM architectures that do not include attention mechanisms.\\nSuppose we want to develop a language translation model that translates text from\\none language into another. As shown in figure 3.3, we can’t simply translate a text word\\nby word due to the grammatical structures in the source and target language.\\nTo address this problem, it is common to use a deep neural network with two submod-\\nules, an encoder and a decoder. The job of the encoder is to first read in and process the\\nentire text, and the decoder then produces the translated text.\\n Before the advent of transformers, recurrent neural networks (RNNs) were the most\\npopular encoder–decoder architecture for language translation. An RNN is a type of\\nneural network where outputs from previous steps are fed as inputs to the current\\ndu\\nKannst\\nhelfen\\nmir\\nSatz\\ndiesen\\nuebersetzen\\nzu\\nyou\\nCan\\nhelp\\nme\\nsentence\\nthis\\ntranslate\\nto\\ndu\\nKannst\\nhelfen\\nmir\\nSatz\\ndiesen\\nuebersetzen\\nzu\\nyou\\nCan\\nme\\nhelp\\ntranslate\\nto\\nsentence\\nthis\\nGerman input sentence to translate\\nThe word-by-word translation results\\nin a grammatically incorrect sentence\\nThe correct translation\\nCertain words in the generated translation\\nrequire access to words that appear earlier\\nor later in the original sentence.\\nFigure 3.3\\nWhen translating text from one language to another, such as German to English, it’s not \\npossible to merely translate word by word. Instead, the translation process requires contextual \\nunderstanding and grammatical alignment.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 74}, page_content='53\\n3.1\\nThe problem with modeling long sequences\\nstep, making them well-suited for sequential data like text. If you are unfamiliar with\\nRNNs, don’t worry—you don’t need to know the detailed workings of RNNs to fol-\\nlow this discussion; our focus here is more on the general concept of the encoder–\\ndecoder setup. \\n In an encoder–decoder RNN, the input text is fed into the encoder, which pro-\\ncesses it sequentially. The encoder updates its hidden state (the internal values at the\\nhidden layers) at each step, trying to capture the entire meaning of the input sen-\\ntence in the final hidden state, as illustrated in figure 3.4. The decoder then takes this\\nfinal hidden state to start generating the translated sentence, one word at a time. It\\nalso updates its hidden state at each step, which is supposed to carry the context nec-\\nessary for the next-word prediction.\\nWhile we don’t need to know the inner workings of these encoder–decoder RNNs,\\nthe key idea here is that the encoder part processes the entire input text into a hid-\\nden state (memory cell). The decoder then takes in this hidden state to produce the\\noutput. You can think of this hidden state as an embedding vector, a concept we dis-\\ncussed in chapter 2.\\n The big limitation of encoder–decoder RNNs is that the RNN can’t directly access\\nearlier hidden states from the encoder during the decoding phase. Consequently, it\\nrelies solely on the current hidden state, which encapsulates all relevant information.\\nThis can lead to a loss of context, especially in complex sentences where dependen-\\ncies might span long distances.\\ndu\\nKannst\\nmir\\nyou\\nCan\\nhelp\\nHidden states\\nOutputs\\nInputs\\nEncoder\\nDecoder\\nHidden states of a\\nneural network\\nA memory cell (hidden state)\\nmemorizing entire input\\nGerman input sentence to translate\\nThe translated English sentence\\nFigure 3.4\\nBefore the advent of transformer models, encoder–decoder RNNs were a popular choice \\nfor machine translation. The encoder takes a sequence of tokens from the source language as input, \\nwhere a hidden state (an intermediate neural network layer) of the encoder encodes a compressed \\nrepresentation of the entire input sequence. Then, the decoder uses its current hidden state to begin \\nthe translation, token by token.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 75}, page_content='54\\nCHAPTER 3\\nCoding attention mechanisms\\n Fortunately, it is not essential to understand RNNs to build an LLM. Just remem-\\nber that encoder–decoder RNNs had a shortcoming that motivated the design of\\nattention mechanisms.\\n3.2\\nCapturing data dependencies with attention \\nmechanisms\\nAlthough RNNs work fine for translating short sentences, they don’t work well for lon-\\nger texts as they don’t have direct access to previous words in the input. One major\\nshortcoming in this approach is that the RNN must remember the entire encoded\\ninput in a single hidden state before passing it to the decoder (figure 3.4).\\n Hence, researchers developed the Bahdanau attention mechanism for RNNs in\\n2014 (named after the first author of the respective paper; for more information, see\\nappendix B), which modifies the encoder–decoder RNN such that the decoder can\\nselectively access different parts of the input sequence at each decoding step as illus-\\ntrated in figure 3.5.\\nInterestingly, only three years later, researchers found that RNN architectures are\\nnot required for building deep neural networks for natural language processing and\\ndu\\nKannst\\nmir\\nyou\\nCan\\nhelp\\nHidden states\\nOutputs\\nInputs\\nWhen generating an output\\ntoken, the model has a way\\nto access to all input tokens.\\nThe dotted line width is proportional\\nto how important the input token is\\nfor the respective output token.\\nWe are focusing on\\ngenerating the second\\noutput token.\\nFigure 3.5\\nUsing an attention mechanism, the text-generating decoder part of the network can \\naccess all input tokens selectively. This means that some input tokens are more important than others \\nfor generating a given output token. The importance is determined by the attention weights, which we \\nwill compute later. Note that this figure shows the general idea behind attention and does not depict \\nthe exact implementation of the Bahdanau mechanism, which is an RNN method outside this book’s \\nscope.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 76}, page_content='55\\n3.3\\nAttending to different parts of the input with self-attention\\nproposed the original transformer architecture (discussed in chapter 1) including a\\nself-attention mechanism inspired by the Bahdanau attention mechanism. \\n Self-attention is a mechanism that allows each position in the input sequence to\\nconsider the relevancy of, or “attend to,” all other positions in the same sequence\\nwhen computing the representation of a sequence. Self-attention is a key component\\nof contemporary LLMs based on the transformer architecture, such as the GPT series. \\n This chapter focuses on coding and understanding this self-attention mechanism\\nused in GPT-like models, as illustrated in figure 3.6. In the next chapter, we will code\\nthe remaining parts of the LLM.\\n3.3\\nAttending to different parts of the input \\nwith self-attention\\nWe’ll now cover the inner workings of the self-attention mechanism and learn how to\\ncode it from the ground up. Self-attention serves as the cornerstone of every LLM\\nbased on the transformer architecture. This topic may require a lot of focus and atten-\\ntion (no pun intended), but once you grasp its fundamentals, you will have con-\\nquered one of the toughest aspects of this book and LLM implementation in general.\\nGPT-like\\ndecoder-only\\ntransformer\\nInput text\\nPreprocessing steps\\nOutput text\\nPostprocessing steps\\nTopic of the\\nprevious\\nchapter\\nSelf-attention module\\nTopic of the\\ncurrent\\nchapter\\nThe remaining parts of the\\nLLM architecture are the\\ntopic of the next chapter\\nFigure 3.6\\nSelf-attention is a mechanism in transformers used to compute \\nmore efficient input representations by allowing each position in a sequence to \\ninteract with and weigh the importance of all other positions within the same \\nsequence. In this chapter, we will code this self-attention mechanism from the \\nground up before we code the remaining parts of the GPT-like LLM in the \\nfollowing chapter.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 77}, page_content='56\\nCHAPTER 3\\nCoding attention mechanisms\\nSince self-attention can appear complex, especially if you are encountering it for the\\nfirst time, we will begin by examining a simplified version of it. Then we will imple-\\nment the self-attention mechanism with trainable weights used in LLMs.\\n3.3.1\\nA simple self-attention mechanism without trainable weights\\nLet’s begin by implementing a simplified variant of self-attention, free from any train-\\nable weights, as summarized in figure 3.7. The goal is to illustrate a few key concepts\\nin self-attention before adding trainable weights.\\nThe “self” in self-attention  \\nIn self-attention, the “self” refers to the mechanism’s ability to compute attention\\nweights by relating different positions within a single input sequence. It assesses and\\nlearns the relationships and dependencies between various parts of the input itself,\\nsuch as words in a sentence or pixels in an image. \\nThis is in contrast to traditional attention mechanisms, where the focus is on the rela-\\ntionships between elements of two different sequences, such as in sequence-to-\\nsequence models where the attention might be between an input sequence and an\\noutput sequence, such as the example depicted in figure 3.5.\\nThe context vector z(2) is\\ncomputed as a combination of\\nall input vectors weighted with\\nrespect to input element x(2)\\nAttention weight to\\nweigh the importance\\nof input x(1)\\nInput vector\\n(token embedding)\\ncorresponding to\\nthe ﬁrst token\\n0.4 0.1 0.8\\n0.5 0.8 0.6\\n0.5 0.8 0.6\\n0.0 0.8 0.5\\n0.4 0.6 0.5\\nFigure 3.7\\nThe goal of self-attention is to compute a context vector for each input \\nelement that combines information from all other input elements. In this example, \\nwe compute the context vector z(2). The importance or contribution of each input \\nelement for computing z(2) is determined by the attention weights \\uf06121 to \\uf0612T. When \\ncomputing z(2), the attention weights are calculated with respect to input element \\nx(2) and all other inputs.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 78}, page_content='57\\n3.3\\nAttending to different parts of the input with self-attention\\nFigure 3.7 shows an input sequence, denoted as x, consisting of T elements repre-\\nsented as x(1) to x(T). This sequence typically represents text, such as a sentence, that\\nhas already been transformed into token embeddings.\\n For example, consider an input text like “Your journey starts with one step.” In this\\ncase, each element of the sequence, such as x(1), corresponds to a d-dimensional\\nembedding vector representing a specific token, like “Your.” Figure 3.7 shows these\\ninput vectors as three-dimensional embeddings.\\n In self-attention, our goal is to calculate context vectors z(i) for each element x(i)\\nin the input sequence. A context vector can be interpreted as an enriched embedding\\nvector.\\n To illustrate this concept, let’s focus on the embedding vector of the second input\\nelement, x(2) (which corresponds to the token “journey”), and the corresponding con-\\ntext vector, z(2), shown at the bottom of figure 3.7. This enhanced context vector, z(2),\\nis an embedding that contains information about x(2) and all other input elements,\\nx(1) to x(T).\\n Context vectors play a crucial role in self-attention. Their purpose is to create\\nenriched representations of each element in an input sequence (like a sentence)\\nby incorporating information from all other elements in the sequence (figure 3.7).\\nThis is essential in LLMs, which need to understand the relationship and relevance\\nof words in a sentence to each other. Later, we will add trainable weights that help\\nan LLM learn to construct these context vectors so that they are relevant for the\\nLLM to generate the next token. But first, let’s implement a simplified self-atten-\\ntion mechanism to compute these weights and the resulting context vector one\\nstep at a time. \\n Consider the following input sentence, which has already been embedded into\\nthree-dimensional vectors (see chapter 2). I’ve chosen a small embedding dimension\\nto ensure it fits on the page without line breaks:\\nimport torch\\ninputs = torch.tensor(\\n  [[0.43, 0.15, 0.89], # Your     (x^1)\\n   [0.55, 0.87, 0.66], # journey  (x^2)\\n   [0.57, 0.85, 0.64], # starts   (x^3)\\n   [0.22, 0.58, 0.33], # with     (x^4)\\n   [0.77, 0.25, 0.10], # one      (x^5)\\n   [0.05, 0.80, 0.55]] # step     (x^6)\\n)\\nThe first step of implementing self-attention is to compute the intermediate values ω,\\nreferred to as attention scores, as illustrated in figure 3.8. Due to spatial constraints,\\nthe figure displays the values of the preceding inputs tensor in a truncated version;\\nfor example, 0.87 is truncated to 0.8. In this truncated version, the embeddings of the\\nwords “journey” and “starts” may appear similar by random chance.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 79}, page_content='58\\nCHAPTER 3\\nCoding attention mechanisms\\nFigure 3.8 illustrates how we calculate the intermediate attention scores between the\\nquery token and each input token. We determine these scores by computing the dot\\nproduct of the query, x(2), with every other input token:\\nquery = inputs[1]                           \\nattn_scores_2 = torch.empty(inputs.shape[0])\\nfor i, x_i in enumerate(inputs):\\n    attn_scores_2[i] = torch.dot(x_i, query)\\nprint(attn_scores_2)\\nThe computed attention scores are\\ntensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\\nUnderstanding dot products \\nA dot product is essentially a concise way of multiplying two vectors element-wise and\\nthen summing the products, which can be demonstrated as follows:\\nres = 0.\\nfor idx, element in enumerate(inputs[0]):\\n    res += inputs[0][idx] * query[idx]\\nprint(res)\\nprint(torch.dot(inputs[0], query))\\nThe output confirms that the sum of the element-wise multiplication gives the same\\nresults as the dot product:\\ntensor(0.9544)\\ntensor(0.9544)\\nEmbedded query token:\\nThe embedded query\\ntoken is one of the\\nembedded input tokens\\n(here, the query is the\\nsecond token).\\n0.4 0.1 0.8\\n0.5 0.8 0.6\\n0.5 0.8 0.6\\n0.0 0.8 0.5\\n0.5 0.8 0.6\\n0.9\\n1.4\\n1.4\\n1.0\\nInputs:\\nAttention score between\\ninput       and query\\nx(1)\\nx(2)\\nAttention score between\\ninput       and query\\nx(3)\\nx(2)\\nFigure 3.8\\nThe overall goal is to illustrate the computation of the context vector z(2) using the \\nsecond input element, x(2) as a query. This figure shows the first intermediate step, computing the \\nattention scores \\uf077 between the query x(2) and all other input elements as a dot product. (Note that \\nthe numbers are truncated to one digit after the decimal point to reduce visual clutter.)\\nThe second input \\ntoken serves as \\nthe query.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 80}, page_content='59\\n3.3\\nAttending to different parts of the input with self-attention\\nIn the next step, as shown in figure 3.9, we normalize each of the attention scores we\\ncomputed previously. The main goal behind the normalization is to obtain attention\\nweights that sum up to 1. This normalization is a convention that is useful for interpre-\\ntation and maintaining training stability in an LLM. Here’s a straightforward method\\nfor achieving this normalization step:\\nattn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\\nprint(\"Attention weights:\", attn_weights_2_tmp)\\nprint(\"Sum:\", attn_weights_2_tmp.sum())\\nAs the output shows, the attention weights now sum to 1: \\nAttention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\\nSum: tensor(1.0000)\\nIn practice, it’s more common and advisable to use the softmax function for normal-\\nization. This approach is better at managing extreme values and offers more favorable\\nBeyond viewing the dot product operation as a mathematical tool that combines\\ntwo vectors to yield a scalar value, the dot product is a measure of similarity\\nbecause it quantifies how closely two vectors are aligned: a higher dot product indi-\\ncates a greater degree of alignment or similarity between the vectors. In the con-\\ntext of self-attention mechanisms, the dot product determines the extent to which\\neach element in a sequence focuses on, or “attends to,” any other element: the\\nhigher the dot product, the higher the similarity and attention score between two\\nelements.\\nAttention weights:\\nWe computed these attention\\nscores in the previous step.\\nWe now normalize the\\nattention scores\\nto obtain\\nω\\nthe attention weights α\\n0.4 0.1 0.8\\n0.5 0.8 0.6\\n0.5 0.8 0.6\\n0.0 0.8 0.5\\n0.5 0.8 0.6\\n0.9\\n1.4\\n1.4\\n1.0\\n0.1\\n0.2\\n0.2\\n0.1\\nFigure 3.9\\nAfter computing the attention scores \\uf07721 to \\uf0772T with respect to the input query x(2), the next \\nstep is to obtain the attention weights \\uf06121 to \\uf0612T by normalizing the attention scores.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 81}, page_content='60\\nCHAPTER 3\\nCoding attention mechanisms\\ngradient properties during training. The following is a basic implementation of the\\nsoftmax function for normalizing the attention scores:\\ndef softmax_naive(x):\\n    return torch.exp(x) / torch.exp(x).sum(dim=0)\\nattn_weights_2_naive = softmax_naive(attn_scores_2)\\nprint(\"Attention weights:\", attn_weights_2_naive)\\nprint(\"Sum:\", attn_weights_2_naive.sum())\\nAs the output shows, the softmax function also meets the objective and normalizes the\\nattention weights such that they sum to 1:\\nAttention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\\nSum: tensor(1.)\\nIn addition, the softmax function ensures that the attention weights are always posi-\\ntive. This makes the output interpretable as probabilities or relative importance,\\nwhere higher weights indicate greater importance.\\n Note that this naive softmax implementation (softmax_naive) may encounter\\nnumerical instability problems, such as overflow and underflow, when dealing with\\nlarge or small input values. Therefore, in practice, it’s advisable to use the PyTorch\\nimplementation of softmax, which has been extensively optimized for performance:\\nattn_weights_2 = torch.softmax(attn_scores_2, dim=0)\\nprint(\"Attention weights:\", attn_weights_2)\\nprint(\"Sum:\", attn_weights_2.sum())\\nIn this case, it yields the same results as our previous softmax_naive function:\\nAttention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\\nSum: tensor(1.)\\nNow that we have computed the normalized attention weights, we are ready for the\\nfinal step, as shown in figure 3.10: calculating the context vector z(2) by multiplying the\\nembedded input tokens, x(i), with the corresponding attention weights and then sum-\\nming the resulting vectors. Thus, context vector z(2) is the weighted sum of all input vec-\\ntors, obtained by multiplying each input vector by its corresponding attention weight:\\nquery = inputs[1]        \\ncontext_vec_2 = torch.zeros(query.shape)\\nfor i,x_i in enumerate(inputs):\\n    context_vec_2 += attn_weights_2[i]*x_i\\nprint(context_vec_2)\\nThe results of this computation are\\ntensor([0.4419, 0.6515, 0.5683])\\nThe second input \\ntoken is the query.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 82}, page_content='61\\n3.3\\nAttending to different parts of the input with self-attention\\nNext, we will generalize this procedure for computing context vectors to calculate all\\ncontext vectors simultaneously.\\n3.3.2\\nComputing attention weights for all input tokens\\nSo far, we have computed attention weights and the context vector for input 2, as\\nshown in the highlighted row in figure 3.11. Now let’s extend this computation to cal-\\nculate attention weights and context vectors for all inputs.\\nAttention weights:\\n0.4 0.1 0.8\\n0.5 0.8 0.6\\n0.5 0.8 0.6\\n0.0 0.8 0.5\\n0.1\\nContext vector:\\n0.4 0.6 0.5\\nInputs:\\n×\\n0.2\\n×\\n0.2\\n×\\n0.1\\n×\\n+\\nMultiply each input vector\\nwith the corresponding\\nattention weight.\\nThis is the second context vector because\\nthe attention weights were computed\\nwith respect to the second input vector\\nin the previous steps.\\nFigure 3.10\\nThe final step, after calculating and normalizing the attention scores to obtain the \\nattention weights for query x(2), is to compute the context vector z(2). This context vector is a \\ncombination of all input vectors x(1) to x(T) weighted by the attention weights.\\nYour\\nstarts\\nwith\\none\\nstep\\njourney\\nThis row contains the attention\\nweights (normalized attention\\nscores) computed previously\\nYour\\n0.20 0.20 0.19\\n0.12 0.12\\n0.14\\nstarts\\n0.13 0.23 0.23 0.12 0.11 0.15\\nwith\\n0.14 0.20 0.20 0.14 0.12 0.17\\none\\n0.15 0.19 0.19 0.13 0.18 0.12\\nstep\\n0.13 0.21 0.21 0.14 0.09 0.18\\njourney\\n0.13 0.23 0.23 0.12 0.10 0.15\\nFigure 3.11\\nThe highlighted row shows the attention weights for the second \\ninput element as a query. Now we will generalize the computation to obtain \\nall other attention weights. (Please note that the numbers in this figure are \\ntruncated to two digits after the decimal point to reduce visual clutter. The \\nvalues in each row should add up to 1.0 or 100%.)'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 83}, page_content='62\\nCHAPTER 3\\nCoding attention mechanisms\\nWe follow the same three steps as before (see figure 3.12), except that we make a few\\nmodifications in the code to compute all context vectors instead of only the second\\none, z(2):\\nattn_scores = torch.empty(6, 6)\\nfor i, x_i in enumerate(inputs):\\n    for j, x_j in enumerate(inputs):\\n        attn_scores[i, j] = torch.dot(x_i, x_j)\\nprint(attn_scores)\\nThe resulting attention scores are as follows:\\ntensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\\n        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\\n        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\\n        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\\n        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\\n        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\\nEach element in the tensor represents an attention score between each pair of inputs,\\nas we saw in figure 3.11. Note that the values in that figure are normalized, which is\\nwhy they differ from the unnormalized attention scores in the preceding tensor. We\\nwill take care of the normalization later. \\n When computing the preceding attention score tensor, we used for loops in\\nPython. However, for loops are generally slow, and we can achieve the same results\\nusing matrix multiplication:\\nattn_scores = inputs @ inputs.T\\nprint(attn_scores)\\nWe can visually confirm that the results are the same as before:\\ntensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\\n        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\\n        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\\n        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\\n1) Compute attention scores\\n2) Compute attention weights\\n3) Compute context vectors\\nCompute the attention scores as\\ndot products between the inputs.\\nThe attention weights are a normalized\\nversion of the attention scores.\\nThe context vectors are computed as\\na weighted sum over the inputs.\\nFigure 3.12\\nIn step 1, we add an additional for loop to compute the dot \\nproducts for all pairs of inputs.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 84}, page_content='63\\n3.3\\nAttending to different parts of the input with self-attention\\n        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\\n        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\\nIn step 2 of figure 3.12, we normalize each row so that the values in each row sum to 1:\\nattn_weights = torch.softmax(attn_scores, dim=-1)\\nprint(attn_weights)\\nThis returns the following attention weight tensor that matches the values shown in\\nfigure 3.10:\\ntensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\\n        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\\n        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\\n        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\\n        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\\n        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\\nIn the context of using PyTorch, the dim parameter in functions like torch.softmax\\nspecifies the dimension of the input tensor along which the function will be com-\\nputed. By setting dim=-1, we are instructing the softmax function to apply the nor-\\nmalization along the last dimension of the attn_scores tensor. If attn_scores is a\\ntwo-dimensional tensor (for example, with a shape of [rows, columns]), it will nor-\\nmalize across the columns so that the values in each row (summing over the column\\ndimension) sum up to 1.\\n We can verify that the rows indeed all sum to 1:\\nrow_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\\nprint(\"Row 2 sum:\", row_2_sum)\\nprint(\"All row sums:\", attn_weights.sum(dim=-1))\\nThe result is\\nRow 2 sum: 1.0\\nAll row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\\nIn the third and final step of figure 3.12, we use these attention weights to compute all\\ncontext vectors via matrix multiplication:\\nall_context_vecs = attn_weights @ inputs\\nprint(all_context_vecs)\\nIn the resulting output tensor, each row contains a three-dimensional context vector:\\ntensor([[0.4421, 0.5931, 0.5790],\\n        [0.4419, 0.6515, 0.5683],\\n        [0.4431, 0.6496, 0.5671],\\n        [0.4304, 0.6298, 0.5510],\\n        [0.4671, 0.5910, 0.5266],\\n        [0.4177, 0.6503, 0.5645]])'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 85}, page_content='64\\nCHAPTER 3\\nCoding attention mechanisms\\nWe can double-check that the code is correct by comparing the second row with the\\ncontext vector z(2) that we computed in section 3.3.1:\\nprint(\"Previous 2nd context vector:\", context_vec_2)\\nBased on the result, we can see that the previously calculated context_vec_2 matches\\nthe second row in the previous tensor exactly: \\nPrevious 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\\nThis concludes the code walkthrough of a simple self-attention mechanism. Next, we\\nwill add trainable weights, enabling the LLM to learn from data and improve its per-\\nformance on specific tasks.\\n3.4\\nImplementing self-attention with trainable weights\\nOur next step will be to implement the self-attention mechanism used in the origi-\\nnal transformer architecture, the GPT models, and most other popular LLMs. This\\nself-attention mechanism is also called scaled dot-product attention. Figure 3.13 shows\\nhow this self-attention mechanism fits into the broader context of implementing\\nan LLM. \\n1) Simpliﬁed\\nself-attention\\n2) Self-attention\\n3) Causal attention\\n4) Multi-head\\nattention\\nWe already implemented\\na simpliﬁed attention\\nmechanism.\\nWe will now extend the\\nself-attention mechanism\\nwith trainable weights.\\n1) Data\\npreparation\\n& sampling\\n2) Attention\\nmechanism\\nBuilding an LLM\\nFoundation model\\nClassiﬁer\\nPersonal assistant\\nDataset with class labels\\nInstruction dataset\\n4) Pretraining\\n9) Fine-tuning\\n5) Training\\nloop\\n6) Model\\nevaluation\\n7) Load\\npretrained\\nweights\\n8) Fine-tuning\\n3) LLM\\narchitecture\\n2) Attention\\nmechanism\\nFigure 3.13\\nPreviously, we coded a simplified attention mechanism to understand the basic mechanism behind \\nattention mechanisms. Now, we add trainable weights to this attention mechanism. Later, we will extend this \\nself-attention mechanism by adding a causal mask and multiple heads.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 86}, page_content='65\\n3.4\\nImplementing self-attention with trainable weights\\nAs illustrated in figure 3.13, the self-attention mechanism with trainable weights builds\\non the previous concepts: we want to compute context vectors as weighted sums over\\nthe input vectors specific to a certain input element. As you will see, there are only slight\\ndifferences compared to the basic self-attention mechanism we coded earlier.\\n The most notable difference is the introduction of weight matrices that are\\nupdated during model training. These trainable weight matrices are crucial so that\\nthe model (specifically, the attention module inside the model) can learn to produce\\n“good” context vectors. (We will train the LLM in chapter 5.)\\n We will tackle this self-attention mechanism in the two subsections. First, we will code\\nit step by step as before. Second, we will organize the code into a compact Python class\\nthat can be imported into the LLM architecture.\\n3.4.1\\nComputing the attention weights step by step\\nWe will implement the self-attention mechanism step by step by introducing the\\nthree trainable weight matrices Wq, Wk, and Wv. These three matrices are used to\\nproject the embedded input tokens, x(i), into query, key, and value vectors, respec-\\ntively, as illustrated in figure 3.14.\\nEarlier, we defined the second input element x(2) as the query when we computed the\\nsimplified attention weights to compute the context vector z(2). Then we generalized\\nthis to compute all context vectors z(1) ... z(T) for the six-word input sentence “Your\\njourney starts with one step.” \\nThe second input token serves as the\\ncurrent input vector to create the query\\n0.4 0.1 0.8\\nkey\\nvalue\\n0.5 0.8 0.6\\n0.0 0.8 0.5\\nThis is the value vector corresponding to the ﬁrst input token obtained via\\nmatrix multiplication between the weight matrix        and input token\\n0.3 0.7\\n0.1 0.8\\n0.4 1.1\\n0.3 1.0\\n0.3 0.9\\n0.3 0.7\\n0.4 1.4\\nFigure 3.14\\nIn the first step of the self-attention mechanism with trainable weight matrices, we compute query \\n(q), key (k), and value (v) vectors for input elements x. Similar to previous sections, we designate the second \\ninput, x(2), as the query input. The query vector q(2) is obtained via matrix multiplication between the input x(2) and \\nthe weight matrix Wq. Similarly, we obtain the key and value vectors via matrix multiplication involving the weight \\nmatrices Wk and Wv.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 87}, page_content='66\\nCHAPTER 3\\nCoding attention mechanisms\\n Similarly, we start here by computing only one context vector, z(2), for illustration\\npurposes. We will then modify this code to calculate all context vectors.\\n Let’s begin by defining a few variables:\\nx_2 = inputs[1]    \\nd_in = inputs.shape[1]     \\nd_out = 2        \\nNote that in GPT-like models, the input and output dimensions are usually the same,\\nbut to better follow the computation, we’ll use different input (d_in=3) and output\\n(d_out=2) dimensions here.\\n Next, we initialize the three weight matrices Wq, Wk, and Wv shown in figure 3.14:\\ntorch.manual_seed(123)\\nW_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\\nW_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\\nW_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\\nWe set requires_grad=False to reduce clutter in the outputs, but if we were to use\\nthe weight matrices for model training, we would set requires_grad=True to update\\nthese matrices during model training.\\n Next, we compute the query, key, and value vectors:\\nquery_2 = x_2 @ W_query \\nkey_2 = x_2 @ W_key \\nvalue_2 = x_2 @ W_value\\nprint(query_2)\\nThe output for the query results in a two-dimensional vector since we set the number\\nof columns of the corresponding weight matrix, via d_out, to 2:\\ntensor([0.4306, 1.4551])\\nWeight parameters vs. attention weights \\nIn the weight matrices W, the term “weight” is short for “weight parameters,” the val-\\nues of a neural network that are optimized during training. This is not to be confused\\nwith the attention weights. As we already saw, attention weights determine the extent\\nto which a context vector depends on the different parts of the input (i.e., to what\\nextent the network focuses on different parts of the input). \\nIn summary, weight parameters are the fundamental, learned coefficients that define\\nthe network’s connections, while attention weights are dynamic, context-specific values.\\nThe second\\ninput element\\nThe input embedding \\nsize, d=3\\nThe output embedding \\nsize, d_out=2'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 88}, page_content='67\\n3.4\\nImplementing self-attention with trainable weights\\nEven though our temporary goal is only to compute the one context vector, z(2), we still\\nrequire the key and value vectors for all input elements as they are involved in com-\\nputing the attention weights with respect to the query q(2) (see figure 3.14).\\n We can obtain all keys and values via matrix multiplication:\\nkeys = inputs @ W_key \\nvalues = inputs @ W_value\\nprint(\"keys.shape:\", keys.shape)\\nprint(\"values.shape:\", values.shape)\\nAs we can tell from the outputs, we successfully projected the six input tokens from a\\nthree-dimensional onto a two-dimensional embedding space:\\nkeys.shape: torch.Size([6, 2])\\nvalues.shape: torch.Size([6, 2])\\nThe second step is to compute the attention scores, as shown in figure 3.15.\\nFirst, let’s compute the attention score ω22:\\nkeys_2 = keys[1]            \\nattn_score_22 = query_2.dot(keys_2)\\nprint(attn_score_22)\\nThe unscaled attention score is computed\\nas a dot product between the query and\\nthe key vectors.\\nSince we want to compute the context vector\\nfor the second input token, the query is derived\\nfrom that second input token.\\n0.4 0.1 0.8\\nquery\\nkey\\nvalue\\n0.5 0.8 0.6\\n0.0 0.8 0.5\\n0.3 0.7\\n0.1 0.8\\n0.4 1.1\\n0.3 1.0\\n0.3 0.9\\n0.3 0.7\\n0.4 1.4\\n0.4 1.4\\n0.4 1.4\\n1.2\\n1.8\\n1.5\\nAttention\\nscore\\nFigure 3.15\\nThe attention score computation is a dot-product computation similar to what we used in the \\nsimplified self-attention mechanism in section 3.3. The new aspect here is that we are not directly computing the \\ndot-product between the input elements but using the query and key obtained by transforming the inputs via the \\nrespective weight matrices.\\nRemember that Python \\nstarts indexing at 0.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 89}, page_content='68\\nCHAPTER 3\\nCoding attention mechanisms\\nThe result for the unnormalized attention score is\\ntensor(1.8524)\\nAgain, we can generalize this computation to all attention scores via matrix\\nmultiplication:\\nattn_scores_2 = query_2 @ keys.T      \\nprint(attn_scores_2)\\nAs we can see, as a quick check, the second element in the output matches the\\nattn_score_22 we computed previously:\\ntensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\\nNow, we want to go from the attention scores to the attention weights, as illustrated in\\nfigure 3.16. We compute the attention weights by scaling the attention scores and\\nusing the softmax function. However, now we scale the attention scores by dividing\\nthem by the square root of the embedding dimension of the keys (taking the square\\nroot is mathematically the same as exponentiating by 0.5):\\nd_k = keys.shape[-1]\\nattn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\\nprint(attn_weights_2)\\nAll attention scores \\nfor given query\\n0.4 0.1 0.8\\nquery q(2)\\nkey\\nvalue\\nq(2)\\n0.5 0.8 0.6\\n0.0 0.8 0.5\\n0.3 0.7\\n0.1 0.8\\n0.4 1.1\\n0.3 1.0\\n0.3 0.9\\n0.3 0.7\\n0.4 1.4\\n0.4 1.4\\n0.4 1.4\\n1.2\\n1.8\\n1.5\\nAttention\\nscore\\n0.1\\n0.2\\n0.1\\nThe unscaled attention\\nscore from the previous\\nstep.\\nThe attention weights\\nare computed using the\\nsoftmax function.\\nAttention\\nweight\\nFigure 3.16\\nAfter computing the attention scores \\uf077, the next step is to normalize these scores using the \\nsoftmax function to obtain the attention weights \\uf061.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 90}, page_content='69\\n3.4\\nImplementing self-attention with trainable weights\\nThe resulting attention weights are\\ntensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\\nNow, the final step is to compute the context vectors, as illustrated in figure 3.17.\\nSimilar to when we computed the context vector as a weighted sum over the input vec-\\ntors (see section 3.3), we now compute the context vector as a weighted sum over the\\nvalue vectors. Here, the attention weights serve as a weighting factor that weighs\\nThe rationale behind scaled-dot product attention\\nThe reason for the normalization by the embedding dimension size is to improve the\\ntraining performance by avoiding small gradients. For instance, when scaling up the\\nembedding dimension, which is typically greater than 1,000 for GPT-like LLMs, large\\ndot products can result in very small gradients during backpropagation due to the\\nsoftmax function applied to them. As dot products increase, the softmax function\\nbehaves more like a step function, resulting in gradients nearing zero. These small\\ngradients can drastically slow down learning or cause training to stagnate.\\nThe scaling by the square root of the embedding dimension is the reason why this\\nself-attention mechanism is also called scaled-dot product attention. \\nquery\\nkey\\n(\\nvalue\\nT\\n0 3\\n.\\n0 7\\n.\\n0 1\\n.\\n0 8\\n.\\n0 4\\n.\\n1 1\\n.\\n0 3\\n.\\n1 0\\n.\\n0 3\\n.\\n0 9\\n.\\n0 3\\n.\\n0 7\\n.\\n0 4\\n.\\n1 4\\n.\\n0 4\\n.\\n1 4\\n.\\n0 4\\n.\\n1 4\\n.\\n1 2\\n.\\n1 8\\n.\\n1 5\\n.\\n0 1\\n.\\n0 2\\n.\\n0 1\\n.\\nAttention\\nweight\\nT\\n0 3\\n.\\n0 8\\n.\\nContext\\nvector\\nThe last step is multiplying each\\nvalue vector with its respective\\nattention weight and then summing\\nthem to obtain the context vector\\n0.5 0.8 0.6\\n0.0 0.8 0.5\\n0.4 0.1 0.8\\nFigure 3.17\\nIn the final step of the self-attention computation, we compute the context vector by combining all \\nvalue vectors via the attention weights.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 91}, page_content='70\\nCHAPTER 3\\nCoding attention mechanisms\\nthe respective importance of each value vector. Also as before, we can use matrix mul-\\ntiplication to obtain the output in one step:\\ncontext_vec_2 = attn_weights_2 @ values\\nprint(context_vec_2)\\nThe contents of the resulting vector are as follows:\\ntensor([0.3061, 0.8210])\\nSo far, we’ve only computed a single context vector, z(2). Next, we will generalize the\\ncode to compute all context vectors in the input sequence, z(1) to z(T).\\n3.4.2\\nImplementing a compact self-attention Python class\\nAt this point, we have gone through a lot of steps to compute the self-attention out-\\nputs. We did so mainly for illustration purposes so we could go through one step at a\\ntime. In practice, with the LLM implementation in the next chapter in mind, it is\\nhelpful to organize this code into a Python class, as shown in the following listing.\\nimport torch.nn as nn\\nclass SelfAttention_v1(nn.Module):\\n    def __init__(self, d_in, d_out):\\n        super().__init__()\\n        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\\n        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\\n        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\\nWhy query, key, and value?\\nThe terms “key,” “query,” and “value” in the context of attention mechanisms are\\nborrowed from the domain of information retrieval and databases, where similar con-\\ncepts are used to store, search, and retrieve information.\\nA query is analogous to a search query in a database. It represents the current item\\n(e.g., a word or token in a sentence) the model focuses on or tries to understand.\\nThe query is used to probe the other parts of the input sequence to determine how\\nmuch attention to pay to them.\\nThe key is like a database key used for indexing and searching. In the attention mech-\\nanism, each item in the input sequence (e.g., each word in a sentence) has an asso-\\nciated key. These keys are used to match the query. \\nThe value in this context is similar to the value in a key-value pair in a database. It\\nrepresents the actual content or representation of the input items. Once the model\\ndetermines which keys (and thus which parts of the input) are most relevant to the\\nquery (the current focus item), it retrieves the corresponding values.\\nListing 3.1\\nA compact self-attention class'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 92}, page_content='71\\n3.4\\nImplementing self-attention with trainable weights\\n    def forward(self, x):\\n        keys = x @ self.W_key\\n        queries = x @ self.W_query\\n        values = x @ self.W_value\\n        attn_scores = queries @ keys.T # omega\\n        attn_weights = torch.softmax(\\n            attn_scores / keys.shape[-1]**0.5, dim=-1\\n        )\\n        context_vec = attn_weights @ values\\n        return context_vec\\nIn this PyTorch code, SelfAttention_v1 is a class derived from nn.Module, which is a\\nfundamental building block of PyTorch models that provides necessary functionalities\\nfor model layer creation and management. \\n The __init__ method initializes trainable weight matrices (W_query, W_key, and\\nW_value) for queries, keys, and values, each transforming the input dimension d_in to\\nan output dimension d_out. \\n During the forward pass, using the forward method, we compute the attention\\nscores (attn_scores) by multiplying queries and keys, normalizing these scores using\\nsoftmax. Finally, we create a context vector by weighting the values with these normal-\\nized attention scores.\\n We can use this class as follows:\\ntorch.manual_seed(123)\\nsa_v1 = SelfAttention_v1(d_in, d_out)\\nprint(sa_v1(inputs))\\nSince inputs contains six embedding vectors, this results in a matrix storing the six\\ncontext vectors:\\ntensor([[0.2996, 0.8053],\\n        [0.3061, 0.8210],\\n        [0.3058, 0.8203],\\n        [0.2948, 0.7939],\\n        [0.2927, 0.7891],\\n        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\\nAs a quick check, notice that the second row ([0.3061, 0.8210]) matches the con-\\ntents of context_vec_2 in the previous section. Figure 3.18 summarizes the self-atten-\\ntion mechanism we just implemented.\\n Self-attention involves the trainable weight matrices Wq, Wk, and Wv. These matrices\\ntransform input data into queries, keys, and values, respectively, which are crucial com-\\nponents of the attention mechanism. As the model is exposed to more data during\\ntraining, it adjusts these trainable weights, as we will see in upcoming chapters.\\n We can improve the SelfAttention_v1 implementation further by utilizing\\nPyTorch’s nn.Linear layers, which effectively perform matrix multiplication when\\nthe bias units are disabled. Additionally, a significant advantage of using nn.Linear'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 93}, page_content='72\\nCHAPTER 3\\nCoding attention mechanisms\\ninstead of manually implementing nn.Parameter(torch.rand(...)) is that nn.Linear\\nhas an optimized weight initialization scheme, contributing to more stable and\\neffective model training.\\nclass SelfAttention_v2(nn.Module):\\n    def __init__(self, d_in, d_out, qkv_bias=False):\\nListing 3.2\\nA self-attention class using PyTorch’s Linear layers\\nContext vector\\ncorresponding to\\nthe second input\\ntoken\\nEmbedded queries, where the\\nsecond row is the query\\nvector q(2) corresponding to\\nthe second input token x(2)\\nWe multiply the inputs X\\nwith weight matrix Wv to\\nget the value matrix V.\\nThe embedding\\nvector x(2) of the\\nsecond input token\\n0.4 0.1 0.8\\n0.4 1.4\\nInputs\\nWeight\\nmatrix\\nWeight\\nmatrix\\nWeight\\nmatrix\\nQueries\\nKeys\\nValues\\n0.3 0.8\\nContext\\nvectors\\n0.4 1.1\\n0.3 1.0\\nAttention weight matrix\\ncontaining the attention\\nscores for each pair of\\ninputs\\nYour\\nstarts\\nwith\\none\\nstep\\njourney\\nYour\\nstarts\\nwith\\none\\nstep\\njourney\\n0.20 0.16 0.16 0.14 0.16 0.14\\n0.19 0.16 0.16\\n0.15 0.17\\n0.15\\n0.20 0.16 0.16 0.14 0.16 0.14\\n0.18 0.16 0.16 0.15 0.16 0.15\\n0.18 0.16 0.16 0.15 0.16 0.15\\n0.19 0.16 0.16 0.15 0.16 0.15\\nFigure 3.18\\nIn self-attention, we transform the input vectors in the input matrix X with the three weight \\nmatrices, Wq, Wk, and Wv. The new compute the attention weight matrix based on the resulting queries (Q) and \\nkeys (K). Using the attention weights and values (V), we then compute the context vectors (Z). For visual clarity, \\nwe focus on a single input text with n tokens, not a batch of multiple inputs. Consequently, the three-dimensional \\ninput tensor is simplified to a two-dimensional matrix in this context. This approach allows for a more straightforward \\nvisualization and understanding of the processes involved. For consistency with later figures, the values in the \\nattention matrix do not depict the real attention weights. (The numbers in this figure are truncated to two digits \\nafter the decimal point to reduce visual clutter. The values in each row should add up to 1.0 or 100%.)'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 94}, page_content='73\\n3.4\\nImplementing self-attention with trainable weights\\n        super().__init__()\\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\\n        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\\n    def forward(self, x):\\n        keys = self.W_key(x)\\n        queries = self.W_query(x)\\n        values = self.W_value(x)\\n        attn_scores = queries @ keys.T\\n        attn_weights = torch.softmax(\\n            attn_scores / keys.shape[-1]**0.5, dim=-1\\n        )\\n        context_vec = attn_weights @ values\\n        return context_vec\\nYou can use the SelfAttention_v2 similar to SelfAttention_v1:\\ntorch.manual_seed(789)\\nsa_v2 = SelfAttention_v2(d_in, d_out)\\nprint(sa_v2(inputs))\\nThe output is\\ntensor([[-0.0739,  0.0713],\\n        [-0.0748,  0.0703],\\n        [-0.0749,  0.0702],\\n        [-0.0760,  0.0685],\\n        [-0.0763,  0.0679],\\n        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\\nNote that SelfAttention_v1 and SelfAttention_v2 give different outputs because\\nthey use different initial weights for the weight matrices since nn.Linear uses a more\\nsophisticated weight initialization scheme.\\nExercise 3.1 Comparing SelfAttention_v1 and SelfAttention_v2\\nNote that nn.Linear in SelfAttention_v2 uses a different weight initialization\\nscheme as nn.Parameter(torch.rand(d_in, d_out)) used in SelfAttention_v1,\\nwhich causes both mechanisms to produce different results. To check that both\\nimplementations, SelfAttention_v1 and SelfAttention_v2, are otherwise simi-\\nlar, we can transfer the weight matrices from a SelfAttention_v2 object to a Self-\\nAttention_v1, such that both objects then produce the same results.\\nYour task is to correctly assign the weights from an instance of SelfAttention_v2\\nto an instance of SelfAttention_v1. To do this, you need to understand the rela-\\ntionship between the weights in both versions. (Hint: nn.Linear stores the weight\\nmatrix in a transposed form.) After the assignment, you should observe that both\\ninstances produce the same outputs.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 95}, page_content='74\\nCHAPTER 3\\nCoding attention mechanisms\\nNext, we will make enhancements to the self-attention mechanism, focusing specifically\\non incorporating causal and multi-head elements. The causal aspect involves modify-\\ning the attention mechanism to prevent the model from accessing future information\\nin the sequence, which is crucial for tasks like language modeling, where each word\\nprediction should only depend on previous words. \\n The multi-head component involves splitting the attention mechanism into multi-\\nple “heads.” Each head learns different aspects of the data, allowing the model to\\nsimultaneously attend to information from different representation subspaces at dif-\\nferent positions. This improves the model’s performance in complex tasks.\\n3.5\\nHiding future words with causal attention\\nFor many LLM tasks, you will want the self-attention mechanism to consider only the\\ntokens that appear prior to the current position when predicting the next token in a\\nsequence. Causal attention, also known as masked attention, is a specialized form of self-\\nattention. It restricts a model to only consider previous and current inputs in a sequence\\nwhen processing any given token when computing attention scores. This is in contrast\\nto the standard self-attention mechanism, which allows access to the entire input\\nsequence at once.\\n Now, we will modify the standard self-attention mechanism to create a causal\\nattention mechanism, which is essential for developing an LLM in the subsequent\\nchapters. To achieve this in GPT-like LLMs, for each token processed, we mask out\\nthe future tokens, which come after the current token in the input text, as illus-\\ntrated in figure 3.19. We mask out the attention weights above the diagonal, and we\\nYour\\nstarts\\nwith\\none\\nstep\\njourney\\nYour\\nstarts\\nwith\\none\\nstep\\njourney\\n0.20 0.16 0.16 0.14 0.16 0.14\\n0.19 0.16 0.16\\n0.15 0.17\\n0.15\\n0.20 0.16 0.16 0.14 0.16 0.14\\n0.18 0.16 0.16 0.15 0.16 0.15\\n0.18 0.16 0.16 0.15 0.16 0.15\\n0.19 0.16 0.16 0.15 0.16 0.15\\nYour\\nstarts\\nwith\\none\\nstep\\njourney\\nYour\\nstarts\\nwith\\none\\nstep\\njourney\\n0.55 0.44\\n1.0\\n0.38 0.30 0.31\\n0.27 0.24 0.24 0.23\\n0.21 0.19 0.19 0.18 0.19\\n0.19 0.16 0.16 0.15 0.16 0.15\\nMasked out\\nfuture tokens\\nfor the “Your”\\ntoken\\nAttention weight for input tokens\\ncorresponding to “step” and “Your”\\nFigure 3.19\\nIn causal attention, we mask out the attention weights above the diagonal such that for \\na given input, the LLM can’t access future tokens when computing the context vectors using the \\nattention weights. For example, for the word “journey” in the second row, we only keep the attention \\nweights for the words before (“Your”) and in the current position (“journey”).'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 96}, page_content='75\\n3.5\\nHiding future words with causal attention\\nnormalize the nonmasked attention weights such that the attention weights sum to 1 in\\neach row. Later, we will implement this masking and normalization procedure in code.\\n3.5.1\\nApplying a causal attention mask\\nOur next step is to implement the causal attention mask in code. To implement the\\nsteps to apply a causal attention mask to obtain the masked attention weights, as sum-\\nmarized in figure 3.20, let’s work with the attention scores and weights from the previ-\\nous section to code the causal attention mechanism. \\nIn the first step, we compute the attention weights using the softmax function as we\\nhave done previously:\\nqueries = sa_v2.W_query(inputs)    \\nkeys = sa_v2.W_key(inputs) \\nattn_scores = queries @ keys.T\\nattn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\\nprint(attn_weights)\\nThis results in the following attention weights:\\ntensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\\n        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\\n        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\\n        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\\n        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\\n        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\\n       grad_fn=<SoftmaxBackward0>)\\nWe can implement the second step using PyTorch’s tril function to create a mask\\nwhere the values above the diagonal are zero:\\ncontext_length = attn_scores.shape[0]\\nmask_simple = torch.tril(torch.ones(context_length, context_length))\\nprint(mask_simple)\\nAttention scores\\n(unnormalized)\\nAttention weights\\n(normalized)\\n1) Apply\\nsoftmax\\n2) Mask with 0’s\\nabove diagonal\\nMasked attention scores\\n(unnormalized)\\nMasked attention weights\\n(normalized)\\n3) Normalize\\nrows\\n“Normalized” means that the\\nvalues in each row sum to 1\\nFigure 3.20\\nOne way to obtain the masked attention weight matrix in causal attention is to apply the \\nsoftmax function to the attention scores, zeroing out the elements above the diagonal and normalizing \\nthe resulting matrix.\\nReuses the query and key weight matrices \\nof the SelfAttention_v2 object from the \\nprevious section for convenience'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 97}, page_content='76\\nCHAPTER 3\\nCoding attention mechanisms\\nThe resulting mask is\\ntensor([[1., 0., 0., 0., 0., 0.],\\n        [1., 1., 0., 0., 0., 0.],\\n        [1., 1., 1., 0., 0., 0.],\\n        [1., 1., 1., 1., 0., 0.],\\n        [1., 1., 1., 1., 1., 0.],\\n        [1., 1., 1., 1., 1., 1.]])\\nNow, we can multiply this mask with the attention weights to zero-out the values above\\nthe diagonal:\\nmasked_simple = attn_weights*mask_simple\\nprint(masked_simple)\\nAs we can see, the elements above the diagonal are successfully zeroed out:\\ntensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\\n        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\\n        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\\n        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\\n        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\\n        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\\n       grad_fn=<MulBackward0>)\\nThe third step is to renormalize the attention weights to sum up to 1 again in each\\nrow. We can achieve this by dividing each element in each row by the sum in each row:\\nrow_sums = masked_simple.sum(dim=-1, keepdim=True)\\nmasked_simple_norm = masked_simple / row_sums\\nprint(masked_simple_norm)\\nThe result is an attention weight matrix where the attention weights above the diago-\\nnal are zeroed-out, and the rows sum to 1:\\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\\n        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\\n        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\\n        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\\n        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\\n        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\\n       grad_fn=<DivBackward0>)\\nInformation leakage\\nWhen we apply a mask and then renormalize the attention weights, it might initially\\nappear that information from future tokens (which we intend to mask) could still influ-\\nence the current token because their values are part of the softmax calculation. How-\\never, the key insight is that when we renormalize the attention weights after masking,'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 98}, page_content='77\\n3.5\\nHiding future words with causal attention\\nWhile we could wrap up our implementation of causal attention at this point, we can\\nstill improve it. Let’s take a mathematical property of the softmax function and imple-\\nment the computation of the masked attention weights more efficiently in fewer steps,\\nas shown in figure 3.21.\\nThe softmax function converts its inputs into a probability distribution. When nega-\\ntive infinity values (-∞) are present in a row, the softmax function treats them as zero\\nprobability. (Mathematically, this is because e –∞ approaches 0.)\\n We can implement this more efficient masking “trick” by creating a mask with 1s\\nabove the diagonal and then replacing these 1s with negative infinity (-inf) values:\\nmask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\\nmasked = attn_scores.masked_fill(mask.bool(), -torch.inf)\\nprint(masked)\\nThis results in the following mask:\\ntensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\\n        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\\n        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\\n        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\\n        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\\n        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\\n       grad_fn=<MaskedFillBackward0>)\\nwhat we’re essentially doing is recalculating the softmax over a smaller subset (since\\nmasked positions don’t contribute to the softmax value).\\nThe mathematical elegance of softmax is that despite initially including all positions\\nin the denominator, after masking and renormalizing, the effect of the masked posi-\\ntions is nullified—they don’t contribute to the softmax score in any meaningful way.\\nIn simpler terms, after masking and renormalization, the distribution of attention\\nweights is as if it was calculated only among the unmasked positions to begin with.\\nThis ensures there’s no information leakage from future (or otherwise masked)\\ntokens as we intended.\\n1) Mask with −∞\\nabove diagonal\\n2) Apply\\nsoftmax\\nAttention scores\\n(unnormalized)\\nMasked attention scores\\n(unnormalized)\\nMasked attention weights\\n(normalized)\\nFigure 3.21\\nA more efficient way to obtain the masked attention weight matrix in \\ncausal attention is to mask the attention scores with negative infinity values before \\napplying the softmax function.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 99}, page_content='78\\nCHAPTER 3\\nCoding attention mechanisms\\nNow all we need to do is apply the softmax function to these masked results, and we\\nare done:\\nattn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\\nprint(attn_weights)\\nAs we can see based on the output, the values in each row sum to 1, and no further\\nnormalization is necessary:\\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\\n        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\\n        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\\n        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\\n        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\\n        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\\n       grad_fn=<SoftmaxBackward0>)\\nWe could now use the modified attention weights to compute the context vectors via\\ncontext_vec = attn_weights @ values, as in section 3.4. However, we will first cover\\nanother minor tweak to the causal attention mechanism that is useful for reducing\\noverfitting when training LLMs.\\n3.5.2\\nMasking additional attention weights with dropout\\nDropout in deep learning is a technique where randomly selected hidden layer units\\nare ignored during training, effectively “dropping” them out. This method helps pre-\\nvent overfitting by ensuring that a model does not become overly reliant on any spe-\\ncific set of hidden layer units. It’s important to emphasize that dropout is only used\\nduring training and is disabled afterward.\\n In the transformer architecture, including models like GPT, dropout in the atten-\\ntion mechanism is typically applied at two specific times: after calculating the atten-\\ntion weights or after applying the attention weights to the value vectors. Here we will\\napply the dropout mask after computing the attention weights, as illustrated in fig-\\nure 3.22, because it’s the more common variant in practice.\\n In the following code example, we use a dropout rate of 50%, which means mask-\\ning out half of the attention weights. (When we train the GPT model in later chapters,\\nwe will use a lower dropout rate, such as 0.1 or 0.2.) We apply PyTorch’s dropout\\nimplementation first to a 6 × 6 tensor consisting of 1s for simplicity:\\ntorch.manual_seed(123)\\ndropout = torch.nn.Dropout(0.5)   \\nexample = torch.ones(6, 6)     \\nprint(dropout(example))\\nWe choose a \\ndropout rate of 50%.\\nHere, we create a \\nmatrix of 1s.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 100}, page_content='79\\n3.5\\nHiding future words with causal attention\\nAs we can see, approximately half of the values are zeroed out:\\ntensor([[2., 2., 0., 2., 2., 0.],\\n        [0., 0., 0., 2., 0., 2.],\\n        [2., 2., 2., 2., 0., 2.],\\n        [0., 2., 2., 0., 0., 2.],\\n        [0., 2., 0., 2., 0., 2.],\\n        [0., 2., 2., 2., 2., 0.]])\\nWhen applying dropout to an attention weight matrix with a rate of 50%, half of the\\nelements in the matrix are randomly set to zero. To compensate for the reduction in\\nactive elements, the values of the remaining elements in the matrix are scaled up by a\\nfactor of 1/0.5 = 2. This scaling is crucial to maintain the overall balance of the atten-\\nYour\\nstarts\\nwith\\none\\nstep\\njourney\\nYour\\nstarts\\nwith\\none\\nstep\\njourney\\n0.55 0.44\\n1.0\\n0.38 0.30 0.31\\n0.27 0.24 0.24 0.23\\n0.21 0.19 0.19 0.18 0.19\\n0.19 0.16 0.16 0.15 0.16 0.15\\nYour\\nstarts\\nwith\\none\\nstep\\njourney\\nYour\\nstarts\\nwith\\none\\nstep\\njourney\\nDropout mask\\nwith random\\npositions to be\\ndropped\\nThe dropout mask\\napplied to the\\nattention scores will\\nzero out certain\\nattention scores\\nAttention weight for input\\ntokens corresponding to\\n“step” and “Your”\\n1.0\\n0.38 0.30 0.31\\n0.24 0.24\\n0.19\\n0.18\\n0.16 0.16 0.15 0.16\\nFigure 3.22\\nUsing the causal attention mask (upper left), we apply an additional \\ndropout mask (upper right) to zero out additional attention weights to reduce overfitting \\nduring training.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 101}, page_content='80\\nCHAPTER 3\\nCoding attention mechanisms\\ntion weights, ensuring that the average influence of the attention mechanism remains\\nconsistent during both the training and inference phases.\\n Now let’s apply dropout to the attention weight matrix itself:\\ntorch.manual_seed(123)\\nprint(dropout(attn_weights))\\nThe resulting attention weight matrix now has additional elements zeroed out and the\\nremaining 1s rescaled:\\ntensor([[2.0000, 0.0000, 0 .0000, 0.0000, 0.0000, 0.0000],\\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\\n        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\\n        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\\n        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\\n        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\\n       grad_fn=<MulBackward0>\\nNote that the resulting dropout outputs may look different depending on your oper-\\nating system; you can read more about this inconsistency here on the PyTorch issue\\ntracker at https://github.com/pytorch/pytorch/issues/121595.\\n Having gained an understanding of causal attention and dropout masking, we can\\nnow develop a concise Python class. This class is designed to facilitate the efficient\\napplication of these two techniques.\\n3.5.3\\nImplementing a compact causal attention class\\nWe will now incorporate the causal attention and dropout modifications into the\\nSelfAttention Python class we developed in section 3.4. This class will then serve as a\\ntemplate for developing multi-head attention, which is the final attention class we will\\nimplement.\\n But before we begin, let’s ensure that the code can handle batches consisting of\\nmore than one input so that the CausalAttention class supports the batch outputs\\nproduced by the data loader we implemented in chapter 2.\\n For simplicity, to simulate such batch inputs, we duplicate the input text example:\\nbatch = torch.stack((inputs, inputs), dim=0)\\nprint(batch.shape)               \\nThis results in a three-dimensional tensor consisting of two input texts with six tokens\\neach, where each token is a three-dimensional embedding vector:\\ntorch.Size([2, 6, 3])\\nThe following CausalAttention class is similar to the SelfAttention class we imple-\\nmented earlier, except that we added the dropout and causal mask components.\\n \\nTwo inputs with six tokens each; each \\ntoken has embedding dimension 3.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 102}, page_content='81\\n3.5\\nHiding future words with causal attention\\nclass CausalAttention(nn.Module):\\n    def __init__(self, d_in, d_out, context_length,\\n                dropout, qkv_bias=False):\\n        super().__init__()\\n        self.d_out = d_out\\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\\n        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\\n        self.dropout = nn.Dropout(dropout)           \\n        self.register_buffer(\\n           \\'mask\\',\\n           torch.triu(torch.ones(context_length, context_length),\\n           diagonal=1)\\n        )            \\n    def forward(self, x):\\n        b, num_tokens, d_in = x.shape                  \\n        keys = self.W_key(x)\\n        queries = self.W_query(x)\\n        values = self.W_value(x)\\n        attn_scores = queries @ keys.transpose(1, 2)   \\n        attn_scores.masked_fill_(                   \\n            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) \\n        attn_weights = torch.softmax(\\n            attn_scores / keys.shape[-1]**0.5, dim=-1\\n        )\\n        attn_weights = self.dropout(attn_weights)\\n        context_vec = attn_weights @ values\\n        return context_vec\\nWhile all added code lines should be familiar at this point, we now added a self\\n.register_buffer() call in the __init__ method. The use of register_buffer in\\nPyTorch is not strictly necessary for all use cases but offers several advantages here. For\\ninstance, when we use the CausalAttention class in our LLM, buffers are automati-\\ncally moved to the appropriate device (CPU or GPU) along with our model, which will\\nbe relevant when training our LLM. This means we don’t need to manually ensure\\nthese tensors are on the same device as your model parameters, avoiding device mis-\\nmatch errors.\\n We can use the CausalAttention class as follows, similar to SelfAttention\\npreviously:\\ntorch.manual_seed(123)\\ncontext_length = batch.shape[1]\\nca = CausalAttention(d_in, d_out, context_length, 0.0)\\ncontext_vecs = ca(batch)\\nprint(\"context_vecs.shape:\", context_vecs.shape)\\nListing 3.3\\nA compact causal attention class\\nCompared \\nto the previous \\nSelfAttention_v1 \\nclass, we added a \\ndropout layer.\\nThe register_buffer call is also a new addition \\n(more information is provided in the following text).\\nWe transpose \\ndimensions 1 and 2, \\nkeeping the batch \\ndimension at the first \\nposition (0).\\nIn PyTorch, operations\\nwith a trailing underscore\\nare performed in-place,\\navoiding unnecessary\\nmemory copies.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 103}, page_content='82\\nCHAPTER 3\\nCoding attention mechanisms\\nThe resulting context vector is a three-dimensional tensor where each token is now\\nrepresented by a two-dimensional embedding:\\ncontext_vecs.shape: torch.Size([2, 6, 2])\\nFigure 3.23 summarizes what we have accomplished so far. We have focused on the\\nconcept and implementation of causal attention in neural networks. Next, we will\\nexpand on this concept and implement a multi-head attention module that imple-\\nments several causal attention mechanisms in parallel.\\n3.6\\nExtending single-head attention to multi-head \\nattention\\nOur final step will be to extend the previously implemented causal attention class over\\nmultiple heads. This is also called multi-head attention. \\n The term “multi-head” refers to dividing the attention mechanism into multiple\\n“heads,” each operating independently. In this context, a single causal attention mod-\\nule can be considered single-head attention, where there is only one set of attention\\nweights processing the input sequentially.\\n We will tackle this expansion from causal attention to multi-head attention. First,\\nwe will intuitively build a multi-head attention module by stacking multiple Causal-\\nAttention modules. Then we will then implement the same multi-head attention\\nmodule in a more complicated but more computationally efficient way.\\n3.6.1\\nStacking multiple single-head attention layers\\nIn practical terms, implementing multi-head attention involves creating multiple\\ninstances of the self-attention mechanism (see figure 3.18), each with its own weights,\\nand then combining their outputs. Using multiple instances of the self-attention\\nmechanism can be computationally intensive, but it’s crucial for the kind of complex\\npattern recognition that models like transformer-based LLMs are known for. \\n1) Simpliﬁed\\nself-attention\\n3) Causal attention\\n2) Self-attention\\n4) Multi-head\\nattention\\nIn the previous section,\\nwe implemented a\\nself-attention mechanism\\nwith trainable weights.\\nIn this section, we extended\\nthe self-attention mechanism\\nwith a causal mask and\\ndropout mask.\\nIn the next section, we\\nextend causal attention\\nto multi-head attention.\\nFigure 3.23\\nHere’s what we’ve done so far. We began with a simplified attention mechanism, added trainable \\nweights, and then added a causal attention mask. Next, we will extend the causal attention mechanism and code \\nmulti-head attention, which we will use in our LLM.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 104}, page_content='83\\n3.6\\nExtending single-head attention to multi-head attention\\n Figure 3.24 illustrates the structure of a multi-head attention module, which con-\\nsists of multiple single-head attention modules, as previously depicted in figure 3.18,\\nstacked on top of each other.\\nAs mentioned before, the main idea behind multi-head attention is to run the attention\\nmechanism multiple times (in parallel) with different, learned linear projections—the\\nresults of multiplying the input data (like the query, key, and value vectors in attention\\nmechanisms) by a weight matrix. In code, we can achieve this by implementing a sim-\\nple MultiHeadAttentionWrapper class that stacks multiple instances of our previously\\nimplemented CausalAttention module.\\n \\n \\n \\n0.7 0.2 0.1\\nInputs\\nFor multi-head attention with\\ntwo heads, we obtain two\\nattention weight matrices,\\nincluding causal and dropout\\nmasks.\\nQueries\\nKeys\\nValues\\n-0.7 -0.1\\nContext\\nvectors\\nWeight\\nmatrix\\nWeight\\nmatrix\\nWeight\\nmatrix\\nThe embedded input tokens\\nremain unchanged.\\nInstead of one query matrix\\n, we\\nQ\\nhave two query matrices Q1and\\n.\\nQ2\\nWe now have two sets of\\ncontext vectors, Z1and\\n.\\nZ2\\n0.7\\n0.4\\n0.7\\n0.4\\n-0.7 -0.1\\nCombined\\ncontext\\nvectors Z\\nThe context vector in Z2\\ncorresponding to the ﬁfth\\ninput that was highlighted\\nin the inputs\\n.\\nX\\nThe values of the 5th row (input)\\nare shown as an example.\\nInstead of one value weight\\nmatrix Wv in single-head\\nattention, use two matrices\\nWv1 and\\n.\\nWv2\\nFigure 3.24\\nThe multi-head attention module includes two single-head attention modules stacked on top of \\neach other. So, instead of using a single matrix Wv for computing the value matrices, in a multi-head attention \\nmodule with two heads, we now have two value weight matrices: Wv1 and Wv2. The same applies to the other \\nweight matrices, WQ and Wk. We obtain two sets of context vectors Z1 and Z2 that we can combine into a single \\ncontext vector matrix Z.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 105}, page_content='84\\nCHAPTER 3\\nCoding attention mechanisms\\nclass MultiHeadAttentionWrapper(nn.Module):\\n    def __init__(self, d_in, d_out, context_length,\\n                 dropout, num_heads, qkv_bias=False):\\n        super().__init__()\\n        self.heads = nn.ModuleList(\\n            [CausalAttention(\\n                 d_in, d_out, context_length, dropout, qkv_bias\\n             ) \\n             for _ in range(num_heads)]\\n        )\\n    def forward(self, x):\\n        return torch.cat([head(x) for head in self.heads], dim=-1)\\nFor example, if we use this MultiHeadAttentionWrapper class with two attention heads\\n(via num_heads=2) and CausalAttention output dimension d_out=2, we get a four-\\ndimensional context vector (d_out*num_heads=4), as depicted in figure 3.25.\\nTo illustrate this further with a concrete example, we can use the MultiHeadAttention-\\nWrapper class similar to the CausalAttention class before:\\ntorch.manual_seed(123)\\ncontext_length = batch.shape[1] # This is the number of tokens\\nd_in, d_out = 3, 2\\nListing 3.4\\nA wrapper class to implement multi-head attention\\nMulti-head attention\\nZ2\\nConcatenated\\ncontext vector\\nmatrices\\nZ\\nZ2\\nZ1\\nd_in = 3\\nd_out = 2\\nd_out = 4\\nTwo attention heads produces a\\ntensor stacking two matrices that\\nrepresent the context vectors.\\nInputs\\nX\\nZ1\\nContext\\nvector\\nmatrices\\nChoosing an embedding dimension of 2 (d_out = 2)\\nfor the context vectors results in a ﬁnal embedding\\ndimension of 4 (d_out × num_heads).\\nFigure 3.25\\nUsing the MultiHeadAttentionWrapper, we specified the number of \\nattention heads (num_heads). If we set num_heads=2, as in this example, we obtain \\na tensor with two sets of context vector matrices. In each context vector matrix, the \\nrows represent the context vectors corresponding to the tokens, and the columns \\ncorrespond to the embedding dimension specified via d_out=4. We concatenate these \\ncontext vector matrices along the column dimension. Since we have two attention \\nheads and an embedding dimension of 2, the final embedding dimension is 2 × 2 = 4.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 106}, page_content='85\\n3.6\\nExtending single-head attention to multi-head attention\\nmha = MultiHeadAttentionWrapper(\\n    d_in, d_out, context_length, 0.0, num_heads=2\\n)\\ncontext_vecs = mha(batch)\\nprint(context_vecs)\\nprint(\"context_vecs.shape:\", context_vecs.shape)\\nThis results in the following tensor representing the context vectors:\\ntensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\\n         [-0.5874,  0.0058,  0.5891,  0.3257],\\n         [-0.6300, -0.0632,  0.6202,  0.3860],\\n         [-0.5675, -0.0843,  0.5478,  0.3589],\\n         [-0.5526, -0.0981,  0.5321,  0.3428],\\n         [-0.5299, -0.1081,  0.5077,  0.3493]],\\n        [[-0.4519,  0.2216,  0.4772,  0.1063],\\n         [-0.5874,  0.0058,  0.5891,  0.3257],\\n         [-0.6300, -0.0632,  0.6202,  0.3860],\\n         [-0.5675, -0.0843,  0.5478,  0.3589],\\n         [-0.5526, -0.0981,  0.5321,  0.3428],\\n         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\\ncontext_vecs.shape: torch.Size([2, 6, 4])\\nThe first dimension of the resulting context_vecs tensor is 2 since we have two input\\ntexts (the input texts are duplicated, which is why the context vectors are exactly the\\nsame for those). The second dimension refers to the 6 tokens in each input. The third\\ndimension refers to the four-dimensional embedding of each token.\\nUp to this point, we have implemented a MultiHeadAttentionWrapper that combined\\nmultiple single-head attention modules. However, these are processed sequentially via\\n[head(x) for head in self.heads] in the forward method. We can improve this\\nimplementation by processing the heads in parallel. One way to achieve this is by com-\\nputing the outputs for all attention heads simultaneously via matrix multiplication.\\nExercise 3.2 Returning two-dimensional embedding vectors \\nChange the input arguments for the MultiHeadAttentionWrapper(..., num_\\nheads=2) call such that the output context vectors are two-dimensional instead of\\nfour dimensional while keeping the setting num_heads=2. Hint: You don’t have to\\nmodify the class implementation; you just have to change one of the other input\\narguments.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 107}, page_content='86\\nCHAPTER 3\\nCoding attention mechanisms\\n3.6.2\\nImplementing multi-head attention with weight splits\\nSo far, we have created a MultiHeadAttentionWrapper to implement multi-head\\nattention by stacking multiple single-head attention modules. This was done by instan-\\ntiating and combining several CausalAttention objects.\\n Instead of maintaining two separate classes, MultiHeadAttentionWrapper and\\nCausalAttention, we can combine these concepts into a single MultiHeadAttention\\nclass. Also, in addition to merging the MultiHeadAttentionWrapper with the Causal-\\nAttention code, we will make some other modifications to implement multi-head\\nattention more efficiently.\\n In the MultiHeadAttentionWrapper, multiple heads are implemented by creating\\na list of CausalAttention objects (self.heads), each representing a separate atten-\\ntion head. The CausalAttention class independently performs the attention mecha-\\nnism, and the results from each head are concatenated. In contrast, the following\\nMultiHeadAttention class integrates the multi-head functionality within a single class.\\nIt splits the input into multiple heads by reshaping the projected query, key, and value\\ntensors and then combines the results from these heads after computing attention.\\n Let’s take a look at the MultiHeadAttention class before we discuss it further.\\nclass MultiHeadAttention(nn.Module):\\n    def __init__(self, d_in, d_out, \\n                 context_length, dropout, num_heads, qkv_bias=False):\\n        super().__init__()\\n        assert (d_out % num_heads == 0), \\\\\\n            \"d_out must be divisible by num_heads\"\\n        self.d_out = d_out\\n        self.num_heads = num_heads\\n        self.head_dim = d_out // num_heads   \\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\\n        self.out_proj = nn.Linear(d_out, d_out)   \\n        self.dropout = nn.Dropout(dropout)\\n        self.register_buffer(\\n            \"mask\",\\n            torch.triu(torch.ones(context_length, context_length),\\n                       diagonal=1)\\n        )\\n    def forward(self, x):\\n        b, num_tokens, d_in = x.shape\\n        keys = self.W_key(x)        \\n        queries = self.W_query(x)   \\n        values = self.W_value(x)    \\nListing 3.5\\nAn efficient multi-head attention class\\nReduces the projection \\ndim to match the \\ndesired output dim\\nUses a Linear \\nlayer to combine \\nhead outputs\\nTensor shape: (b, \\nnum_tokens, d_out)'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 108}, page_content='87\\n3.6\\nExtending single-head attention to multi-head attention\\n        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)      \\n        values = values.view(b, num_tokens, self.num_heads, self.head_dim)  \\n        queries = queries.view(                                             \\n            b, num_tokens, self.num_heads, self.head_dim                    \\n        )                                                                   \\n        keys = keys.transpose(1, 2)         \\n        queries = queries.transpose(1, 2)   \\n        values = values.transpose(1, 2)     \\n        attn_scores = queries @ keys.transpose(2, 3)  \\n        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]   \\n  \\n        attn_scores.masked_fill_(mask_bool, -torch.inf)    \\n        attn_weights = torch.softmax(\\n            attn_scores / keys.shape[-1]**0.5, dim=-1)\\n        attn_weights = self.dropout(attn_weights)\\n        context_vec = (attn_weights @ values).transpose(1, 2)  \\n        \\n        context_vec = context_vec.contiguous().view(\\n            b, num_tokens, self.d_out\\n        )\\n        context_vec = self.out_proj(context_vec)   \\n        return context_vec\\nEven though the reshaping (.view) and transposing (.transpose) of tensors inside\\nthe MultiHeadAttention class looks very mathematically complicated, the Multi-\\nHeadAttention class implements the same concept as the MultiHeadAttention-\\nWrapper earlier. \\n On a big-picture level, in the previous MultiHeadAttentionWrapper, we stacked\\nmultiple single-head attention layers that we combined into a multi-head attention\\nlayer. The MultiHeadAttention class takes an integrated approach. It starts with a\\nmulti-head layer and then internally splits this layer into individual attention heads, as\\nillustrated in figure 3.26.\\n The splitting of the query, key, and value tensors is achieved through tensor reshap-\\ning and transposing operations using PyTorch’s .view and .transpose methods. The\\ninput is first transformed (via linear layers for queries, keys, and values) and then\\nreshaped to represent multiple heads. \\n The key operation is to split the d_out dimension into num_heads and head_dim,\\nwhere head_dim = d_out / num_heads. This splitting is then achieved using the .view\\nmethod: a tensor of dimensions (b, num_tokens, d_out) is reshaped to dimension\\n(b, num_tokens, num_heads, head_dim).\\nWe implicitly\\nsplit the matrix\\nby adding a\\nnum_heads\\ndimension. Then\\nwe unroll the\\nlast dim: (b,\\nnum_tokens,\\nd_out) -> (b,\\nnum_tokens,\\nnum_heads,\\nhead_dim).\\nTransposes from shape (b, num_tokens, \\nnum_heads, head_dim) to (b, num_heads, \\nnum_tokens, head_dim)\\nComputes\\ndot product\\nfor each head\\nMasks \\ntruncated to \\nthe number \\nof tokens\\nUses the \\nmask to fill \\nattention \\nscores\\nTensor shape: \\n(b, num_tokens, \\nn_heads, \\nhead_dim)\\nCombines heads, where self.d_out \\n= self.num_heads * self.head_dim\\nAdds an optional \\nlinear projection'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 109}, page_content='88\\nCHAPTER 3\\nCoding attention mechanisms\\nThe tensors are then transposed to bring the num_heads dimension before the num_\\ntokens dimension, resulting in a shape of (b, num_heads, num_tokens, head_dim). This\\ntransposition is crucial for correctly aligning the queries, keys, and values across the\\ndifferent heads and performing batched matrix multiplications efficiently.\\n To illustrate this batched matrix multiplication, suppose we have the following\\ntensor:\\na = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],   \\n                    [0.8993, 0.0390, 0.9268, 0.7388],\\n                    [0.7179, 0.7058, 0.9156, 0.4340]],\\n                   [[0.0772, 0.3565, 0.1479, 0.5331],\\n                    [0.4066, 0.2318, 0.4545, 0.9737],\\n                    [0.4606, 0.5159, 0.4220, 0.5786]]]])\\nWeight\\nmatrix\\nWeight\\nmatrix\\nInputs X\\nQueries\\nQueries\\nInputs\\nQueries\\nQueries\\nWeight\\nmatrix\\nQueries\\nPerform two matrix multiplications to obtain\\nthe two query matrices, Q1and\\n.\\nQ2\\nObtain queries\\nwith only one\\nQ\\nmatrix multiplication.\\nThen split queries\\ninto\\nQ\\nQ1and\\n.\\nQ2\\nFigure 3.26\\nIn the MultiHeadAttentionWrapper class with two attention heads, \\nwe initialized two weight matrices, Wq1 and Wq2, and computed two query matrices, Q1 \\nand Q2 (top). In the MultiheadAttention class, we initialize one larger weight matrix \\nWq, only perform one matrix multiplication with the inputs to obtain a query matrix Q, and \\nthen split the query matrix into Q1 and Q2 (bottom). We do the same for the keys and \\nvalues, which are not shown to reduce visual clutter.\\nThe shape of this \\ntensor is (b, num_heads, \\nnum_tokens, head_dim) \\n= (1, 2, 3, 4).'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 110}, page_content='89\\n3.6\\nExtending single-head attention to multi-head attention\\nNow we perform a batched matrix multiplication between the tensor itself and a view\\nof the tensor where we transposed the last two dimensions, num_tokens and head_dim:\\nprint(a @ a.transpose(2, 3))\\nThe result is\\ntensor([[[[1.3208, 1.1631, 1.2879],\\n          [1.1631, 2.2150, 1.8424],\\n          [1.2879, 1.8424, 2.0402]],\\n         [[0.4391, 0.7003, 0.5903],\\n          [0.7003, 1.3737, 1.0620],\\n          [0.5903, 1.0620, 0.9912]]]])\\nIn this case, the matrix multiplication implementation in PyTorch handles the four-\\ndimensional input tensor so that the matrix multiplication is carried out between the two\\nlast dimensions (num_tokens, head_dim) and then repeated for the individual heads. \\n For instance, the preceding becomes a more compact way to compute the matrix\\nmultiplication for each head separately:\\nfirst_head = a[0, 0, :, :]\\nfirst_res = first_head @ first_head.T\\nprint(\"First head:\\\\n\", first_res)\\nsecond_head = a[0, 1, :, :]\\nsecond_res = second_head @ second_head.T\\nprint(\"\\\\nSecond head:\\\\n\", second_res)\\nThe results are exactly the same results as those we obtained when using the batched\\nmatrix multiplication print(a @ a.transpose(2, 3)):\\nFirst head:\\n tensor([[1.3208, 1.1631, 1.2879],\\n        [1.1631, 2.2150, 1.8424],\\n        [1.2879, 1.8424, 2.0402]])\\nSecond head:\\n tensor([[0.4391, 0.7003, 0.5903],\\n        [0.7003, 1.3737, 1.0620],\\n        [0.5903, 1.0620, 0.9912]])\\nContinuing with MultiHeadAttention, after computing the attention weights and con-\\ntext vectors, the context vectors from all heads are transposed back to the shape (b,\\nnum_tokens, num_heads, head_dim). These vectors are then reshaped (flattened) into\\nthe shape (b, num_tokens, d_out), effectively combining the outputs from all heads.\\n Additionally, we added an output projection layer (self.out_proj) to Multi-\\nHeadAttention after combining the heads, which is not present in the Causal-\\nAttention class. This output projection layer is not strictly necessary (see appendix B for'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 111}, page_content='90\\nCHAPTER 3\\nCoding attention mechanisms\\nmore details), but it is commonly used in many LLM architectures, which is why I\\nadded it here for completeness.\\n Even though the MultiHeadAttention class looks more complicated than the\\nMultiHeadAttentionWrapper due to the additional reshaping and transposition of\\ntensors, it is more efficient. The reason is that we only need one matrix multiplication\\nto compute the keys, for instance, keys = self.W_key(x) (the same is true for the que-\\nries and values). In the MultiHeadAttentionWrapper, we needed to repeat this matrix\\nmultiplication, which is computationally one of the most expensive steps, for each\\nattention head.\\n The MultiHeadAttention class can be used similar to the SelfAttention and\\nCausalAttention classes we implemented earlier:\\ntorch.manual_seed(123)\\nbatch_size, context_length, d_in = batch.shape\\nd_out = 2\\nmha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\\ncontext_vecs = mha(batch)\\nprint(context_vecs)\\nprint(\"context_vecs.shape:\", context_vecs.shape)\\nThe results show that the output dimension is directly controlled by the d_out\\nargument:\\ntensor([[[0.3190, 0.4858],\\n         [0.2943, 0.3897],\\n         [0.2856, 0.3593],\\n         [0.2693, 0.3873],\\n         [0.2639, 0.3928],\\n         [0.2575, 0.4028]],\\n        [[0.3190, 0.4858],\\n         [0.2943, 0.3897],\\n         [0.2856, 0.3593],\\n         [0.2693, 0.3873],\\n         [0.2639, 0.3928],\\n         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\\ncontext_vecs.shape: torch.Size([2, 6, 2])\\nWe have now implemented the MultiHeadAttention class that we will use when we\\nimplement and train the LLM. Note that while the code is fully functional, I used\\nrelatively small embedding sizes and numbers of attention heads to keep the outputs\\nreadable.\\n For comparison, the smallest GPT-2 model (117 million parameters) has 12 atten-\\ntion heads and a context vector embedding size of 768. The largest GPT-2 model (1.5\\nbillion parameters) has 25 attention heads and a context vector embedding size of\\n1,600. The embedding sizes of the token inputs and context embeddings are the same\\nin GPT models (d_in = d_out).'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 112}, page_content='91\\nSummary\\nSummary\\n\\uf0a1Attention mechanisms transform input elements into enhanced context vector\\nrepresentations that incorporate information about all inputs.\\n\\uf0a1A self-attention mechanism computes the context vector representation as a\\nweighted sum over the inputs.\\n\\uf0a1In a simplified attention mechanism, the attention weights are computed via\\ndot products.\\n\\uf0a1A dot product is a concise way of multiplying two vectors element-wise and then\\nsumming the products.\\n\\uf0a1Matrix multiplications, while not strictly required, help us implement computa-\\ntions more efficiently and compactly by replacing nested for loops. \\n\\uf0a1In self-attention mechanisms used in LLMs, also called scaled-dot product\\nattention, we include trainable weight matrices to compute intermediate trans-\\nformations of the inputs: queries, values, and keys.\\n\\uf0a1When working with LLMs that read and generate text from left to right, we add\\na causal attention mask to prevent the LLM from accessing future tokens.\\n\\uf0a1In addition to causal attention masks to zero-out attention weights, we can add\\na dropout mask to reduce overfitting in LLMs.\\n\\uf0a1The attention modules in transformer-based LLMs involve multiple instances of\\ncausal attention, which is called multi-head attention.\\n\\uf0a1We can create a multi-head attention module by stacking multiple instances of\\ncausal attention modules.\\n\\uf0a1A more efficient way of creating multi-head attention modules involves batched\\nmatrix multiplications.\\nExercise 3.3 Initializing GPT-2 size attention modules\\nUsing the MultiHeadAttention class, initialize a multi-head attention module that\\nhas the same number of attention heads as the smallest GPT-2 model (12 attention\\nheads). Also ensure that you use the respective input and output embedding sizes\\nsimilar to GPT-2 (768 dimensions). Note that the smallest GPT-2 model supports a\\ncontext length of 1,024 tokens.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 113}, page_content='92\\nImplementing\\na GPT model from\\nscratch to generate text\\nYou’ve already learned and coded the multi-head attention mechanism, one of the\\ncore components of LLMs. Now, we will code the other building blocks of an LLM\\nand assemble them into a GPT-like model that we will train in the next chapter to\\ngenerate human-like text.\\n The LLM architecture referenced in figure 4.1, consists of several building\\nblocks. We will begin with a top-down view of the model architecture before cover-\\ning the individual components in more detail.\\nThis chapter covers\\n\\uf0a1Coding a GPT-like large language model (LLM) \\nthat can be trained to generate human-like text\\n\\uf0a1Normalizing layer activations to stabilize neural \\nnetwork training\\n\\uf0a1Adding shortcut connections in deep neural \\nnetworks \\n\\uf0a1Implementing transformer blocks to create GPT \\nmodels of various sizes\\n\\uf0a1Computing the number of parameters and \\nstorage requirements of GPT models'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 114}, page_content='93\\n4.1\\nCoding an LLM architecture\\n4.1\\nCoding an LLM architecture\\nLLMs, such as GPT (which stands for generative pretrained transformer), are large deep\\nneural network architectures designed to generate new text one word (or token) at a\\ntime. However, despite their size, the model architecture is less complicated than you\\nmight think, since many of its components are repeated, as we will see later. Figure 4.2\\nprovides a top-down view of a GPT-like LLM, with its main components highlighted.\\n We have already covered several aspects of the LLM architecture, such as input\\ntokenization and embedding and the masked multi-head attention module. Now, we\\nwill implement the core structure of the GPT model, including its transformer blocks,\\nwhich we will later train to generate human-like text.\\n Previously, we used smaller embedding dimensions for simplicity, ensuring that the\\nconcepts and examples could comfortably fit on a single page. Now, we are scaling up\\nto the size of a small GPT-2 model, specifically the smallest version with 124 million\\nparameters, as described in “Language Models Are Unsupervised Multitask Learners,”\\nby Radford et al. (https://mng.bz/yoBq). Note that while the original report men-\\ntions 117 million parameters, this was later corrected. In chapter 6, we will focus on\\nloading pretrained weights into our implementation and adapting it for larger GPT-2\\nmodels with 345, 762, and 1,542 million parameters. \\n In the context of deep learning and LLMs like GPT, the term “parameters” refers\\nto the trainable weights of the model. These weights are essentially the internal vari-\\nables of the model that are adjusted and optimized during the training process to\\nminimize a specific loss function. This optimization allows the model to learn from\\nthe training data.\\nA building block of an LLM\\nthat we previously implemented\\nin the previous chapter\\nIn this chapter, we will\\nnow implement the\\nother parts of the LLM\\nIn the next chapter, we\\nadd the training loop\\nand pretrain the LLM\\n1) Data\\npreparation\\n& sampling\\n2) Attention\\nmechanism\\nBuilding an LLM\\nSTAGE 1\\nFoundation model\\nSTAGE 2\\nSTAGE 3\\nClassiﬁer\\nPersonal assistant\\nDataset with class labels\\nInstruction dataset\\n4) Pretraining\\n9) Fine-tuning\\n5) Training\\nloop\\n6) Model\\nevaluation\\n7) Load\\npretrained\\nweights\\n8) Fine-tuning\\n3) LLM\\narchitecture\\nFigure 4.1\\nThe three main stages of coding an LLM. This chapter focuses on step 3 of stage 1: implementing the \\nLLM architecture.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 115}, page_content='94\\nCHAPTER 4\\nImplementing a GPT model from scratch to generate text\\nFor example, in a neural network layer that is represented by a 2,048 × 2,048–dimensional\\nmatrix (or tensor) of weights, each element of this matrix is a parameter. Since there\\nare 2,048 rows and 2,048 columns, the total number of parameters in this layer is 2,048\\nmultiplied by 2,048, which equals 4,194,304 parameters.\\nGPT-2 vs. GPT-3 \\nNote that we are focusing on GPT-2 because OpenAI has made the weights of the\\npretrained model publicly available, which we will load into our implementation in\\nchapter 6. GPT-3 is fundamentally the same in terms of model architecture, except\\nthat it is scaled up from 1.5 billion parameters in GPT-2 to 175 billion parameters\\nin GPT-3, and it is trained on more data. As of this writing, the weights for GPT-3\\nare not publicly available. GPT-2 is also a better choice for learning how to imple-\\nment LLMs, as it can be run on a single laptop computer, whereas GPT-3 requires a\\nGPU cluster for training and inference. According to Lambda Labs (https://lambdalabs\\n.com/), it would take 355 years to train GPT-3 on a single V100 datacenter GPU\\nand 665 years on a consumer RTX 8000 GPU.\\nOutput layers\\nTokenized text\\nEmbedding layers\\nTransformer block\\nMasked multi-head\\nattention\\nEmbedding layers and\\ntokenization were\\ncovered in hapter 2.\\nc\\nWe implemented the\\nattention module in\\nthe previous chapter.\\nIn this chapter, we\\nimplement a GPT model\\nincluding all of its\\nsubcomponents.\\nGPT\\nmodel\\nTransformer blocks are\\na key component of\\nGPT-like LLMs.\\nThe goal is to generate new\\ntext one word at a time.\\n“Every eﬀort moves you\\n”\\nforward\\n“Every eﬀort moves you”\\nFigure 4.2\\nA GPT model. In addition to the embedding layers, it consists of one or more \\ntransformer blocks containing the masked multi-head attention module we previously \\nimplemented.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 116}, page_content='95\\n4.1\\nCoding an LLM architecture\\nWe specify the configuration of the small GPT-2 model via the following Python dictio-\\nnary, which we will use in the code examples later:\\nGPT_CONFIG_124M = {\\n    \"vocab_size\": 50257,     # Vocabulary size\\n    \"context_length\": 1024,  # Context length\\n    \"emb_dim\": 768,          # Embedding dimension\\n    \"n_heads\": 12,           # Number of attention heads\\n    \"n_layers\": 12,          # Number of layers\\n    \"drop_rate\": 0.1,        # Dropout rate\\n    \"qkv_bias\": False        # Query-Key-Value bias\\n}\\nIn the GPT_CONFIG_124M dictionary, we use concise variable names for clarity and to\\nprevent long lines of code:\\n\\uf0a1\\nvocab_size refers to a vocabulary of 50,257 words, as used by the BPE tokenizer\\n(see chapter 2).\\n\\uf0a1\\ncontext_length denotes the maximum number of input tokens the model can\\nhandle via the positional embeddings (see chapter 2).\\n\\uf0a1\\nemb_dim represents the embedding size, transforming each token into a 768-\\ndimensional vector.\\n\\uf0a1\\nn_heads indicates the count of attention heads in the multi-head attention\\nmechanism (see chapter 3).\\n\\uf0a1\\nn_layers specifies the number of transformer blocks in the model, which we\\nwill cover in the upcoming discussion.\\n\\uf0a1\\ndrop_rate indicates the intensity of the dropout mechanism (0.1 implies a 10%\\nrandom drop out of hidden units) to prevent overfitting (see chapter 3).\\n\\uf0a1\\nqkv_bias determines whether to include a bias vector in the Linear layers of\\nthe multi-head attention for query, key, and value computations. We will initially\\ndisable this, following the norms of modern LLMs, but we will revisit it in chap-\\nter 6 when we load pretrained GPT-2 weights from OpenAI into our model (see\\nchapter 6).\\nUsing this configuration, we will implement a GPT placeholder architecture (Dummy-\\nGPTModel), as shown in figure 4.3. This will provide us with a big-picture view of how\\neverything fits together and what other components we need to code to assemble the\\nfull GPT model architecture.\\n The numbered boxes in figure 4.3 illustrate the order in which we tackle the indi-\\nvidual concepts required to code the final GPT architecture. We will start with step 1,\\na placeholder GPT backbone we will call DummyGPTModel.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 117}, page_content='96\\nCHAPTER 4\\nImplementing a GPT model from scratch to generate text\\nimport torch\\nimport torch.nn as nn\\nclass DummyGPTModel(nn.Module):\\n    def __init__(self, cfg):\\n        super().__init__()\\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\\n        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\\n        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\\n        self.trf_blocks = nn.Sequential(              \\n            *[DummyTransformerBlock(cfg)              \\n              for _ in range(cfg[\"n_layers\"])]        \\n        )                                             \\n        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])    \\n        self.out_head = nn.Linear(\\n            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\\n        )\\n    def forward(self, in_idx):\\n        batch_size, seq_len = in_idx.shape\\n        tok_embeds = self.tok_emb(in_idx)\\n        pos_embeds = self.pos_emb(\\n            torch.arange(seq_len, device=in_idx.device)\\n        )\\n        x = tok_embeds + pos_embeds\\n        x = self.drop_emb(x)\\n        x = self.trf_blocks(x)\\n        x = self.final_norm(x)\\n        logits = self.out_head(x)\\n        return logits\\nListing 4.1\\nA placeholder GPT model architecture class\\n3) GELU\\nactivation\\n4) Feed forward\\nnetwork\\n2) Layer\\nnormalization\\n5) Shortcut\\nconnections\\nWe developed a GPT\\nplaceholder model to see\\nthe overall structure of\\nthe model.\\nNext, we will\\nimplement building\\nblocks 2 5.\\n–\\nThen we will combine building\\nblocks 2 5, including the\\n–\\nmulti-head attention\\nmodule from chapter 3,\\ninto a transformer block.\\nFinally, we will use multiple\\ntransformer blocks to\\nimplement the untrained\\nGPT model.\\n1) GPT\\nbackbone\\n6) Transformer\\nblock\\n7) Final GPT\\narchitecture\\nFigure 4.3\\nThe order in which we code the GPT architecture. We start with the GPT \\nbackbone, a placeholder architecture, before getting to the individual core pieces and \\neventually assembling them in a transformer block for the final GPT architecture.\\nUses a placeholder \\nfor TransformerBlock\\nUses a \\nplaceholder for \\nLayerNorm'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 118}, page_content='97\\n4.1\\nCoding an LLM architecture\\nclass DummyTransformerBlock(nn.Module):   \\n    def __init__(self, cfg):\\n        super().__init__()\\n    def forward(self, x):    \\n        return x\\nclass DummyLayerNorm(nn.Module):          \\n    def __init__(self, normalized_shape, eps=1e-5):   \\n        super().__init__()\\n    def forward(self, x):\\n        return x\\nThe DummyGPTModel class in this code defines a simplified version of a GPT-like\\nmodel using PyTorch’s neural network module (nn.Module). The model architecture\\nin the DummyGPTModel class consists of token and positional embeddings, dropout,\\na series of transformer blocks (DummyTransformerBlock), a final layer normalization\\n(DummyLayerNorm), and a linear output layer (out_head). The configuration is\\npassed in via a Python dictionary, for instance, the GPT_CONFIG_124M dictionary we\\ncreated earlier.\\n The forward method describes the data flow through the model: it computes token\\nand positional embeddings for the input indices, applies dropout, processes the data\\nthrough the transformer blocks, applies normalization, and finally produces logits\\nwith the linear output layer.\\n The code in listing 4.1 is already functional. However, for now, note that we use\\nplaceholders (DummyLayerNorm and DummyTransformerBlock) for the transformer block\\nand layer normalization, which we will develop later.\\n Next, we will prepare the input data and initialize a new GPT model to illustrate\\nits usage. Building on our coding of the tokenizer (see chapter 2), let’s now con-\\nsider a high-level overview of how data flows in and out of a GPT model, as shown in\\nfigure 4.4.\\n To implement these steps, we tokenize a batch consisting of two text inputs for the\\nGPT model using the tiktoken tokenizer from chapter 2:\\nimport tiktoken\\ntokenizer = tiktoken.get_encoding(\"gpt2\")\\nbatch = []\\ntxt1 = \"Every effort moves you\"\\ntxt2 = \"Every day holds a\"\\nbatch.append(torch.tensor(tokenizer.encode(txt1)))\\nbatch.append(torch.tensor(tokenizer.encode(txt2)))\\nbatch = torch.stack(batch, dim=0)\\nprint(batch)\\nA simple placeholder class that will be \\nreplaced by a real TransformerBlock later\\nThis block does nothing and \\njust returns its input.\\nA simple placeholder class that will be \\nreplaced by a real LayerNorm later\\nThe parameters here \\nare just to mimic the \\nLayerNorm interface.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 119}, page_content='98\\nCHAPTER 4\\nImplementing a GPT model from scratch to generate text\\nThe resulting token IDs for the two texts are as follows:\\ntensor([[6109,  3626,  6100,   345],   \\n        [6109,  1110,  6622,   257]])  \\nNext, we initialize a new 124-million-parameter DummyGPTModel instance and feed it\\nthe tokenized batch: \\ntorch.manual_seed(123)\\nmodel = DummyGPTModel(GPT_CONFIG_124M)\\nlogits = model(batch)\\nprint(\"Output shape:\", logits.shape)\\nprint(logits)\\nGPT model\\nInput text:\\nToken embeddings:\\nTokenized text:\\nOutput text\\nPostprocessing steps\\nToken IDs:\\nWe tokenize the input\\ntext and convert it into\\ntoken embeddings.\\nOutputs:\\nEvery effort moves you\\nyou\\nmoves\\neffort\\nEvery\\n345\\n6100\\n3626\\n6109\\n2.4\\n2.4\\n-2.6 -1.3\\n2.0\\n1.8\\n-1.6 2.1\\n-1.2 0.3\\n-0.1 0.4\\n0.5\\n1.6\\n0.0 1.6\\nThe LLM returns one 768-\\ndimensional output vector\\nfor each 768-dimensional\\ninput token embedding.\\nFor the smallest GPT-2 model,\\neach embedding vector consists\\nof 768 dimensions (only the ﬁrst\\n2 dimensions are shown).\\neffort moves you forward\\nThe goal is to generate\\nthe next word, “forward.”\\nThe number of input tokens matches\\nthe number of output tokens; hence,\\nthe ﬁrst token (“Every”) will not be\\ncontained in the output.\\nFigure 4.4\\nA big-picture overview showing how the input data is tokenized, embedded, and fed to the GPT model. \\nNote that in our DummyGPTClass coded earlier, the token embedding is handled inside the GPT model. In LLMs, \\nthe embedded input token dimension typically matches the output dimension. The output embeddings here \\nrepresent the context vectors (see chapter 3).\\nThe first row corresponds to the first text, and \\nthe second row corresponds to the second text.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 120}, page_content='99\\n4.2\\nNormalizing activations with layer normalization\\nThe model outputs, which are commonly referred to as logits, are as follows:\\nOutput shape: torch.Size([2, 4, 50257])\\ntensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\\n         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\\n         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\\n         [ 0.0139,  1.6755, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\\n        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\\n         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\\n         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\\n         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\\n       grad_fn=<UnsafeViewBackward0>)\\nThe output tensor has two rows corresponding to the two text samples. Each text sam-\\nple consists of four tokens; each token is a 50,257-dimensional vector, which matches\\nthe size of the tokenizer’s vocabulary.\\n The embedding has 50,257 dimensions because each of these dimensions refers to\\na unique token in the vocabulary. When we implement the postprocessing code, we\\nwill convert these 50,257-dimensional vectors back into token IDs, which we can then\\ndecode into words.\\n Now that we have taken a top-down look at the GPT architecture and its inputs and\\noutputs, we will code the individual placeholders, starting with the real layer normal-\\nization class that will replace the DummyLayerNorm in the previous code.\\n4.2\\nNormalizing activations with layer normalization\\nTraining deep neural networks with many layers can sometimes prove challenging\\ndue to problems like vanishing or exploding gradients. These problems lead to unsta-\\nble training dynamics and make it difficult for the network to effectively adjust its\\nweights, which means the learning process struggles to find a set of parameters\\n(weights) for the neural network that minimizes the loss function. In other words, the\\nnetwork has difficulty learning the underlying patterns in the data to a degree that\\nwould allow it to make accurate predictions or decisions. \\nNOTE\\nIf you are new to neural network training and the concepts of gradi-\\nents, a brief introduction to these concepts can be found in section A.4 in\\nappendix A. However, a deep mathematical understanding of gradients is not\\nrequired to follow the contents of this book.\\nLet’s now implement layer normalization to improve the stability and efficiency of neu-\\nral network training. The main idea behind layer normalization is to adjust the activa-\\ntions (outputs) of a neural network layer to have a mean of 0 and a variance of 1, also\\nknown as unit variance. This adjustment speeds up the convergence to effective\\nweights and ensures consistent, reliable training. In GPT-2 and modern transformer\\narchitectures, layer normalization is typically applied before and after the multi-head\\nattention module, and, as we have seen with the DummyLayerNorm placeholder, before'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 121}, page_content='100\\nCHAPTER 4\\nImplementing a GPT model from scratch to generate text\\nthe final output layer. Figure 4.5 provides a visual overview of how layer normalization\\nfunctions.\\nWe can recreate the example shown in figure 4.5 via the following code, where we\\nimplement a neural network layer with five inputs and six outputs that we apply to two\\ninput examples:\\ntorch.manual_seed(123)\\nbatch_example = torch.randn(2, 5)    \\nlayer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\\nout = layer(batch_example)\\nprint(out)\\nThis prints the following tensor, where the first row lists the layer outputs for the first\\ninput and the second row lists the layer outputs for the second row:\\ntensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\\n        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\\n       grad_fn=<ReluBackward0>)\\nThe neural network layer we have coded consists of a Linear layer followed by a non-\\nlinear activation function, ReLU (short for rectified linear unit), which is a standard\\nactivation function in neural networks. If you are unfamiliar with ReLU, it simply\\nthresholds negative inputs to 0, ensuring that a layer outputs only positive values,\\nwhich explains why the resulting layer output does not contain any negative values.\\nLater, we will use another, more sophisticated activation function in GPT.\\n0.22\\n0.34\\n0.00\\n0.22\\n0.00\\n0.00\\n-0.11\\n0.12\\n-0.36\\n-0.24\\n-1.19\\nLayer inputs, where\\nthe ﬁve values represent\\na single training example\\nLayer\\noutputs\\nApply layer normalization\\n0.61\\n1.41\\n-0.87\\n0.58\\n-0.87\\n-0.87\\nMean = 0.13\\nVariance = 0.39\\nMean = 0.00\\nVariance = 1.00\\nZero-centered\\nmean and unit\\nvariance after\\nnormalization\\nFigure 4.5\\nAn illustration of layer normalization where the six outputs of the layer, also \\ncalled activations, are normalized such that they have a 0 mean and a variance of 1.\\nCreates two training \\nexamples with five \\ndimensions (features) each'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 122}, page_content='101\\n4.2\\nNormalizing activations with layer normalization\\n Before we apply layer normalization to these outputs, let’s examine the mean and\\nvariance:\\nmean = out.mean(dim=-1, keepdim=True)\\nvar = out.var(dim=-1, keepdim=True)\\nprint(\"Mean:\\\\n\", mean)\\nprint(\"Variance:\\\\n\", var)\\nThe output is\\nMean:\\n  tensor([[0.1324],\\n          [0.2170]], grad_fn=<MeanBackward1>)\\nVariance:\\n  tensor([[0.0231],\\n          [0.0398]], grad_fn=<VarBackward0>)\\nThe first row in the mean tensor here contains the mean value for the first input row,\\nand the second output row contains the mean for the second input row. \\n Using keepdim=True in operations like mean or variance calculation ensures that the\\noutput tensor retains the same number of dimensions as the input tensor, even though\\nthe operation reduces the tensor along the dimension specified via dim. For instance,\\nwithout keepdim=True, the returned mean tensor would be a two-dimensional vector\\n[0.1324, 0.2170] instead of a 2 × 1–dimensional matrix [[0.1324], [0.2170]].\\n The dim parameter specifies the dimension along which the calculation of the statis-\\ntic (here, mean or variance) should be performed in a tensor. As figure 4.6 explains, for\\n0.22   0.34   0.00   0.22   0.00   0.00\\n0.21   0.23   0.00   0.51   0.32   0.00\\n0.13\\n0.21\\ndim=1\\ndim=-1\\nor\\ncalculates mean across the\\ncolumn dimension to obtain one mean per row\\nMean\\nInput 1\\nInput 2\\n0.22   0.34   0.00   0.22   0.00   0.00\\n0.21   0.23   0.00   0.51   0.32   0.00\\nInput 1\\nInput 2\\n0.21 0.29\\n0.00\\n0.37 0.16 0.00\\nMean\\ndim=0 calculates mean across the row\\ndimension to obtain one mean per column\\nFigure 4.6\\nAn illustration of the dim parameter when calculating the mean \\nof a tensor. For instance, if we have a two-dimensional tensor (matrix) with \\ndimensions [rows, columns], using dim=0 will perform the operation \\nacross rows (vertically, as shown at the bottom), resulting in an output that \\naggregates the data for each column. Using dim=1 or dim=-1 will perform \\nthe operation across columns (horizontally, as shown at the top), resulting in \\nan output aggregating the data for each row.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 123}, page_content='102\\nCHAPTER 4\\nImplementing a GPT model from scratch to generate text\\na two-dimensional tensor (like a matrix), using dim=-1 for operations such as mean or\\nvariance calculation is the same as using dim=1. This is because -1 refers to the tensor’s\\nlast dimension, which corresponds to the columns in a two-dimensional tensor. Later,\\nwhen adding layer normalization to the GPT model, which produces three-dimensional\\ntensors with the shape [batch_size, num_tokens, embedding_size], we can still use\\ndim=-1 for normalization across the last dimension, avoiding a change from dim=1 to\\ndim=2.\\n Next, let’s apply layer normalization to the layer outputs we obtained earlier. The\\noperation consists of subtracting the mean and dividing by the square root of the vari-\\nance (also known as the standard deviation):\\nout_norm = (out - mean) / torch.sqrt(var)\\nmean = out_norm.mean(dim=-1, keepdim=True)\\nvar = out_norm.var(dim=-1, keepdim=True)\\nprint(\"Normalized layer outputs:\\\\n\", out_norm)\\nprint(\"Mean:\\\\n\", mean)\\nprint(\"Variance:\\\\n\", var)\\nAs we can see based on the results, the normalized layer outputs, which now also con-\\ntain negative values, have 0 mean and a variance of 1:\\nNormalized layer outputs:\\n tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\\n        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\\n       grad_fn=<DivBackward0>)\\nMean:\\n tensor([[-5.9605e-08],\\n        [1.9868e-08]], grad_fn=<MeanBackward1>)\\nVariance:\\n tensor([[1.],\\n        [1.]], grad_fn=<VarBackward0>)\\nNote that the value –5.9605e-08 in the output tensor is the scientific notation for\\n–5.9605 × 10-8, which is –0.000000059605 in decimal form. This value is very close to 0,\\nbut it is not exactly 0 due to small numerical errors that can accumulate because of\\nthe finite precision with which computers represent numbers. \\n To improve readability, we can also turn off the scientific notation when printing\\ntensor values by setting sci_mode to False:\\ntorch.set_printoptions(sci_mode=False)\\nprint(\"Mean:\\\\n\", mean)\\nprint(\"Variance:\\\\n\", var)\\nThe output is\\nMean:\\n tensor([[    0.0000],\\n        [    0.0000]], grad_fn=<MeanBackward1>)'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 124}, page_content='103\\n4.2\\nNormalizing activations with layer normalization\\nVariance:\\n tensor([[1.],\\n        [1.]], grad_fn=<VarBackward0>)\\nSo far, we have coded and applied layer normalization in a step-by-step process. Let’s\\nnow encapsulate this process in a PyTorch module that we can use in the GPT model\\nlater.\\nclass LayerNorm(nn.Module):\\n    def __init__(self, emb_dim):\\n        super().__init__()\\n        self.eps = 1e-5\\n        self.scale = nn.Parameter(torch.ones(emb_dim))\\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\\n    def forward(self, x):\\n        mean = x.mean(dim=-1, keepdim=True)\\n        var = x.var(dim=-1, keepdim=True, unbiased=False)\\n        norm_x = (x - mean) / torch.sqrt(var + self.eps)\\n        return self.scale * norm_x + self.shift\\nThis specific implementation of layer normalization operates on the last dimension of\\nthe input tensor x, which represents the embedding dimension (emb_dim). The vari-\\nable eps is a small constant (epsilon) added to the variance to prevent division by zero\\nduring normalization. The scale and shift are two trainable parameters (of the\\nsame dimension as the input) that the LLM automatically adjusts during training if it\\nis determined that doing so would improve the model’s performance on its training\\ntask. This allows the model to learn appropriate scaling and shifting that best suit the\\ndata it is processing.\\nLet’s now try the LayerNorm module in practice and apply it to the batch input:\\nListing 4.2\\nA layer normalization class\\nBiased variance \\nIn our variance calculation method, we use an implementation detail by setting\\nunbiased=False. For those curious about what this means, in the variance calcula-\\ntion, we divide by the number of inputs n in the variance formula. This approach does\\nnot apply Bessel’s correction, which typically uses n – 1 instead of n in the denomi-\\nnator to adjust for bias in sample variance estimation. This decision results in a so-\\ncalled biased estimate of the variance. For LLMs, where the embedding dimension n\\nis significantly large, the difference between using n and n – 1 is practically negligible.\\nI chose this approach to ensure compatibility with the GPT-2 model’s normalization\\nlayers and because it reflects TensorFlow’s default behavior, which was used to\\nimplement the original GPT-2 model. Using a similar setting ensures our method is\\ncompatible with the pretrained weights we will load in chapter 6.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 125}, page_content='104\\nCHAPTER 4\\nImplementing a GPT model from scratch to generate text\\nln = LayerNorm(emb_dim=5)\\nout_ln = ln(batch_example)\\nmean = out_ln.mean(dim=-1, keepdim=True)\\nvar = out_ln.var(dim=-1, unbiased=False, keepdim=True)\\nprint(\"Mean:\\\\n\", mean)\\nprint(\"Variance:\\\\n\", var)\\nThe results show that the layer normalization code works as expected and normalizes\\nthe values of each of the two inputs such that they have a mean of 0 and a variance of 1:\\nMean:\\n tensor([[    -0.0000],\\n        [     0.0000]], grad_fn=<MeanBackward1>)\\nVariance:\\n tensor([[1.0000],\\n        [1.0000]], grad_fn=<VarBackward0>)\\nWe have now covered two of the building blocks we will need to implement the GPT\\narchitecture, as shown in figure 4.7. Next, we will look at the GELU activation func-\\ntion, which is one of the activation functions used in LLMs, instead of the traditional\\nReLU function we used previously.\\nLayer normalization vs. batch normalization\\nIf you are familiar with batch normalization, a common and traditional normalization\\nmethod for neural networks, you may wonder how it compares to layer normalization.\\nUnlike batch normalization, which normalizes across the batch dimension, layer nor-\\nmalization normalizes across the feature dimension. LLMs often require significant\\n3) GELU\\nactivation\\n4) Feed forward\\nnetwork\\n2) Layer\\nnormalization\\n5) Shortcut\\nconnections\\nNext, we will implement\\ncomponents 3 and 4.\\nWe implemented a GPT\\nplaceholder model to see\\nthe overall structure of\\nthe model.\\nWe implemented\\nlayer normalization.\\n1) GPT\\nbackbone\\n6) Transformer\\nblock\\n7) Final GPT\\narchitecture\\nFigure 4.7\\nThe building blocks necessary to build the GPT architecture. So far, we \\nhave completed the GPT backbone and layer normalization. Next, we will focus on \\nGELU activation and the feed forward network.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 126}, page_content='105\\n4.3\\nImplementing a feed forward network with GELU activations\\n4.3\\nImplementing a feed forward network \\nwith GELU activations\\nNext, we will implement a small neural network submodule used as part of the trans-\\nformer block in LLMs. We begin by implementing the GELU activation function,\\nwhich plays a crucial role in this neural network submodule. \\nNOTE\\nFor additional information on implementing neural networks in\\nPyTorch, see section A.5 in appendix A.\\nHistorically, the ReLU activation function has been commonly used in deep learning\\ndue to its simplicity and effectiveness across various neural network architectures.\\nHowever, in LLMs, several other activation functions are employed beyond the tradi-\\ntional ReLU. Two notable examples are GELU (Gaussian error linear unit) and SwiGLU\\n(Swish-gated linear unit).\\n GELU and SwiGLU are more complex and smooth activation functions incorpo-\\nrating Gaussian and sigmoid-gated linear units, respectively. They offer improved per-\\nformance for deep learning models, unlike the simpler ReLU.\\n The GELU activation function can be implemented in several ways; the exact ver-\\nsion is defined as GELU(x) = x⋅Φ(x), where Φ(x) is the cumulative distribution func-\\ntion of the standard Gaussian distribution. In practice, however, it’s common to\\nimplement a computationally cheaper approximation (the original GPT-2 model was\\nalso trained with this approximation, which was found via curve fitting):\\nIn code, we can implement this function as a PyTorch module.\\nclass GELU(nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n    def forward(self, x):\\n        return 0.5 * x * (1 + torch.tanh(\\n            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \\n            (x + 0.044715 * torch.pow(x, 3))\\n        ))\\ncomputational resources, and the available hardware or the specific use case can\\ndictate the batch size during training or inference. Since layer normalization normal-\\nizes each input independently of the batch size, it offers more flexibility and stability\\nin these scenarios. This is particularly beneficial for distributed training or when\\ndeploying models in environments where resources are constrained.\\nListing 4.3\\nAn implementation of the GELU activation function'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 127}, page_content='106\\nCHAPTER 4\\nImplementing a GPT model from scratch to generate text\\nNext, to get an idea of what this GELU function looks like and how it compares to the\\nReLU function, let’s plot these functions side by side:\\nimport matplotlib.pyplot as plt\\ngelu, relu = GELU(), nn.ReLU()\\nx = torch.linspace(-3, 3, 100)    \\ny_gelu, y_relu = gelu(x), relu(x)\\nplt.figure(figsize=(8, 3))\\nfor i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\\n    plt.subplot(1, 2, i)\\n    plt.plot(x, y)\\n    plt.title(f\"{label} activation function\")\\n    plt.xlabel(\"x\")\\n    plt.ylabel(f\"{label}(x)\")\\n    plt.grid(True)\\nplt.tight_layout()\\nplt.show()\\nAs we can see in the resulting plot in figure 4.8, ReLU (right) is a piecewise linear\\nfunction that outputs the input directly if it is positive; otherwise, it outputs zero.\\nGELU (left) is a smooth, nonlinear function that approximates ReLU but with a non-\\nzero gradient for almost all negative values (except at approximately x = –0.75).\\nThe smoothness of GELU can lead to better optimization properties during training,\\nas it allows for more nuanced adjustments to the model’s parameters. In contrast,\\nReLU has a sharp corner at zero (figure 4.18, right), which can sometimes make opti-\\nmization harder, especially in networks that are very deep or have complex architec-\\ntures. Moreover, unlike ReLU, which outputs zero for any negative input, GELU\\nallows for a small, non-zero output for negative values. This characteristic means that\\nduring the training process, neurons that receive negative input can still contribute to\\nthe learning process, albeit to a lesser extent than positive inputs.\\nCreates 100 sample \\ndata points in the \\nrange –3 to 3\\nFigure 4.8\\nThe output of the GELU and ReLU plots using matplotlib. The x-axis shows the function \\ninputs and the y-axis shows the function outputs.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 128}, page_content='107\\n4.3\\nImplementing a feed forward network with GELU activations\\n Next, let’s use the GELU function to implement the small neural network module,\\nFeedForward, that we will be using in the LLM’s transformer block later.\\nclass FeedForward(nn.Module):\\n    def __init__(self, cfg):\\n        super().__init__()\\n        self.layers = nn.Sequential(\\n            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\\n            GELU(),\\n            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\\n        )\\n    def forward(self, x):\\n        return self.layers(x)\\nAs we can see, the FeedForward module is a small neural network consisting of two\\nLinear layers and a GELU activation function. In the 124-million-parameter GPT\\nmodel, it receives the input batches with tokens that have an embedding size of 768\\neach via the GPT_CONFIG_124M dictionary where GPT_CONFIG_ 124M[\"emb_dim\"] = 768.\\nFigure 4.9 shows how the embedding size is manipulated inside this small feed for-\\nward neural network when we pass it some inputs.\\nListing 4.4\\nA feed forward neural network module\\nLinear layer\\nGELU activation\\nLinear layer\\nInput:    (2, 3, 768)\\nOutput: (2, 3, 3072)\\nInput:    (2, 3, 3072)\\nOutput: (2, 3, 3072)\\nInput:    (2, 3, 3072)\\nOutput: (2, 3, 768)\\nThe ﬁrst linear layer\\nincreases the embedding\\ndimension by a factor of 4.\\nInput tensor with\\nshape (2, 3, 768)\\nThe three values represent\\nthe batch size (2), number\\nof tokens (3), and embedding\\nsize (768).\\nOutput tensor with\\nshape (2, 3, 768)\\nThe second linear layer\\ndecreases the embedding\\ndimension by a factor of 4.\\nFigure 4.9\\nAn overview of the connections between the layers of the \\nfeed forward neural network. This neural network can accommodate \\nvariable batch sizes and numbers of tokens in the input. However, the \\nembedding size for each token is determined and fixed when initializing \\nthe weights.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 129}, page_content='108\\nCHAPTER 4\\nImplementing a GPT model from scratch to generate text\\nFollowing the example in figure 4.9, let’s initialize a new FeedForward module with a\\ntoken embedding size of 768 and feed it a batch input with two samples and three\\ntokens each:\\nffn = FeedForward(GPT_CONFIG_124M)\\nx = torch.rand(2, 3, 768)         \\nout = ffn(x)\\nprint(out.shape)\\nAs we can see, the shape of the output tensor is the same as that of the input tensor:\\ntorch.Size([2, 3, 768])\\nThe FeedForward module plays a crucial role in enhancing the model’s ability to learn\\nfrom and generalize the data. Although the input and output dimensions of this\\nmodule are the same, it internally expands the embedding dimension into a higher-\\ndimensional space through the first linear layer, as illustrated in figure 4.10. This expan-\\nsion is followed by a nonlinear GELU activation and then a contraction back to the orig-\\ninal dimension with the second linear transformation. Such a design allows for the\\nexploration of a richer representation space.\\nMoreover, the uniformity in input and output dimensions simplifies the architecture\\nby enabling the stacking of multiple layers, as we will do later, without the need to\\nadjust dimensions between them, thus making the model more scalable.\\nCreates sample input \\nwith batch dimension 2\\n...\\n...\\n...\\nInputs\\nLinear layer 1\\nOutputs\\nLinear layer 2\\nThe inputs are projected into\\na four-times larger space via\\nthe ﬁrst linear layer.\\nThe second linear layer shrinks the\\noutputs by a factor of 4, so that they\\nmatch the original input dimensions.\\nFigure 4.10\\nAn illustration of the expansion and contraction of the layer outputs in the feed \\nforward neural network. First, the inputs expand by a factor of 4 from 768 to 3,072 values. Then, \\nthe second layer compresses the 3,072 values back into a 768-dimensional representation.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 130}, page_content='109\\n4.4\\nAdding shortcut connections\\n As figure 4.11 shows, we have now implemented most of the LLM’s building blocks.\\nNext, we will go over the concept of shortcut connections that we insert between dif-\\nferent layers of a neural network, which are important for improving the training\\nperformance in deep neural network architectures.\\n4.4\\nAdding shortcut connections\\nLet’s discuss the concept behind shortcut connections, also known as skip or residual\\nconnections. Originally, shortcut connections were proposed for deep networks in\\ncomputer vision (specifically, in residual networks) to mitigate the challenge of van-\\nishing gradients. The vanishing gradient problem refers to the issue where gradients\\n(which guide weight updates during training) become progressively smaller as they\\npropagate backward through the layers, making it difficult to effectively train earlier\\nlayers.\\n Figure 4.12 shows that a shortcut connection creates an alternative, shorter path\\nfor the gradient to flow through the network by skipping one or more layers, which is\\nachieved by adding the output of one layer to the output of a later layer. This is why\\nthese connections are also known as skip connections. They play a crucial role in pre-\\nserving the flow of gradients during the backward pass in training. \\n In the following list, we implement the neural network in figure 4.12 to see how\\nwe can add shortcut connections in the forward method.\\n \\n \\n \\n \\n \\n \\n3) GELU\\nactivation\\n4) Feed forward\\nnetwork\\n2) Layer\\nnormalization\\n5) Shortcut\\nconnections\\nNext, we implement shortcut\\nconnections so that we can\\nassemble the transformer block.\\nWe implemented building\\nblocks 2-4, which we need for\\nimplementing a GPT model.\\n1) GPT\\nbackbone\\n6) Transformer\\nblock\\n7) Final GPT\\narchitecture\\nFigure 4.11\\nThe building blocks necessary to build the GPT architecture. The black checkmarks \\nindicating those we have already covered.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 131}, page_content='110\\nCHAPTER 4\\nImplementing a GPT model from scratch to generate text\\nclass ExampleDeepNeuralNetwork(nn.Module):\\n    def __init__(self, layer_sizes, use_shortcut):\\n        super().__init__()\\n        self.use_shortcut = use_shortcut\\n        self.layers = nn.ModuleList([      \\nListing 4.5\\nA neural network to illustrate shortcut connections\\nGELU\\nLinear\\nGELU\\nLinear\\nGELU\\nLinear\\n[1.0,  0.0,  -1.0]\\n[1.0,  0.0,  -1.0]\\nGELU\\nLinear\\nGELU\\nLinear\\nLayer 5\\nGELU\\nLinear\\nGELU\\nLinear\\nGELU\\nLinear\\n[1.0,  0.0,  -1.0]\\n[1.0,  0.0,  -1.0]\\n[1.0,  0.0,  -1.0]\\n[1.0,  0.0,  -1.0]\\n[1.0,  0.0,  -1.0]\\nGELU\\nLinear\\nGELU\\nLinear\\nDeep neural network\\nDeep neural network with\\nshortcut connections\\nGradient: 0.0002\\nLayer 4\\nGradient: 0.0001\\nLayer 3\\nGradient: 0.0007\\nLayer 2\\nGradient: 0.0013\\nLayer 1\\nGradient: 0.0050\\nLayer 5\\nGradient: 1.32\\nLayer 4\\nGradient: 0.26\\nLayer 3\\nGradient: 0.32\\nLayer 2\\nGradient: 0.20\\nLayer 1\\nGradient: 0.22\\nShortcut connection\\nadds input values to\\nthe outputs of layer 1\\nIn very deep networks, the\\ngradient values in early layers\\nbecome vanishingly small\\nThe shortcut connections\\nhelp with maintaining\\nrelatively large gradient\\nvalues even in early layers\\nFigure 4.12\\nA comparison between a deep neural network consisting of five layers without (left) and with \\nshortcut connections (right). Shortcut connections involve adding the inputs of a layer to its outputs, effectively \\ncreating an alternate path that bypasses certain layers. The gradients denote the mean absolute gradient at each \\nlayer, which we compute in listing 4.5.\\nImplements \\nfive layers'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 132}, page_content='111\\n4.4\\nAdding shortcut connections\\n            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), \\n                          GELU()),\\n            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), \\n                          GELU()),\\n            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), \\n                          GELU()),\\n            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), \\n                          GELU()),\\n            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), \\n                          GELU())\\n        ])\\n    def forward(self, x):\\n        for layer in self.layers:\\n            layer_output = layer(x)        \\n            if self.use_shortcut and x.shape == layer_output.shape:   \\n                x = x + layer_output\\n            else:\\n                x = layer_output\\n        return x\\nThe code implements a deep neural network with five layers, each consisting of a\\nLinear layer and a GELU activation function. In the forward pass, we iteratively pass the\\ninput through the layers and optionally add the shortcut connections if the self.use_\\nshortcut attribute is set to True. \\n Let’s use this code to initialize a neural network without shortcut connections.\\nEach layer will be initialized such that it accepts an example with three input values\\nand returns three output values. The last layer returns a single output value:\\nlayer_sizes = [3, 3, 3, 3, 3, 1]  \\nsample_input = torch.tensor([[1., 0., -1.]])\\ntorch.manual_seed(123)                           \\nmodel_without_shortcut = ExampleDeepNeuralNetwork(\\n    layer_sizes, use_shortcut=False\\n)\\nNext, we implement a function that computes the gradients in the model’s back-\\nward pass:\\ndef print_gradients(model, x):\\n    output = model(x)            \\n    target = torch.tensor([[0.]])\\n    loss = nn.MSELoss()\\n    loss = loss(output, target)   \\n    loss.backward()         \\nCompute the \\noutput of the \\ncurrent layer\\nCheck if\\nshortcut can\\nbe applied\\nSpecifies random seed \\nfor the initial weights \\nfor reproducibility\\nForward pass\\nCalculates loss based \\non how close the target \\nand output are\\nBackward pass to \\ncalculate the gradients'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 133}, page_content='112\\nCHAPTER 4\\nImplementing a GPT model from scratch to generate text\\n    for name, param in model.named_parameters():\\n        if \\'weight\\' in name:\\n            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\\nThis code specifies a loss function that computes how close the model output and a\\nuser-specified target (here, for simplicity, the value 0) are. Then, when calling\\nloss.backward(), PyTorch computes the loss gradient for each layer in the model. We\\ncan iterate through the weight parameters via model.named_parameters(). Suppose we\\nhave a 3 × 3 weight parameter matrix for a given layer. In that case, this layer will have\\n3 × 3 gradient values, and we print the mean absolute gradient of these 3 × 3 gradient\\nvalues to obtain a single gradient value per layer to compare the gradients between\\nlayers more easily.\\n In short, the .backward() method is a convenient method in PyTorch that com-\\nputes loss gradients, which are required during model training, without implement-\\ning the math for the gradient calculation ourselves, thereby making working with\\ndeep neural networks much more accessible. \\nNOTE\\nIf you are unfamiliar with the concept of gradients and neural network\\ntraining, I recommend reading sections A.4 and A.7 in appendix A.\\nLet’s now use the print_gradients function and apply it to the model without skip\\nconnections:\\nprint_gradients(model_without_shortcut, sample_input)\\nThe output is\\nlayers.0.0.weight has gradient mean of 0.00020173587836325169\\nlayers.1.0.weight has gradient mean of 0.0001201116101583466\\nlayers.2.0.weight has gradient mean of 0.0007152041653171182\\nlayers.3.0.weight has gradient mean of 0.001398873864673078\\nlayers.4.0.weight has gradient mean of 0.005049646366387606\\nThe output of the print_gradients function shows, the gradients become smaller\\nas we progress from the last layer (layers.4) to the first layer (layers.0), which is\\na phenomenon called the vanishing gradient problem.\\n Let’s now instantiate a model with skip connections and see how it compares:\\ntorch.manual_seed(123)\\nmodel_with_shortcut = ExampleDeepNeuralNetwork(\\n    layer_sizes, use_shortcut=True\\n)\\nprint_gradients(model_with_shortcut, sample_input)\\nThe output is'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 134}, page_content='113\\n4.5\\nConnecting attention and linear layers in a transformer block\\nlayers.0.0.weight has gradient mean of 0.22169792652130127\\nlayers.1.0.weight has gradient mean of 0.20694105327129364\\nlayers.2.0.weight has gradient mean of 0.32896995544433594\\nlayers.3.0.weight has gradient mean of 0.2665732502937317\\nlayers.4.0.weight has gradient mean of 1.3258541822433472\\nThe last layer (layers.4) still has a larger gradient than the other layers. However,\\nthe gradient value stabilizes as we progress toward the first layer (layers.0) and\\ndoesn’t shrink to a vanishingly small value. \\n In conclusion, shortcut connections are important for overcoming the limitations\\nposed by the vanishing gradient problem in deep neural networks. Shortcut connec-\\ntions are a core building block of very large models such as LLMs, and they will help\\nfacilitate more effective training by ensuring consistent gradient flow across layers\\nwhen we train the GPT model in the next chapter. \\n Next, we’ll connect all of the previously covered concepts (layer normalization,\\nGELU activations, feed forward module, and shortcut connections) in a transformer\\nblock, which is the final building block we need to code the GPT architecture.\\n4.5\\nConnecting attention and linear layers \\nin a transformer block\\nNow, let’s implement the transformer block, a fundamental building block of GPT and\\nother LLM architectures. This block, which is repeated a dozen times in the 124-million-\\nparameter GPT-2 architecture, combines several concepts we have previously covered:\\nmulti-head attention, layer normalization, dropout, feed forward layers, and GELU\\nactivations. Later, we will connect this transformer block to the remaining parts of the\\nGPT architecture.\\n Figure 4.13 shows a transformer block that combines several components, includ-\\ning the masked multi-head attention module (see chapter 3) and the FeedForward\\nmodule we previously implemented (see section 4.3). When a transformer block pro-\\ncesses an input sequence, each element in the sequence (for example, a word or sub-\\nword token) is represented by a fixed-size vector (in this case, 768 dimensions). The\\noperations within the transformer block, including multi-head attention and feed for-\\nward layers, are designed to transform these vectors in a way that preserves their\\ndimensionality.\\n The idea is that the self-attention mechanism in the multi-head attention block iden-\\ntifies and analyzes relationships between elements in the input sequence. In contrast,\\nthe feed forward network modifies the data individually at each position. This combina-\\ntion not only enables a more nuanced understanding and processing of the input but\\nalso enhances the model’s overall capacity for handling complex data patterns.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 135}, page_content='114\\nCHAPTER 4\\nImplementing a GPT model from scratch to generate text\\nMasked multi-head\\nattention\\nLayerNorm 2\\nFeed forward\\nLayerNorm 1\\nShortcut connection\\nLinear layer\\nGELU activation\\nLinear layer\\n[[0.2961, ..., 0.4604],\\n[0.2238, ..., 0.7598],\\n[0.6945, ..., 0.5963],\\n[0.0890, ..., 0.5833]]\\nEvery\\neffort\\nmoves\\nyou\\nEach row is a 768-dimensional\\nvector representing an embedded\\ninput token.\\nA view into the “ eed\\nf\\nforward” block\\nThe transformer\\nblock\\n[[-0.0256, ...,  0.6890],\\n[-0.0178, ...,  0.7431],\\n[ 0.4558, ...,  0.7814],\\n[ 0.0702, ...,  0.7134]]\\nOutputs have the same\\nform and dimensions\\nas the inputs.\\nThis tensor represents an\\nembedded text sample\\nthat serves as input to the\\ntransformer block.\\nDropout\\nDropout\\nThe input tokens to be\\nembedded\\nFigure 4.13\\nAn illustration of a transformer block. Input tokens have been embedded into 768-\\ndimensional vectors. Each row corresponds to one token’s vector representation. The outputs of the \\ntransformer block are vectors of the same dimension as the input, which can then be fed into \\nsubsequent layers in an LLM.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 136}, page_content='115\\n4.5\\nConnecting attention and linear layers in a transformer block\\nWe can create the TransformerBlock in code.\\nfrom chapter03 import MultiHeadAttention\\nclass TransformerBlock(nn.Module):\\n    def __init__(self, cfg):\\n        super().__init__()\\n        self.att = MultiHeadAttention(\\n            d_in=cfg[\"emb_dim\"],\\n            d_out=cfg[\"emb_dim\"],\\n            context_length=cfg[\"context_length\"],\\n            num_heads=cfg[\"n_heads\"], \\n            dropout=cfg[\"drop_rate\"],\\n            qkv_bias=cfg[\"qkv_bias\"])\\n        self.ff = FeedForward(cfg)\\n        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\\n        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\\n        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\\n    def forward(self, x):\\n                           \\n        shortcut = x\\n        x = self.norm1(x)\\n        x = self.att(x)\\n        x = self.drop_shortcut(x)\\n        x = x + shortcut     \\n        shortcut = x        \\n        x = self.norm2(x)\\n        x = self.ff(x)\\n        x = self.drop_shortcut(x)\\n        x = x + shortcut     \\n        return x\\nThe given code defines a TransformerBlock class in PyTorch that includes a multi-head\\nattention mechanism (MultiHeadAttention) and a feed forward network (Feed-\\nForward), both configured based on a provided configuration dictionary (cfg), such\\nas GPT_CONFIG_124M.\\n Layer normalization (LayerNorm) is applied before each of these two components,\\nand dropout is applied after them to regularize the model and prevent overfitting. This\\nis also known as Pre-LayerNorm. Older architectures, such as the original transformer\\nmodel, applied layer normalization after the self-attention and feed forward networks\\ninstead, known as Post-LayerNorm, which often leads to worse training dynamics.\\nListing 4.6\\nThe transformer block component of GPT\\nShortcut connection \\nfor attention block\\nAdd the original \\ninput back\\nShortcut connection \\nfor feed forward block\\nAdds the original \\ninput back'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 137}, page_content='116\\nCHAPTER 4\\nImplementing a GPT model from scratch to generate text\\n The class also implements the forward pass, where each component is followed by\\na shortcut connection that adds the input of the block to its output. This critical fea-\\nture helps gradients flow through the network during training and improves the\\nlearning of deep models (see section 4.4).\\n Using the GPT_CONFIG_124M dictionary we defined earlier, let’s instantiate a trans-\\nformer block and feed it some sample data:\\ntorch.manual_seed(123)\\nx = torch.rand(2, 4, 768)                  \\nblock = TransformerBlock(GPT_CONFIG_124M)\\noutput = block(x)\\nprint(\"Input shape:\", x.shape)\\nprint(\"Output shape:\", output.shape)\\nThe output is\\nInput shape: torch.Size([2, 4, 768])\\nOutput shape: torch.Size([2, 4, 768])\\nAs we can see, the transformer block maintains the input dimensions in its output, indi-\\ncating that the transformer architecture processes sequences of data without altering\\ntheir shape throughout the network.\\n The preservation of shape throughout the transformer block architecture is not\\nincidental but a crucial aspect of its design. This design enables its effective applica-\\ntion across a wide range of sequence-to-sequence tasks, where each output vector\\ndirectly corresponds to an input vector, maintaining a one-to-one relationship. How-\\never, the output is a context vector that encapsulates information from the entire\\ninput sequence (see chapter 3). This means that while the physical dimensions of the\\nsequence (length and feature size) remain unchanged as it passes through the trans-\\nformer block, the content of each output vector is re-encoded to integrate contextual\\ninformation from across the entire input sequence.\\n With the transformer block implemented, we now have all the building blocks\\nneeded to implement the GPT architecture. As illustrated in figure 4.14, the trans-\\nformer block combines layer normalization, the feed forward network, GELU activa-\\ntions, and shortcut connections. As we will eventually see, this transformer block will\\nmake up the main component of the GPT architecture.\\n \\n \\n \\n \\nCreates sample input of shape \\n[batch_size, num_tokens, emb_dim]'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 138}, page_content='117\\n4.6\\nCoding the GPT model\\n4.6\\nCoding the GPT model\\nWe started this chapter with a big-picture overview of a GPT architecture that we\\ncalled DummyGPTModel. In this DummyGPTModel code implementation, we showed the\\ninput and outputs to the GPT model, but its building blocks remained a black box\\nusing a DummyTransformerBlock and DummyLayerNorm class as placeholders.\\n Let’s now replace the DummyTransformerBlock and DummyLayerNorm placeholders\\nwith the real TransformerBlock and LayerNorm classes we coded previously to assem-\\nble a fully working version of the original 124-million-parameter version of GPT-2. In\\nchapter 5, we will pretrain a GPT-2 model, and in chapter 6, we will load in the pre-\\ntrained weights from OpenAI.\\n Before we assemble the GPT-2 model in code, let’s look at its overall structure, as\\nshown in figure 4.15, which includes all the concepts we have covered so far. As we can\\nsee, the transformer block is repeated many times throughout a GPT model architec-\\nture. In the case of the 124-million-parameter GPT-2 model, it’s repeated 12 times,\\nwhich we specify via the n_layers entry in the GPT_CONFIG_124M dictionary. This\\ntransform block is repeated 48 times in the largest GPT-2 model with 1,542 million\\nparameters.\\n The output from the final transformer block then goes through a final layer normal-\\nization step before reaching the linear output layer. This layer maps the transformer’s\\noutput to a high-dimensional space (in this case, 50,257 dimensions, corresponding to\\nthe model’s vocabulary size) to predict the next token in the sequence.\\n Let’s now code the architecture in figure 4.15.\\n \\n \\n \\n \\nWe have completed building\\nblocks\\n6, which we need to\\n1–\\nimplement a GPT model.\\n3) GELU\\nactivation\\n4) Feed forward\\nnetwork\\n2) Layer\\nnormalization\\n5) Shortcut\\nconnections\\nNext, we will assemble\\nthese building blocks to\\ncreate a GPT model.\\n1) GPT\\nbackbone\\n6) Transformer\\nblock\\n7) Final GPT\\narchitecture\\nFigure 4.14\\nThe building blocks necessary to build the GPT architecture. The black \\nchecks indicate the blocks we have completed.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 139}, page_content='118\\nCHAPTER 4\\nImplementing a GPT model from scratch to generate text\\nGPT\\nmodel\\nMasked multi-head\\nattention\\nLayerNorm 2\\nFeed forward\\nLayerNorm 1\\nDropout\\nDropout\\nToken embedding layer\\nTokenized text\\nEvery effort moves you\\n[[-0.0055, ..., -0.4747],\\n[ 0.2663, ..., -0.4224],\\n[ 1.1146, ...,  0.0276],\\n[-0.8239, ..., -0.3993]]\\nFinal LayerNorm\\nLinear output layer\\nThe transformer block\\nis repeated 12 times.\\n12\\nThe GPT code implementation\\nincludes a token embedding\\nand positional embedding layer\\n(see chapter 2).\\nThe last linear layer embeds\\neach token vector into a 50,257-\\ndimensional embedding, where\\n50,257 is the size of the\\nvocabulary.\\nA 4\\n50,257 dimensional\\n×\\n–\\ntensor\\nThe goal is for these embeddings to\\nbe converted back into text such\\nthat the last row represents the\\nword the model is supposed to\\ngenerate (here, the word “forward”).\\nDropout\\nPositional embedding layer\\nFigure 4.15\\nAn overview of the GPT model architecture showing the flow of data through the GPT model. \\nStarting from the bottom, tokenized text is first converted into token embeddings, which are then augmented \\nwith positional embeddings. This combined information forms a tensor that is passed through a series of \\ntransformer blocks shown in the center (each containing multi-head attention and feed forward neural network \\nlayers with dropout and layer normalization), which are stacked on top of each other and repeated 12 times.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 140}, page_content='119\\n4.6\\nCoding the GPT model\\nclass GPTModel(nn.Module):\\n    def __init__(self, cfg):\\n        super().__init__()\\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\\n        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\\n        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\\n        \\n        self.trf_blocks = nn.Sequential(\\n            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\\n       \\n        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\\n        self.out_head = nn.Linear(\\n            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\\n        )\\n    def forward(self, in_idx):\\n        batch_size, seq_len = in_idx.shape\\n        tok_embeds = self.tok_emb(in_idx)\\n                                            \\n        pos_embeds = self.pos_emb(\\n            torch.arange(seq_len, device=in_idx.device)\\n        )\\n        x = tok_embeds + pos_embeds\\n        x = self.drop_emb(x)\\n        x = self.trf_blocks(x)\\n        x = self.final_norm(x)\\n        logits = self.out_head(x)\\n        return logits\\nThanks to the TransformerBlock class, the GPTModel class is relatively small and\\ncompact. \\n The __init__ constructor of this GPTModel class initializes the token and posi-\\ntional embedding layers using the configurations passed in via a Python dictionary,\\ncfg. These embedding layers are responsible for converting input token indices into\\ndense vectors and adding positional information (see chapter 2). \\n Next, the __init__ method creates a sequential stack of TransformerBlock mod-\\nules equal to the number of layers specified in cfg. Following the transformer blocks,\\na LayerNorm layer is applied, standardizing the outputs from the transformer blocks to\\nstabilize the learning process. Finally, a linear output head without bias is defined,\\nwhich projects the transformer’s output into the vocabulary space of the tokenizer to\\ngenerate logits for each token in the vocabulary. \\n The forward method takes a batch of input token indices, computes their embed-\\ndings, applies the positional embeddings, passes the sequence through the transformer\\nblocks, normalizes the final output, and then computes the logits, representing the next\\ntoken’s unnormalized probabilities. We will convert these logits into tokens and text\\noutputs in the next section. \\nListing 4.7\\nThe GPT model architecture implementation\\nThe device setting will allow \\nus to train the model on a CPU \\nor GPU, depending on which \\ndevice the input data sits on.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 141}, page_content='120\\nCHAPTER 4\\nImplementing a GPT model from scratch to generate text\\n Let’s now initialize the 124-million-parameter GPT model using the GPT_CONFIG_\\n124M dictionary we pass into the cfg parameter and feed it with the batch text input\\nwe previously created:\\ntorch.manual_seed(123)\\nmodel = GPTModel(GPT_CONFIG_124M)\\nout = model(batch)\\nprint(\"Input batch:\\\\n\", batch)\\nprint(\"\\\\nOutput shape:\", out.shape)\\nprint(out)\\nThis code prints the contents of the input batch followed by the output tensor:\\nInput batch:\\n tensor([[6109,  3626,  6100,   345],     \\n         [6109,  1110,  6622,   257]])    \\nOutput shape: torch.Size([2, 4, 50257])\\ntensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\\n         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\\n         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\\n         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\\n        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\\n         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\\n         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\\n         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\\n       grad_fn=<UnsafeViewBackward0>)\\nAs we can see, the output tensor has the shape [2, 4, 50257], since we passed in two\\ninput texts with four tokens each. The last dimension, 50257, corresponds to the\\nvocabulary size of the tokenizer. Later, we will see how to convert each of these 50,257-\\ndimensional output vectors back into tokens.\\n Before we move on to coding the function that converts the model outputs into\\ntext, let’s spend a bit more time with the model architecture itself and analyze its size.\\nUsing the numel() method, short for “number of elements,” we can collect the total\\nnumber of parameters in the model’s parameter tensors:\\ntotal_params = sum(p.numel() for p in model.parameters())\\nprint(f\"Total number of parameters: {total_params:,}\")\\nThe result is\\nTotal number of parameters: 163,009,536\\nNow, a curious reader might notice a discrepancy. Earlier, we spoke of initializing\\na 124-million-parameter GPT model, so why is the actual number of parameters\\n163 million?\\nToken IDs of text 1\\nToken IDs of text 2'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 142}, page_content='121\\n4.6\\nCoding the GPT model\\n The reason is a concept called weight tying, which was used in the original GPT-2\\narchitecture. It means that the original GPT-2 architecture reuses the weights from\\nthe token embedding layer in its output layer. To understand better, let’s take a look at\\nthe shapes of the token embedding layer and linear output layer that we initialized on\\nthe model via the GPTModel earlier:\\nprint(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\\nprint(\"Output layer shape:\", model.out_head.weight.shape)\\nAs we can see from the print outputs, the weight tensors for both these layers have the\\nsame shape:\\nToken embedding layer shape: torch.Size([50257, 768])\\nOutput layer shape: torch.Size([50257, 768])\\nThe token embedding and output layers are very large due to the number of rows for\\nthe 50,257 in the tokenizer’s vocabulary. Let’s remove the output layer parameter\\ncount from the total GPT-2 model count according to the weight tying:\\ntotal_params_gpt2 = (\\n    total_params - sum(p.numel()\\n    for p in model.out_head.parameters())\\n)\\nprint(f\"Number of trainable parameters \"\\n      f\"considering weight tying: {total_params_gpt2:,}\"\\n)\\nThe output is\\nNumber of trainable parameters considering weight tying: 124,412,160\\nAs we can see, the model is now only 124 million parameters large, matching the orig-\\ninal size of the GPT-2 model. \\n Weight tying reduces the overall memory footprint and computational complexity\\nof the model. However, in my experience, using separate token embedding and out-\\nput layers results in better training and model performance; hence, we use separate\\nlayers in our GPTModel implementation. The same is true for modern LLMs. However,\\nwe will revisit and implement the weight tying concept later in chapter 6 when we load\\nthe pretrained weights from OpenAI.\\nExercise 4.1 Number of parameters in feed forward and attention modules \\nCalculate and compare the number of parameters that are contained in the feed for-\\nward module and those that are contained in the multi-head attention module.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 143}, page_content='122\\nCHAPTER 4\\nImplementing a GPT model from scratch to generate text\\nLastly, let’s compute the memory requirements of the 163 million parameters in our\\nGPTModel object:\\ntotal_size_bytes = total_params * 4      \\ntotal_size_mb = total_size_bytes / (1024 * 1024)    \\nprint(f\"Total size of the model: {total_size_mb:.2f} MB\")\\nThe result is\\nTotal size of the model: 621.83 MB\\nIn conclusion, by calculating the memory requirements for the 163 million parame-\\nters in our GPTModel object and assuming each parameter is a 32-bit float taking up 4\\nbytes, we find that the total size of the model amounts to 621.83 MB, illustrating the\\nrelatively large storage capacity required to accommodate even relatively small LLMs. \\n Now that we’ve implemented the GPTModel architecture and saw that it outputs\\nnumeric tensors of shape [batch_size, num_tokens, vocab_size], let’s write the code\\nto convert these output tensors into text.\\n4.7\\nGenerating text\\nWe will now implement the code that converts the tensor outputs of the GPT model\\nback into text. Before we get started, let’s briefly review how a generative model like\\nan LLM generates text one word (or token) at a time.\\n Figure 4.16 illustrates the step-by-step process by which a GPT model generates\\ntext given an input context, such as “Hello, I am.” With each iteration, the input con-\\ntext grows, allowing the model to generate coherent and contextually appropriate\\ntext. By the sixth iteration, the model has constructed a complete sentence: “Hello, I\\nam a model ready to help.” We’ve seen that our current GPTModel implementation\\noutputs tensors with shape [batch_size, num_token, vocab_size]. Now the question\\nis: How does a GPT model go from these output tensors to the generated text? \\n The process by which a GPT model goes from output tensors to generated text\\ninvolves several steps, as illustrated in figure 4.17. These steps include decoding the\\nExercise 4.2 Initializing larger GPT models \\nWe initialized a 124-million-parameter GPT model, which is known as “GPT-2 small.”\\nWithout making any code modifications besides updating the configuration file, use\\nthe GPTModel class to implement GPT-2 medium (using 1,024-dimensional embed-\\ndings, 24 transformer blocks, 16 multi-head attention heads), GPT-2 large (1,280-\\ndimensional embeddings, 36 transformer blocks, 20 multi-head attention heads),\\nand GPT-2 XL (1,600-dimensional embeddings, 48 transformer blocks, 25 multi-head\\nattention heads). As a bonus, calculate the total number of parameters in each GPT\\nmodel.\\nCalculates the total size in \\nbytes (assuming float32, 4 \\nbytes per parameter)\\nConverts to \\nmegabytes'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 144}, page_content='123\\n4.7\\nGenerating text\\noutput tensors, selecting tokens based on a probability distribution, and converting\\nthese tokens into human-readable text.\\n The next-token generation process detailed in figure 4.17 illustrates a single step\\nwhere the GPT model generates the next token given its input. In each step, the model\\noutputs a matrix with vectors representing potential next tokens. The vector corre-\\nsponding to the next token is extracted and converted into a probability distribution via\\nthe softmax function. Within the vector containing the resulting probability scores, the\\nindex of the highest value is located, which translates to the token ID. This token ID is\\nthen decoded back into text, producing the next token in the sequence. Finally, this\\ntoken is appended to the previous inputs, forming a new input sequence for the subse-\\nquent iteration. This step-by-step process enables the model to generate text sequen-\\ntially, building coherent phrases and sentences from the initial input context.\\n In practice, we repeat this process over many iterations, such as shown in figure 4.16,\\nuntil we reach a user-specified number of generated tokens. In code, we can imple-\\nment the token-generation process as shown in the following listing.\\n \\n \\nHello , I am a model\\nHello , I am a model\\nHello , I am a model\\nHello , I am a model\\nHello , I am a model\\nHello , I am a model\\nThe next generated\\ntoken\\nThe input context\\nfor the model\\nThe token generated in\\nthe previous round is\\nappended to the input for\\nthe next iteration\\nThe input context\\ngrows in each\\niteration\\n1st iteration:\\n2nd iteration:\\n3rd iteration:\\n6th iteration:\\nHello , I am a\\nHello , I am a model ready\\nHello , I am a model ready to help .\\nFigure 4.16\\nThe step-by-step process by which an LLM generates text, one \\ntoken at a time. Starting with an initial input context (“Hello, I am”), the \\nmodel predicts a subsequent token during each iteration, appending it to the \\ninput context for the next round of prediction. As shown, the first iteration \\nadds “a,” the second “model,” and the third “ready,” progressively building \\nthe sentence.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 145}, page_content='124\\nCHAPTER 4\\nImplementing a GPT model from scratch to generate text\\ndef generate_text_simple(model, idx,                \\n                         max_new_tokens, context_size): \\n    for _ in range(max_new_tokens):\\n        idx_cond = idx[:, -context_size:]   \\n        with torch.no_grad():\\n            logits = model(idx_cond)\\n       \\n        logits = logits[:, -1, :]                   \\n        probas = torch.softmax(logits, dim=-1)          \\n        idx_next = torch.argmax(probas, dim=-1, keepdim=True)   \\n        idx = torch.cat((idx, idx_next), dim=1)    \\n    return idx\\nListing 4.8\\nA function for the GPT model to generate text\\n[[-0.2949, ..., -0.8141],\\n[ 1.2199, ..., -0.3599],\\n[ 1.0446, ...,  0.0020],\\n[-0.4929, ..., -0.6093]]\\nGPT\\n[15496,\\n11,\\n314,\\n716]\\n[-0.4929, ..., -0.6093]]\\n[-0.4929, ..., 2.4812, ..., -0.6093]]\\n0\\n257\\n50257\\n1. Encodes text input\\ninto four token IDs\\n2. The GPT model returns a matrix consisting\\nof four vectors (rows), where each vector\\nhas 50257 dimensions (columns).\\n3. Extracts the last vector,\\nwhich corresponds to\\nthe next token that the\\nGPT model is supposed\\nto generate\\n5. Indentifies the index\\nposition of the\\nlargest value, which\\nalso represents the\\ntoken ID\\nIf the largest element\\nis at position 257, we\\nobtain token ID 257.\\nToken ID decoded\\ninto text\\n6. Appends token to the previous\\ninputs for the next round\\n[ 0.0001, ..., 0.0200, ...,  0.0001]]\\n4. Converts logits\\ninto probability\\ndistribution\\nusing the softmax\\nfunction\\nSoftmax\\n257\\nProbabilities:\\nLogits:\\n\"Hello\"\\n\",\"\\n\"I\"\\n\"am\"\\n\"a\"\\n\"a\"\\nFigure 4.17\\nThe mechanics of text generation in a GPT model by showing a single iteration in the token \\ngeneration process. The process begins by encoding the input text into token IDs, which are then fed into the \\nGPT model. The outputs of the model are then converted back into text and appended to the original input text.\\nidx is a (batch, n_tokens) \\narray of indices in the \\ncurrent context.\\nCrops current context if it exceeds the supported context size, \\ne.g., if LLM supports only 5 tokens, and the context size is 10, \\nthen only the last 5 tokens are used as context\\nFocuses only on the last time step, \\nso that (batch, n_token, vocab_size) \\nbecomes (batch, vocab_size)\\nprobas has \\nshape (batch, \\nvocab_size).\\nidx_next has \\nshape (batch, 1).\\nAppends sampled index to the\\nrunning sequence, where idx has\\nshape (batch, n_tokens+1)'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 146}, page_content='125\\n4.7\\nGenerating text\\nThis code demonstrates a simple implementation of a generative loop for a lan-\\nguage model using PyTorch. It iterates for a specified number of new tokens to be\\ngenerated, crops the current context to fit the model’s maximum context size, com-\\nputes predictions, and then selects the next token based on the highest probability\\nprediction. \\n To code the generate_text_simple function, we use a softmax function to con-\\nvert the logits into a probability distribution from which we identify the position with\\nthe highest value via torch.argmax. The softmax function is monotonic, meaning it\\npreserves the order of its inputs when transformed into outputs. So, in practice, the\\nsoftmax step is redundant since the position with the highest score in the softmax out-\\nput tensor is the same position in the logit tensor. In other words, we could apply the\\ntorch.argmax function to the logits tensor directly and get identical results. However,\\nI provide the code for the conversion to illustrate the full process of transforming log-\\nits to probabilities, which can add additional intuition so that the model generates the\\nmost likely next token, which is known as greedy decoding.\\n When we implement the GPT training code in the next chapter, we will use addi-\\ntional sampling techniques to modify the softmax outputs such that the model doesn’t\\nalways select the most likely token. This introduces variability and creativity in the gen-\\nerated text. \\n This process of generating one token ID at a time and appending it to the context\\nusing the generate_text_simple function is further illustrated in figure 4.18. (The\\ntoken ID generation process for each iteration is detailed in figure 4.17.) We generate\\nthe token IDs in an iterative fashion. For instance, in iteration 1, the model is pro-\\nvided with the tokens corresponding to “Hello, I am,” predicts the next token (with\\nID 257, which is “a”), and appends it to the input. This process is repeated until the\\nmodel produces the complete sentence “Hello, I am a model ready to help” after six\\niterations.\\n Let’s now try out the generate_text_simple function with the \"Hello, I am\" con-\\ntext as model input. First, we encode the input context into token IDs:\\nstart_context = \"Hello, I am\"\\nencoded = tokenizer.encode(start_context)\\nprint(\"encoded:\", encoded)\\nencoded_tensor = torch.tensor(encoded).unsqueeze(0)   \\nprint(\"encoded_tensor.shape:\", encoded_tensor.shape)\\nThe encoded IDs are\\nencoded: [15496, 11, 314, 716]\\nencoded_tensor.shape: torch.Size([1, 4])\\nAdds batch \\ndimension'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 147}, page_content='126\\nCHAPTER 4\\nImplementing a GPT model from scratch to generate text\\nNext, we put the model into .eval() mode. This disables random components like\\ndropout, which are only used during training, and use the generate_text_simple\\nfunction on the encoded input tensor:\\nmodel.eval()                 \\nout = generate_text_simple(\\n    model=model,\\n    idx=encoded_tensor, \\n    max_new_tokens=6, \\n    context_size=GPT_CONFIG_124M[\"context_length\"]\\n)\\nprint(\"Output:\", out)\\nprint(\"Output length:\", len(out[0]))\\nThe resulting output token IDs are\\nOutput: tensor([[15496,    11,   314,   716, 27018, 24086, 47843,\\n30961, 42348,  7267]])\\nOutput length: 10\\nHello, I am a model ready to help.\\nHello\\n[15496, 11, 314, 716]\\n[15496, 11, 314, 716, 257]\\n[15496, 11, 314, 716, 257, 2746]\\nIteration\\n1\\n2\\n3\\nID\\n[257]\\n[2746]\\nPredict\\nam\\n,\\nI\\na\\nHello\\nam\\n,\\nI\\na\\nmodel\\nHello\\nam\\n,\\nI\\na\\nmodel\\n[3492]\\nready\\nAppend\\n[15496, ..., 3492, 284, 1037, 13]\\n...\\n6\\n...\\nThe initial tokens (context)\\nprovided as input to the LLM\\nThe output tokens\\nafter six iterations\\n(max_new_tokens=6)\\nThe predicted token\\nID is appended to the\\ncontext for the next round.\\nThe token IDs converted\\ninto a text representation\\nfor illustration purposes\\nFigure 4.18\\nThe six iterations of a token prediction cycle, where the model takes a sequence of initial token IDs \\nas input, predicts the next token, and appends this token to the input sequence for the next iteration. (The token \\nIDs are also translated into their corresponding text for better understanding.) \\nDisables dropout since \\nwe are not training \\nthe model'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 148}, page_content='127\\nSummary\\nUsing the .decode method of the tokenizer, we can convert the IDs back into text:\\ndecoded_text = tokenizer.decode(out.squeeze(0).tolist())\\nprint(decoded_text)\\nThe model output in text format is\\nHello, I am Featureiman Byeswickattribute argue\\nAs we can see, the model generated gibberish, which is not at all like the coherent text\\nHello, I am a model ready to help. What happened? The reason the model is unable to\\nproduce coherent text is that we haven’t trained it yet. So far, we have only implemented\\nthe GPT architecture and initialized a GPT model instance with initial random weights.\\nModel training is a large topic in itself, and we will tackle it in the next chapter.\\nSummary\\n\\uf0a1Layer normalization stabilizes training by ensuring that each layer’s outputs\\nhave a consistent mean and variance.\\n\\uf0a1Shortcut connections are connections that skip one or more layers by feeding\\nthe output of one layer directly to a deeper layer, which helps mitigate the van-\\nishing gradient problem when training deep neural networks, such as LLMs.\\n\\uf0a1Transformer blocks are a core structural component of GPT models, combin-\\ning masked multi-head attention modules with fully connected feed forward\\nnetworks that use the GELU activation function.\\n\\uf0a1GPT models are LLMs with many repeated transformer blocks that have mil-\\nlions to billions of parameters.\\n\\uf0a1GPT models come in various sizes, for example, 124, 345, 762, and 1,542 mil-\\nlion parameters, which we can implement with the same GPTModel Python class.\\n\\uf0a1The text-generation capability of a GPT-like LLM involves decoding output ten-\\nsors into human-readable text by sequentially predicting one token at a time\\nbased on a given input context. \\n\\uf0a1Without training, a GPT model generates incoherent text, which underscores\\nthe importance of model training for coherent text generation.\\nExercise 4.3 Using separate dropout parameters \\nAt the beginning of this chapter, we defined a global drop_rate setting in the GPT_\\nCONFIG_124M dictionary to set the dropout rate in various places throughout the\\nGPTModel architecture. Change the code to specify a separate dropout value for the\\nvarious dropout layers throughout the model architecture. (Hint: there are three dis-\\ntinct places where we used dropout layers: the embedding layer, shortcut layer, and\\nmulti-head attention module.)'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 149}, page_content='128\\nPretraining\\non unlabeled data\\nThus far, we have implemented the data sampling and attention mechanism and\\ncoded the LLM architecture. It is now time to implement a training function and\\npretrain the LLM. We will learn about basic model evaluation techniques to mea-\\nsure the quality of the generated text, which is a requirement for optimizing the\\nLLM during the training process. Moreover, we will discuss how to load pretrained\\nweights, giving our LLM a solid starting point for fine-tuning. Figure 5.1 lays out\\nour overall plan, highlighting what we will discuss in this chapter.\\nThis chapter covers\\n\\uf0a1Computing the training and validation set losses \\nto assess the quality of LLM-generated text \\nduring training\\n\\uf0a1Implementing a training function and pretraining \\nthe LLM\\n\\uf0a1Saving and loading model weights to continue \\ntraining an LLM\\n\\uf0a1Loading pretrained weights from OpenAI'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 150}, page_content='129\\n5.1\\nEvaluating generative text models\\n5.1\\nEvaluating generative text models\\nAfter briefly recapping the text generation from chapter 4, we will set up our LLM for\\ntext generation and then discuss basic ways to evaluate the quality of the generated text.\\nWe will then calculate the training and validation losses. Figure 5.2 shows the topics\\ncovered in this chapter, with these first three steps highlighted.\\n \\nWeight parameters \\nIn the context of LLMs and other deep learning models, weights refer to the trainable\\nparameters that the learning process adjusts. These weights are also known as\\nweight parameters or simply parameters. In frameworks like PyTorch, these weights\\nare stored in linear layers; we used these to implement the multi-head attention mod-\\nule in chapter 3 and the GPTModel in chapter 4. After initializing a layer (new_layer\\n= torch.nn.Linear(...)), we can access its weights through the .weight attri-\\nbute, new_layer.weight. Additionally, for convenience, PyTorch allows direct\\naccess to all a model’s trainable parameters, including weights and biases, through\\nthe method model.parameters(), which we will use later when implementing the\\nmodel training.\\nIn this chapter, we will\\npretrain the LLM model.\\nIn the previous chapter, we\\nimplemented a GPT-like\\nLLM architecture.\\nFor the pretraining, we will\\nimplement the training\\nloop along with model\\nevaluation metrics.\\nFinally, we load openly\\navailable pretrained\\nweights into the\\nmodel.\\n1) Data\\npreparation\\n& sampling\\n2) Attention\\nmechanism\\nBuilding an LLM\\nSTAGE 1\\nFoundation model\\nSTAGE 2\\nSTAGE 3\\nClassiﬁer\\nPersonal assistant\\nDataset with class labels\\nInstruction dataset\\n4) Pretraining\\n9) Fine-tuning\\n5) Training\\nloop\\n6) Model\\nevaluation\\n7) Load\\npretrained\\nweights\\n8) Fine-tuning\\n3) LLM\\narchitecture\\nFigure 5.1\\nThe three main stages of coding an LLM. This chapter focuses on stage 2: pretraining the LLM (step \\n4), which includes implementing the training code (step 5), evaluating the performance (step 6), and saving and \\nloading model weights (step 7).'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 151}, page_content='130\\nCHAPTER 5\\nPretraining on unlabeled data\\n5.1.1\\nUsing GPT to generate text\\nLet’s set up the LLM and briefly recap the text generation process we implemented in\\nchapter 4. We begin by initializing the GPT model that we will later evaluate and train\\nusing the GPTModel class and GPT_CONFIG_124M dictionary (see chapter 4):\\nimport torch\\nfrom chapter04 import GPTModel\\nGPT_CONFIG_124M = {\\n    \"vocab_size\": 50257,\\n    \"context_length\": 256,   \\n    \"emb_dim\": 768,\\n    \"n_heads\": 12,\\n    \"n_layers\": 12, \\n    \"drop_rate\": 0.1,      \\n    \"qkv_bias\": False\\n}\\ntorch.manual_seed(123)\\nmodel = GPTModel(GPT_CONFIG_124M)\\nmodel.eval()\\nConsidering the GPT_CONFIG_124M dictionary, the only adjustment we have made com-\\npared to the previous chapter is that we have reduced the context length (context_\\nlength) to 256 tokens. This modification reduces the computational demands of\\ntraining the model, making it possible to carry out the training on a standard laptop\\ncomputer.\\n Originally, the GPT-2 model with 124 million parameters was configured to handle\\nup to 1,024 tokens. After the training process, we will update the context size setting\\n1) Text\\ngeneration\\n2) Text\\nevaluation\\n3) Training\\n& validation\\nlosses\\n4) LLM training\\nfunction\\n5) Text generation\\nstrategies\\n6) Weight saving &\\nloading\\n7) Pretrained weights\\nfrom OpenAI\\nTrain the model\\nto generate\\nhuman-like text\\nImplement additional LLM text\\ngeneration strategies to reduce\\ntraining data memorization\\nEvaluate how well\\nthe model performs\\nLoad pretrained weights from\\nOpenAI into our LLM model\\nImplement functions to save and\\nload the LLM weights to use or\\ncontinue training the LLM later\\nFigure 5.2\\nAn overview of the topics covered in this chapter. We begin by recapping text generation \\n(step 1) before moving on to discuss basic model evaluation techniques (step 2) and training and \\nvalidation losses (step 3).\\nWe shorten the \\ncontext length from \\n1,024 to 256 tokens.\\nIt’s possible and common \\nto set dropout to 0.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 152}, page_content=\"131\\n5.1\\nEvaluating generative text models\\nand load pretrained weights to work with a model configured for a 1,024-token con-\\ntext length.\\n Using the GPTModel instance, we adopt the generate_text_simple function from\\nchapter 4 and introduce two handy functions: text_to_token_ ids and token_ids_\\nto_text. These functions facilitate the conversion between text and token represen-\\ntations, a technique we will utilize throughout this chapter. \\nFigure 5.3 illustrates a three-step text generation process using a GPT model. First,\\nthe tokenizer converts input text into a series of token IDs (see chapter 2). Second,\\nthe model receives these token IDs and generates corresponding logits, which are vec-\\ntors representing the probability distribution for each token in the vocabulary (see\\nchapter 4). Third, these logits are converted back into token IDs, which the tokenizer\\ndecodes into human-readable text, completing the cycle from textual input to tex-\\ntual output.\\n We can implement the text generation process, as shown in the following listing.\\nimport tiktoken\\nfrom chapter04 import generate_text_simple\\ndef text_to_token_ids(text, tokenizer):\\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\\nListing 5.1\\nUtility functions for text to token ID conversion\\nGPTModel\\nEvery effort moves you\\ntensor([[ 6109, 3626, 6100, 345 ]])\\nTokenizer\\ntensor([[[-0.2968, ..., -0.1714],\\n[-1.3747, ...,  0.3993],\\n[ 1.8251, ..., -0.9297],\\n[-0.0922, ..., -0.6768]]])\\neffort moves you forward\\nTokenizer\\n1. Use the tokenizer to encode input\\ntext into a token ID representation.\\n2. Given four input token IDs, the model\\nproduces 4 logit vectors (rows) where\\neach vector has 50,257 elements\\n(columns) equal to the vocabulary size.\\n3. After converting the logits to\\ntoken IDs, we use the tokenizer\\nto decode these IDs back into\\na text representation.\\ntext_to_token_ids()\\ntoken_ids_to_text()\\nFigure 5.3\\nGenerating text involves encoding text into token IDs that the LLM processes into logit vectors. The \\nlogit vectors are then converted back into token IDs, detokenized into a text representation.\"),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 153}, page_content='132\\nCHAPTER 5\\nPretraining on unlabeled data\\n    encoded_tensor = torch.tensor(encoded).unsqueeze(0)   \\n    return encoded_tensor\\ndef token_ids_to_text(token_ids, tokenizer):\\n    flat = token_ids.squeeze(0)               \\n    return tokenizer.decode(flat.tolist())\\nstart_context = \"Every effort moves you\"\\ntokenizer = tiktoken.get_encoding(\"gpt2\")\\ntoken_ids = generate_text_simple(\\n    model=model,\\n    idx=text_to_token_ids(start_context, tokenizer),\\n    max_new_tokens=10,\\n    context_size=GPT_CONFIG_124M[\"context_length\"]\\n)\\nprint(\"Output text:\\\\n\", token_ids_to_text(token_ids, tokenizer))\\nUsing this code, the model generates the following text:\\nOutput text:\\n Every effort moves you rentingetic wasn? refres RexMeCHicular stren\\nClearly, the model isn’t yet producing coherent text because it hasn’t undergone\\ntraining. To define what makes text “coherent” or “high quality,” we have to imple-\\nment a numerical method to evaluate the generated content. This approach will\\nenable us to monitor and enhance the model’s performance throughout its training\\nprocess.\\n Next, we will calculate a loss metric for the generated outputs. This loss serves as a\\nprogress and success indicator of the training progress. Furthermore, in later chap-\\nters, when we fine-tune our LLM, we will review additional methodologies for assess-\\ning model quality.\\n5.1.2\\nCalculating the text generation loss\\nNext, let’s explore techniques for numerically assessing text quality generated\\nduring training by calculating a text generation loss. We will go over this topic step by\\nstep with a practical example to make the concepts clear and applicable, beginning\\nwith a short recap of how the data is loaded and how the text is generated via the\\ngenerate_text_simple function. \\n Figure 5.4 illustrates the overall flow from input text to LLM-generated text using a\\nfive-step procedure. This text-generation process shows what the generate_text_simple\\nfunction does internally. We need to perform these same initial steps before we can\\ncompute a loss that measures the generated text quality later in this section.\\n Figure 5.4 outlines the text generation process with a small seven-token vocabulary\\nto fit this image on a single page. However, our GPTModel works with a much larger\\n.unsqueeze(0) \\nadds the batch \\ndimension\\nRemoves batch \\ndimension'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 154}, page_content='133\\n5.1\\nEvaluating generative text models\\nvocabulary consisting of 50,257 words; hence, the token IDs in the following code will\\nrange from 0 to 50,256 rather than 0 to 6. \\n Also, figure 5.4 only shows a single text example (\"every effort moves\") for sim-\\nplicity. In the following hands-on code example that implements the steps in the fig-\\nure, we will work with two input examples for the GPT model (\"every effort moves\"\\nand \"I really like\").\\n Consider these two input examples, which have already been mapped to token IDs\\n(figure 5.4, step 1):\\ninputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\\n                       [40,    1107, 588]])   #  \"I really like\"]\\nMatching these inputs, the targets contain the token IDs we want the model to\\nproduce:\\ntargets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\\n                        [1107, 588, 11311]])  #  \" really like chocolate\"]\\nNote that the targets are the inputs but shifted one position forward, a concept we\\ncovered in chapter 2 during the implementation of the data loader. This shifting strat-\\negy is crucial for teaching the model to predict the next token in a sequence.\\n[2,\\n1,\\n4]\\nevery\\neﬀort\\nmoves\\nvocabulary = {\\n\"a\":       0,\\n\"effort\":  1,\\n\"every\":   2,\\n\"forward\": 3,\\n\"moves\":   4,\\n\"you\":     5,\\n\"zoo\":     6\\n}\\n[0.10, 0.60, 0.20, 0.05, 0.00, 0.02, 0.01]\\n[0.06, 0.07, 0.01, 0.26, 0.35, 0.13, 0.12]\\n[0.01, 0.10, 0.10, 0.20, 0.12, 0.34, 0.13]\\n0.12, 0.34, 0.13]\\neffort\\nmoves\\nyou\\ninverse_vocabulary = {\\n0: \"a\",\\n1: \"effort\",\\n2: \"every\",\\n3: \"forward\",\\n4: \"moves\",\\n5: \"you\",\\n6: \"zoo\"\\n}\\n1. Use vocabulary\\nto map the input\\ntext to token IDs.\\n2. Obtain seven-dimensional\\nprobability row vector\\nfor each input token via\\nthe\\nfunction.\\nsoftmax\\n4. Obtain all predicted\\ntoken IDs as the index\\npositions with the\\nhighest probabilities.\\n0     1     2     3     4     5     6\\nIndex position:\\n3. Locate the index position\\nwith the highest probability\\nvalue in each row vector, which\\nis done via the\\nfunction.\\nargmax\\n5. Map index\\npositions back\\ninto text via\\nthe inverse\\nvocabulary.\\nInput text\\nThe output text\\ngenerated by\\nthe LLM\\n1\\n4\\n5\\n[ 1, ,\\n,\\n]\\n1\\n4\\n5\\nFigure 5.4\\nFor each of the three input tokens, shown on the left, we compute a vector containing probability \\nscores corresponding to each token in the vocabulary. The index position of the highest probability score in each \\nvector represents the most likely next token ID. These token IDs associated with the highest probability scores \\nare selected and mapped back into a text that represents the text generated by the model.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 155}, page_content='134\\nCHAPTER 5\\nPretraining on unlabeled data\\n Now we feed the inputs into the model to calculate logits vectors for the two input\\nexamples, each comprising three tokens. Then we apply the softmax function to\\ntransform these logits into probability scores (probas; figure 5.4, step 2):\\nwith torch.no_grad():    \\n    logits = model(inputs)\\nprobas = torch.softmax(logits, dim=-1)    \\nprint(probas.shape)\\nThe resulting tensor dimension of the probability score (probas) tensor is\\ntorch.Size([2, 3, 50257])\\nThe first number, 2, corresponds to the two examples (rows) in the inputs, also known\\nas batch size. The second number, 3, corresponds to the number of tokens in each\\ninput (row). Finally, the last number corresponds to the embedding dimensionality,\\nwhich is determined by the vocabulary size. Following the conversion from logits to\\nprobabilities via the softmax function, the generate_text_simple function then con-\\nverts the resulting probability scores back into text (figure 5.4, steps 3–5).\\n We can complete steps 3 and 4 by applying the argmax function to the probability\\nscores to obtain the corresponding token IDs:\\ntoken_ids = torch.argmax(probas, dim=-1, keepdim=True)\\nprint(\"Token IDs:\\\\n\", token_ids)\\nGiven that we have two input batches, each containing three tokens, applying the\\nargmax function to the probability scores (figure 5.4, step 3) yields two sets of outputs,\\neach with three predicted token IDs:\\nToken IDs:\\n tensor([[[16657],      \\n         [  339],\\n         [42826]],\\n        [[49906],       \\n         [29669],\\n         [41751]]])\\nFinally, step 5 converts the token IDs back into text:\\nprint(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\\nprint(f\"Outputs batch 1:\"\\n      f\" {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")\\nWhen we decode these tokens, we find that these output tokens are quite different\\nfrom the target tokens we want the model to generate:\\nTargets batch 1:  effort moves you\\nOutputs batch 1:  Armed heNetflix\\nDisables gradient tracking \\nsince we are not training yet\\nProbability of each \\ntoken in vocabulary\\nFirst batch\\nSecond batch'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 156}, page_content='135\\n5.1\\nEvaluating generative text models\\nThe model produces random text that is different from the target text because it has\\nnot been trained yet. We now want to evaluate the performance of the model’s gen-\\nerated text numerically via a loss (figure 5.5). Not only is this useful for measuring\\nthe quality of the generated text, but it’s also a building block for implementing the\\ntraining function, which we will use to update the model’s weight to improve the\\ngenerated text.\\nPart of the text evaluation process that we implement, as shown in figure 5.5, is to mea-\\nsure “how far” the generated tokens are from the correct predictions (targets). The\\ntraining function we implement later will use this information to adjust the model\\nweights to generate text that is more similar to (or, ideally, matches) the target text.\\n The model training aims to increase the softmax probability in the index positions\\ncorresponding to the correct target token IDs, as illustrated in figure 5.6. This softmax\\nprobability is also used in the evaluation metric we will implement next to numerically\\nassess the model’s generated outputs: the higher the probability in the correct posi-\\ntions, the better.\\n Remember that figure 5.6 displays the softmax probabilities for a compact seven-\\ntoken vocabulary to fit everything into a single figure. This implies that the starting\\nrandom values will hover around 1/7, which equals approximately 0.14. However, the\\nvocabulary we are using for our GPT-2 model has 50,257 tokens, so most of the initial\\nprobabilities will hover around 0.00002 (1/50,257). \\n \\n \\nImplement\\nthe loss\\ncomputation\\nto evaluate how\\nwell the model\\nperforms.\\n1) Text\\ngeneration\\n2) Text\\nevaluation\\n3) Training\\n& validation\\nlosses\\n4) LLM training\\nfunction\\n5) Text generation\\nstrategies\\n6) Weight saving &\\nloading\\n7) Pretrained weights\\nfrom OpenAI\\nTrain the model\\nto generate\\nhuman-like text\\nImplement additional LLM text\\ngeneration strategies to reduce\\ntraining data memorization\\nApply the loss to the entire dataset,\\nwhich we split into a training and\\nvalidation portion\\nImplement functions to save and\\nload the LLM weights to use or\\ncontinue training the LLM later\\nLoad pretrained weights from\\nOpenAI into our LLM model\\nFigure 5.5\\nAn overview of the topics covered in this chapter. We have completed step 1. We are now ready to  \\nimplement the text evaluation function (step 2).'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 157}, page_content='136\\nCHAPTER 5\\nPretraining on unlabeled data\\nFor each of the two input texts, we can print the initial softmax probability scores cor-\\nresponding to the target tokens using the following code:\\ntext_idx = 0\\ntarget_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\\nprint(\"Text 1:\", target_probas_1)\\ntext_idx = 1\\ntarget_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\\nprint(\"Text 2:\", target_probas_2)\\nThe three target token ID probabilities for each batch are\\nText 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\\nText 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\\nThe goal of training an LLM is to maximize the likelihood of the correct token, which\\ninvolves increasing its probability relative to other tokens. This way, we ensure the\\nLLM consistently picks the target token—essentially the next word in the sentence—\\nas the next token it generates.\\n \\n \\n[0.14, 0.14, 0.13, 0.17, 0.15, 0.13, 0.14]\\n[0.15, 0.13, 0.13, 0.16, 0.14, 0.15, 0.14]\\n[0.13, 0.14, 0.15, 0.16, 0.15, 0.13, 0.14]\\n0     1     2     3     4     5     6\\nIndex position:\\n[2,\\n1,\\n4]\\nevery\\neﬀort\\nmoves\\n\"a\"\\n\"effort\"\\n\"every\"\\n\"forward\"\\n\"you\"\\n\"zoo\"\\n\"moves\"\\n1. The model receives\\nthree input tokens and\\ngenerates three vectors.\\n2. Each vector index position\\nin the model-generated\\ntensors corresponds to a\\nword in the vocabulary.\\n[0.14, 0.14, 0.1\\n3. An untrained model\\nproduces random\\nvectors for each token.\\n3, 0.16, 0.14, 0.15, 0.14]\\n5, 0.16, 0.15, 0.13, 0.14]\\n4. In model training, the goal is\\nto maximize the values that\\ncorrespond to the index of\\nthe token in the target vector.\\nInputs:\\nTargets:\\n[ 1, ,\\n,\\n]\\neffort\\nmoves\\nyou\\n1\\n4\\n5\\n1\\n4\\n5\\nFigure 5.6\\nBefore training, the model produces random next-token probability vectors. The goal of model \\ntraining is to ensure that the probability values corresponding to the highlighted target token IDs are maximized.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 158}, page_content='137\\n5.1\\nEvaluating generative text models\\nNext, we will calculate the loss for the probability scores of the two example batches,\\ntarget_probas_1 and target_probas_2. The main steps are illustrated in figure 5.7.\\nSince we already applied steps 1 to 3 to obtain target_probas_1 and target_\\nprobas_2, we proceed with step 4, applying the logarithm to the probability scores:\\nlog_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\\nprint(log_probas)\\nThis results in the following values:\\ntensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\\nBackpropagation\\nHow do we maximize the softmax probability values corresponding to the target\\ntokens? The big picture is that we update the model weights so that the model outputs\\nhigher values for the respective token IDs we want to generate. The weight update is\\ndone via a process called backpropagation, a standard technique for training deep\\nneural networks (see sections A.3 to A.7 in appendix A for more details about back-\\npropagation and model training).\\nBackpropagation requires a loss function, which calculates the difference between\\nthe model’s predicted output (here, the probabilities corresponding to the target\\ntoken IDs) and the actual desired output. This loss function measures how far off the\\nmodel’s predictions are from the target values.\\nLogits\\nProbabilities\\nTarget\\nprobabilities\\nLog probabilities\\nAverage\\nlog probability\\nNegative average\\nlog probability\\n= [[[ 0.1113, -0.1057, -0.3666,  ..., ]]]\\n= [[[1.8849e-05, 1.5172e-05, 1.1687e-05,  ..., ]]]\\n= [7.4541e-05, 3.1061e-05, 1.1563e-05, ..., ]\\n= [-9.5042, -10.3796, -11.3677, ..., ]\\n= -10.7940\\n= 10.7940\\nThe negative average\\nlog probability is the\\nloss we want to\\ncompute\\n1\\n2\\n3\\n4\\n5\\n6\\nFigure 5.7\\nCalculating the loss involves several steps. Steps 1 to 3, which we have already \\ncompleted, calculate the token probabilities corresponding to the target tensors. These \\nprobabilities are then transformed via a logarithm and averaged in steps 4 to 6.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 159}, page_content='138\\nCHAPTER 5\\nPretraining on unlabeled data\\nWorking with logarithms of probability scores is more manageable in mathematical\\noptimization than handling the scores directly. This topic is outside the scope of this\\nbook, but I’ve detailed it further in a lecture, which can be found in appendix B.\\n Next, we combine these log probabilities into a single score by computing the aver-\\nage (step 5 in figure 5.7):\\navg_log_probas = torch.mean(log_probas)\\nprint(avg_log_probas)\\nThe resulting average log probability score is\\ntensor(-10.7940)\\nThe goal is to get the average log probability as close to 0 as possible by updating the\\nmodel’s weights as part of the training process. However, in deep learning, the com-\\nmon practice isn’t to push the average log probability up to 0 but rather to bring the\\nnegative average log probability down to 0. The negative average log probability is\\nsimply the average log probability multiplied by –1, which corresponds to step 6 in\\nfigure 5.7:\\nneg_avg_log_probas = avg_log_probas * -1\\nprint(neg_avg_log_probas)\\nThis prints tensor(10.7940). In deep learning, the term for turning this negative\\nvalue, –10.7940, into 10.7940, is known as the cross entropy loss. PyTorch comes in\\nhandy here, as it already has a built-in cross_entropy function that takes care of all\\nthese six steps in figure 5.7 for us.\\nBefore we apply the cross_entropy function, let’s briefly recall the shape of the logits\\nand target tensors:\\nprint(\"Logits shape:\", logits.shape)\\nprint(\"Targets shape:\", targets.shape)\\nCross entropy loss\\nAt its core, the cross entropy loss is a popular measure in machine learning and deep\\nlearning that measures the difference between two probability distributions—typi-\\ncally, the true distribution of labels (here, tokens in a dataset) and the predicted dis-\\ntribution from a model (for instance, the token probabilities generated by an LLM). \\nIn the context of machine learning and specifically in frameworks like PyTorch, the\\ncross_entropy function computes this measure for discrete outcomes, which is\\nsimilar to the negative average log probability of the target tokens given the model’s\\ngenerated token probabilities, making the terms “cross entropy” and “negative aver-\\nage log probability” related and often used interchangeably in practice.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 160}, page_content='139\\n5.1\\nEvaluating generative text models\\nThe resulting shapes are\\nLogits shape: torch.Size([2, 3, 50257])\\nTargets shape: torch.Size([2, 3])\\nAs we can see, the logits tensor has three dimensions: batch size, number of tokens,\\nand vocabulary size. The targets tensor has two dimensions: batch size and number\\nof tokens.\\n For the cross_entropy loss function in PyTorch, we want to flatten these tensors\\nby combining them over the batch dimension:\\nlogits_flat = logits.flatten(0, 1)\\ntargets_flat = targets.flatten()\\nprint(\"Flattened logits:\", logits_flat.shape)\\nprint(\"Flattened targets:\", targets_flat.shape)\\nThe resulting tensor dimensions are\\nFlattened logits: torch.Size([6, 50257])\\nFlattened targets: torch.Size([6])\\nRemember that the targets are the token IDs we want the LLM to generate, and the\\nlogits contain the unscaled model outputs before they enter the softmax function to\\nobtain the probability scores.\\n Previously, we applied the softmax function, selected the probability scores corre-\\nsponding to the target IDs, and computed the negative average log probabilities.\\nPyTorch’s cross_entropy function will take care of all these steps for us:\\nloss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\\nprint(loss)\\nThe resulting loss is the same that we obtained previously when applying the individ-\\nual steps in figure 5.7 manually:\\ntensor(10.7940)\\nPerplexity\\nPerplexity is a measure often used alongside cross entropy loss to evaluate the per-\\nformance of models in tasks like language modeling. It can provide a more interpre-\\ntable way to understand the uncertainty of a model in predicting the next token in a\\nsequence. \\nPerplexity measures how well the probability distribution predicted by the model\\nmatches the actual distribution of the words in the dataset. Similar to the loss, a lower\\nperplexity indicates that the model predictions are closer to the actual distribution.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 161}, page_content='140\\nCHAPTER 5\\nPretraining on unlabeled data\\nWe have now calculated the loss for two small text inputs for illustration purposes.\\nNext, we will apply the loss computation to the entire training and validation sets.\\n5.1.3\\nCalculating the training and validation set losses\\nWe must first prepare the training and validation datasets that we will use to train the\\nLLM. Then, as highlighted in figure 5.8, we will calculate the cross entropy for the\\ntraining and validation sets, which is an important component of the model training\\nprocess.\\nTo compute the loss on the training and validation datasets, we use a very small text\\ndataset, the “The Verdict” short story by Edith Wharton, which we have already\\nworked with in chapter 2. By selecting a text from the public domain, we circumvent\\nany concerns related to usage rights. Additionally, using such a small dataset allows\\nfor the execution of code examples on a standard laptop computer in a matter of\\n(continued)\\nPerplexity can be calculated as perplexity = torch.exp(loss), which returns\\ntensor(48725.8203) when applied to the previously calculated loss. \\nPerplexity is often considered more interpretable than the raw loss value because it sig-\\nnifies the effective vocabulary size about which the model is uncertain at each step. In\\nthe given example, this would translate to the model being unsure about which among\\n48,725 tokens in the vocabulary to generate as the next token.\\nImplement\\nthe loss\\ncomputation to\\nevaluate how\\nwell the model\\nperforms.\\n1) Text\\ngeneration\\n2) Text\\nevaluation\\n3) Training\\n& validation\\nlosses\\n4) LLM training\\nfunction\\n5) Text generation\\nstrategies\\n6) Weight saving &\\nloading\\n7) Pretrained weights\\nfrom OpenAI\\nTrain the model\\nto generate\\nhuman-like text.\\nImplement additional LLM text\\ngeneration strategies to reduce\\ntraining data memorization.\\nApply the loss to the entire dataset,\\nwhich we split into a training and\\nvalidation portion.\\nImplement functions to save and\\nload the LLM weights to use or\\ncontinue training the LLM later.\\nLoad pretrained weights from\\nOpenAI into our LLM model.\\nFigure 5.8\\nHaving completed steps 1 and 2, including computing the cross entropy loss, we can now apply this \\nloss computation to the entire text dataset that we will use for model training.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 162}, page_content='141\\n5.1\\nEvaluating generative text models\\nminutes, even without a high-end GPU, which is particularly advantageous for edu-\\ncational purposes. \\nNOTE\\nInterested readers can also use the supplementary code for this book\\nto prepare a larger-scale dataset consisting of more than 60,000 public domain\\nbooks from Project Gutenberg and train an LLM on these (see appendix D\\nfor details).\\nThe following code loads the “The Verdict” short story:\\nfile_path = \"the-verdict.txt\"\\nwith open(file_path, \"r\", encoding=\"utf-8\") as file:\\n    text_data = file.read()\\nAfter loading the dataset, we can check the number of characters and tokens in the\\ndataset:\\ntotal_characters = len(text_data)\\ntotal_tokens = len(tokenizer.encode(text_data))\\nprint(\"Characters:\", total_characters)\\nprint(\"Tokens:\", total_tokens)\\nThe output is\\nCharacters: 20479\\nTokens: 5145\\nWith just 5,145 tokens, the text might seem too small to train an LLM, but as men-\\ntioned earlier, it’s for educational purposes so that we can run the code in minutes\\ninstead of weeks. Plus, later we will load pretrained weights from OpenAI into our\\nGPTModel code.\\n Next, we divide the dataset into a training and a validation set and use the data\\nloaders from chapter 2 to prepare the batches for LLM training. This process is visual-\\nized in figure 5.9. Due to spatial constraints, we use a max_length=6. However, for the\\nactual data loaders, we set the max_length equal to the 256-token context length that\\nthe LLM supports so that the LLM sees longer texts during training.\\nThe cost of pretraining LLMs \\nTo put the scale of our project into perspective, consider the training of the 7 billion\\nparameter Llama 2 model, a relatively popular openly available LLM. This model\\nrequired 184,320 GPU hours on expensive A100 GPUs, processing 2 trillion tokens.\\nAt the time of writing, running an 8 × A100 cloud server on AWS costs around $30\\nper hour. A rough estimate puts the total training cost of such an LLM at around\\n$690,000 (calculated as 184,320 hours divided by 8, then multiplied by $30).'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 163}, page_content='142\\nCHAPTER 5\\nPretraining on unlabeled data\\nNOTE\\nWe are training the model with training data presented in similarly\\nsized chunks for simplicity and efficiency. However, in practice, it can also be\\nbeneficial to train an LLM with variable-length inputs to help the LLM to bet-\\nter generalize across different types of inputs when it is being used.\\nTo implement the data splitting and loading, we first define a train_ratio to use 90%\\nof the data for training and the remaining 10% as validation data for model evalua-\\ntion during training:\\ntrain_ratio = 0.90\\nsplit_idx = int(train_ratio * len(text_data))\\ntrain_data = text_data[:split_idx]\\nval_data = text_data[split_idx:]\\nIn the realm of visual compositions, where content reigns\\nsupreme, there exists a tranquil harbor known as\\nPlaceholder Bay. Here, amidst the gentle sway of design\\nelements, words meander without true purpose, yet with a\\nsemblance of meaning. This text, neither too captivating nor\\ntoo bland, serves as a beacon for eyes that scan, seeking\\nthe eventual substance that will ﬁll the void.\\ncasts\\nAs the sun\\nits golden hues over Placeholder Bay, sentences ﬂow like a\\ngentle river, words ripple like waves against the shore.\\n[[  818,   262, 13360,   286,  5874, 33543 ],\\n[    11,   810,  2695, 13580,    82, 17700 ],\\n[    11,   612,  7160,   257, 46944, 25451 ],\\n[  1900,   355,  8474, 13829,  4696,    13 ],\\n[  3423,    11, 31095,   262, 10296, 20009 ],\\n...\\n[  7850,    11,  2456, 42462,   588,  9813 ]]\\n[[[ 1900,   355,  8474, 13829,  4696,    13 ],\\n[    11,   810,  2695, 13580,    82, 17700 ]],\\n[[ 7850,    11,  2456, 42462,   588,  9813 ],\\n[  3423,    11, 31095,   262, 10296, 20009 ]],\\n...\\n[[   11,   612,  7160,   257, 46944, 25451 ],\\n[   818,   262, 13360,   286,  5874, 33543 ]]]\\nWe use a small portion of the data\\nto construct the validation set.\\nWe use a large portion of\\nthe input text dataset for\\nmodel training.\\n1) Input text dataset\\nTokenizer\\nThe ﬁrst sample\\nin the tokenized\\ndataset of length 6\\n[[  818,   262, 13360,   286,  5874, 33543 ],\\n[    11,   810,  2695, 13580,    82, 17700 ],\\nProcess the dataset\\nwith a max length\\nof 6\\nThe second sample\\nin the tokenized\\ndataset\\nOrganize dataset\\ninto batches; here\\nwith batch size 2\\nand shufﬂing\\nenabled.\\n2) Tokenized\\ntraining dataset\\n3) Training datasets\\nin chunks of length 6\\n4) Training datasets\\nin batches\\nBatch 1\\nBatch 2\\nLast batch\\n[ 818, 262, 13360, 286, 5874, 33543, 11, 810,\\n2695, 13580, 82, 17700, 11, 612, 7160, 257,\\n46944, 25451, 1900, 355, 8474, 13829, 4696, 13,\\n3423, 11, 31095, 262, 10296, 20009, 286, 1486,\\n4847, 11, 2456, 502, 4066, 1231, 2081, 4007, 11,\\n1865, 351, 257, 45960, 286, 3616, 13, 770, 2420,\\n11, 6159, 1165, 3144, 39438, 4249, 1165, 34377,\\n11, 9179, 355, 257, 34538, 329, 2951, 326, 9367,\\n11, 6095, 262, 19657, 9136, 326, 481, 6070, 262,\\n7951, 13, 1081, 262, 4252, 3350 ]\\nThe second sample in the\\ntokenized dataset when\\nthe stride is set to 6\\nFigure 5.9\\nWhen preparing the data loaders, we split the input text into training and validation set portions. Then \\nwe tokenize the text (only shown for the training set portion for simplicity) and divide the tokenized text into \\nchunks of a user-specified length (here, 6). Finally, we shuffle the rows and organize the chunked text into batches \\n(here, batch size 2), which we can use for model training.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 164}, page_content='143\\n5.1\\nEvaluating generative text models\\nUsing the train_data and val_data subsets, we can now create the respective data\\nloader reusing the create_dataloader_v1 code from chapter 2:\\nfrom chapter02 import create_dataloader_v1\\ntorch.manual_seed(123)\\ntrain_loader = create_dataloader_v1(\\n    train_data,\\n    batch_size=2,\\n    max_length=GPT_CONFIG_124M[\"context_length\"],\\n    stride=GPT_CONFIG_124M[\"context_length\"],\\n    drop_last=True,\\n    shuffle=True,\\n    num_workers=0\\n)\\nval_loader = create_dataloader_v1(\\n    val_data,\\n    batch_size=2,\\n    max_length=GPT_CONFIG_124M[\"context_length\"],\\n    stride=GPT_CONFIG_124M[\"context_length\"],\\n    drop_last=False,\\n    shuffle=False,\\n    num_workers=0\\n)\\nWe used a relatively small batch size to reduce the computational resource demand\\nbecause we were working with a very small dataset. In practice, training LLMs with\\nbatch sizes of 1,024 or larger is not uncommon.\\n As an optional check, we can iterate through the data loaders to ensure that they\\nwere created correctly:\\nprint(\"Train loader:\")\\nfor x, y in train_loader:\\n    print(x.shape, y.shape)\\nprint(\"\\\\nValidation loader:\")\\nfor x, y in val_loader:\\n    print(x.shape, y.shape)\\nWe should see the following outputs:\\nTrain loader:\\ntorch.Size([2, 256]) torch.Size([2, 256])\\ntorch.Size([2, 256]) torch.Size([2, 256])\\ntorch.Size([2, 256]) torch.Size([2, 256])\\ntorch.Size([2, 256]) torch.Size([2, 256])\\ntorch.Size([2, 256]) torch.Size([2, 256])\\ntorch.Size([2, 256]) torch.Size([2, 256])\\ntorch.Size([2, 256]) torch.Size([2, 256])\\ntorch.Size([2, 256]) torch.Size([2, 256])\\ntorch.Size([2, 256]) torch.Size([2, 256])'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 165}, page_content='144\\nCHAPTER 5\\nPretraining on unlabeled data\\nValidation loader:\\ntorch.Size([2, 256]) torch.Size([2, 256])\\nBased on the preceding code output, we have nine training set batches with two sam-\\nples and 256 tokens each. Since we allocated only 10% of the data for validation, there\\nis only one validation batch consisting of two input examples. As expected, the input\\ndata (x) and target data (y) have the same shape (the batch size times the number of\\ntokens in each batch) since the targets are the inputs shifted by one position, as dis-\\ncussed in chapter 2.\\n Next, we implement a utility function to calculate the cross entropy loss of a given\\nbatch returned via the training and validation loader:\\ndef calc_loss_batch(input_batch, target_batch, model, device):\\n    input_batch = input_batch.to(device)        \\n    target_batch = target_batch.to(device)      \\n    logits = model(input_batch)\\n    loss = torch.nn.functional.cross_entropy(\\n        logits.flatten(0, 1), target_batch.flatten()\\n    )\\n    return loss\\nWe can now use this calc_loss_batch utility function, which computes the loss for a\\nsingle batch, to implement the following calc_loss_loader function that computes\\nthe loss over all the batches sampled by a given data loader.\\ndef calc_loss_loader(data_loader, model, device, num_batches=None):\\n    total_loss = 0.\\n    if len(data_loader) == 0:\\n        return float(\"nan\")\\n    elif num_batches is None:\\n        num_batches = len(data_loader)    \\n    else:\\n        num_batches = min(num_batches, len(data_loader))  \\n    for i, (input_batch, target_batch) in enumerate(data_loader):\\n        if i < num_batches:\\n            loss = calc_loss_batch(\\n                input_batch, target_batch, model, device\\n            )\\n            total_loss += loss.item()   \\n        else:\\n            break\\n    return total_loss / num_batches   \\nBy default, the calc_loss_loader function iterates over all batches in a given data\\nloader, accumulates the loss in the total_loss variable, and then computes and\\nListing 5.2\\nFunction to compute the training and validation loss\\nThe transfer to a \\ngiven device allows \\nus to transfer the \\ndata to a GPU.\\nIteratives over all \\nbatches if no fixed \\nnum_batches is specified\\nReduces the number\\nof batches to match\\nthe total number of\\nbatches in the data\\nloader if num_batches\\nexceeds the number\\nof batches in the\\ndata loader\\nSums loss \\nfor each \\nbatch\\nAverages the loss over all batches'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 166}, page_content='145\\n5.1\\nEvaluating generative text models\\naverages the loss over the total number of batches. Alternatively, we can specify a\\nsmaller number of batches via num_batches to speed up the evaluation during model\\ntraining.\\n Let’s now see this calc_loss_loader function in action, applying it to the training\\nand validation set loaders:\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\nmodel.to(device)  \\nwith torch.no_grad():                                       \\n    train_loss = calc_loss_loader(train_loader, model, device)   \\n    val_loss = calc_loss_loader(val_loader, model, device)\\nprint(\"Training loss:\", train_loss)\\nprint(\"Validation loss:\", val_loss)\\nThe resulting loss values are\\nTraining loss: 10.98758347829183\\nValidation loss: 10.98110580444336\\nThe loss values are relatively high because the model has not yet been trained. For\\ncomparison, the loss approaches 0 if the model learns to generate the next tokens as\\nthey appear in the training and validation sets.\\n Now that we have a way to measure the quality of the generated text, we will train\\nthe LLM to reduce this loss so that it becomes better at generating text, as illustrated\\nin figure 5.10.\\nIf you have a machine with a \\nCUDA-supported GPU, the LLM \\nwill train on the GPU without \\nmaking any changes to the code.\\nDisables gradient tracking\\nfor efficiency because we\\nare not training yet\\nVia the “device” setting,\\nwe ensure the data is loaded onto\\nthe same device as the LLM model.\\n1) Text\\ngeneration\\n2) Text\\nevaluation\\n3) Training\\n& validation\\nlosses\\n4) LLM training\\nfunction\\n5) Text generation\\nstrategies\\n6) Weight saving &\\nloading\\n7) Pretrained weights\\nfrom OpenAI\\nTrain the model\\nto generate\\nhuman-like text.\\nImplement additional LLM text\\ngeneration strategies to reduce\\ntraining data memorization.\\nImplement functions to save and\\nload the LLM weights to use or\\ncontinue training the LLM later.\\nLoad pretrained weights from\\nOpenAI into our LLM model.\\nFigure 5.10\\nWe have recapped the text generation process (step 1) and implemented basic model \\nevaluation techniques (step 2) to compute the training and validation set losses (step 3). Next, we \\nwill go to the training functions and pretrain the LLM (step 4).'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 167}, page_content='146\\nCHAPTER 5\\nPretraining on unlabeled data\\nNext, we will focus on pretraining the LLM. After model training, we will implement\\nalternative text generation strategies and save and load pretrained model weights.\\n5.2\\nTraining an LLM\\nIt is finally time to implement the code for pretraining the LLM, our GPTModel. For this,\\nwe focus on a straightforward training loop to keep the code concise and readable. \\nNOTE\\nInterested readers can learn about more advanced techniques, includ-\\ning learning rate warmup, cosine annealing, and gradient clipping, in appendix D.\\nThe flowchart in figure 5.11 depicts a typical PyTorch neural network training work-\\nflow, which we use for training an LLM. It outlines eight steps, starting with iterating\\nover each epoch, processing batches, resetting gradients, calculating the loss and new\\n1) Iterate over training epochs\\n2) Iterate over batches in\\neach training epoch\\n3) Reset loss gradients from\\nprevious batch iteration\\n4) Calculate loss on\\ncurrent batch\\n5) Backward pass to\\ncalculate loss gradients\\n6) Update model weights\\nusing loss gradients\\n7) Print training and\\nvalidation set losses\\n8) Generate sample text\\nfor visual inspection\\nOptional steps for tracking\\nthe training progress.\\nThese are the usual steps\\nused for training deep\\nneural networks in PyTorch.\\nOne epoch is one complete\\npass over a training set.\\nThe number of batches is\\ndetermined by the training\\nset size divided by the size\\nof each batch.\\nFigure 5.11\\nA typical training loop for training deep neural networks in \\nPyTorch consists of numerous steps, iterating over the batches in the training \\nset for several epochs. In each loop, we calculate the loss for each training \\nset batch to determine loss gradients, which we use to update the model \\nweights so that the training set loss is minimized.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 168}, page_content='147\\n5.2\\nTraining an LLM\\ngradients, and updating weights and concluding with monitoring steps like printing\\nlosses and generating text samples. \\nNOTE\\nIf you are relatively new to training deep neural networks with PyTorch\\nand any of these steps are unfamiliar, consider reading sections A.5 to A.8 in\\nappendix A.\\nWe can implement this training flow via the train_model_simple function in code.\\ndef train_model_simple(model, train_loader, val_loader,\\n                       optimizer, device, num_epochs,\\n                       eval_freq, eval_iter, start_context, tokenizer):\\n    train_losses, val_losses, track_tokens_seen = [], [], []   \\n    tokens_seen, global_step = 0, -1\\n    for epoch in range(num_epochs):   \\n        model.train()\\n        for input_batch, target_batch in train_loader:\\n            optimizer.zero_grad()  \\n            loss = calc_loss_batch(\\n                input_batch, target_batch, model, device\\n            )\\n            loss.backward()                    \\n            optimizer.step()                   \\n            tokens_seen += input_batch.numel()\\n            global_step += 1\\n            if global_step % eval_freq == 0:   \\n                train_loss, val_loss = evaluate_model(\\n                    model, train_loader, val_loader, device, eval_iter)\\n                train_losses.append(train_loss)\\n                val_losses.append(val_loss)\\n                track_tokens_seen.append(tokens_seen)\\n                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\\n                      f\"Train loss {train_loss:.3f}, \"\\n                      f\"Val loss {val_loss:.3f}\"\\n                )\\n        generate_and_print_sample(                     \\n            model, tokenizer, device, start_context\\n        )\\n    return train_losses, val_losses, track_tokens_seen\\nNote that the train_model_simple function we just created uses two functions we\\nhave not defined yet: evaluate_model and generate_and_print_sample. \\n The evaluate_model function corresponds to step 7 in figure 5.11. It prints the\\ntraining and validation set losses after each model update so we can evaluate whether\\nthe training improves the model. More specifically, the evaluate_model function cal-\\nculates the loss over the training and validation set while ensuring the model is in eval-\\nListing 5.3\\nThe main function for pretraining LLMs\\nInitializes lists to\\ntrack losses and\\ntokens seen\\nStarts the main \\ntraining loop\\nResets loss gradients \\nfrom the previous \\nbatch iteration\\nCalculates loss gradients\\nUpdates model weights \\nusing loss gradients\\nOptional evaluation step\\nPrints a sample text \\nafter each epoch'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 169}, page_content='148\\nCHAPTER 5\\nPretraining on unlabeled data\\nuation mode with gradient tracking and dropout disabled when calculating the loss\\nover the training and validation sets:\\ndef evaluate_model(model, train_loader, val_loader, device, eval_iter):\\n    model.eval() \\n    with torch.no_grad():                             \\n        train_loss = calc_loss_loader(\\n            train_loader, model, device, num_batches=eval_iter\\n        )\\n        val_loss = calc_loss_loader(\\n            val_loader, model, device, num_batches=eval_iter\\n        )\\n    model.train()\\n    return train_loss, val_loss\\nSimilar to evaluate_model, the generate_and_print_sample function is a convenience\\nfunction that we use to track whether the model improves during the training. In partic-\\nular, the generate_and_print_sample function takes a text snippet (start_context) as\\ninput, converts it into token IDs, and feeds it to the LLM to generate a text sample\\nusing the generate_text_simple function we used earlier:\\ndef generate_and_print_sample(model, tokenizer, device, start_context):\\n    model.eval()\\n    context_size = model.pos_emb.weight.shape[0]\\n    encoded = text_to_token_ids(start_context, tokenizer).to(device)\\n    with torch.no_grad():\\n        token_ids = generate_text_simple(\\n            model=model, idx=encoded,\\n            max_new_tokens=50, context_size=context_size\\n        )\\n    decoded_text = token_ids_to_text(token_ids, tokenizer)\\n    print(decoded_text.replace(\"\\\\n\", \" \"))     \\n    model.train()\\nWhile the evaluate_model function gives us a numeric estimate of the model’s train-\\ning progress, this generate_and_print_sample text function provides a concrete text\\nexample generated by the model to judge its capabilities during training.\\nAdamW \\nAdam optimizers are a popular choice for training deep neural networks. However, in\\nour training loop, we opt for the AdamW optimizer. AdamW is a variant of Adam that\\nimproves the weight decay approach, which aims to minimize model complexity and\\nprevent overfitting by penalizing larger weights. This adjustment allows AdamW to\\nachieve more effective regularization and better generalization; thus, AdamW is fre-\\nquently used in the training of LLMs.\\nDropout is disabled during \\nevaluation for stable, \\nreproducible results.\\nDisables gradient tracking, which is not\\nrequired during evaluation, to reduce\\nthe computational overhead\\nCompact \\nprint format'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 170}, page_content='149\\n5.2\\nTraining an LLM\\nLet’s see this all in action by training a GPTModel instance for 10 epochs using an\\nAdamW optimizer and the train_model_simple function we defined earlier: \\ntorch.manual_seed(123)\\nmodel = GPTModel(GPT_CONFIG_124M)\\nmodel.to(device)\\noptimizer = torch.optim.AdamW(\\n     model.parameters(),          \\n    lr=0.0004, weight_decay=0.1\\n)\\nnum_epochs = 10\\ntrain_losses, val_losses, tokens_seen = train_model_simple(\\n    model, train_loader, val_loader, optimizer, device,\\n    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\\n    start_context=\"Every effort moves you\", tokenizer=tokenizer\\n)\\nExecuting the train_model_simple function starts the training process, which takes\\nabout 5 minutes to complete on a MacBook Air or a similar laptop. The output\\nprinted during this execution is as follows:\\nEp 1 (Step 000000): Train loss 9.781, Val loss 9.933\\nEp 1 (Step 000005): Train loss 8.111, Val loss 8.339\\nEvery effort moves you,,,,,,,,,,,,.                                     \\nEp 2 (Step 000010): Train loss 6.661, Val loss 7.048\\nEp 2 (Step 000015): Train loss 5.961, Val loss 6.616\\nEvery effort moves you, and, and, and, and, and, and, and, and, and, and,\\n and, and, and, and, and, and, and, and, and, and, and, and,, and, and,\\n[...]                                                  \\nEp 9 (Step 000080): Train loss 0.541, Val loss 6.393\\nEvery effort moves you?\"  \"Yes--quite insensible to the irony. She wanted\\nhim vindicated--and by me!\"  He laughed again, and threw back the \\nwindow-curtains, I had the donkey. \"There were days when I\\nEp 10 (Step 000085): Train loss 0.391, Val loss 6.452\\nEvery effort moves you know,\" was one of the axioms he laid down across the\\nSevres and silver of an exquisitely appointed luncheon-table, when, on a\\nlater day, I had again run over from Monte Carlo; and Mrs. Gis\\nAs we can see, the training loss improves drastically, starting with a value of 9.781\\nand converging to 0.391. The language skills of the model have improved quite a\\nlot. In the beginning, the model is only able to append commas to the start context\\n(Every effort moves you,,,,,,,,,,,,) or repeat the word and. At the end of the\\ntraining, it can generate grammatically correct text. \\n Similar to the training set loss, we can see that the validation loss starts high\\n(9.933) and decreases during the training. However, it never becomes as small as the\\ntraining set loss and remains at 6.452 after the 10th epoch.\\n Before discussing the validation loss in more detail, let’s create a simple plot that\\nshows the training and validation set losses side by side:\\n \\nThe .parameters() method \\nreturns all trainable weight \\nparameters of the model.\\nIntermediate\\nresults removed\\nto save space'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 171}, page_content='150\\nCHAPTER 5\\nPretraining on unlabeled data\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.ticker import MaxNLocator\\ndef plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\\n    fig, ax1 = plt.subplots(figsize=(5, 3))\\n    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\\n    ax1.plot(\\n        epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\\n    )\\n    ax1.set_xlabel(\"Epochs\")\\n    ax1.set_ylabel(\"Loss\")\\n    ax1.legend(loc=\"upper right\")\\n    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\\n    ax2 = ax1.twiny()                  \\n    ax2.plot(tokens_seen, train_losses, alpha=0)    \\n    ax2.set_xlabel(\"Tokens seen\")\\n    fig.tight_layout()\\n    plt.show()\\nepochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\\nplot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\\nThe resulting training and validation loss plot is shown in figure 5.12. As we can see,\\nboth the training and validation losses start to improve for the first epoch. However,\\nthe losses start to diverge past the second epoch. This divergence and the fact that the\\nvalidation loss is much larger than the training loss indicate that the model is overfit-\\nting to the training data. We can confirm that the model memorizes the training data\\nverbatim by searching for the generated text snippets, such as quite insensible to\\nthe irony in the “The Verdict” text file. \\nCreates a second \\nx-axis that shares \\nthe same y-axis\\nInvisible plot for \\naligning ticks\\nFigure 5.12\\nAt the beginning of the training, both the training and validation \\nset losses sharply decrease, which is a sign that the model is learning. However, \\nthe training set loss continues to decrease past the second epoch, whereas the \\nvalidation loss stagnates. This is a sign that the model is still learning, but it’s \\noverfitting to the training set past epoch 2.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 172}, page_content='151\\n5.3\\nDecoding strategies to control randomness\\nThis memorization is expected since we are working with a very, very small training\\ndataset and training the model for multiple epochs. Usually, it’s common to train a\\nmodel on a much larger dataset for only one epoch. \\nNOTE\\nAs mentioned earlier, interested readers can try to train the model on\\n60,000 public domain books from Project Gutenberg, where this overfitting\\ndoes not occur; see appendix B for details. \\nAs illustrated in figure 5.13, we have completed four of our objectives for this chaper.\\nNext, we will cover text generation strategies for LLMs to reduce training data memo-\\nrization and increase the originality of the LLM-generated text before we cover weight\\nloading and saving and loading pretrained weights from OpenAI’s GPT model.\\n5.3\\nDecoding strategies to control randomness\\nLet’s look at text generation strategies (also called decoding strategies) to generate\\nmore original text. First, we will briefly revisit the generate_text_simple function that\\nwe used inside generate_and_print_sample earlier. Then we will cover two techniques,\\ntemperature scaling and top-k sampling, to improve this function.\\n We begin by transferring the model back from the GPU to the CPU since infer-\\nence with a relatively small model does not require a GPU. Also, after training, we put\\nthe model into evaluation mode to turn off random components such as dropout:\\nmodel.to(\"cpu\")\\nmodel.eval()\\nNext, we plug the GPTModel instance (model) into the generate_text_simple func-\\ntion, which uses the LLM to generate one token at a time:\\n1) Text\\ngeneration\\n2) Text\\nevaluation\\n3) Training\\n& validation\\nlosses\\n4) LLM training\\nfunction\\n5) Text generation\\nstrategies\\n6) Weight saving &\\nloading\\n7) Pretrained weights\\nfrom OpenAI\\nTrain the model\\nto generate\\nhuman-like text.\\nImplement additional LLM text\\ngeneration strategies to reduce\\ntraining data memorization.\\nAt the end of this chapter,\\nload pretrained weights from\\nOpenAI into our LLM model.\\nImplement functions to save and\\nload the LLM weights to use or\\ncontinue training the LLM later.\\nFigure 5.13\\nOur model can generate coherent text after implementing the training function. \\nHowever, it often memorizes passages from the training set verbatim. Next, we will discuss \\nstrategies to generate more diverse output texts.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 173}, page_content='152\\nCHAPTER 5\\nPretraining on unlabeled data\\ntokenizer = tiktoken.get_encoding(\"gpt2\")\\ntoken_ids = generate_text_simple(\\n    model=model,\\n    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\\n    max_new_tokens=25,\\n    context_size=GPT_CONFIG_124M[\"context_length\"]\\n)\\nprint(\"Output text:\\\\n\", token_ids_to_text(token_ids, tokenizer))\\nThe generated text is\\nOutput text:\\nEvery effort moves you know,\" was one of the axioms he laid down across the\\nSevres and silver of an exquisitely appointed lun\\nAs explained earlier, the generated token is selected at each generation step corre-\\nsponding to the largest probability score among all tokens in the vocabulary. This\\nmeans that the LLM will always generate the same outputs even if we run the preced-\\ning generate_text_simple function multiple times on the same start context (Every\\neffort moves you).\\n5.3.1\\nTemperature scaling\\nLet’s now look at temperature scaling, a technique that adds a probabilistic selection\\nprocess to the next-token generation task. Previously, inside the generate_text_simple\\nfunction, we always sampled the token with the highest probability as the next token\\nusing torch.argmax, also known as greedy decoding. To generate text with more variety,\\nwe can replace argmax with a function that samples from a probability distribution\\n(here, the probability scores the LLM generates for each vocabulary entry at each\\ntoken generation step).\\n To illustrate the probabilistic sampling with a concrete example, let’s briefly dis-\\ncuss the next-token generation process using a very small vocabulary for illustration\\npurposes:\\nvocab = { \\n    \"closer\": 0,\\n    \"every\": 1, \\n    \"effort\": 2, \\n    \"forward\": 3,\\n    \"inches\": 4,\\n    \"moves\": 5, \\n    \"pizza\": 6,\\n    \"toward\": 7,\\n    \"you\": 8,\\n} \\ninverse_vocab = {v: k for k, v in vocab.items()}'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 174}, page_content='153\\n5.3\\nDecoding strategies to control randomness\\nNext, assume the LLM is given the start context \"every effort moves you\" and gener-\\nates the following next-token logits:\\nnext_token_logits = torch.tensor(\\n    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\\n)\\nAs discussed in chapter 4, inside generate_text_simple, we convert the logits into\\nprobabilities via the softmax function and obtain the token ID corresponding to the\\ngenerated token via the argmax function, which we can then map back into text via\\nthe inverse vocabulary:\\nprobas = torch.softmax(next_token_logits, dim=0)\\nnext_token_id = torch.argmax(probas).item()\\nprint(inverse_vocab[next_token_id])\\nSince the largest logit value and, correspondingly, the largest softmax probability\\nscore are in the fourth position (index position 3 since Python uses 0 indexing), the\\ngenerated word is \"forward\". \\n To implement a probabilistic sampling process, we can now replace argmax with\\nthe multinomial function in PyTorch:\\ntorch.manual_seed(123) \\nnext_token_id = torch.multinomial(probas, num_samples=1).item()\\nprint(inverse_vocab[next_token_id])\\nThe printed output is \"forward\" just like before. What happened? The multinomial\\nfunction samples the next token proportional to its probability score. In other words,\\n\"forward\" is still the most likely token and will be selected by multinomial most of\\nthe time but not all the time. To illustrate this, let’s implement a function that repeats\\nthis sampling 1,000 times:\\ndef print_sampled_tokens(probas):\\n    torch.manual_seed(123)\\n    sample = [torch.multinomial(probas, num_samples=1).item()\\n             for i in range(1_000)]\\n    sampled_ids = torch.bincount(torch.tensor(sample))\\n    for i, freq in enumerate(sampled_ids):\\n        print(f\"{freq} x {inverse_vocab[i]}\")\\nprint_sampled_tokens(probas)\\nThe sampling output is\\n73 x closer\\n0 x every\\n0 x effort\\n582 x forward\\n2 x inches'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 175}, page_content=\"154\\nCHAPTER 5\\nPretraining on unlabeled data\\n0 x moves\\n0 x pizza\\n343 x toward\\nAs we can see, the word forward is sampled most of the time (582 out of 1,000 times),\\nbut other tokens such as closer, inches, and toward will also be sampled some of\\nthe time. This means that if we replaced the argmax function with the multinomial\\nfunction inside the generate_and_print_sample function, the LLM would some-\\ntimes generate texts such as every effort moves you toward, every effort moves\\nyou inches, and every effort moves you closer instead of every effort moves you\\nforward.\\n We can further control the distribution and selection process via a concept called\\ntemperature scaling. Temperature scaling is just a fancy description for dividing the logits\\nby a number greater than 0:\\ndef softmax_with_temperature(logits, temperature):\\n    scaled_logits = logits / temperature\\n    return torch.softmax(scaled_logits, dim=0)\\nTemperatures greater than 1 result in more uniformly distributed token probabilities,\\nand temperatures smaller than 1 will result in more confident (sharper or more peaky)\\ndistributions. Let’s illustrate this by plotting the original probabilities alongside proba-\\nbilities scaled with different temperature values:\\ntemperatures = [1, 0.1, 5]                                    \\nscaled_probas = [softmax_with_temperature(next_token_logits, T)\\n                for T in temperatures]\\nx = torch.arange(len(vocab))\\nbar_width = 0.15\\nfig, ax = plt.subplots(figsize=(5, 3))\\nfor i, T in enumerate(temperatures):\\n    rects = ax.bar(x + i * bar_width, scaled_probas[i], \\n                   bar_width, label=f'Temperature = {T}')\\nax.set_ylabel('Probability')\\nax.set_xticks(x)\\nax.set_xticklabels(vocab.keys(), rotation=90)\\nax.legend()\\nplt.tight_layout()\\nplt.show()\\nThe resulting plot is shown in figure 5.14.\\n A temperature of 1 divides the logits by 1 before passing them to the softmax func-\\ntion to compute the probability scores. In other words, using a temperature of 1 is the\\nsame as not using any temperature scaling. In this case, the tokens are selected with a\\nprobability equal to the original softmax probability scores via the multinomial sam-\\npling function in PyTorch. For example, for the temperature setting 1, the token cor-\\nresponding to “forward” would be selected about 60% of the time, as we can see in\\nfigure 5.14.\\nOriginal, lower, \\nand higher \\nconfidence\"),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 176}, page_content='155\\n5.3\\nDecoding strategies to control randomness\\nAlso, as we can see in figure 5.14, applying very small temperatures, such as 0.1, will\\nresult in sharper distributions such that the behavior of the multinomial function\\nselects the most likely token (here, \"forward\") almost 100% of the time, approaching\\nthe behavior of the argmax function. Likewise, a temperature of 5 results in a more\\nuniform distribution where other tokens are selected more often. This can add more\\nvariety to the generated texts but also more often results in nonsensical text. For\\nexample, using the temperature of 5 results in texts such as every effort moves you\\npizza about 4% of the time.\\n5.3.2\\nTop-k sampling\\nWe’ve now implemented a probabilistic sampling approach coupled with temperature\\nscaling to increase the diversity of the outputs. We saw that higher temperature values\\nresult in more uniformly distributed next-token probabilities, which result in more\\ndiverse outputs as it reduces the likelihood of the model repeatedly selecting the most\\nprobable token. This method allows for the exploring of less likely but potentially\\nmore interesting and creative paths in the generation process. However, one down-\\nside of this approach is that it sometimes leads to grammatically incorrect or com-\\npletely nonsensical outputs such as every effort moves you pizza.\\nExercise 5.1\\nUse the print_sampled_tokens function to print the sampling frequencies of the\\nsoftmax probabilities scaled with the temperatures shown in figure 5.14. How often\\nis the word pizza sampled in each case? Can you think of a faster and more accurate\\nway to determine how often the word pizza is sampled?\\nFigure 5.14\\nA temperature of 1 represents the unscaled probability \\nscores for each token in the vocabulary. Decreasing the temperature to \\n0.1 sharpens the distribution, so the most likely token (here, “forward”) \\nwill have an even higher probability score. Likewise, increasing the \\ntemperature to 5 makes the distribution more uniform.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 177}, page_content='156\\nCHAPTER 5\\nPretraining on unlabeled data\\n Top-k sampling, when combined with probabilistic sampling and temperature scal-\\ning, can improve the text generation results. In top-k sampling, we can restrict the\\nsampled tokens to the top-k most likely tokens and exclude all other tokens from the\\nselection process by masking their probability scores, as illustrated in figure 5.15.\\nThe top-k approach replaces all nonselected logits with negative infinity value (-inf),\\nsuch that when computing the softmax values, the probability scores of the non-top-k\\ntokens are 0, and the remaining probabilities sum up to 1. (Careful readers may\\nremember this masking trick from the causal attention module we implemented in\\nchapter 3, section 3.5.1.)\\n In code, we can implement the top-k procedure in figure 5.15 as follows, starting\\nwith the selection of the tokens with the largest logit values:\\ntop_k = 3\\ntop_logits, top_pos = torch.topk(next_token_logits, top_k)\\nprint(\"Top logits:\", top_logits)\\nprint(\"Top positions:\", top_pos)\\nLogits\\nTop-k (k = 3)\\n-inf mask\\nSoftmax\\n= [ 4.51,  0.89, -1.90,  6.75,  1.63, -1.62, -1.89,  6.28,  1.79 ]\\n= [ 4.51,  0.89, -1.90,  6.75,  1.63, -1.62, -1.89,  6.28,  1.79 ]\\n4.51,\\n0       1     2      3      4      5      6      7      8\\nIndex position:\\n\"closer\"\\n\"every\"\\n\"effort\"\\n\"forward\"\\n\"inches\"\\n\"moves\"\\n\"pizza\"\\n\"toward\"\\n\"you\"\\nVocabulary:\\n= [ 4.51,  -inf, -inf,   6.75,  -inf,  -inf,  -inf,  6.28,  -inf ]\\n= [ 0.06,  0.00,  0.00,  0.57,  0.00,  0.00,  0.00,  0.36,  0.00 ]\\n,  6.75,  1.63, -1.62, -1.89,  6.28,  1.79 ]\\n,  6.75,  1.63, -1.62, -1.89,  6.28,  1.79 ]\\n4.51,\\n6.75,  -inf,  -inf,  -inf,  6.28,  -inf ]\\n6.75,  -inf,  -inf,  -inf,  6.28,  -inf ]\\n0.06,\\n,  0.57,  0.00,  0.00,  0.00,  0.36,  0.00 ]\\n,  0.57,  0.00,  0.00,  0.00,  0.36,  0.00 ]\\nBy assigning zero probabilities to the\\nnon-top-k positions, we ensure that\\nthe next token is always sampled\\nfrom a top-k position.\\nFigure 5.15\\nUsing top-k sampling with k = 3, we focus on the three tokens associated with the highest logits \\nand mask out all other tokens with negative infinity (–inf) before applying the softmax function. This results \\nin a probability distribution with a probability value 0 assigned to all non-top-k tokens. (The numbers in this figure \\nare truncated to two digits after the decimal point to reduce visual clutter. The values in the “Softmax” row \\nshould add up to 1.0.)'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 178}, page_content=\"157\\n5.3\\nDecoding strategies to control randomness\\nThe logits values and token IDs of the top three tokens, in descending order, are\\nTop logits: tensor([6.7500, 6.2800, 4.5100])\\nTop positions: tensor([3, 7, 0])\\nSubsequently, we apply PyTorch’s where function to set the logit values of tokens that are\\nbelow the lowest logit value within our top-three selection to negative infinity (-inf): \\nnew_logits = torch.where(\\n    condition=next_token_logits < top_logits[-1],   \\n    input=torch.tensor(float('-inf')),    \\n    other=next_token_logits    \\n)\\nprint(new_logits)\\nThe resulting logits for the next token in the nine-token vocabulary are\\ntensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,\\n     -inf])\\nLastly, let’s apply the softmax function to turn these into next-token probabilities:\\ntopk_probas = torch.softmax(new_logits, dim=0)\\nprint(topk_probas)\\nAs we can see, the result of this top-three approach are three non-zero probability\\nscores:\\ntensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610,\\n        0.0000])\\nWe can now apply the temperature scaling and multinomial function for probabilistic\\nsampling to select the next token among these three non-zero probability scores to\\ngenerate the next token. We do this next by modifying the text generation function.\\n5.3.3\\nModifying the text generation function\\nNow, let’s combine temperature sampling and top-k sampling to modify the generate_\\ntext_simple function we used to generate text via the LLM earlier, creating a new\\ngenerate function.\\ndef generate(model, idx, max_new_tokens, context_size,\\n             temperature=0.0, top_k=None, eos_id=None):\\n    for _ in range(max_new_tokens):           \\n        idx_cond = idx[:, -context_size:]\\n        with torch.no_grad():\\n            logits = model(idx_cond)\\n        logits = logits[:, -1, :]\\nListing 5.4\\nA modified text generation function with more diversity\\nIdentifies logits less than \\nthe minimum in the top 3\\nAssigns –inf to these lower logits\\nRetains the original logits \\nfor all other tokens\\nThe for loop is the same \\nas before: gets logits \\nand only focuses on the \\nlast time step.\"),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 179}, page_content='158\\nCHAPTER 5\\nPretraining on unlabeled data\\n        if top_k is not None:               \\n            top_logits, _ = torch.topk(logits, top_k)\\n            min_val = top_logits[:, -1]\\n            logits = torch.where(\\n                logits < min_val,\\n                torch.tensor(float(\\'-inf\\')).to(logits.device),\\n                logits\\n            )\\n        if temperature > 0.0:                 \\n            logits = logits / temperature\\n            probs = torch.softmax(logits, dim=-1)\\n            idx_next = torch.multinomial(probs, num_samples=1)\\n        else:   \\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\\n        if idx_next == eos_id:             \\n            break\\n        idx = torch.cat((idx, idx_next), dim=1)\\n    return idx\\nLet’s now see this new generate function in action:\\ntorch.manual_seed(123)\\ntoken_ids = generate(\\n    model=model,\\n    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\\n    max_new_tokens=15,\\n    context_size=GPT_CONFIG_124M[\"context_length\"],\\n    top_k=25,\\n    temperature=1.4\\n)\\nprint(\"Output text:\\\\n\", token_ids_to_text(token_ids, tokenizer))\\nThe generated text is\\nOutput text:\\n Every effort moves you stand to work on surprise, a one of us had gone\\n with random-\\nAs we can see, the generated text is very different from the one we previously gener-\\nated via the generate_simple function in section 5.3 (\"Every effort moves you know,\"\\nwas one of the axioms he laid...! ), which was a memorized passage from the train-\\ning set.\\nExercise 5.2\\nPlay around with different temperatures and top-k settings. Based on your observa-\\ntions, can you think of applications where lower temperature and top-k settings are\\ndesired? Likewise, can you think of applications where higher temperature and top-k\\nsettings are preferred? (It’s recommended to also revisit this exercise at the end of\\nthe chapter after loading the pretrained weights from OpenAI.)\\nFilters logits with \\ntop_k sampling\\nApplies \\ntemperature \\nscaling\\nCarries out \\ngreedy next-\\ntoken selection \\nas before when \\ntemperature \\nscaling is \\ndisabled\\nStops generating early \\nif end-of-sequence \\ntoken is encountered'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 180}, page_content='159\\n5.4\\nLoading and saving model weights in PyTorch\\n5.4\\nLoading and saving model weights in PyTorch\\nThus far, we have discussed how to numerically evaluate the training progress and pre-\\ntrain an LLM from scratch. Even though both the LLM and dataset were relatively\\nsmall, this exercise showed that pretraining LLMs is computationally expensive. Thus,\\nit is important to be able to save the LLM so that we don’t have to rerun the training\\nevery time we want to use it in a new session. \\n So, let’s discuss how to save and load a pretrained model, as highlighted in fig-\\nure 5.16. Later, we will load a more capable pretrained GPT model from OpenAI into\\nour GPTModel instance.\\nFortunately, saving a PyTorch model is relatively straightforward. The recommended\\nway is to save a model’s state_dict, a dictionary mapping each layer to its parameters,\\nusing the torch.save function:\\ntorch.save(model.state_dict(), \"model.pth\")\\n\"model.pth\" is the filename where the state_dict is saved. The .pth extension is a\\nconvention for PyTorch files, though we could technically use any file extension. \\n Then, after saving the model weights via the state_dict, we can load the model\\nweights into a new GPTModel model instance:\\nExercise 5.3\\nWhat are the different combinations of settings for the generate function to force\\ndeterministic behavior, that is, disabling the random sampling such that it always pro-\\nduces the same outputs similar to the generate_simple function?\\n1) Text\\ngeneration\\n2) Text\\nevaluation\\n3) Training\\n& validation\\nlosses\\n4) LLM training\\nfunction\\n5) Text generation\\nstrategies\\n6) Weight saving &\\nloading\\n7) Pretrained weights\\nfrom OpenAI\\nTrain the model\\nto generate\\nhuman-like text\\nImplement additional LLM text\\ngeneration strategies to reduce\\ntraining data memorization\\nAt the end of this chapter,\\nload pretrained weights from\\nOpenAI into our LLM model\\nImplement functions to save and\\nload the LLM weights to use or\\ncontinue training the LLM later\\nFigure 5.16\\nAfter training and inspecting the model, it is often helpful to save the model so that \\nwe can use or continue training it later (step 6).'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 181}, page_content='160\\nCHAPTER 5\\nPretraining on unlabeled data\\nmodel = GPTModel(GPT_CONFIG_124M)\\nmodel.load_state_dict(torch.load(\"model.pth\", map_location=device))\\nmodel.eval()\\nAs discussed in chapter 4, dropout helps prevent the model from overfitting to the\\ntraining data by randomly “dropping out” of a layer’s neurons during training. How-\\never, during inference, we don’t want to randomly drop out any of the information\\nthe network has learned. Using model.eval() switches the model to evaluation mode\\nfor inference, disabling the dropout layers of the model. If we plan to continue pre-\\ntraining a model later—for example, using the train_model_simple function we\\ndefined earlier in this chapter—saving the optimizer state is also recommended.\\n Adaptive optimizers such as AdamW store additional parameters for each model\\nweight. AdamW uses historical data to adjust learning rates for each model parameter\\ndynamically. Without it, the optimizer resets, and the model may learn suboptimally\\nor even fail to converge properly, which means it will lose the ability to generate coher-\\nent text. Using torch.save, we can save both the model and optimizer state_dict\\ncontents:\\ntorch.save({\\n    \"model_state_dict\": model.state_dict(),\\n    \"optimizer_state_dict\": optimizer.state_dict(),\\n    }, \\n    \"model_and_optimizer.pth\"\\n)\\nThen we can restore the model and optimizer states by first loading the saved data via\\ntorch.load and then using the load_state_dict method:\\ncheckpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\\nmodel = GPTModel(GPT_CONFIG_124M)\\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\\noptimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\\nmodel.train();\\n5.5\\nLoading pretrained weights from OpenAI\\nPreviously, we trained a small GPT-2 model using a limited dataset comprising a short-\\nstory book. This approach allowed us to focus on the fundamentals without the need\\nfor extensive time and computational resources. \\nExercise 5.4\\nAfter saving the weights, load the model and optimizer in a new Python session or\\nJupyter notebook file and continue pretraining it for one more epoch using the\\ntrain_model_simple function.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 182}, page_content='161\\n5.5\\nLoading pretrained weights from OpenAI\\n Fortunately, OpenAI openly shared the weights of their GPT-2 models, thus elimi-\\nnating the need to invest tens to hundreds of thousands of dollars in retraining the\\nmodel on a large corpus ourselves. So, let’s load these weights into our GPTModel class\\nand use the model for text generation. Here, weights refer to the weight parameters\\nstored in the .weight attributes of PyTorch’s Linear and Embedding layers, for exam-\\nple. We accessed them earlier via model.parameters() when training the model. In\\nchapter 6, will reuse these pretrained weights to fine-tune the model for a text classifi-\\ncation task and follow instructions similar to ChatGPT.\\n Note that OpenAI originally saved the GPT-2 weights via TensorFlow, which we\\nhave to install to load the weights in Python. The following code will use a progress\\nbar tool called tqdm to track the download process, which we also have to install.\\n You can install these libraries by executing the following command in your terminal:\\npip install tensorflow>=2.15.0  tqdm>=4.66\\nThe download code is relatively long, mostly boilerplate, and not very interesting.\\nHence, instead of devoting precious space to discussing Python code for fetching files\\nfrom the internet, we download the gpt_download.py Python module directly from\\nthis chapter’s online repository:\\nimport urllib.request\\nurl = (\\n    \"https://raw.githubusercontent.com/rasbt/\"\\n    \"LLMs-from-scratch/main/ch05/\"\\n    \"01_main-chapter-code/gpt_download.py\"\\n)\\nfilename = url.split(\\'/\\')[-1]\\nurllib.request.urlretrieve(url, filename)\\nNext, after downloading this file to the local directory of your Python session, you\\nshould briefly inspect the contents of this file to ensure that it was saved correctly and\\ncontains valid Python code.\\n We can now import the download_and_load_gpt2 function from the gpt_download\\n.py file as follows, which will load the GPT-2 architecture settings (settings) and\\nweight parameters (params) into our Python session:\\nfrom gpt_download import download_and_load_gpt2\\nsettings, params = download_and_load_gpt2(\\n    model_size=\"124M\", models_dir=\"gpt2\"\\n)\\nExecuting this code downloads the following seven files associated with the 124M\\nparameter GPT-2 model:\\ncheckpoint: 100%|███████████████████████████| 77.0/77.0 [00:00<00:00, \\n                                                         63.9kiB/s]\\nencoder.json: 100%|█████████████████████████| 1.04M/1.04M [00:00<00:00,\\n                                                           2.20MiB/s]'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 183}, page_content='162\\nCHAPTER 5\\nPretraining on unlabeled data\\nhprams.json: 100%|██████████████████████████| 90.0/90.0 [00:00<00:00,\\n                                                         78.3kiB/s]\\nmodel.ckpt.data-00000-of-00001: 100%|███████| 498M/498M [01:09<00:00,\\n                                                         7.16MiB/s]\\nmodel.ckpt.index: 100%|█████████████████████| 5.21k/5.21k [00:00<00:00,\\n                                                           3.24MiB/s]\\nmodel.ckpt.meta: 100%|██████████████████████| 471k/471k [00:00<00:00, \\n                                                         2.46MiB/s]\\nvocab.bpe: 100%|████████████████████████████| 456k/456k [00:00<00:00,\\n                                                         1.70MiB/s]\\nNOTE\\nIf the download code does not work for you, it could be due to inter-\\nmittent internet connection, server problems, or changes in how OpenAI\\nshares the weights of the open-source GPT-2 model. In this case, please visit\\nthis chapter’s online code repository at https://github.com/rasbt/LLMs-\\nfrom-scratch for alternative and updated instructions, and reach out via the\\nManning Forum for further questions. \\nAssuming the execution of the previous code has completed, let’s inspect the contents\\nof settings and params:\\nprint(\"Settings:\", settings)\\nprint(\"Parameter dictionary keys:\", params.keys())\\nThe contents are\\nSettings: {\\'n_vocab\\': 50257, \\'n_ctx\\': 1024, \\'n_embd\\': 768, \\'n_head\\': 12,\\n           \\'n_layer\\': 12}\\nParameter dictionary keys: dict_keys([\\'blocks\\', \\'b\\', \\'g\\', \\'wpe\\', \\'wte\\'])\\nBoth settings and params are Python dictionaries. The settings dictionary stores the\\nLLM architecture settings similarly to our manually defined GPT_CONFIG_124M settings.\\nThe params dictionary contains the actual weight tensors. Note that we only printed\\nthe dictionary keys because printing the weight contents would take up too much\\nscreen space; however, we can inspect these weight tensors by printing the whole dic-\\ntionary via print(params) or by selecting individual tensors via the respective dictio-\\nnary keys, for example, the embedding layer weights:\\nprint(params[\"wte\"])\\nprint(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)\\nThe weights of the token embedding layer are\\n[[-0.11010301 ... -0.1363697   0.01506208   0.04531523]\\n [ 0.04034033 ...  0.08605453  0.00253983   0.04318958]\\n [-0.12746179  ...  0.08991534 -0.12972379 -0.08785918]\\n ...\\n [-0.04453601 ...   0.10435229  0.09783269 -0.06952604]\\n [ 0.1860082  ...  -0.09625227  0.07847701 -0.02245961]'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 184}, page_content='163\\n5.5\\nLoading pretrained weights from OpenAI\\n [ 0.05135201 ...   0.00704835  0.15519823  0.12067825]]\\nToken embedding weight tensor dimensions: (50257, 768)\\nWe downloaded and loaded the weights of the smallest GPT-2 model via the download_\\nand_load_gpt2(model_size=\"124M\", ...) setting. OpenAI also shares the weights of\\nlarger models: 355M, 774M, and 1558M. The overall architecture of these differently\\nsized GPT models is the same, as illustrated in figure 5.17, except that different\\nRepeat this transformer block:\\n• 12 × in “gpt2-small”\\n• 24 × in “gpt2-medium”\\n• 36 × in “gpt2-large”\\n• 48 × in “gpt2-xl”\\nNumber of heads in\\nmulti-head attention:\\n• 12 in “gpt2-small”\\n• 16 in “gpt2-medium”\\n• 20 in “gpt2-large”\\n• 25 in gpt2-xl”\\n“\\nEmbedding dimensions:\\n• 768 in “gpt2-small”\\n• 1,024 in “gpt2-medium”\\n• 1,280 in “gpt2-large”\\n• 1,600 in “gpt2-xl”\\nGPT\\nmodel\\nMasked multi-head\\nattention\\nLayerNorm 2\\nFeed forward\\nLayerNorm 1\\nDropout\\nDropout\\nToken embedding layer\\nTokenized text\\nEvery effort moves you\\nFinal LayerNorm\\nLinear output layer\\nN\\nDropout\\nPositional embedding layer\\nTotal number of parameters:\\n• 124 M in “gpt2-small”\\n• 355 M in “gpt2-medium”\\n• 774 M in “gpt2-large”\\n• 1558 M in “gpt2-xl”\\nFigure 5.17\\nGPT-2 LLMs come in several different model sizes, ranging from 124 million to 1,558 million \\nparameters. The core architecture is the same, with the only difference being the embedding sizes and the number \\nof times individual components like the attention heads and transformer blocks are repeated.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 185}, page_content='164\\nCHAPTER 5\\nPretraining on unlabeled data\\narchitectural elements are repeated different numbers of times and the embedding\\nsize differs. The remaining code in this chapter is also compatible with these larger\\nmodels.\\n After loading the GPT-2 model weights into Python, we still need to transfer them\\nfrom the settings and params dictionaries into our GPTModel instance. First, we cre-\\nate a dictionary that lists the differences between the different GPT model sizes in\\nfigure 5.17:\\nmodel_configs = {\\n    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\\n    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\\n    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\\n    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\\n}\\nSuppose we are interested in loading the smallest model, \"gpt2-small (124M)\". We can\\nuse the corresponding settings from the model_configs table to update our full-length\\nGPT_CONFIG_124M we defined and used earlier:\\nmodel_name = \"gpt2-small (124M)\"\\nNEW_CONFIG = GPT_CONFIG_124M.copy()\\nNEW_CONFIG.update(model_configs[model_name])\\nCareful readers may remember that we used a 256-token length earlier, but the origi-\\nnal GPT-2 models from OpenAI were trained with a 1,024-token length, so we have to\\nupdate the NEW_CONFIG accordingly:\\nNEW_CONFIG.update({\"context_length\": 1024})\\nAlso, OpenAI used bias vectors in the multi-head attention module’s linear layers to\\nimplement the query, key, and value matrix computations. Bias vectors are not com-\\nmonly used in LLMs anymore as they don’t improve the modeling performance and\\nare thus unnecessary. However, since we are working with pretrained weights, we need\\nto match the settings for consistency and enable these bias vectors:\\nNEW_CONFIG.update({\"qkv_bias\": True})\\nWe can now use the updated NEW_CONFIG dictionary to initialize a new GPTModel\\ninstance:\\ngpt = GPTModel(NEW_CONFIG)\\ngpt.eval()'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 186}, page_content='165\\n5.5\\nLoading pretrained weights from OpenAI\\nBy default, the GPTModel instance is initialized with random weights for pretraining.\\nThe last step to using OpenAI’s model weights is to override these random weights\\nwith the weights we loaded into the params dictionary. For this, we will first define a\\nsmall assign utility function that checks whether two tensors or arrays (left and\\nright)  have the same dimensions or shape and returns the right tensor as trainable\\nPyTorch parameters:\\ndef assign(left, right):\\n    if left.shape != right.shape:\\n        raise ValueError(f\"Shape mismatch. Left: {left.shape}, \"\\n                          \"Right: {right.shape}\"\\n        )\\n    return torch.nn.Parameter(torch.tensor(right))\\nNext, we define a load_weights_into_gpt function that loads the weights from the\\nparams dictionary into a GPTModel instance gpt.\\nimport numpy as np\\ndef load_weights_into_gpt(gpt, params):          \\n    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params[\\'wpe\\'])\\n    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params[\\'wte\\'])\\n    \\n    for b in range(len(params[\"blocks\"])):    \\n        q_w, k_w, v_w = np.split(                           \\n            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\\n        gpt.trf_blocks[b].att.W_query.weight = assign(\\n            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\\n        gpt.trf_blocks[b].att.W_key.weight = assign(\\n            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\\n        gpt.trf_blocks[b].att.W_value.weight = assign(\\n            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\\n        q_b, k_b, v_b = np.split(\\n            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\\n        gpt.trf_blocks[b].att.W_query.bias = assign(\\n            gpt.trf_blocks[b].att.W_query.bias, q_b)\\n        gpt.trf_blocks[b].att.W_key.bias = assign(\\n            gpt.trf_blocks[b].att.W_key.bias, k_b)\\n        gpt.trf_blocks[b].att.W_value.bias = assign(\\n            gpt.trf_blocks[b].att.W_value.bias, v_b)\\n        gpt.trf_blocks[b].att.out_proj.weight = assign(\\n            gpt.trf_blocks[b].att.out_proj.weight, \\n            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\\nListing 5.5\\nLoading OpenAI weights into our GPT model code\\nSets the model’s positional \\nand token embedding weights \\nto those specified in params.\\nIterates over each transformer block in the model\\nThe np.split function is used to divide the attention and bias weights\\ninto three equal parts for the query, key, and value components.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 187}, page_content='166\\nCHAPTER 5\\nPretraining on unlabeled data\\n        gpt.trf_blocks[b].att.out_proj.bias = assign(\\n            gpt.trf_blocks[b].att.out_proj.bias, \\n            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\\n        gpt.trf_blocks[b].ff.layers[0].weight = assign(\\n            gpt.trf_blocks[b].ff.layers[0].weight, \\n            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\\n        gpt.trf_blocks[b].ff.layers[0].bias = assign(\\n            gpt.trf_blocks[b].ff.layers[0].bias, \\n            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\\n        gpt.trf_blocks[b].ff.layers[2].weight = assign(\\n            gpt.trf_blocks[b].ff.layers[2].weight, \\n            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\\n        gpt.trf_blocks[b].ff.layers[2].bias = assign(\\n            gpt.trf_blocks[b].ff.layers[2].bias, \\n            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\\n        gpt.trf_blocks[b].norm1.scale = assign(\\n            gpt.trf_blocks[b].norm1.scale, \\n            params[\"blocks\"][b][\"ln_1\"][\"g\"])\\n        gpt.trf_blocks[b].norm1.shift = assign(\\n            gpt.trf_blocks[b].norm1.shift, \\n            params[\"blocks\"][b][\"ln_1\"][\"b\"])\\n        gpt.trf_blocks[b].norm2.scale = assign(\\n            gpt.trf_blocks[b].norm2.scale, \\n            params[\"blocks\"][b][\"ln_2\"][\"g\"])\\n        gpt.trf_blocks[b].norm2.shift = assign(\\n            gpt.trf_blocks[b].norm2.shift, \\n            params[\"blocks\"][b][\"ln_2\"][\"b\"])\\n    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\\n    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\\n    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])   \\nIn the load_weights_into_gpt function, we carefully match the weights from\\nOpenAI’s implementation with our GPTModel implementation. To pick a specific\\nexample, OpenAI stored the weight tensor for the output projection layer for the\\nfirst transformer block as params[\"blocks\"][0][\"attn\"][\"c_proj\"][\"w\"]. In our\\nimplementation, this weight tensor corresponds to gpt.trf_blocks[b].att.out_proj\\n.weight, where gpt is a GPTModel instance.\\n Developing the load_weights_into_gpt function took a lot of guesswork since\\nOpenAI used a slightly different naming convention from ours. However, the assign\\nfunction would alert us if we try to match two tensors with different dimensions. Also,\\nif we made a mistake in this function, we would notice this, as the resulting GPT\\nmodel would be unable to produce coherent text.\\n Let’s now try the load_weights_into_gpt out in practice and load the OpenAI\\nmodel weights into our GPTModel instance gpt:\\nload_weights_into_gpt(gpt, params)\\ngpt.to(device)\\nThe original GPT-2 model\\nby OpenAI reused the token\\nembedding weights in the\\noutput layer to reduce the\\ntotal number of parameters,\\nwhich is a concept known as\\nweight tying.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 188}, page_content='167\\nSummary\\nIf the model is loaded correctly, we can now use it to generate new text using our pre-\\nvious generate function:\\ntorch.manual_seed(123)\\ntoken_ids = generate(\\n    model=gpt,\\n    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\\n    max_new_tokens=25,\\n    context_size=NEW_CONFIG[\"context_length\"],\\n    top_k=50,\\n    temperature=1.5\\n)\\nprint(\"Output text:\\\\n\", token_ids_to_text(token_ids, tokenizer))\\nThe resulting text is as follows:\\nOutput text:\\n Every effort moves you toward finding an ideal new way to practice \\nsomething!\\nWhat makes us want to be on top of that?\\nWe can be confident that we loaded the model weights correctly because the model can\\nproduce coherent text. A tiny mistake in this process would cause the model to fail. In\\nthe following chapters, we will work further with this pretrained model and fine-tune it\\nto classify text and follow instructions.\\nSummary\\n\\uf0a1When LLMs generate text, they output one token at a time.\\n\\uf0a1By default, the next token is generated by converting the model outputs into\\nprobability scores and selecting the token from the vocabulary that corresponds\\nto the highest probability score, which is known as “greedy decoding.”\\n\\uf0a1Using probabilistic sampling and temperature scaling, we can influence the\\ndiversity and coherence of the generated text.\\n\\uf0a1Training and validation set losses can be used to gauge the quality of text gener-\\nated by LLM during training.\\nExercise 5.5\\nCalculate the training and validation set losses of the GPTModel with the pretrained\\nweights from OpenAI on the “The Verdict” dataset.\\nExercise 5.6\\nExperiment with GPT-2 models of different sizes—for example, the largest 1,558 mil-\\nlion parameter model—and compare the generated text to the 124 million model.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 189}, page_content='168\\nCHAPTER 5\\nPretraining on unlabeled data\\n\\uf0a1Pretraining an LLM involves changing its weights to minimize the training loss.\\n\\uf0a1The training loop for LLMs itself is a standard procedure in deep learning,\\nusing a conventional cross entropy loss and AdamW optimizer.\\n\\uf0a1Pretraining an LLM on a large text corpus is time- and resource-intensive, so we\\ncan load openly available weights as an alternative to pretraining the model on\\na large dataset ourselves.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 190}, page_content='169\\nFine-tuning\\nfor classification\\nSo far, we have coded the LLM architecture, pretrained it, and learned how to\\nimport pretrained weights from an external source, such as OpenAI, into our\\nmodel. Now we will reap the fruits of our labor by fine-tuning the LLM on a specific\\ntarget task, such as classifying text. The concrete example we examine is classifying\\ntext messages as “spam” or “not spam.” Figure 6.1 highlights the two main ways of\\nfine-tuning an LLM: fine-tuning for classification (step 8) and fine-tuning to follow\\ninstructions (step 9).\\nThis chapter covers\\n\\uf0a1Introducing different LLM fine-tuning approaches\\n\\uf0a1Preparing a dataset for text classification\\n\\uf0a1Modifying a pretrained LLM for fine-tuning \\n\\uf0a1Fine-tuning an LLM to identify spam messages\\n\\uf0a1Evaluating the accuracy of a fine-tuned LLM \\nclassifier\\n\\uf0a1Using a fine-tuned LLM to classify new data'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 191}, page_content='170\\nCHAPTER 6\\nFine-tuning for classification\\n6.1\\nDifferent categories of fine-tuning\\nThe most common ways to fine-tune language models are instruction fine-tuning and\\nclassification fine-tuning. Instruction fine-tuning involves training a language model on\\na set of tasks using specific instructions to improve its ability to understand and exe-\\ncute tasks described in natural language prompts, as illustrated in figure 6.2. \\nIn classification fine-tuning, a concept you might already be acquainted with if you\\nhave a background in machine learning, the model is trained to recognize a specific\\nIn this chapter, we will\\nﬁne-tune the pretrained\\nLLM to classify texts\\nIn chapter 5, we\\npretrained an LLM\\nIn chapter 4, we\\nimplemented a GPT-like\\nLLM architecture\\nIn chapter 5, we also loaded\\npretrained model weights\\ninto the LLM architecture\\n1) Data\\npreparation\\n& sampling\\n2) Attention\\nmechanism\\nBuilding an LLM\\nSTAGE 1\\nFoundation model\\nSTAGE 2\\nSTAGE 3\\nClassiﬁer\\nPersonal assistant\\nDataset with class labels\\nInstruction dataset\\n4) Pretraining\\n9) Fine-tuning\\n5) Training\\nloop\\n6) Model\\nevaluation\\n7) Load\\npretrained\\nweights\\n8) Fine-tuning\\n3) LLM\\narchitecture\\nFigure 6.1\\nThe three main stages of coding an LLM. This chapter focus on stage 3 (step 8): fine-tuning a \\npretrained LLM as a classifier.\\nYes.\\nAdd instructions\\nfor the model\\nLLM\\nLLM\\nModel input\\nModel output\\nAnswer with \\'yes\\' or \\'no\\'.\\nIs the following text ‘spam’?\\n“You are a winner you have been\\nspecially selected to receive $1000\\ncash or a $2000 award.”\\nTranslate into German:\\n“The quick brown fox\\njumps over the lazy dog.”\\n\"Der schnelle braune\\nFuchs springt über den\\nfaulen Hund.\"\\nFigure 6.2\\nTwo different instruction fine-tuning scenarios. At the top, the model is tasked with determining \\nwhether a given text is spam. At the bottom, the model is given an instruction on how to translate an English \\nsentence into German.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 192}, page_content=\"171\\n6.1\\nDifferent categories of fine-tuning\\nset of class labels, such as “spam” and “not spam.” Examples of classification tasks extend\\nbeyond LLMs and email filtering: they include identifying different species of plants\\nfrom images; categorizing news articles into topics like sports, politics, and technology;\\nand distinguishing between benign and malignant tumors in medical imaging.\\n The key point is that a classification fine-tuned model is restricted to predicting\\nclasses it has encountered during its training. For instance, it can determine whether\\nsomething is “spam” or “not spam,” as illustrated in figure 6.3, but it can’t say anything\\nelse about the input text. \\nIn contrast to the classification fine-tuned model depicted in figure 6.3, an instruction\\nfine-tuned model typically can undertake a broader range of tasks. We can view a clas-\\nsification fine-tuned model as highly specialized, and generally, it is easier to develop a\\nspecialized model than a generalist model that works well across various tasks.\\nChoosing the right approach \\nInstruction fine-tuning improves a model’s ability to understand and generate responses\\nbased on specific user instructions. Instruction fine-tuning is best suited for models\\nthat need to handle a variety of tasks based on complex user instructions, improving\\nflexibility and interaction quality. Classification fine-tuning is ideal for projects requir-\\ning precise categorization of data into predefined classes, such as sentiment analy-\\nsis or spam detection. \\nWhile instruction fine-tuning is more versatile, it demands larger datasets and greater\\ncomputational resources to develop models proficient in various tasks. In contrast,\\nclassification fine-tuning requires less data and compute power, but its use is con-\\nfined to the specific classes on which the model has been trained.\\nSpam\\nLLM\\nLLM\\nModel input without\\ninstructions\\nModel can only output\\ntwo types of responses:\\n“Spam” and “Not spam.”\\nNot\\nspam\\n“You are a winner you have been\\nspecially selected to receive $1000\\ncash or a $2000 award.”\\n“Hey, just wanted to check if\\nwe're still on for dinner tonight?\\nLet me know!”\\nFigure 6.3\\nA text classification scenario using an LLM. A model fine-tuned for spam \\nclassification does not require further instruction alongside the input. In contrast to \\nan instruction fine-tuned model, it can only respond with “spam” or “not spam.”\"),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 193}, page_content='172\\nCHAPTER 6\\nFine-tuning for classification\\n6.2\\nPreparing the dataset\\nWe will modify and classification fine-tune the GPT model we previously implemented\\nand pretrained. We begin by downloading and preparing the dataset, as highlighted\\nin figure 6.4. To provide an intuitive and useful example of classification fine-tuning,\\nwe will work with a text message dataset that consists of spam and non-spam messages. \\nNOTE\\nText messages typically sent via phone, not email. However, the same\\nsteps also apply to email classification, and interested readers can find links to\\nemail spam classification datasets in appendix B.\\nThe first step is to download the dataset.\\nimport urllib.request\\nimport zipfile\\nimport os\\nfrom pathlib import Path\\nurl = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\\nzip_path = \"sms_spam_collection.zip\"\\nextracted_path = \"sms_spam_collection\"\\ndata_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\\ndef download_and_unzip_spam_data(\\n        url, zip_path, extracted_path, data_file_path):\\n    if data_file_path.exists():\\nListing 6.1\\nDownloading and unzipping the dataset\\nStage 1:\\nDataset preparation\\n1) Download\\nthe dataset\\n2) Preprocess\\ndataset\\n3) Create data\\nloaders\\n4) Initialize\\nmodel\\n5) Load pretrained\\nweights\\n6) Modify model\\nfor ﬁne-tuning\\n7) Implement\\nevaluation utilities\\n8) Fine-tune\\nmodel\\n9) Evaluate\\nﬁne-tuned model\\n10) Use model\\non new data\\nStage 2:\\nModel setup\\nStage 3:\\nModel ﬁne-tuning\\nand usage\\nWe start with downloading,\\ninspecting, and preparing the\\ndataset that we will use to\\nﬁne-tune the model.\\nFigure 6.4\\nThe three-stage process for classification fine-tuning an LLM. Stage 1 involves dataset \\npreparation. Stage 2 focuses on model setup. Stage 3 covers fine-tuning and evaluating the model.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 194}, page_content='173\\n6.2\\nPreparing the dataset\\n        print(f\"{data_file_path} already exists. Skipping download \"\\n              \"and extraction.\"\\n        )\\n        return\\n    with urllib.request.urlopen(url) as response:   \\n        with open(zip_path, \"wb\") as out_file:\\n            out_file.write(response.read())\\n    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:   \\n        zip_ref.extractall(extracted_path)\\n    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\\n    os.rename(original_file_path, data_file_path)              \\n    print(f\"File downloaded and saved as {data_file_path}\")\\ndownload_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\\nAfter executing the preceding code, the dataset is saved as a tab-separated text file,\\nSMSSpamCollection.tsv, in the sms_spam_collection folder. We can load it into a\\npandas DataFrame as follows:\\nimport pandas as pd\\ndf = pd.read_csv(\\n    data_file_path, sep=\"\\\\t\", header=None, names=[\"Label\", \"Text\"]\\n)\\ndf     \\nFigure 6.5 shows the resulting data frame of the spam dataset.\\nLet’s examine the class label distribution:\\nprint(df[\"Label\"].value_counts())\\nDownloads \\nthe file\\nUnzips the file\\nAdds a .tsv \\nfile extension\\nRenders the data frame in a Jupyter \\nnotebook. Alternatively, use print(df).\\nFigure 6.5\\nPreview of the \\nSMSSpamCollection dataset \\nin a pandas DataFrame, showing \\nclass labels (“ham” or “spam”) and \\ncorresponding text messages. The \\ndataset consists of 5,572 rows \\n(text messages and labels).'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 195}, page_content='174\\nCHAPTER 6\\nFine-tuning for classification\\nExecuting the previous code, we find that the data contains “ham” (i.e., not spam) far\\nmore frequently than “spam”:\\nLabel\\nham     4825\\nspam     747\\nName: count, dtype: int64\\nFor simplicity, and because we prefer a small dataset (which will facilitate faster fine-\\ntuning of the LLM), we choose to undersample the dataset to include 747 instances\\nfrom each class. \\nNOTE\\nThere are several other methods to handle class imbalances, but these\\nare beyond the scope of this book. Readers interested in exploring methods for\\ndealing with imbalanced data can find additional information in appendix B.\\nWe can use the code in the following listing to undersample and create a balanced\\ndataset.\\ndef create_balanced_dataset(df):\\n    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]    \\n    ham_subset = df[df[\"Label\"] == \"ham\"].sample(\\n        num_spam, random_state=123\\n    )                                        \\n    balanced_df = pd.concat([\\n        ham_subset, df[df[\"Label\"] == \"spam\"]\\n    ])                              \\n    return balanced_df\\nbalanced_df = create_balanced_dataset(df)\\nprint(balanced_df[\"Label\"].value_counts())\\nAfter executing the previous code to balance the dataset, we can see that we now have\\nequal amounts of spam and non-spam messages:\\nLabel\\nham     747\\nspam    747\\nName: count, dtype: int64\\nNext, we convert the “string” class labels \"ham\" and \"spam\" into integer class labels 0\\nand 1, respectively:\\nbalanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\\nThis process is similar to converting text into token IDs. However, instead of using the\\nGPT vocabulary, which consists of more than 50,000 words, we are dealing with just\\ntwo token IDs: 0 and 1.\\nListing 6.2\\nCreating a balanced dataset\\nCounts the instances \\nof “spam”\\nRandomly samples “ham” \\ninstances to match the number \\nof “spam” instances\\nCombines ham \\nsubset with “spam”'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 196}, page_content='175\\n6.3\\nCreating data loaders\\n Next, we create a random_split function to split the dataset into three parts: 70%\\nfor training, 10% for validation, and 20% for testing. (These ratios are common in\\nmachine learning to train, adjust, and evaluate models.)\\ndef random_split(df, train_frac, validation_frac):\\n    \\n    df = df.sample(\\n        frac=1, random_state=123\\n    ).reset_index(drop=True)              \\n    train_end = int(len(df) * train_frac)         \\n    validation_end = train_end + int(len(df) * validation_frac)\\n                                 \\n    train_df = df[:train_end]\\n    validation_df = df[train_end:validation_end]\\n    test_df = df[validation_end:]\\n    return train_df, validation_df, test_df\\ntrain_df, validation_df, test_df = random_split(\\n    balanced_df, 0.7, 0.1)                    \\nLet’s save the dataset as CSV (comma-separated value) files so we can reuse it later:\\ntrain_df.to_csv(\"train.csv\", index=None)\\nvalidation_df.to_csv(\"validation.csv\", index=None)\\ntest_df.to_csv(\"test.csv\", index=None)\\nThus far, we have downloaded the dataset, balanced it, and split it into training and\\nevaluation subsets. Now we will set up the PyTorch data loaders that will be used to\\ntrain the model.\\n6.3\\nCreating data loaders\\nWe will develop PyTorch data loaders conceptually similar to those we implemented\\nwhile working with text data. Previously, we utilized a sliding window technique to\\ngenerate uniformly sized text chunks, which we then grouped into batches for more\\nefficient model training. Each chunk functioned as an individual training instance.\\nHowever, we are now working with a spam dataset that contains text messages of vary-\\ning lengths. To batch these messages as we did with the text chunks, we have two pri-\\nmary options:\\n\\uf0a1Truncate all messages to the length of the shortest message in the dataset or batch.\\n\\uf0a1Pad all messages to the length of the longest message in the dataset or batch.\\nThe first option is computationally cheaper, but it may result in significant informa-\\ntion loss if shorter messages are much smaller than the average or longest messages,\\nListing 6.3\\nSplitting the dataset\\nShuffles the entire \\nDataFrame\\nCalculates \\nsplit indices\\nSplits the DataFrame\\nTest size is implied \\nto be 0.2 as the \\nremainder.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 197}, page_content='176\\nCHAPTER 6\\nFine-tuning for classification\\npotentially reducing model performance. So, we opt for the second option, which\\npreserves the entire content of all messages.\\n To implement batching, where all messages are padded to the length of the lon-\\ngest message in the dataset, we add padding tokens to all shorter messages. For this\\npurpose, we use \"<|endoftext|>\" as a padding token. \\n However, instead of appending the string \"<|endoftext|>\" to each of the text\\nmessages directly, we can add the token ID corresponding to \"<|endoftext|>\" to the\\nencoded text messages, as illustrated in figure 6.6. 50256 is the token ID of the padding\\ntoken \"<|endoftext|>\". We can double-check whether the token ID is correct by\\nencoding the \"<|endoftext|>\" using the GPT-2 tokenizer from the tiktoken package\\nthat we used previously:\\nimport tiktoken\\ntokenizer = tiktoken.get_encoding(\"gpt2\")\\nprint(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))\\nIndeed, executing the preceding code returns [50256].\\n We first need to implement a PyTorch Dataset, which specifies how the data is\\nloaded and processed before we can instantiate the data loaders. For this purpose,\\nwe define the SpamDataset class, which implements the concepts in figure 6.6. This\\nSpamDataset class handles several key tasks: it identifies the longest sequence in the\\ntraining dataset, encodes the text messages, and ensures that all other sequences are\\npadded with a padding token to match the length of the longest sequence.\\n \\nThis is the first text\\nmessage\\nThis is another text\\nmessage\\nThis is the third text\\nmessage, which is\\nvery long\\n1212, 318, 262, 717,\\n2420, 3275\\n1212, 318, 1194,\\n2420, 3275\\n1212, 318, 262, 2368,\\n2420, 3275, 11, 543, 318,\\n845, 890\\n1. Tokenize\\ntexts\\nInput text\\nmessage\\nToken IDs\\n2. Pad to longest\\nsequence\\n1212, 318, 262, 2368,\\n2420, 3275, 11, 543,\\n318, 845, 890\\n1212, 318, 262, 717,\\n2420, 3275, 50256, 50256,\\n50256, 50256, 50256\\nPadded\\ntoken IDs\\n1212, 318, 1194, 2420,\\n3275, 50256, 50256, 50256,\\n50256, 50256, 50256\\nWe use 50256 as\\na padding token.\\nNo padding in this last\\nexample because it is\\nthe longest message\\nFigure 6.6\\nThe input text preparation process. First, each input text message is converted into a sequence of \\ntoken IDs. Then, to ensure uniform sequence lengths, shorter sequences are padded with a padding token (in this \\ncase, token ID 50256) to match the length of the longest sequence.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 198}, page_content='177\\n6.3\\nCreating data loaders\\nimport torch\\nfrom torch.utils.data import Dataset\\nclass SpamDataset(Dataset):\\n    def __init__(self, csv_file, tokenizer, max_length=None,\\n                 pad_token_id=50256):\\n        self.data = pd.read_csv(csv_file)\\n                                            \\n        self.encoded_texts = [\\n            tokenizer.encode(text) for text in self.data[\"Text\"]\\n        ]\\n        if max_length is None:\\n            self.max_length = self._longest_encoded_length()\\n        else:\\n            self.max_length = max_length\\n                                           \\n            self.encoded_texts = [\\n                encoded_text[:self.max_length]\\n                for encoded_text in self.encoded_texts\\n            ]\\n                                         \\n        self.encoded_texts = [\\n            encoded_text + [pad_token_id] * \\n            (self.max_length - len(encoded_text))\\n            for encoded_text in self.encoded_texts\\n        ]\\n    def __getitem__(self, index):\\n        encoded = self.encoded_texts[index]\\n        label = self.data.iloc[index][\"Label\"]\\n        return (\\n            torch.tensor(encoded, dtype=torch.long),\\n            torch.tensor(label, dtype=torch.long)\\n        )\\n    def __len__(self):\\n        return len(self.data)\\n    def _longest_encoded_length(self):\\n        max_length = 0\\n        for encoded_text in self.encoded_texts:\\n            encoded_length = len(encoded_text)\\n            if encoded_length > max_length:\\n                max_length = encoded_length\\n        return max_length\\nListing 6.4\\nSetting up a Pytorch Dataset class\\nPretokenizes texts\\nTruncates sequences if they \\nare longer than max_length\\nPads sequences to \\nthe longest sequence'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 199}, page_content='178\\nCHAPTER 6\\nFine-tuning for classification\\nThe SpamDataset class loads data from the CSV files we created earlier, tokenizes\\nthe text using the GPT-2 tokenizer from tiktoken, and allows us to pad or truncate\\nthe sequences to a uniform length determined by either the longest sequence or a\\npredefined maximum length. This ensures each input tensor is of the same size,\\nwhich is necessary to create the batches in the training data loader we implement\\nnext:\\ntrain_dataset = SpamDataset(\\n    csv_file=\"train.csv\",\\n    max_length=None,\\n    tokenizer=tokenizer\\n)\\nThe longest sequence length is stored in the dataset’s max_length attribute. If you are\\ncurious to see the number of tokens in the longest sequence, you can use the follow-\\ning code:\\nprint(train_dataset.max_length)\\nThe code outputs 120, showing that the longest sequence contains no more than\\n120 tokens, a common length for text messages. The model can handle sequences\\nof up to 1,024 tokens, given its context length limit. If your dataset includes longer\\ntexts, you can pass max_length=1024 when creating the training dataset in the pre-\\nceding code to ensure that the data does not exceed the model’s supported input\\n(context) length.\\n Next, we pad the validation and test sets to match the length of the longest train-\\ning sequence. Importantly, any validation and test set samples exceeding the length of\\nthe longest training example are truncated using encoded_text[:self.max_length]\\nin the SpamDataset code we defined earlier. This truncation is optional; you can set\\nmax_length=None for both validation and test sets, provided there are no sequences\\nexceeding 1,024 tokens in these sets:\\nval_dataset = SpamDataset(\\n    csv_file=\"validation.csv\",\\n    max_length=train_dataset.max_length,\\n    tokenizer=tokenizer\\n)\\ntest_dataset = SpamDataset(\\n    csv_file=\"test.csv\",\\n    max_length=train_dataset.max_length,\\n    tokenizer=tokenizer\\n)'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 200}, page_content='179\\n6.3\\nCreating data loaders\\nUsing the datasets as inputs, we can instantiate the data loaders similarly to when we\\nwere working with text data. However, in this case, the targets represent class labels\\nrather than the next tokens in the text. For instance, if we choose a batch size of 8,\\neach batch will consist of eight training examples of length 120 and the correspond-\\ning class label of each example, as illustrated in figure 6.7.\\nExercise 6.1 Increasing the context length \\nPad the inputs to the maximum number of tokens the model supports and observe\\nhow it affects the predictive performance.\\n20266, 40621, 15762, ..., 50256\\n2953,   644,   640, ..., 50256\\n55, 31180, 15895, ..., 50256\\nAt what time are\\nyou ...\\nXMAS Prize\\ndraws!  ...\\nDear voucher\\nholder ...\\nK. I will sent it\\nagain ...\\n42,    13,   314, ..., 50256\\n1\\n2\\n3\\n8\\n0\\n1\\n1\\n0\\nOriginal message text\\nEach batch consists of\\neight training examples.\\nEach entry (row) represents\\nthe token IDs corresponding\\nto the original message text\\nThe class label of the\\neighth training example\\nClass label array, where 1\\nstands for “spam” and 0\\nfor “ham” (not spam)\\nEach text message is\\npadded to 120 tokens.\\n1\\n2\\n3\\n120\\nFigure 6.7\\nA single training batch consisting of eight text messages represented as token IDs. Each \\ntext message consists of 120 token IDs. A class label array stores the eight class labels corresponding \\nto the text messages, which can be either 0 (“not spam”) or 1 (“spam”).'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 201}, page_content='180\\nCHAPTER 6\\nFine-tuning for classification\\nThe code in the following listing creates the training, validation, and test set data load-\\ners that load the text messages and labels in batches of size 8.\\nfrom torch.utils.data import DataLoader\\nnum_workers = 0     \\nbatch_size = 8\\ntorch.manual_seed(123)\\ntrain_loader = DataLoader(\\n    dataset=train_dataset,\\n    batch_size=batch_size,\\n    shuffle=True,\\n    num_workers=num_workers,\\n    drop_last=True,\\n)\\nval_loader = DataLoader(\\n    dataset=val_dataset,\\n    batch_size=batch_size,\\n    num_workers=num_workers,\\n    drop_last=False,\\n)\\ntest_loader = DataLoader(\\n    dataset=test_dataset,\\n    batch_size=batch_size,\\n    num_workers=num_workers,\\n    drop_last=False,\\n)\\nTo ensure that the data loaders are working and are, indeed, returning batches of the\\nexpected size, we iterate over the training loader and then print the tensor dimen-\\nsions of the last batch:\\nfor input_batch, target_batch in train_loader:\\n    pass\\nprint(\"Input batch dimensions:\", input_batch.shape)\\nprint(\"Label batch dimensions\", target_batch.shape)\\nThe output is\\nInput batch dimensions: torch.Size([8, 120])\\nLabel batch dimensions torch.Size([8])\\nAs we can see, the input batches consist of eight training examples with 120 tokens\\neach, as expected. The label tensor stores the class labels corresponding to the eight\\ntraining examples.\\n Lastly, to get an idea of the dataset size, let’s print the total number of batches in\\neach dataset:\\nListing 6.5\\nCreating PyTorch data loaders\\nThis setting ensures compatibility \\nwith most computers.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 202}, page_content='181\\n6.4\\nInitializing a model with pretrained weights\\nprint(f\"{len(train_loader)} training batches\")\\nprint(f\"{len(val_loader)} validation batches\")\\nprint(f\"{len(test_loader)} test batches\")\\nThe number of batches in each dataset are\\n130 training batches\\n19 validation batches\\n38 test batches\\nNow that we’ve prepared the data, we need to prepare the model for fine-tuning.\\n6.4\\nInitializing a model with pretrained weights\\nWe must prepare the model for classification fine-tuning to identify spam messages.\\nWe start by initializing our pretrained model, as highlighted in figure 6.8.\\nTo begin the model preparation process, we employ the same configurations we used\\nto pretrain unlabeled data:\\nCHOOSE_MODEL = \"gpt2-small (124M)\"\\nINPUT_PROMPT = \"Every effort moves\"\\nIn this section, we initialize the\\npretrained model from the previous\\nchapter that we will ﬁne-tune.\\nStage 1:\\nDataset preparation\\n1) Download\\nthe dataset\\n2) Preprocess\\ndataset\\n3) Create data\\nloaders\\n4) Initialize\\nmodel\\n5) Load pretrained\\nweights\\n6) Modify model\\nfor ﬁne-tuning\\n7) Implement\\nevaluation utilities\\n8) Fine-tune\\nmodel\\n9) Evaluate\\nﬁne-tuned model\\n10) Use model\\non new data\\n4) Initialize\\nmodel\\n5) Load pretrained\\nweights\\nStage 2:\\nModel setup\\nStage 3:\\nModel ﬁne-tuning\\nand usage\\nIn the previous section,\\nwe prepared the SPAM\\nprediction dataset for\\nclassiﬁcation ﬁne-tuning.\\nFigure 6.8\\nThe three-stage process for classification fine-tuning the LLM. Having completed stage 1, \\npreparing the dataset, we now must initialize the LLM, which we will then fine-tune to classify spam \\nmessages.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 203}, page_content='182\\nCHAPTER 6\\nFine-tuning for classification\\nBASE_CONFIG = {\\n    \"vocab_size\": 50257,         \\n    \"context_length\": 1024,      \\n    \"drop_rate\": 0.0,            \\n    \"qkv_bias\": True             \\n}\\nmodel_configs = {\\n    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\\n    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\\n    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\\n    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\\n}\\nBASE_CONFIG.update(model_configs[CHOOSE_MODEL])\\nNext, we import the download_and_load_gpt2 function from the gpt_download.py\\nfile and reuse the GPTModel class and load_weights_into_gpt function from pretrain-\\ning (see chapter 5) to load the downloaded weights into the GPT model.\\nfrom gpt_download import download_and_load_gpt2\\nfrom chapter05 import GPTModel, load_weights_into_gpt\\nmodel_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\\nsettings, params = download_and_load_gpt2(\\n    model_size=model_size, models_dir=\"gpt2\"\\n)\\nmodel = GPTModel(BASE_CONFIG)\\nload_weights_into_gpt(model, params)\\nmodel.eval()\\nAfter loading the model weights into the GPTModel, we reuse the text generation util-\\nity function from chapters 4 and 5 to ensure that the model generates coherent text:\\nfrom chapter04 import generate_text_simple\\nfrom chapter05 import text_to_token_ids, token_ids_to_text\\ntext_1 = \"Every effort moves you\"\\ntoken_ids = generate_text_simple(\\n    model=model,\\n    idx=text_to_token_ids(text_1, tokenizer),\\n    max_new_tokens=15,\\n    context_size=BASE_CONFIG[\"context_length\"]\\n)\\nprint(token_ids_to_text(token_ids, tokenizer))\\nThe following output shows the model generates coherent text, which is indicates that\\nthe model weights have been loaded correctly:\\nEvery effort moves you forward.\\nThe first step is to understand the importance of your work\\nListing 6.6\\nLoading a pretrained GPT model\\nVocabulary size\\nContext length\\nDropout rate\\nQuery-key-value bias'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 204}, page_content='183\\n6.5\\nAdding a classification head\\nBefore we start fine-tuning the model as a spam classifier, let’s see whether the model\\nalready classifies spam messages by prompting it with instructions:\\ntext_2 = (\\n    \"Is the following text \\'spam\\'? Answer with \\'yes\\' or \\'no\\':\"\\n    \" \\'You are a winner you have been specially\"\\n    \" selected to receive $1000 cash or a $2000 award.\\'\"\\n)\\ntoken_ids = generate_text_simple(\\n    model=model,\\n    idx=text_to_token_ids(text_2, tokenizer),\\n    max_new_tokens=23,\\n    context_size=BASE_CONFIG[\"context_length\"]\\n)\\nprint(token_ids_to_text(token_ids, tokenizer))\\nThe model output is\\nIs the following text \\'spam\\'? Answer with \\'yes\\' or \\'no\\': \\'You are a winner\\nyou have been specially selected to receive $1000 cash \\nor a $2000 award.\\'\\nThe following text \\'spam\\'? Answer with \\'yes\\' or \\'no\\': \\'You are a winner\\nBased on the output, it’s apparent that the model is struggling to follow instructions.\\nThis result is expected, as it has only undergone pretraining and lacks instruction\\nfine-tuning. So, let’s prepare the model for classification fine-tuning.\\n6.5\\nAdding a classification head\\nWe must modify the pretrained LLM to prepare it for classification fine-tuning. To do\\nso, we replace the original output layer, which maps the hidden representation to a\\nvocabulary of 50,257, with a smaller output layer that maps to two classes: 0 (“not\\nspam”) and 1 (“spam”), as shown in figure 6.9. We use the same model as before, except\\nwe replace the output layer.\\n \\nOutput layer nodes \\nWe could technically use a single output node since we are dealing with a binary clas-\\nsification task. However, it would require modifying the loss function, as I discuss in\\n“Losses Learned—Optimizing Negative Log-Likelihood and Cross-Entropy in PyTorch”\\n(https://mng.bz/NRZ2). Therefore, we choose a more general approach, where the\\nnumber of output nodes matches the number of classes. For example, for a three-\\nclass problem, such as classifying news articles as “Technology,” “Sports,” or “Pol-\\nitics,” we would use three output nodes, and so forth.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 205}, page_content='184\\nCHAPTER 6\\nFine-tuning for classification\\nThe GPT model we implemented\\nin chapter 5 and loaded in the\\nprevious section\\nOutputs\\nThe original linear output layer mapped 768\\nhidden units to 50,257 units (the number of\\ntokens in the vocabulary).\\n1\\n768\\n50,257\\n1\\n1\\n768\\n1\\n2\\nWe replace the original linear output layer above\\nwith a layer that maps from 768 hidden units to\\nonly 2 units, where the 2 units represent the two\\nclasses (\"spam\" and \"not spam\").\\nGPT\\nmodel\\nMasked multihead\\nattention\\nLayerNorm 2\\nFeed forward\\nLayerNorm 1\\nDropout\\nDropout\\nToken embedding layer\\nTokenized text\\nFinal LayerNorm\\nLinear output layer\\n12\\nDropout\\nPositional embedding layer\\nInputs\\nFigure 6.9\\nAdapting a GPT model for spam classification by altering its architecture. Initially, the model’s linear \\noutput layer mapped 768 hidden units to a vocabulary of 50,257 tokens. To detect spam, we replace this layer \\nwith a new output layer that maps the same 768 hidden units to just two classes, representing “spam” and “not \\nspam.”'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 206}, page_content='185\\n6.5\\nAdding a classification head\\nBefore we attempt the modification shown in figure 6.9, let’s print the model architec-\\nture via print(model):\\nGPTModel(\\n  (tok_emb): Embedding(50257, 768)\\n  (pos_emb): Embedding(1024, 768)\\n  (drop_emb): Dropout(p=0.0, inplace=False)\\n  (trf_blocks): Sequential(\\n...\\n    (11): TransformerBlock(\\n      (att): MultiHeadAttention(\\n        (W_query): Linear(in_features=768, out_features=768, bias=True)\\n        (W_key): Linear(in_features=768, out_features=768, bias=True)\\n        (W_value): Linear(in_features=768, out_features=768, bias=True)\\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\\n        (dropout): Dropout(p=0.0, inplace=False)\\n      )\\n      (ff): FeedForward(\\n        (layers): Sequential(\\n          (0): Linear(in_features=768, out_features=3072, bias=True)\\n          (1): GELU()\\n          (2): Linear(in_features=3072, out_features=768, bias=True)\\n        )\\n      )\\n      (norm1): LayerNorm()\\n      (norm2): LayerNorm()\\n      (drop_resid): Dropout(p=0.0, inplace=False)\\n    )\\n  )\\n  (final_norm): LayerNorm()\\n  (out_head): Linear(in_features=768, out_features=50257, bias=False)\\n)\\nThis output neatly lays out the architecture we laid out in chapter 4. As previously dis-\\ncussed, the GPTModel consists of embedding layers followed by 12 identical transformer\\nblocks (only the last block is shown for brevity), followed by a final LayerNorm and the\\noutput layer, out_head. \\n Next, we replace the out_head with a new output layer (see figure 6.9) that we will\\nfine-tune.\\nFine-tuning selected layers vs. all layers\\nSince we start with a pretrained model, it’s not necessary to fine-tune all model layers.\\nIn neural network-based language models, the lower layers generally capture basic lan-\\nguage structures and semantics applicable across a wide range of tasks and datasets.\\nSo, fine-tuning only the last layers (i.e., layers near the output), which are more specific\\nto nuanced linguistic patterns and task-specific features, is often sufficient to adapt the\\nmodel to new tasks. A nice side effect is that it is computationally more efficient to fine-\\ntune only a small number of layers. Interested readers can find more information,\\nincluding experiments, on which layers to fine-tune in appendix B.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 207}, page_content='186\\nCHAPTER 6\\nFine-tuning for classification\\nTo get the model ready for classification fine-tuning, we first freeze the model, meaning\\nthat we make all layers nontrainable:\\nfor param in model.parameters():\\n    param.requires_grad = False\\nThen, we replace the output layer (model.out_head), which originally maps the layer\\ninputs to 50,257 dimensions, the size of the vocabulary (see figure 6.9).\\ntorch.manual_seed(123)\\nnum_classes = 2\\nmodel.out_head = torch.nn.Linear(\\n    in_features=BASE_CONFIG[\"emb_dim\"], \\n    out_features=num_classes\\n)\\nTo keep the code more general, we use BASE_CONFIG[\"emb_dim\"], which is equal to\\n768 in the \"gpt2-small (124M)\" model. Thus, we can also use the same code to work\\nwith the larger GPT-2 model variants.\\n This new model.out_head output layer has its requires_grad attribute set to\\nTrue by default, which means that it’s the only layer in the model that will be\\nupdated during training. Technically, training the output layer we just added is suffi-\\ncient. However, as I found in experiments, fine-tuning additional layers can notice-\\nably improve the predictive performance of the model. (For more details, refer to\\nappendix B.) We also configure the last transformer block and the final LayerNorm\\nmodule, which connects this block to the output layer, to be trainable, as depicted\\nin figure 6.10.\\n To make the final LayerNorm and last transformer block trainable, we set their\\nrespective requires_grad to True:\\nfor param in model.trf_blocks[-1].parameters():\\n    param.requires_grad = True\\nfor param in model.final_norm.parameters():\\n    param.requires_grad = True\\nEven though we added a new output layer and marked certain layers as trainable or\\nnontrainable, we can still use this model similarly to how we have previously. For\\nListing 6.7\\nAdding a classification layer\\nExercise 6.2 Fine-tuning the whole model \\nInstead of fine-tuning just the final transformer block, fine-tune the entire model and\\nassess the effect on predictive performance.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 208}, page_content='187\\n6.5\\nAdding a classification head\\ninstance, we can feed it an example text identical to our previously used example\\ntext:\\ninputs = tokenizer.encode(\"Do you have time\")\\ninputs = torch.tensor(inputs).unsqueeze(0)\\nprint(\"Inputs:\", inputs)\\nprint(\"Inputs dimensions:\", inputs.shape)   \\nAs we saw in chapter 5,\\nthis transformer block\\nis repeated 12x in the\\n124M-parameter GPT-2\\nmodel\\nWe make the output layer,\\nﬁnal LayerNorm, and the last\\ntransformer block trainable\\nThe GPT model we\\nimplemented in chapter 5\\nand loaded in the previous\\nsection\\nOutputs\\nGPT\\nmodel\\nMasked multihead\\nattention\\nLayerNorm 2\\nFeed forward\\nLayerNorm 1\\nDropout\\nDropout\\nToken embedding layer\\nTokenized text\\nFinal LayerNorm\\nLinear output layer\\n12\\nDropout\\nPositional embedding layer\\nInputs\\nTransformer\\nblock\\nFigure 6.10\\nThe GPT model includes 12 repeated transformer blocks. Alongside the output layer, we set the final \\nLayerNorm and the last transformer block as trainable. The remaining 11 transformer blocks and the embedding \\nlayers are kept nontrainable.\\nshape: (batch_size, \\nnum_tokens)'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 209}, page_content='188\\nCHAPTER 6\\nFine-tuning for classification\\nThe print output shows that the preceding code encodes the inputs into a tensor con-\\nsisting of four input tokens:\\nInputs: tensor([[5211,  345,  423,  640]])\\nInputs dimensions: torch.Size([1, 4])\\nThen, we can pass the encoded token IDs to the model as usual:\\nwith torch.no_grad():\\n    outputs = model(inputs)\\nprint(\"Outputs:\\\\n\", outputs)\\nprint(\"Outputs dimensions:\", outputs.shape)\\nThe output tensor looks like the following:\\nOutputs:\\n tensor([[[-1.5854,  0.9904],\\n          [-3.7235,  7.4548],\\n          [-2.2661,  6.6049],\\n          [-3.5983,  3.9902]]])\\nOutputs dimensions: torch.Size([1, 4, 2])\\nA similar input would have previously produced an output tensor of [1, 4, 50257],\\nwhere 50257 represents the vocabulary size. The number of output rows corresponds\\nto the number of input tokens (in this case, four). However, each output’s embedding\\ndimension (the number of columns) is now 2 instead of 50,257 since we replaced the\\noutput layer of the model.\\n Remember that we are interested in fine-tuning this model to return a class label\\nindicating whether a model input is “spam” or “not spam.” We don’t need to fine-\\ntune all four output rows; instead, we can focus on a single output token. In particu-\\nlar, we will focus on the last row corresponding to the last output token, as shown in\\nfigure 6.11.\\n To extract the last output token from the output tensor, we use the following code:\\nprint(\"Last output token:\", outputs[:, -1, :])\\nThis prints\\nLast output token: tensor([[-3.5983,  3.9902]])\\nWe still need to convert the values into a class-label prediction. But first, let’s under-\\nstand why we are particularly interested in the last output token only.\\n We have already explored the attention mechanism, which establishes a relationship\\nbetween each input token and every other input token, and the concept of a causal\\nattention mask, commonly used in GPT-like models (see chapter 3). This mask restricts a'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 210}, page_content='189\\n6.5\\nAdding a classification head\\n[[-1.5854,  0.9904],\\n[-3.7235,  7.4548],\\n[-2.2661,  6.6049],\\n[-3.5983,  3.9902]]\\nA 4 × 2–dimensional tensor\\nThe number of rows corresponds\\nto the number of input tokens,\\nas discussed in chapter 4.\\nThe last row corresponds\\nto the last token.\\nGPT\\nmodel\\nMasked multihead\\nattention\\nLayerNorm 2\\nFeed forward\\nLayerNorm 1\\nDropout\\nDropout\\nToken embedding layer\\nTokenized text\\nFinal LayerNorm\\nLinear output layer\\n12\\nDropout\\nPositional embedding layer\\nDo you have time\\nThis transformer block\\nis repeated 12x in the\\n124M-parameter GPT-2\\nmodel.\\nThe GPT model we\\nimplemented in chapter 5\\nand loaded in the previous\\nsection\\nFigure 6.11\\nThe GPT model with a four-token example input and output. The output tensor consists of \\ntwo columns due to the modified output layer. We are only interested in the last row corresponding to \\nthe last token when fine-tuning the model for spam classification.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 211}, page_content='190\\nCHAPTER 6\\nFine-tuning for classification\\ntoken’s focus to its current position and the those before it, ensuring that each token\\ncan only be influenced by itself and the preceding tokens, as illustrated in figure 6.12.\\nGiven the causal attention mask setup in figure 6.12, the last token in a sequence accu-\\nmulates the most information since it is the only token with access to data from all the\\nprevious tokens. Therefore, in our spam classification task, we focus on this last token\\nduring the fine-tuning process.\\n We are now ready to transform the last token into class label predictions and calcu-\\nlate the model’s initial prediction accuracy. Subsequently, we will fine-tune the model\\nfor the spam classification task.\\n6.6\\nCalculating the classification loss and accuracy\\nOnly one small task remains before we fine-tune the model: we must implement the\\nmodel evaluation functions used during fine-tuning, as illustrated in figure 6.13.\\n Before implementing the evaluation utilities, let’s briefly discuss how we convert\\nthe model outputs into class label predictions. We previously computed the token ID\\nof the next token generated by the LLM by converting the 50,257 outputs into proba-\\nbilities via the softmax function and then returning the position of the highest proba-\\nbility via the argmax function. We take the same approach here to calculate whether\\nthe model outputs a “spam” or “not spam” prediction for a given input, as shown in\\nfigure 6.14. The only difference is that we work with 2-dimensional instead of 50,257-\\ndimensional outputs. \\n \\nExercise 6.3 Fine-tuning the first vs. last token \\nTry fine-tuning the first output token. Notice the changes in predictive performance\\ncompared to fine-tuning the last output token.\\nDo\\nhave\\ntime\\nyou 0.55 0.45\\n1.0\\n0.38 0.30 0.32\\n0.27 0.24 0.24 0.25\\nDo\\nhave\\ntime\\nyou\\nTokens masked out via\\nthe causal attention mask.\\nThe last token is the only\\ntoken with an attention\\nscore to all other tokens.\\nFigure 6.12\\nThe causal attention \\nmechanism, where the attention scores \\nbetween input tokens are displayed in a \\nmatrix format. The empty cells indicate \\nmasked positions due to the causal attention \\nmask, preventing tokens from attending to \\nfuture tokens. The values in the cells \\nrepresent attention scores; the last token, \\ntime, is the only one that computes \\nattention scores for all preceding tokens.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 212}, page_content=\"191\\n6.6\\nCalculating the classification loss and accuracy\\nImplement the utility\\nfunction to calculate the\\nclassiﬁcation loss and\\naccuracy of the model.\\nStage 1:\\nDataset preparation\\n1) Download\\nthe dataset\\n2) Preprocess\\ndataset\\n3) Create data\\nloaders\\n4) Initialize\\nmodel\\n5) Load pretrained\\nweights\\n6) Modify model\\nfor ﬁne-tuning\\n7) Implement\\nevaluation utilities\\n8) Fine-tune\\nmodel\\n9) Evaluate\\nﬁne-tuned model\\n10) Use model\\non new data\\n7) Implement\\nevaluation utilities\\nStage 2:\\nModel setup\\nStage 3:\\nModel ﬁne-tuning\\nand usage\\nFigure 6.13\\nThe three-stage process for classification fine-tuning the LLM. \\nWe've completed the first six steps. We are now ready to undertake the last step \\nof stage 2: implementing the functions to evaluate the model’s performance to \\nclassify spam messages before, during, and after the fine-tuning.\\nYou won the lottery\\nDo you have time\\nInput text\\nmessage\\n[ -3.9846,  5.2940 ]\\nLLM\\nLLM\\nOutputs corresponding\\nto the last row (token)\\n0        1\\nIndex position:\\n2. Locate the index position with\\nthe highest probability value in\\neach row vector, which is done\\nvia the\\nfunction.\\nargmax\\n[ 3.5983,  -3.9902 ]\\n[ 0.01,   0.99 ]\\n[ 0.99,   0.01 ]\\n(spam)\\n(not spam)\\n10\\n11\\n(spam)\\n(not spam)\\n1. Convert outputs to\\nsoftmax probabilities.\\nThe predicted\\nlabels\\nFigure 6.14\\nThe model outputs corresponding to the last token are converted into probability scores for each \\ninput text. The class labels are obtained by looking up the index position of the highest probability score. The \\nmodel predicts the spam labels incorrectly because it has not yet been trained.\"),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 213}, page_content='192\\nCHAPTER 6\\nFine-tuning for classification\\nLet’s consider the last token output using a concrete example:\\nprint(\"Last output token:\", outputs[:, -1, :])\\nThe values of the tensor corresponding to the last token are\\nLast output token: tensor([[-3.5983,  3.9902]])\\nWe can obtain the class label:\\nprobas = torch.softmax(outputs[:, -1, :], dim=-1)\\nlabel = torch.argmax(probas)\\nprint(\"Class label:\", label.item())\\nIn this case, the code returns 1, meaning the model predicts that the input text is\\n“spam.” Using the softmax function here is optional because the largest outputs\\ndirectly correspond to the highest probability scores. Hence, we can simplify the code\\nwithout using softmax:\\nlogits = outputs[:, -1, :]\\nlabel = torch.argmax(logits)\\nprint(\"Class label:\", label.item())\\nThis concept can be used to compute the classification accuracy, which measures the\\npercentage of correct predictions across a dataset.\\n To determine the classification accuracy, we apply the argmax-based prediction\\ncode to all examples in the dataset and calculate the proportion of correct predictions\\nby defining a calc_accuracy_loader function.\\ndef calc_accuracy_loader(data_loader, model, device, num_batches=None):\\n    model.eval()\\n    correct_predictions, num_examples = 0, 0\\n    if num_batches is None:\\n        num_batches = len(data_loader)\\n    else:\\n        num_batches = min(num_batches, len(data_loader))\\n    for i, (input_batch, target_batch) in enumerate(data_loader):\\n        if i < num_batches:\\n            input_batch = input_batch.to(device)\\n            target_batch = target_batch.to(device)\\n            with torch.no_grad():\\n                logits = model(input_batch)[:, -1, :]    \\n            predicted_labels = torch.argmax(logits, dim=-1)\\n            num_examples += predicted_labels.shape[0]\\n            correct_predictions += (\\nListing 6.8\\nCalculating the classification accuracy\\nLogits of last \\noutput token'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 214}, page_content='193\\n6.6\\nCalculating the classification loss and accuracy\\n                (predicted_labels == target_batch).sum().item()\\n            )\\n        else:\\n            break\\n    return correct_predictions / num_examples\\nLet’s use the function to determine the classification accuracies across various datasets\\nestimated from 10 batches for efficiency:\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\nmodel.to(device)\\ntorch.manual_seed(123)\\ntrain_accuracy = calc_accuracy_loader(\\n    train_loader, model, device, num_batches=10\\n)\\nval_accuracy = calc_accuracy_loader(\\n    val_loader, model, device, num_batches=10\\n)\\ntest_accuracy = calc_accuracy_loader(\\n    test_loader, model, device, num_batches=10\\n)\\nprint(f\"Training accuracy: {train_accuracy*100:.2f}%\")\\nprint(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\\nprint(f\"Test accuracy: {test_accuracy*100:.2f}%\")\\nVia the device setting, the model automatically runs on a GPU if a GPU with Nvidia\\nCUDA support is available and otherwise runs on a CPU. The output is\\nTraining accuracy: 46.25%\\nValidation accuracy: 45.00%\\nTest accuracy: 48.75%\\nAs we can see, the prediction accuracies are near a random prediction, which would be\\n50% in this case. To improve the prediction accuracies, we need to fine-tune the model.\\n However, before we begin fine-tuning the model, we must define the loss function\\nwe will optimize during training. Our objective is to maximize the spam classification\\naccuracy of the model, which means that the preceding code should output the cor-\\nrect class labels: 0 for non-spam and 1 for spam. \\n Because classification accuracy is not a differentiable function, we use cross-\\nentropy loss as a proxy to maximize accuracy. Accordingly, the calc_loss_batch func-\\ntion remains the same, with one adjustment: we focus on optimizing only the last\\ntoken, model(input_batch)[:, -1, :], rather than all tokens, model(input_batch):\\ndef calc_loss_batch(input_batch, target_batch, model, device):\\n    input_batch = input_batch.to(device)\\n    target_batch = target_batch.to(device)\\n    logits = model(input_batch)[:, -1, :]    \\nLogits of last \\noutput token'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 215}, page_content='194\\nCHAPTER 6\\nFine-tuning for classification\\n    loss = torch.nn.functional.cross_entropy(logits, target_batch)\\n    return loss\\nWe use the calc_loss_batch function to compute the loss for a single batch obtained\\nfrom the previously defined data loaders. To calculate the loss for all batches in a data\\nloader, we define the calc_loss_loader function as before.\\ndef calc_loss_loader(data_loader, model, device, num_batches=None):\\n    total_loss = 0.\\n    if len(data_loader) == 0:\\n        return float(\"nan\")\\n    elif num_batches is None:\\n        num_batches = len(data_loader)\\n    else:                                       \\n        num_batches = min(num_batches, len(data_loader))\\n    for i, (input_batch, target_batch) in enumerate(data_loader):\\n        if i < num_batches:\\n            loss = calc_loss_batch(\\n                input_batch, target_batch, model, device\\n            )\\n            total_loss += loss.item()\\n        else:\\n            break\\n    return total_loss / num_batches\\nSimilar to calculating the training accuracy, we now compute the initial loss for each\\ndata set:\\nwith torch.no_grad():                \\n    train_loss = calc_loss_loader(\\n        train_loader, model, device, num_batches=5\\n    )\\n    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\\n    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\\nprint(f\"Training loss: {train_loss:.3f}\")\\nprint(f\"Validation loss: {val_loss:.3f}\")\\nprint(f\"Test loss: {test_loss:.3f}\")\\nThe initial loss values are\\nTraining loss: 2.453\\nValidation loss: 2.583\\nTest loss: 2.322\\nNext, we will implement a training function to fine-tune the model, which means\\nadjusting the model to minimize the training set loss. Minimizing the training set loss\\nwill help increase the classification accuracy, which is our overall goal.\\nListing 6.9\\nCalculating the classification loss\\nEnsures number of \\nbatches doesn’t exceed \\nbatches in data loader\\nDisables gradient tracking \\nfor efficiency because we \\nare not training yet'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 216}, page_content='195\\n6.7\\nFine-tuning the model on supervised data\\n6.7\\nFine-tuning the model on supervised data\\nWe must define and use the training function to fine-tune the pretrained LLM and\\nimprove its spam classification accuracy. The training loop, illustrated in figure 6.15,\\nis the same overall training loop we used for pretraining; the only difference is that\\nwe calculate the classification accuracy instead of generating a sample text to evalu-\\nate the model.\\nThe training function implementing the concepts shown in figure 6.15 also closely mir-\\nrors the train_model_simple function used for pretraining the model. The only two dis-\\ntinctions are that we now track the number of training examples seen (examples_seen)\\ninstead of the number of tokens, and we calculate the accuracy after each epoch instead\\nof printing a sample text.\\n \\n \\n1) For each training epoch\\n2) For each batch in training set\\n3) Reset loss gradients from\\nthe previous batch iteration\\n4) Calculate loss on\\ncurrent batch\\n5) Backward pass to\\ncalculate loss gradients\\n6) Update model weights\\nusing loss gradients\\n7) Print training and\\nvalidation set losses\\n8) Generate sample text\\nfor visual inspection\\nOptional steps for tracking\\nthe training progress\\nThese are the usual steps\\nused for training deep\\nneural networks in PyTorch.\\nOne epoch is one complete\\npass over a training set.\\nThe number of batches is\\ndetermined by the training\\nset size divided by the size\\nof each batch.\\nFigure 6.15\\nA typical training loop for training deep neural networks in \\nPyTorch consists of several steps, iterating over the batches in the training \\nset for several epochs. In each loop, we calculate the loss for each training \\nset batch to determine loss gradients, which we use to update the model \\nweights to minimize the training set loss.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 217}, page_content='196\\nCHAPTER 6\\nFine-tuning for classification\\ndef train_classifier_simple(\\n        model, train_loader, val_loader, optimizer, device,\\n        num_epochs, eval_freq, eval_iter):\\n    train_losses, val_losses, train_accs, val_accs = [], [], [], []  \\n    examples_seen, global_step = 0, -1\\n    for epoch in range(num_epochs):   \\n        model.train()            \\n        for input_batch, target_batch in train_loader:\\n            optimizer.zero_grad()                     \\n            loss = calc_loss_batch(\\n                input_batch, target_batch, model, device\\n            )\\n            loss.backward()                         \\n            optimizer.step()                         \\n            examples_seen += input_batch.shape[0]   \\n            global_step += 1\\n              \\n            if global_step % eval_freq == 0:\\n                train_loss, val_loss = evaluate_model(\\n                    model, train_loader, val_loader, device, eval_iter)\\n                train_losses.append(train_loss)\\n                val_losses.append(val_loss)\\n                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\\n                      f\"Train loss {train_loss:.3f}, \"\\n                      f\"Val loss {val_loss:.3f}\"\\n                )\\n                                                  \\n        train_accuracy = calc_accuracy_loader(\\n            train_loader, model, device, num_batches=eval_iter\\n        )\\n        val_accuracy = calc_accuracy_loader(\\n            val_loader, model, device, num_batches=eval_iter\\n        )\\n        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\\n        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\\n        train_accs.append(train_accuracy)\\n        val_accs.append(val_accuracy)\\n    return train_losses, val_losses, train_accs, val_accs, examples_seen\\nThe evaluate_model function is identical to the one we used for pretraining:\\ndef evaluate_model(model, train_loader, val_loader, device, eval_iter):\\n    model.eval()\\n    with torch.no_grad():\\nListing 6.10\\nFine-tuning the model to classify spam\\nInitialize lists to\\ntrack losses and\\nexamples seen\\nMain training loop\\nSets model to training mode\\nResets loss gradients \\nfrom the previous \\nbatch iteration\\nCalculates loss \\ngradients\\nUpdates model \\nweights using \\nloss gradients\\nNew: tracks examples \\ninstead of tokens\\nOptional\\nevaluation\\nstep\\nCalculates accuracy \\nafter each epoch'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 218}, page_content='197\\n6.7\\nFine-tuning the model on supervised data\\n        train_loss = calc_loss_loader(\\n            train_loader, model, device, num_batches=eval_iter\\n        )\\n        val_loss = calc_loss_loader(\\n            val_loader, model, device, num_batches=eval_iter\\n        )\\n    model.train()\\n    return train_loss, val_loss\\nNext, we initialize the optimizer, set the number of training epochs, and initiate the\\ntraining using the train_classifier_simple function. The training takes about 6\\nminutes on an M3 MacBook Air laptop computer and less than half a minute on a\\nV100 or A100 GPU:\\nimport time\\nstart_time = time.time()\\ntorch.manual_seed(123)\\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\\nnum_epochs = 5\\ntrain_losses, val_losses, train_accs, val_accs, examples_seen = \\\\\\n    train_classifier_simple(\\n        model, train_loader, val_loader, optimizer, device,\\n        num_epochs=num_epochs, eval_freq=50,\\n        eval_iter=5\\n    )\\nend_time = time.time()\\nexecution_time_minutes = (end_time - start_time) / 60\\nprint(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\\nThe output we see during the training is as follows:\\nEp 1 (Step 000000): Train loss 2.153, Val loss 2.392\\nEp 1 (Step 000050): Train loss 0.617, Val loss 0.637\\nEp 1 (Step 000100): Train loss 0.523, Val loss 0.557\\nTraining accuracy: 70.00% | Validation accuracy: 72.50%\\nEp 2 (Step 000150): Train loss 0.561, Val loss 0.489\\nEp 2 (Step 000200): Train loss 0.419, Val loss 0.397\\nEp 2 (Step 000250): Train loss 0.409, Val loss 0.353\\nTraining accuracy: 82.50% | Validation accuracy: 85.00%\\nEp 3 (Step 000300): Train loss 0.333, Val loss 0.320\\nEp 3 (Step 000350): Train loss 0.340, Val loss 0.306\\nTraining accuracy: 90.00% | Validation accuracy: 90.00%\\nEp 4 (Step 000400): Train loss 0.136, Val loss 0.200\\nEp 4 (Step 000450): Train loss 0.153, Val loss 0.132\\nEp 4 (Step 000500): Train loss 0.222, Val loss 0.137\\nTraining accuracy: 100.00% | Validation accuracy: 97.50%\\nEp 5 (Step 000550): Train loss 0.207, Val loss 0.143\\nEp 5 (Step 000600): Train loss 0.083, Val loss 0.074\\nTraining accuracy: 100.00% | Validation accuracy: 97.50%\\nTraining completed in 5.65 minutes.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 219}, page_content='198\\nCHAPTER 6\\nFine-tuning for classification\\nWe then use Matplotlib to plot the loss function for the training and validation set.\\nimport matplotlib.pyplot as plt\\ndef plot_values(\\n        epochs_seen, examples_seen, train_values, val_values,\\n        label=\"loss\"):\\n    fig, ax1 = plt.subplots(figsize=(5, 3))\\n                                     \\n    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\\n    ax1.plot(\\n        epochs_seen, val_values, linestyle=\"-.\",\\n        label=f\"Validation {label}\"\\n    )\\n    ax1.set_xlabel(\"Epochs\")\\n    ax1.set_ylabel(label.capitalize())\\n    ax1.legend()\\n                                  \\n    ax2 = ax1.twiny()\\n    ax2.plot(examples_seen, train_values, alpha=0)   \\n    ax2.set_xlabel(\"Examples seen\")\\n    fig.tight_layout()            \\n    plt.savefig(f\"{label}-plot.pdf\")\\n    plt.show()\\nepochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\\nexamples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\\nplot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)\\nFigure 6.16 plots the resulting loss curves.\\nListing 6.11\\nPlotting the classification loss\\nPlots training \\nand validation loss \\nagainst epochs\\nCreates a \\nsecond x-axis for \\nexamples seen\\nInvisible plot for \\naligning ticks\\nAdjusts layout \\nto make room\\nFigure 6.16\\nThe model’s training and \\nvalidation loss over the five training \\nepochs. Both the training loss, \\nrepresented by the solid line, and the \\nvalidation loss, represented by the \\ndashed line, sharply decline in the first \\nepoch and gradually stabilize toward the \\nfifth epoch. This pattern indicates good \\nlearning progress and suggests that the \\nmodel learned from the training data \\nwhile generalizing well to the unseen \\nvalidation data.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 220}, page_content='199\\n6.7\\nFine-tuning the model on supervised data\\nAs we can see based on the sharp downward slope in figure 6.16, the model is learning\\nwell from the training data, and there is little to no indication of overfitting; that is,\\nthere is no noticeable gap between the training and validation set losses.\\nUsing the same plot_values function, let’s now plot the classification accuracies:\\nepochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\\nexamples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\\nplot_values(\\n    epochs_tensor, examples_seen_tensor, train_accs, val_accs,\\n    label=\"accuracy\"\\n)\\nFigure 6.17 graphs the resulting accuracy. The model achieves a relatively high training\\nand validation accuracy after epochs 4 and 5. Importantly, we previously set eval_iter=5\\nChoosing the number of epochs \\nEarlier, when we initiated the training, we set the number of epochs to five. The num-\\nber of epochs depends on the dataset and the task’s difficulty, and there is no uni-\\nversal solution or recommendation, although an epoch number of five is usually a\\ngood starting point. If the model overfits after the first few epochs as a loss plot (see\\nfigure 6.16), you may need to reduce the number of epochs. Conversely, if the trend-\\nline suggests that the validation loss could improve with further training, you should\\nincrease the number of epochs. In this concrete case, five epochs is a reasonable\\nnumber as there are no signs of early overfitting, and the validation loss is close to 0.\\nFigure 6.17\\nBoth the training accuracy (solid line) and the validation \\naccuracy (dashed line) increase substantially in the early epochs and \\nthen plateau, achieving almost perfect accuracy scores of 1.0. The \\nclose proximity of the two lines throughout the epochs suggests that \\nthe model does not overfit the training data very much.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 221}, page_content='200\\nCHAPTER 6\\nFine-tuning for classification\\nwhen using the train_classifier_simple function, which means our estimations of\\ntraining and validation performance are based on only five batches for efficiency\\nduring training. \\n Now we must calculate the performance metrics for the training, validation, and\\ntest sets across the entire dataset by running the following code, this time without\\ndefining the eval_iter value:\\ntrain_accuracy = calc_accuracy_loader(train_loader, model, device)\\nval_accuracy = calc_accuracy_loader(val_loader, model, device)\\ntest_accuracy = calc_accuracy_loader(test_loader, model, device)\\nprint(f\"Training accuracy: {train_accuracy*100:.2f}%\")\\nprint(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\\nprint(f\"Test accuracy: {test_accuracy*100:.2f}%\")\\nThe resulting accuracy values are\\nTraining accuracy: 97.21%\\nValidation accuracy: 97.32%\\nTest accuracy: 95.67%\\nThe training and test set performances are almost identical. The slight discrepancy\\nbetween the training and test set accuracies suggests minimal overfitting of the train-\\ning data. Typically, the validation set accuracy is somewhat higher than the test set\\naccuracy because the model development often involves tuning hyperparameters to\\nperform well on the validation set, which might not generalize as effectively to the test\\nset. This situation is common, but the gap could potentially be minimized by adjusting\\nthe model’s settings, such as increasing the dropout rate (drop_rate) or the weight_\\ndecay parameter in the optimizer configuration.\\n6.8\\nUsing the LLM as a spam classifier\\nHaving fine-tuned and evaluated the model, we are now ready to classify spam mes-\\nsages (see figure 6.18). Let’s use our fine-tuned GPT-based spam classification model.\\nThe following classify_review function follows data preprocessing steps similar\\nto those we used in the SpamDataset implemented earlier. Then, after processing\\ntext into token IDs, the function uses the model to predict an integer class label,\\nsimilar to what we implemented in section 6.6, and then returns the corresponding\\nclass name.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 222}, page_content='201\\n6.8\\nUsing the LLM as a spam classifier\\ndef classify_review(\\n        text, model, tokenizer, device, max_length=None,\\n        pad_token_id=50256):\\n    model.eval()\\n    input_ids = tokenizer.encode(text)         \\n    supported_context_length = model.pos_emb.weight.shape[1]\\n    input_ids = input_ids[:min(             \\n        max_length, supported_context_length\\n    )]\\n    input_ids += [pad_token_id] * (max_length - len(input_ids))   \\n    \\n    input_tensor = torch.tensor(\\n        input_ids, device=device\\n    ).unsqueeze(0)             \\n    \\n    with torch.no_grad():                               \\n        logits = model(input_tensor)[:, -1, :]    \\n    predicted_label = torch.argmax(logits, dim=-1).item()\\n    return \"spam\" if predicted_label == 1 else \"not spam\"    \\nListing 6.12\\nUsing the model to classify new texts\\nStage 1:\\nDataset preparation\\n1) Download\\nthe dataset\\n2) Preprocess\\ndataset\\n3) Create data\\nloaders\\n4) Initialize\\nmodel\\n5) Load pretrained\\nweights\\n6) Modify model\\nfor ﬁne-tuning\\n7) Implement\\nevaluation utilities\\n8) Fine-tune\\nmodel\\n9) Evaluate\\nﬁne-tuned model\\n10) Use model\\non new data\\n10) Use model\\non new data\\nStage 2:\\nModel setup\\nStage 3:\\nModel ﬁne-tuning\\nand usage\\nWe are ready to try the model\\non new text messages.\\nFigure 6.18\\nThe three-stage process for classification fine-tuning our LLM. Step \\n10 is the final step of stage 3—using the fine-tuned model to classify new spam \\nmessages.\\nPrepares inputs \\nto the model\\nTruncates sequences if \\nthey are too long\\nPads sequences\\nto the longest\\nsequence\\nAdds batch \\ndimension\\nModels inference \\nwithout gradient \\ntracking\\nLogits of the last output token\\nReturns the classified result'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 223}, page_content='202\\nCHAPTER 6\\nFine-tuning for classification\\nLet’s try this classify_review function on an example text: \\ntext_1 = (\\n    \"You are a winner you have been specially\"\\n    \" selected to receive $1000 cash or a $2000 award.\"\\n)\\nprint(classify_review(\\n    text_1, model, tokenizer, device, max_length=train_dataset.max_length\\n))\\nThe resulting model correctly predicts \"spam\". Let’s try another example:\\ntext_2 = (\\n    \"Hey, just wanted to check if we\\'re still on\"\\n    \" for dinner tonight? Let me know!\"\\n)\\nprint(classify_review(\\n    text_2, model, tokenizer, device, max_length=train_dataset.max_length\\n))\\nThe model again makes a correct prediction and returns a “not spam” label.\\n Finally, let’s save the model in case we want to reuse the model later without having\\nto train it again. We can use the torch.save method:\\ntorch.save(model.state_dict(), \"review_classifier.pth\")\\nOnce saved, the model can be loaded:\\nmodel_state_dict = torch.load(\"review_classifier.pth, map_location=device\")\\nmodel.load_state_dict(model_state_dict)\\nSummary\\n\\uf0a1There are different strategies for fine-tuning LLMs, including classification\\nfine-tuning and instruction fine-tuning.\\n\\uf0a1Classification fine-tuning involves replacing the output layer of an LLM via a\\nsmall classification layer.\\n\\uf0a1In the case of classifying text messages as “spam” or “not spam,” the new classifi-\\ncation layer consists of only two output nodes. Previously, we used the number\\nof output nodes equal to the number of unique tokens in the vocabulary\\n(i.e., 50,256).\\n\\uf0a1Instead of predicting the next token in the text as in pretraining, classification\\nfine-tuning trains the model to output a correct class label—for example,\\n“spam” or “not spam.”\\n\\uf0a1The model input for fine-tuning is text converted into token IDs, similar to\\npretraining.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 224}, page_content='203\\nSummary\\n\\uf0a1Before fine-tuning an LLM, we load the pretrained model as a base model.\\n\\uf0a1Evaluating a classification model involves calculating the classification accuracy\\n(the fraction or percentage of correct predictions).\\n\\uf0a1Fine-tuning a classification model uses the same cross entropy loss function as\\nwhen pretraining the LLM.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 225}, page_content='204\\nFine-tuning to follow\\ninstructions\\nPreviously, we implemented the LLM architecture, carried out pretraining, and\\nimported pretrained weights from external sources into our model. Then, we\\nfocused on fine-tuning our LLM for a specific classification task: distinguishing\\nbetween spam and non-spam text messages. Now we’ll implement the process for\\nfine-tuning an LLM to follow human instructions, as illustrated in figure 7.1.\\nInstruction fine-tuning is one of the main techniques behind developing LLMs for\\nchatbot applications, personal assistants, and other conversational tasks.\\nThis chapter covers\\n\\uf0a1The instruction fine-tuning process of LLMs\\n\\uf0a1Preparing a dataset for supervised instruction \\nfine-tuning\\n\\uf0a1Organizing instruction data in training batches\\n\\uf0a1Loading a pretrained LLM and fine-tuning it to \\nfollow human instructions\\n\\uf0a1Extracting LLM-generated instruction responses \\nfor evaluation\\n\\uf0a1Evaluating an instruction-fine-tuned LLM'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 226}, page_content='205\\n7.1\\nIntroduction to instruction fine-tuning\\nFigure 7.1 shows two main ways of fine-tuning an LLM: fine-tuning for classification\\n(step 8) and fine-tuning an LLM to follow instructions (step 9). We implemented step\\n8 in chapter 6. Now we will fine-tune an LLM using an instruction dataset.\\n7.1\\nIntroduction to instruction fine-tuning\\nWe now know that pretraining an LLM involves a training procedure where it learns\\nto generate one word at a time. The resulting pretrained LLM is capable of text comple-\\ntion, meaning it can finish sentences or write text paragraphs given a fragment as\\ninput. However, pretrained LLMs often struggle with specific instructions, such as “Fix\\nthe grammar in this text” or “Convert this text into passive voice.” Later, we will exam-\\nine a concrete example where we load the pretrained LLM as the basis for instruction\\nfine-tuning, also known as supervised instruction fine-tuning.\\n Here, we focus on improving the LLM’s ability to follow such instructions and gen-\\nerate a desired response, as illustrated in figure 7.2. Preparing the dataset is a key\\naspect of instruction fine-tuning. Then we’ll complete all the steps in the three stages\\nof the instruction fine-tuning process, beginning with the dataset preparation, as\\nshown in figure 7.3.\\nFigure 7.1\\nThe three main stages of coding an LLM. This chapter focuses on step 9 of stage 3: fine-tuning a \\npretrained LLM to follow human instructions.\\nIn the previous chapter,\\nwe ﬁne-tuned the pretrained\\nLLM to classify texts.\\nIn chapter 5, we\\npretrained an LLM.\\nIn chapter 4, we\\nimplemented a GPT-like\\nLLM architecture.\\nIn chapter 5, we also loaded\\npretrained model weights\\ninto the LLM architecture.\\n1) Data\\npreparation\\nand sampling\\n2) Attention\\nmechanism\\nBuilding an LLM\\nSTAGE 1\\nFoundation model\\nSTAGE 2\\nSTAGE 3\\nClassiﬁer\\nPersonal assistant\\nDataset with class labels\\nInstruction dataset\\n4) Pretraining\\n9) Fine-tuning\\n5) Training\\nloop\\n6) Model\\nevaluation\\n7) Load\\npretrained\\nweights\\n9) Fine-tuning\\n8) Fine-tuning\\n3) LLM\\narchitecture\\nIn this chapter, we\\nﬁne-tune the pretrained\\nLLM to follow instructions.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 227}, page_content='206\\nCHAPTER 7\\nFine-tuning to follow instructions\\nFigure 7.2\\nExamples of instructions that are processed by an LLM to \\ngenerate desired responses\\nConvert 45 kilometers to meters.\\n45 kilometers is 45000 meters.\\nEdit the following sentence to\\nremove all passive voice: “The\\nsong was composed by the artist.”\\nThe artist composed the song.\\nInstruction\\nDesired response\\nThe instructions serve\\nas inputs for the LLM.\\nThe goal for the\\nLLM is to generate\\na desired response.\\nProvide a synonym for “bright.”\\nA synonym for “bright” is “radiant.”\\nFigure 7.3\\nThe three-stage process for instruction fine-tuning an LLM. Stage 1 involves \\ndataset preparation, stage 2 focuses on model setup and fine-tuning, and stage 3 covers \\nthe evaluation of the model. We will begin with step 1 of stage 1: downloading and \\nformatting the dataset.\\n2) Batching the\\ndataset\\nStage 1:\\nPreparing the dataset\\n1) Dataset\\ndownload and\\nformatting\\n3) Creating\\ndata loaders\\n4) Loading a\\npretrained LLM\\n5) Instruction\\nﬁne-tuning the\\nLLM\\n9) Scoring the\\nresponses\\nStage 2:\\nFine-tuning the LLM\\nStage 3:\\nEvaluating the LLM\\n7) Extracting\\nresponses\\n6) Inspecting\\nthe modeling\\nloss\\n8) Qualitative\\nevaluation\\nWe start with downloading,\\ninspecting, and preparing\\nthe dataset that we will use\\nto ﬁne-tune the model.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 228}, page_content='207\\n7.2\\nPreparing a dataset for supervised instruction fine-tuning\\n7.2\\nPreparing a dataset for supervised instruction \\nfine-tuning\\nLet’s download and format the instruction dataset for instruction fine-tuning a pre-\\ntrained LLM. The dataset consists of 1,100 instruction–response pairs similar to those in\\nfigure 7.2. This dataset was created specifically for this book, but interested readers\\ncan find alternative, publicly available instruction datasets in appendix B.\\n The following code implements and executes a function to download this dataset,\\nwhich is a relatively small file (only 204 KB) in JSON format. JSON, or JavaScript Object\\nNotation, mirrors the structure of Python dictionaries, providing a simple structure\\nfor data interchange that is both human readable and machine friendly.\\nimport json\\nimport os\\nimport urllib\\ndef download_and_load_file(file_path, url):\\n    if not os.path.exists(file_path):\\n        with urllib.request.urlopen(url) as response:\\n            text_data = response.read().decode(\"utf-8\")\\n        with open(file_path, \"w\", encoding=\"utf-8\") as file:\\n            file.write(text_data)\\n    else:                                               \\n        with open(file_path, \"r\", encoding=\"utf-8\") as file:\\n            text_data = file.read()\\n    with open(file_path, \"r\") as file:\\n        data = json.load(file)\\n    return data\\nfile_path = \"instruction-data.json\"\\nurl = (\\n    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\\n    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\\n)\\ndata = download_and_load_file(file_path, url)\\nprint(\"Number of entries:\", len(data))\\nThe output of executing the preceding code is\\nNumber of entries: 1100\\nThe data list that we loaded from the JSON file contains the 1,100 entries of the\\ninstruction dataset. Let’s print one of the entries to see how each entry is structured:\\nprint(\"Example entry:\\\\n\", data[50])\\nListing 7.1\\nDownloading the dataset\\nSkips download if \\nfile was already \\ndownloaded'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 229}, page_content='208\\nCHAPTER 7\\nFine-tuning to follow instructions\\nThe content of the example entry is \\nExample entry:\\n {\\'instruction\\': \\'Identify the correct spelling of the following word.\\',\\n  \\'input\\': \\'Ocassion\\', \\'output\\': \"The correct spelling is \\'Occasion.\\'\"}\\nAs we can see, the example entries are Python dictionary objects containing an\\n\\'instruction\\', \\'input\\', and \\'output\\'. Let’s take a look at another example:\\nprint(\"Another example entry:\\\\n\", data[999])\\nBased on the contents of this entry, the \\'input\\' field may occasionally be empty:\\nAnother example entry:\\n {\\'instruction\\': \"What is an antonym of \\'complicated\\'?\", \\n  \\'input\\': \\'\\',\\n  \\'output\\': \"An antonym of \\'complicated\\' is \\'simple\\'.\"}\\nInstruction fine-tuning involves training a model on a dataset where the input-output\\npairs, like those we extracted from the JSON file, are explicitly provided. There are\\nvarious methods to format these entries for LLMs. Figure 7.4 illustrates two different\\nFigure 7.4\\nComparison of prompt styles for instruction fine-tuning in LLMs. The Alpaca style (left) uses a \\nstructured format with defined sections for instruction, input, and response, while the Phi-3 style (right) employs \\na simpler format with designated <|user|> and <|assistant|> tokens.\\n{\\n\"instruction\": \"Identify the correct spelling of the following word.\",\\n\"input\": \"Ocassion\",\\n\"output\": \"The correct spelling is \\'Occasion.\\'\"\\n},\\nApply Alpaca prompt style template.\\nApply Phi-3 prompt style template.\\nAn entry in the\\ninstruction dataset\\nBelow is an instruction that\\ndescribes a task. Write a response\\nthat appropriately completes the\\nrequest.\\n### Instruction:\\nIdentify the correct spelling of the\\nfollowing word.\\n### Input:\\nOcassion\\n### Response:\\nThe correct spelling is \\'Occasion\\'.\\n<|user|>\\nIdentify the correct spelling of the\\nfollowing word: \\'Ocassion\\'\\n<|assistant|>\\nThe correct spelling is \\'Occasion\\'.\\nOne way to format\\nthe data entry to\\ntrain the LLM'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 230}, page_content='209\\n7.2\\nPreparing a dataset for supervised instruction fine-tuning\\nexample formats, often referred to as prompt styles, used in the training of notable\\nLLMs such as Alpaca and Phi-3. \\n Alpaca was one of the early LLMs to publicly detail its instruction fine-tuning pro-\\ncess. Phi-3, developed by Microsoft, is included to demonstrate the diversity in prompt\\nstyles. The rest of this chapter uses the Alpaca prompt style since it is one of the most\\npopular ones, largely because it helped define the original approach to fine-tuning.\\nLet’s define a format_input function that we can use to convert the entries in the\\ndata list into the Alpaca-style input format. \\ndef format_input(entry):\\n    instruction_text = (\\n        f\"Below is an instruction that describes a task. \"\\n        f\"Write a response that appropriately completes the request.\"\\n        f\"\\\\n\\\\n### Instruction:\\\\n{entry[\\'instruction\\']}\"\\n    )\\n    \\n    input_text = (\\n        f\"\\\\n\\\\n### Input:\\\\n{entry[\\'input\\']}\" if entry[\"input\"] else \"\"\\n    )\\n    return instruction_text + input_text\\nThis format_input function takes a dictionary entry as input and constructs a format-\\nted string. Let’s test it to dataset entry data[50], which we looked at earlier:\\nmodel_input = format_input(data[50])\\ndesired_response = f\"\\\\n\\\\n### Response:\\\\n{data[50][\\'output\\']}\"\\nprint(model_input + desired_response)\\nThe formatted input looks like as follows:\\nBelow is an instruction that describes a task. Write a response that \\nappropriately completes the request.\\n### Instruction:\\nIdentify the correct spelling of the following word.\\n### Input:\\nOcassion\\n### Response:\\nThe correct spelling is \\'Occasion.\\'\\nExercise 7.1 Changing prompt styles \\nAfter fine-tuning the model with the Alpaca prompt style, try the Phi-3 prompt style\\nshown in figure 7.4 and observe whether it affects the response quality of the model.\\nListing 7.2\\nImplementing the prompt formatting function'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 231}, page_content='210\\nCHAPTER 7\\nFine-tuning to follow instructions\\nNote that the format_input skips the optional ### Input: section if the \\'input\\' field\\nis empty, which we can test out by applying the format_input function to entry\\ndata[999] that we inspected earlier:\\nmodel_input = format_input(data[999])\\ndesired_response = f\"\\\\n\\\\n### Response:\\\\n{data[999][\\'output\\']}\"\\nprint(model_input + desired_response)\\nThe output shows that entries with an empty \\'input\\' field don’t contain an ###\\nInput: section in the formatted input:\\nBelow is an instruction that describes a task. Write a response that \\nappropriately completes the request.\\n### Instruction:\\nWhat is an antonym of \\'complicated\\'?\\n### Response:\\nAn antonym of \\'complicated\\' is \\'simple\\'.\\nBefore we move on to setting up the PyTorch data loaders in the next section, let’s\\ndivide the dataset into training, validation, and test sets analogous to what we have\\ndone with the spam classification dataset in the previous chapter. The following listing\\nshows how we calculate the portions.\\ntrain_portion = int(len(data) * 0.85)   \\ntest_portion = int(len(data) * 0.1)           \\nval_portion = len(data) - train_portion - test_portion   \\ntrain_data = data[:train_portion]\\ntest_data = data[train_portion:train_portion + test_portion]\\nval_data = data[train_portion + test_portion:]\\nprint(\"Training set length:\", len(train_data))\\nprint(\"Validation set length:\", len(val_data))\\nprint(\"Test set length:\", len(test_data))\\nThis partitioning results in the following dataset sizes:\\nTraining set length: 935\\nValidation set length: 55\\nTest set length: 110\\nListing 7.3\\nPartitioning the dataset\\nUse 85% of the data for training\\nUse 10% for \\ntesting\\nUse remaining \\n5% for validation'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 232}, page_content='211\\n7.3\\nOrganizing data into training batches\\nHaving successfully downloaded and partitioned the dataset and gained a clear under-\\nstanding of the dataset prompt formatting, we are now ready for the core implementa-\\ntion of the instruction fine-tuning process. Next, we focus on developing the method\\nfor constructing the training batches for fine-tuning the LLM.\\n7.3\\nOrganizing data into training batches\\nAs we progress into the implementation phase of our instruction fine-tuning process,\\nthe next step, illustrated in figure 7.5, focuses on constructing the training batches\\neffectively. This involves defining a method that will ensure our model receives the\\nformatted training data during the fine-tuning process.\\nIn the previous chapter, the training batches were created automatically by the PyTorch\\nDataLoader class, which employs a default collate function to combine lists of samples\\ninto batches. A collate function is responsible for taking a list of individual data sam-\\nples and merging them into a single batch that can be processed efficiently by the\\nmodel during training. \\n However, the batching process for instruction fine-tuning is a bit more involved\\nand requires us to create our own custom collate function that we will later plug into\\nFigure 7.5\\nThe three-stage process for instruction fine-tuning an LLM. Next, we look at step 2 of stage \\n1: assembling the training batches.\\n2) Batching the\\ndataset\\nStage 1:\\nPreparing the dataset\\n1) Dataset\\ndownload and\\nformatting\\n3) Creating\\ndata loaders\\n4) Loading a\\npretrained LLM\\n5) Instruction\\nﬁne-tuning the\\nLLM\\n9) Scoring the\\nresponses\\nStage 2:\\nFine-tuning the LLM\\nStage 3:\\nEvaluating the LLM\\n7) Extracting\\nresponses\\n6) Inspecting\\nthe modeling\\nloss\\n8) Qualitative\\nevaluation\\nIn this section, we learn how to efﬁciently\\npad the data samples to equal lengths\\nso we can assemble multiple instruction\\nexamples in a batch.\\nThen, we create the PyTorch\\ndata loaders we will use for\\nﬁne-tuning the LLM.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 233}, page_content='212\\nCHAPTER 7\\nFine-tuning to follow instructions\\nthe DataLoader. We implement this custom collate function to handle the specific\\nrequirements and formatting of our instruction fine-tuning dataset. \\n Let’s tackle the batching process in several steps, including coding the custom col-\\nlate function, as illustrated in figure 7.6. First, to implement steps 2.1 and 2.2, we\\ncode an InstructionDataset class that applies format_input and pretokenizes all\\ninputs in the dataset, similar to the SpamDataset in chapter 6. This two-step process,\\ndetailed in figure 7.7, is implemented in the __init__ constructor method of the\\nInstructionDataset.\\n \\n \\nFigure 7.6\\nThe five substeps involved in implementing the batching process: (2.1) applying the \\nprompt template, (2.2) using tokenization from previous chapters, (2.3) adding padding tokens, \\n(2.4) creating target token IDs, and (2.5) replacing -100 placeholder tokens to mask padding \\ntokens in the loss function.\\n2.1) Format data\\nusing prompt\\ntemplate.\\n2.2) Tokenize\\nformatted data.\\n2.5) Replace\\npadding tokens\\nwith placeholders.\\n2.3)\\nh\\nAdjust to t e\\nsame length with\\npadding tokens.\\nBelow is an instruction that describes a task. Write a\\nresponse that appropriately completes the request.\\n[21106, 318, 281, 12064, 326,\\n8477, 257, 4876, 13, ...,]\\n2.4) Create target\\ntoken IDs for\\ntraining.\\n[21106, 318, 281, 12064, 326,\\n8477, 257, 4876, 13, ...,\\n50256, 50256, 50256]\\n[21106, 318, 281, 12064, 326,\\n8477, 257, 4876, 13, ...,\\n50256, 50256, 50256]\\n[318, 281, 12064, 326, 8477,\\n257, 4876, 13, ...,\\n50256, 50256, 50256, 50256]\\n[21106, 318, 281, 12064, 326,\\n8477, 257, 4876, 13, ...,\\n50256, 50256, 50256]\\n[318, 281, 12064, 326, 8477,\\n257, 4876, 13, ...,\\n50256, -100, -100, -100]\\nFormat input into an instruction-\\nresponse template.\\nConvert instruction-response\\nentry into token IDs.\\nAdd end-of-text tokens (50256)\\nto pad data samples to the same\\nlength.\\nCreate a list of target token IDs\\nfor the model to learn (these are\\nthe inputs shifted by 1, plus an\\nadditional padding token).\\nReplace certain padding tokens\\nby -100 to exclude them from\\nthe training loss.\\n### Instruction: …\\n### Response: …'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 234}, page_content='213\\n7.3\\nOrganizing data into training batches\\nimport torch\\nfrom torch.utils.data import Dataset\\nclass InstructionDataset(Dataset):\\n    def __init__(self, data, tokenizer):\\n        self.data = data\\n        self.encoded_texts = []\\n        for entry in data:        \\n            instruction_plus_input = format_input(entry)\\n            response_text = f\"\\\\n\\\\n### Response:\\\\n{entry[\\'output\\']}\"\\n            full_text = instruction_plus_input + response_text\\n            self.encoded_texts.append(\\n                tokenizer.encode(full_text)\\n            )\\n    def __getitem__(self, index):\\n        return self.encoded_texts[index]\\n    def __len__(self):\\n        return len(self.data)\\nListing 7.4\\nImplementing an instruction dataset class\\nFigure 7.7\\nThe first two steps involved in implementing the batching process. Entries are first formatted using \\na specific prompt template (2.1) and then tokenized (2.2), resulting in a sequence of token IDs that the model \\ncan process.\\nBelow is an instruction that\\ndescribes a task. Write a response\\nthat appropriately completes the\\nrequest.\\n### Instruction:\\nConvert 45 kilometers to meters.\\n### Response:\\n45 kilometers is 45000 meters.\\nBelow is an instruction that\\ndescribes a task. Write a response\\nthat appropriately completes the\\nrequest.\\n### Instruction:\\nIdentify the correct spelling of the\\nfollowing word.\\n### Input:\\nOcassion\\n### Response:\\nThe correct spelling is \\'Occasion\\'.\\n{\\n\"instruction\": \"Identify the correct\\nspelling of the following word.\",\\n\"input\": \"Ocassion\",\\n\"output\": \"The correct\\nspelling is \\'Occasion.\\'\"\\n}\\n{\\n\"instruction\": \"Convert 45 kilometers\\nto meters.\",\\n\"input\":           ,\\n\"Ocassion\"\\n\"output\": \"45 kilometers is 45000\\nmeters.\\'\"\\n}\\n[21106, 318, 281, 12064,\\n326, 8477, 257, 4876, 13,\\n19430, ..., 29223, 4247,\\n4458]\\n[21106, 318, 281, 12064,\\n326, 8477, 257, 4876, 13,\\n19430, ... , 830, 10700,\\n13]\\n2.1) Format\\ndataset entry.\\n2.2) Tokenize\\nformatted entry.\\nThe input entry is formatted\\nusing the prompt template.\\nThe token IDs that the\\nLLM will receive as input.\\nPretokenizes \\ntexts'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 235}, page_content='214\\nCHAPTER 7\\nFine-tuning to follow instructions\\nSimilar to the approach used for classification fine-tuning, we want to accelerate train-\\ning by collecting multiple training examples in a batch, which necessitates padding all\\ninputs to a similar length. As with classification fine-tuning, we use the <|endoftext|>\\ntoken as a padding token. \\n Instead of appending the <|endoftext|> tokens to the text inputs, we can append\\nthe token ID corresponding to <|endoftext|> to the pretokenized inputs directly. We\\ncan use the tokenizer’s .encode method on an <|endoftext|> token to remind us\\nwhich token ID we should use:\\nimport tiktoken\\ntokenizer = tiktoken.get_encoding(\"gpt2\")\\nprint(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))\\nThe resulting token ID is 50256. \\n Moving on to step 2.3 of the process (see figure 7.6), we adopt a more sophisti-\\ncated approach by developing a custom collate function that we can pass to the data\\nloader. This custom collate function pads the training examples in each batch to the\\nsame length while allowing different batches to have different lengths, as demon-\\nstrated in figure 7.8. This approach minimizes unnecessary padding by only extending\\nsequences to match the longest one in each batch, not the whole dataset.\\nFigure 7.8\\nThe padding of training examples in batches using token ID 50256 to ensure uniform length within \\neach batch. Each batch may have different lengths, as shown by the first and second.\\n[ 0,     1,     2,     3,     4]\\n[ 5,     6]\\n[7,      8,     9]\\n[    0,     1,     2,     3,     4]\\n[    5,     6, 50256, 50256, 50256]\\n[    7,     8,     9, 50256, 50256]\\nInput 1\\nInput 2\\nInput 3\\n[ 8,     1]\\n[10,     3,     11,     6]\\n[ 5,     22,    13,    13]\\nInput 4\\nInput 5\\nInput 6\\n[    8,     1, 50256, 50256]\\n[   10,     3,    11,     6]\\n[    5,    22,    13,    13]\\nThe ﬁrst\\nbatch\\nThe\\nsecond\\nbatch\\nToken ID 50256 is used\\nas the padding token.\\nToken IDs corresponding to\\nthe ﬁrst training example\\nPad all training examples in a batch\\nso that they have the same length.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 236}, page_content='215\\n7.3\\nOrganizing data into training batches\\nWe can implement the padding process with a custom collate function:\\ndef custom_collate_draft_1(\\n    batch,\\n    pad_token_id=50256,\\n    device=\"cpu\"\\n):\\n    batch_max_length = max(len(item)+1 for item in batch)  \\n    inputs_lst = []\\n    for item in batch:    \\n        new_item = item.copy()\\n        new_item += [pad_token_id]\\n        padded = (\\n            new_item + [pad_token_id] * \\n            (batch_max_length - len(new_item))\\n        )\\n        inputs = torch.tensor(padded[:-1])   \\n        inputs_lst.append(inputs)\\n    inputs_tensor = torch.stack(inputs_lst).to(device)    \\n    return inputs_tensor\\nThe custom_collate_draft_1 we implemented is designed to be integrated into a\\nPyTorch DataLoader, but it can also function as a standalone tool. Here, we use it\\nindependently to test and verify that it operates as intended. Let’s try it on three dif-\\nferent inputs that we want to assemble into a batch, where each example gets padded\\nto the same length:\\ninputs_1 = [0, 1, 2, 3, 4]\\ninputs_2 = [5, 6]\\ninputs_3 = [7, 8, 9]\\nbatch = (\\n    inputs_1,\\n    inputs_2,\\n    inputs_3\\n)\\nprint(custom_collate_draft_1(batch))\\nThe resulting batch looks like the following:\\ntensor([[    0,     1,     2,     3,     4],  \\n        [    5,     6, 50256, 50256, 50256],\\n        [    7,     8,     9, 50256, 50256]])\\nThis output shows all inputs have been padded to the length of the longest input list,\\ninputs_1, containing five token IDs.\\nFinds the longest \\nsequence in the \\nbatch\\nPads and \\nprepares inputs\\nRemoves extra \\npadded token \\nadded earlier\\nConverts the list of \\ninputs to a tensor \\nand transfers it to \\nthe target device'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 237}, page_content='216\\nCHAPTER 7\\nFine-tuning to follow instructions\\n We have just implemented our first custom collate function to create batches from\\nlists of inputs. However, as we previously learned, we also need to create batches with\\nthe target token IDs corresponding to the batch of input IDs. These target IDs, as\\nshown in figure 7.9, are crucial because they represent what we want the model to\\ngenerate and what we need during training to calculate the loss for the weight\\nupdates. That is, we modify our custom collate function to return the target token IDs\\nin addition to the input token IDs.\\nSimilar to the process we used to pretrain an LLM, the target token IDs match the\\ninput token IDs but are shifted one position to the right. This setup, as shown in fig-\\nure 7.10, allows the LLM to learn how to predict the next token in a sequence.\\n \\n \\nFigure 7.9\\nThe five substeps involved in implementing the batching process. We are now focusing on \\nstep 2.4, the creation of target token IDs. This step is essential as it enables the model to learn and \\npredict the tokens it needs to generate.\\n2.1) Format data\\nusing prompt\\ntemplate.\\n2.2) Tokenize\\nformatted data.\\n2.5) Replace\\npadding tokens\\nwith placeholders.\\n2.3)\\nh\\nAdjust to t e\\nsame length with\\npadding tokens.\\nBelow is an instruction that describes a task. Write a\\nresponse that appropriately completes the request.\\n[21106, 318, 281, 12064, 326,\\n8477, 257, 4876, 13, ...,]\\n2.4) Create target\\ntoken IDs for\\ntraining.\\n[21106, 318, 281, 12064, 326,\\n8477, 257, 4876, 13, ...,\\n50256, 50256, 50256]\\n[21106, 318, 281, 12064, 326,\\n8477, 257, 4876, 13, ...,\\n50256, 50256, 50256]\\n[318, 281, 12064, 326, 8477,\\n257, 4876, 13, ...,\\n50256, 50256, 50256, 50256]\\n[21106, 318, 281, 12064, 326,\\n8477, 257, 4876, 13, ...,\\n50256, 50256, 50256]\\n[318, 281, 12064, 326, 8477,\\n257, 4876, 13, ...,\\n50256, -100, -100, -100]\\nFormat input into an instruction-\\nresponse template.\\nConvert instruction-response\\nentry into token IDs.\\nAdd end-of-text tokens (50256)\\nto pad data samples to the same\\nlength.\\nCreate a list of target token IDs\\nfor the model to learn (these are\\nthe inputs shifted by 1, plus an\\nadditional padding token).\\nReplace certain padding tokens\\nby -100 to exclude them from\\nthe training loss.\\n### Instruction: …\\n### Response: …'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 238}, page_content='217\\n7.3\\nOrganizing data into training batches\\nThe following updated collate function generates the target token IDs from the input\\ntoken IDs:\\ndef custom_collate_draft_2(\\n    batch,\\n    pad_token_id=50256,\\n    device=\"cpu\"\\n):\\n    batch_max_length = max(len(item)+1 for item in batch)\\n    inputs_lst, targets_lst = [], []\\n    for item in batch:\\n        new_item = item.copy()\\n        new_item += [pad_token_id]\\nFigure 7.10\\nThe input and target token alignment used in the instruction \\nfine-tuning process of an LLM. For each input sequence, the corresponding \\ntarget sequence is created by shifting the token IDs one position to the right, \\nomitting the first token of the input, and appending an end-of-text token. \\n[    0,     1,     2,     3,     4       ]\\nInput 1\\n[    1,     2,     3,     4,    50256    ]\\nTarget 1\\nThe token IDs in the target\\nare similar to the input IDs\\nbut shifted by 1 position.\\nThe target vector does not\\ncontain the ﬁrst input ID.\\nWe add an end-of-text\\n(padding) token.\\n[    5,     6, 50256, 50256, 50256    ]\\nInput 2\\n[    6, 50256, 50256, 50256, 50256    ]\\nTarget 2\\nWe always add an end-of-text\\n(padding) token to the target.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 239}, page_content='218\\nCHAPTER 7\\nFine-tuning to follow instructions\\n        padded = (\\n            new_item + [pad_token_id] * \\n            (batch_max_length - len(new_item))\\n        )\\n        inputs = torch.tensor(padded[:-1])    \\n        targets = torch.tensor(padded[1:])   \\n        inputs_lst.append(inputs)\\n        targets_lst.append(targets)\\n    inputs_tensor = torch.stack(inputs_lst).to(device)\\n    targets_tensor = torch.stack(targets_lst).to(device)\\n    return inputs_tensor, targets_tensor\\ninputs, targets = custom_collate_draft_2(batch)\\nprint(inputs)\\nprint(targets)\\nApplied to the example batch consisting of three input lists we defined earlier, the\\nnew custom_collate_draft_2 function now returns the input and the target batch:\\ntensor([[    0,     1,     2,     3,     4],   \\n        [    5,     6, 50256, 50256, 50256],\\n        [    7,     8,     9, 50256, 50256]])\\ntensor([[    1,     2,     3,     4, 50256],  \\n        [    6, 50256, 50256, 50256, 50256],\\n        [    8,     9, 50256, 50256, 50256]])\\nIn the next step, we assign a -100 placeholder value to all padding tokens, as high-\\nlighted in figure 7.11. This special value allows us to exclude these padding tokens\\nfrom contributing to the training loss calculation, ensuring that only meaningful data\\ninfluences model learning. We will discuss this process in more detail after we imple-\\nment this modification. (When fine-tuning for classification, we did not have to worry\\nabout this since we only trained the model based on the last output token.)\\n However, note that we retain one end-of-text token, ID 50256, in the target list, as\\ndepicted in figure 7.12. Retaining it allows the LLM to learn when to generate an end-\\nof-text token in response to instructions, which we use as an indicator that the gener-\\nated response is complete.\\n In the following listing, we modify our custom collate function to replace tokens\\nwith ID 50256 with -100 in the target lists. Additionally, we introduce an allowed_\\nmax_length parameter to optionally limit the length of the samples. This adjustment\\nwill be useful if you plan to work with your own datasets that exceed the 1,024-token\\ncontext size supported by the GPT-2 model. \\n \\n \\n \\n \\n \\nTruncates the \\nlast token for \\ninputs\\nShifts +1 to the \\nright for targets\\nThe first tensor \\nrepresents inputs.\\nThe second tensor \\nrepresents the targets.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 240}, page_content='219\\n7.3\\nOrganizing data into training batches\\nFigure 7.11\\nThe five substeps involved in implementing the batching process. After creating the \\ntarget sequence by shifting token IDs one position to the right and appending an end-of-text token, in \\nstep 2.5, we replace the end-of-text padding tokens with a placeholder value (-100).\\n2.1) Format data\\nusing prompt\\ntemplate.\\n2.2) Tokenize\\nformatted data.\\n2.5) Replace\\npadding tokens\\nwith placeholders.\\n2.3)\\nh\\nAdjust to t e\\nsame length with\\npadding tokens.\\nBelow is an instruction that describes a task. Write a\\nresponse that appropriately completes the request.\\n[21106, 318, 281, 12064, 326,\\n8477, 257, 4876, 13, ...,]\\n2.4) Create target\\ntoken IDs for\\ntraining.\\n[21106, 318, 281, 12064, 326,\\n8477, 257, 4876, 13, ...,\\n50256, 50256, 50256]\\n[21106, 318, 281, 12064, 326,\\n8477, 257, 4876, 13, ...,\\n50256, 50256, 50256]\\n[318, 281, 12064, 326, 8477,\\n257, 4876, 13, ...,\\n50256, 50256, 50256, 50256]\\n[21106, 318, 281, 12064, 326,\\n8477, 257, 4876, 13, ...,\\n50256, 50256, 50256]\\n[318, 281, 12064, 326, 8477,\\n257, 4876, 13, ...,\\n50256, -100, -100, -100]\\nFormat input into an instruction-\\nresponse template.\\nConvert instruction-response\\nentry into token IDs.\\nAdd end-of-text tokens (50256)\\nto pad data samples to the same\\nlength.\\nCreate a list of target token IDs\\nfor the model to learn (these are\\nthe inputs shifted by 1, plus an\\nadditional padding token).\\nReplace certain padding tokens\\nby -100 to exclude them from\\nthe training loss.\\n### Instruction: …\\n### Response: …\\nFigure 7.12\\nStep 2.4 in the token replacement process in the target batch for the training data preparation. We \\nreplace all but the first instance of the end-of-text token, which we use as padding, with the placeholder value \\n-100, while keeping the initial end-of-text token in each target sequence.\\n[    1,     2,     3,     4, 50256    ]\\nTarget 1\\n[    6, 50256, 50256, 50256, 50256    ]\\nTarget 2\\n[    8,     9, 50256, 50256, 50256    ]\\nTarget 3\\n[    1,     2,     3,     4, 50256     ]\\n[    6, 50256,  -100,  -100,   -100    ]\\n[    8,     9, 50256,  -100,   -100    ]\\nWe don’t modify the ﬁrst\\ninstance of the end-of-text\\n(padding) token.\\nWe replace all but the ﬁrst\\ninstance of the end-of-text\\n(padding) token with -100.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 241}, page_content='220\\nCHAPTER 7\\nFine-tuning to follow instructions\\ndef custom_collate_fn(\\n    batch,\\n    pad_token_id=50256,\\n    ignore_index=-100,\\n    allowed_max_length=None,\\n    device=\"cpu\"\\n):\\n    batch_max_length = max(len(item)+1 for item in batch)\\n    inputs_lst, targets_lst = [], []\\n    for item in batch:\\n        new_item = item.copy()\\n        new_item += [pad_token_id]\\n        \\n        padded = (                              \\n            new_item + [pad_token_id] *         \\n            (batch_max_length - len(new_item))  \\n        )\\n        inputs = torch.tensor(padded[:-1])     \\n        targets = torch.tensor(padded[1:])    \\n        mask = targets == pad_token_id             \\n        indices = torch.nonzero(mask).squeeze()    \\n        if indices.numel() > 1:                    \\n            targets[indices[1:]] = ignore_index    \\n        if allowed_max_length is not None:\\n            inputs = inputs[:allowed_max_length]      \\n            targets = targets[:allowed_max_length]    \\n        inputs_lst.append(inputs)\\n        targets_lst.append(targets)\\n    inputs_tensor = torch.stack(inputs_lst).to(device)\\n    targets_tensor = torch.stack(targets_lst).to(device)\\n    return inputs_tensor, targets_tensor\\nAgain, let’s try the collate function on the sample batch that we created earlier to\\ncheck that it works as intended:\\ninputs, targets = custom_collate_fn(batch)\\nprint(inputs)\\nprint(targets)\\nThe results are as follows, where the first tensor represents the inputs and the second\\ntensor represents the targets:\\ntensor([[    0,     1,     2,     3,     4],\\n        [    5,     6, 50256, 50256, 50256],\\n        [    7,     8,     9, 50256, 50256]])\\nListing 7.5\\nImplementing a custom batch collate function\\nPads sequences \\nto max_length\\nTruncates the last token for inputs\\nShifts +1 to the right for targets\\nReplaces all but the first \\npadding tokens in targets \\nby ignore_index\\nOptionally truncates to the \\nmaximum sequence length'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 242}, page_content='221\\n7.3\\nOrganizing data into training batches\\ntensor([[    1,     2,     3,     4, 50256],\\n        [    6, 50256,  -100,  -100,  -100],\\n        [    8,     9, 50256,  -100,  -100]])\\nThe modified collate function works as expected, altering the target list by inserting\\nthe token ID -100. What is the logic behind this adjustment? Let’s explore the under-\\nlying purpose of this modification.\\n For demonstration purposes, consider the following simple and self-contained\\nexample where each output logit corresponds to a potential token from the model’s\\nvocabulary. Here’s how we might calculate the cross entropy loss (introduced in chap-\\nter 5) during training when the model predicts a sequence of tokens, which is similar\\nto what we did when we pretrained the model and fine-tuned it for classification:\\nlogits_1 = torch.tensor(\\n    [[-1.0, 1.0],    \\n     [-0.5, 1.5]]     \\n)\\ntargets_1 = torch.tensor([0, 1]) # Correct token indices to generate\\nloss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\\nprint(loss_1)\\nThe loss value calculated by the previous code is 1.1269:\\ntensor(1.1269)\\nAs we would expect, adding an additional token ID affects the loss calculation:\\nlogits_2 = torch.tensor(\\n    [[-1.0, 1.0],\\n     [-0.5, 1.5],\\n     [-0.5, 1.5]]     \\n)\\ntargets_2 = torch.tensor([0, 1, 1])\\nloss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\\nprint(loss_2)\\nAfter adding the third token, the loss value is 0.7936. \\n So far, we have carried out some more or less obvious example calculations using\\nthe cross entropy loss function in PyTorch, the same loss function we used in the\\ntraining functions for pretraining and fine-tuning for classification. Now let’s get to\\nthe interesting part and see what happens if we replace the third target token ID\\nwith -100:\\ntargets_3 = torch.tensor([0, 1, -100])\\nloss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\\nprint(loss_3)\\nprint(\"loss_1 == loss_3:\", loss_1 == loss_3)\\npredictions for 1st token \\npredictions for 2nd token\\nNew third token \\nID prediction'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 243}, page_content='222\\nCHAPTER 7\\nFine-tuning to follow instructions\\nThe resulting output is \\ntensor(1.1269)\\nloss_1 == loss_3: tensor(True)\\nThe resulting loss on these three training examples is identical to the loss we calcu-\\nlated from the two training examples earlier. In other words, the cross entropy loss\\nfunction ignored the third entry in the targets_3 vector, the token ID corresponding\\nto -100. (Interested readers can try to replace the -100 value with another token ID\\nthat is not 0 or 1; it will result in an error.)\\n So what’s so special about -100 that it’s ignored by the cross entropy loss? The\\ndefault setting of the cross entropy function in PyTorch is cross_entropy(...,\\nignore_index=-100). This means that it ignores targets labeled with -100. We take\\nadvantage of this ignore_index to ignore the additional end-of-text (padding) tokens\\nthat we used to pad the training examples to have the same length in each batch.\\nHowever, we want to keep one 50256 (end-of-text) token ID in the targets because it\\nhelps the LLM to learn to generate end-of-text tokens, which we can use as an indica-\\ntor that a response is complete.\\n In addition to masking out padding tokens, it is also common to mask out the tar-\\nget token IDs that correspond to the instruction, as illustrated in figure 7.13. By mask-\\ning out the LLM’s target token IDs corresponding to the instruction, the cross\\nentropy loss is only computed for the generated response target IDs. Thus, the model\\nis trained to focus on generating accurate responses rather than memorizing instruc-\\ntions, which can help reduce overfitting.\\nFigure 7.13\\nLeft: The formatted input text we tokenize and then feed to the LLM during training. Right: The \\ntarget text we prepare for the LLM where we can optionally mask out the instruction section, which means \\nreplacing the corresponding token IDs with the -100 ignore_index value.\\nBelow is an instruction that describes a task. Write a\\nresponse that appropriately completes the request.\\n### Instruction:\\nRewrite the following sentence using passive voice.\\n### Input:\\nThe team achieved great results.\\n### Response:\\nGreat results were achieved by the team.\\nis an instruction that describes a task. Write a response\\nthat appropriately completes the request.\\n### Instruction:\\nRewrite the following sentence using passive voice.\\n### Input:\\nThe team achieved great results.\\n### Response:\\nGreat results were achieved by the team.<|endoftext|>\\nInput text:\\nTarget text:\\n[21106, 318, 281, 12064, 326, ..., 13]\\n[21106, 318, 281, 12064, 326, ..., 13]\\n[-100, -100, -100, -100, -100, ..., 13, 50256]\\n[-100, -100, -100, -100, -100, ..., 13, 50256]\\nMask out the instruction\\nwhen calculating the loss.\\nThe instruction tokens\\nare replaced by -100.\\nThe token IDs corresponding\\nto the input text\\nTokenize\\nTokenize'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 244}, page_content='223\\n7.4\\nCreating data loaders for an instruction dataset\\nAs of this writing, researchers are divided on whether masking the instructions is uni-\\nversally beneficial during instruction fine-tuning. For instance, the 2024 paper by Shi\\net al., “Instruction Tuning With Loss Over Instructions” (https://arxiv.org/abs/\\n2405.14394), demonstrated that not masking the instructions benefits the LLM per-\\nformance (see appendix B for more details). Here, we will not apply masking and\\nleave it as an optional exercise for interested readers.\\n7.4\\nCreating data loaders for an instruction dataset\\nWe have completed several stages to implement an InstructionDataset class and a\\ncustom_collate_fn function for the instruction dataset. As shown in figure 7.14, we\\nare ready to reap the fruits of our labor by simply plugging both InstructionDataset\\nobjects and the custom_collate_fn function into PyTorch data loaders. These loaders\\nExercise 7.2 Instruction and input masking\\nAfter completing the chapter and fine-tuning the model with InstructionDataset,\\nreplace the instruction and input tokens with the -100 mask to use the instruction\\nmasking method illustrated in figure 7.13. Then evaluate whether this has a positive\\neffect on model performance.\\nFigure 7.14\\nThe three-stage process for instruction fine-tuning an LLM. Thus far, we have prepared the \\ndataset and implemented a custom collate function to batch the instruction dataset. Now, we can \\ncreate and apply the data loaders to the training, validation, and test sets needed for the LLM \\ninstruction fine-tuning and evaluation.\\n2) Batching the\\ndataset\\nStage 1:\\nPreparing the dataset\\n1) Dataset\\ndownload and\\nformatting\\n3) Creating\\ndata loaders\\n4) Loading a\\npretrained LLM\\n5) Instruction\\nﬁne-tuning the\\nLLM\\n9) Scoring the\\nresponses\\nStage 2:\\nFine-tuning the LLM\\nStage 3:\\nEvaluating the LLM\\n7) Extracting\\nresponses\\n6) Inspecting\\nthe modeling\\nloss\\n8) Qualitative\\nevaluation\\nIn the previous section, we\\nassembled multiple instruction\\nexamples in a batch.\\nNow, we create the PyTorch\\ndata loaders we will use for\\nﬁne-tuning the LLM.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 245}, page_content='224\\nCHAPTER 7\\nFine-tuning to follow instructions\\nwill automatically shuffle and organize the batches for the LLM instruction fine-tun-\\ning process.\\n Before we implement the data loader creation step, we have to briefly talk about\\nthe device setting of the custom_collate_fn. The custom_collate_fn includes code\\nto move the input and target tensors (for example, torch.stack(inputs_lst).to\\n(device)) to a specified device, which can be either \"cpu\" or \"cuda\" (for NVIDIA\\nGPUs) or, optionally, \"mps\" for Macs with Apple Silicon chips. \\nNOTE\\nUsing an \"mps\" device may result in numerical differences compared\\nto the contents of this chapter, as Apple Silicon support in PyTorch is still\\nexperimental.\\nPreviously, we moved the data onto the target device (for example, the GPU memory\\nwhen device=\"cuda\") in the main training loop. Having this as part of the collate\\nfunction offers the advantage of performing this device transfer process as a back-\\nground process outside the training loop, preventing it from blocking the GPU\\nduring model training.\\n The following code initializes the device variable:\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n# if torch.backends.mps.is_available():  \\n#     device = torch.device(\"mps\")\"      \\nprint(\"Device:\", device)\\nThis will either print \"Device: cpu\" or \"Device: cuda\", depending on your machine.\\n Next, to reuse the chosen device setting in custom_collate_fn when we plug it\\ninto the PyTorch DataLoader class, we use the partial function from Python’s\\nfunctools standard library to create a new version of the function with the device\\nargument prefilled. Additionally, we set the allowed_max_length to 1024, which trun-\\ncates the data to the maximum context length supported by the GPT-2 model, which\\nwe will fine-tune later:\\nfrom functools import partial\\ncustomized_collate_fn = partial(\\n    custom_collate_fn,\\n    device=device,\\n    allowed_max_length=1024\\n)\\nNext, we can set up the data loaders as we did previously, but this time, we will use our\\ncustom collate function for the batching process.\\n \\n \\n \\n \\nUncomments these two \\nlines to use the GPU on \\nan Apple Silicon chip'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 246}, page_content='225\\n7.4\\nCreating data loaders for an instruction dataset\\nfrom torch.utils.data import DataLoader\\nnum_workers = 0     \\nbatch_size = 8\\ntorch.manual_seed(123)\\ntrain_dataset = InstructionDataset(train_data, tokenizer)\\ntrain_loader = DataLoader(\\n    train_dataset,\\n    batch_size=batch_size,\\n    collate_fn=customized_collate_fn,\\n    shuffle=True,\\n    drop_last=True,\\n    num_workers=num_workers\\n)\\nval_dataset = InstructionDataset(val_data, tokenizer)\\nval_loader = DataLoader(\\n    val_dataset,\\n    batch_size=batch_size,\\n    collate_fn=customized_collate_fn,\\n    shuffle=False,\\n    drop_last=False,\\n    num_workers=num_workers\\n)\\ntest_dataset = InstructionDataset(test_data, tokenizer)\\ntest_loader = DataLoader(\\n    test_dataset,\\n    batch_size=batch_size,\\n    collate_fn=customized_collate_fn,\\n    shuffle=False,\\n    drop_last=False,\\n    num_workers=num_workers\\n)\\nLet’s examine the dimensions of the input and target batches generated by the train-\\ning loader:\\nprint(\"Train loader:\")\\nfor inputs, targets in train_loader:\\n    print(inputs.shape, targets.shape)\\nThe output is as follows (truncated to conserve space):\\nTrain loader:\\ntorch.Size([8, 61]) torch.Size([8, 61])\\ntorch.Size([8, 76]) torch.Size([8, 76])\\ntorch.Size([8, 73]) torch.Size([8, 73])\\n...\\nListing 7.6\\nInitializing the data loaders\\nYou can try to increase this number if \\nparallel Python processes are supported \\nby your operating system.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 247}, page_content='226\\nCHAPTER 7\\nFine-tuning to follow instructions\\ntorch.Size([8, 74]) torch.Size([8, 74])\\ntorch.Size([8, 69]) torch.Size([8, 69])\\nThis output shows that the first input and target batch have dimensions 8 × 61, where\\n8 represents the batch size and 61 is the number of tokens in each training example in\\nthis batch. The second input and target batch have a different number of tokens—for\\ninstance, 76. Thanks to our custom collate function, the data loader is able to create\\nbatches of different lengths. In the next section, we load a pretrained LLM that we\\ncan then fine-tune with this data loader.\\n7.5\\nLoading a pretrained LLM\\nWe have spent a lot of time preparing the dataset for instruction fine-tuning, which is\\na key aspect of the supervised fine-tuning process. Many other aspects are the same as\\nin pretraining, allowing us to reuse much of the code from earlier chapters.\\n Before beginning instruction fine-tuning, we must first load a pretrained GPT\\nmodel that we want to fine-tune (see figure 7.15), a process we have undertaken previ-\\nously. However, instead of using the smallest 124-million-parameter model as before,\\nwe load the medium-sized model with 355 million parameters. The reason for this\\nchoice is that the 124-million-parameter model is too limited in capacity to achieve\\nFigure 7.15\\nThe three-stage process for instruction fine-tuning an LLM. After the dataset \\npreparation, the process of fine-tuning an LLM for instruction-following begins with loading \\na pretrained LLM, which serves as the foundation for subsequent training. \\n2) Batching the\\ndataset\\nStage 1:\\nPreparing the dataset\\n1) Dataset\\ndownload and\\nformatting\\n3) Creating\\ndata loaders\\n4) Loading a\\npretrained LLM\\n5) Instruction\\nﬁne-tuning the\\nLLM\\n9) Scoring the\\nresponses\\nStage 2:\\nFine-tuning the LLM\\nStage 3:\\nEvaluating the LLM\\n7) Extracting\\nresponses\\n6) Inspecting\\nthe modeling\\nloss\\n8) Qualitative\\nevaluation\\nNow, we are loading\\nthe LLM for ﬁne-tuning.\\nNow, we create the PyTorch\\ndata loaders we will use for\\nﬁne-tuning the LLM.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 248}, page_content='227\\n7.5\\nLoading a pretrained LLM\\nsatisfactory results via instruction fine-tuning. Specifically, smaller models lack the\\nnecessary capacity to learn and retain the intricate patterns and nuanced behaviors\\nrequired for high-quality instruction-following tasks. \\n Loading our pretrained models requires the same code as when we pretrained the\\ndata (section 5.5) and fine-tuned it for classification (section 6.4), except that we now\\nspecify \"gpt2-medium (355M)\" instead of \"gpt2-small (124M)\". \\nNOTE\\nExecuting this code will initiate the download of the medium-sized\\nGPT model, which has a storage requirement of approximately 1.42 giga-\\nbytes. This is roughly three times larger than the storage space needed for the\\nsmall model.\\nfrom gpt_download import download_and_load_gpt2\\nfrom chapter04 import GPTModel\\nfrom chapter05 import load_weights_into_gpt\\nBASE_CONFIG = {\\n    \"vocab_size\": 50257,     # Vocabulary size\\n    \"context_length\": 1024,  # Context length\\n    \"drop_rate\": 0.0,        # Dropout rate\\n    \"qkv_bias\": True         # Query-key-value bias\\n}\\nmodel_configs = {\\n    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\\n    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\\n    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\\n    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\\n}\\nCHOOSE_MODEL = \"gpt2-medium (355M)\"\\nBASE_CONFIG.update(model_configs[CHOOSE_MODEL])\\nmodel_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\\nsettings, params = download_and_load_gpt2(\\n    model_size=model_size, \\n    models_dir=\"gpt2\"\\n)\\nmodel = GPTModel(BASE_CONFIG)\\nload_weights_into_gpt(model, params)\\nmodel.eval();\\nAfter executing the code, several files will be downloaded:\\ncheckpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 156kiB/s]\\nencoder.json: 100%|██████████| 1.04M/1.04M [00:02<00:00, 467kiB/s]\\nhparams.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 198kiB/s]\\nmodel.ckpt.data-00000-of-00001: 100%|██████████| 1.42G/1.42G \\nListing 7.7\\nLoading the pretrained model'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 249}, page_content='228\\nCHAPTER 7\\nFine-tuning to follow instructions\\n[05:50<00:00, 4.05MiB/s]\\nmodel.ckpt.index: 100%|██████████| 10.4k/10.4k [00:00<00:00, 18.1MiB/s]\\nmodel.ckpt.meta: 100%|██████████| 927k/927k [00:02<00:00, 454kiB/s]\\nvocab.bpe: 100%|██████████| 456k/456k [00:01<00:00, 283kiB/s]\\nNow, let’s take a moment to assess the pretrained LLM’s performance on one of the\\nvalidation tasks by comparing its output to the expected response. This will give us a\\nbaseline understanding of how well the model performs on an instruction-following\\ntask right out of the box, prior to fine-tuning, and will help us appreciate the effect\\nof fine-tuning later on. We will use the first example from the validation set for this\\nassessment:\\ntorch.manual_seed(123)\\ninput_text = format_input(val_data[0])\\nprint(input_text)\\nThe content of the instruction is as follows:\\nBelow is an instruction that describes a task. Write a response that \\nappropriately completes the request.\\n### Instruction:\\nConvert the active sentence to passive: \\'The chef cooks the meal every day.\\'\\nNext we generate the model’s response using the same generate function we used to\\npretrain the model in chapter 5:\\nfrom chapter05 import generate, text_to_token_ids, token_ids_to_text\\ntoken_ids = generate(\\n    model=model,\\n    idx=text_to_token_ids(input_text, tokenizer),\\n    max_new_tokens=35,\\n    context_size=BASE_CONFIG[\"context_length\"],\\n    eos_id=50256,\\n)\\ngenerated_text = token_ids_to_text(token_ids, tokenizer)\\nThe generate function returns the combined input and output text. This behavior was\\npreviously convenient since pretrained LLMs are primarily designed as text-completion\\nmodels, where the input and output are concatenated to create coherent and legible\\ntext. However, when evaluating the model’s performance on a specific task, we often\\nwant to focus solely on the model’s generated response.\\n To isolate the model’s response text, we need to subtract the length of the input\\ninstruction from the start of the generated_text:\\nresponse_text = generated_text[len(input_text):].strip()\\nprint(response_text)'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 250}, page_content=\"229\\n7.6\\nFine-tuning the LLM on instruction data\\nThis code removes the input text from the beginning of the generated_text, leaving\\nus with only the model’s generated response. The strip() function is then applied to\\nremove any leading or trailing whitespace characters. The output is \\n### Response:\\nThe chef cooks the meal every day.\\n### Instruction:\\nConvert the active sentence to passive: 'The chef cooks the\\nThis output shows that the pretrained model is not yet capable of correctly following\\nthe given instruction. While it does create a Response section, it simply repeats the\\noriginal input sentence and part of the instruction, failing to convert the active sen-\\ntence to passive voice as requested. So, let’s now implement the fine-tuning process\\nto improve the model’s ability to comprehend and appropriately respond to such\\nrequests.\\n7.6\\nFine-tuning the LLM on instruction data\\nIt’s time to fine-tune the LLM for instructions (figure 7.16). We will take the loaded\\npretrained model in the previous section and further train it using the previously pre-\\npared instruction dataset prepared earlier in this chapter. We already did all the hard\\nwork when we implemented the instruction dataset processing at the beginning of\\nFigure 7.16\\nThe three-stage process for instruction fine-tuning an LLM. In step 5, we train the pretrained model \\nwe previously loaded on the instruction dataset we prepared earlier.\\n2) Batching the\\ndataset\\nStage 1:\\nPreparing the dataset\\n1) Dataset\\ndownload and\\nformatting\\n3) Creating\\ndata loaders\\n4) Loading a\\npretrained LLM\\n5) Instruction\\nﬁne-tuning the\\nLLM\\n9) Scoring the\\nresponses\\nStage 2:\\nFine-tuning the LLM\\nStage 3:\\nEvaluating the LLM\\n7) Extracting\\nresponses\\n6) Inspecting\\nthe modeling\\nloss\\n8) Qualitative\\nevaluation\\nAfter preparing the\\ndataset and loading a\\npretrained model, we\\nnow ﬁne-tune the model\\non the instruction data.\"),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 251}, page_content='230\\nCHAPTER 7\\nFine-tuning to follow instructions\\nthis chapter. For the fine-tuning process itself, we can reuse the loss calculation and\\ntraining functions implemented in chapter 5:\\nfrom chapter05 import (\\n    calc_loss_loader,\\n    train_model_simple\\n)\\nBefore we begin training, let’s calculate the initial loss for the training and valida-\\ntion sets:\\nmodel.to(device)\\ntorch.manual_seed(123)\\nwith torch.no_grad():\\n    train_loss = calc_loss_loader(\\n        train_loader, model, device, num_batches=5\\n    )\\n    val_loss = calc_loss_loader(\\n        val_loader, model, device, num_batches=5\\n)\\nprint(\"Training loss:\", train_loss)\\nprint(\"Validation loss:\", val_loss)\\nThe initial loss values are as follows; as previously, our goal is to minimize the loss:\\nTraining loss: 3.825908660888672\\nValidation loss: 3.7619335651397705\\nDealing with hardware limitations\\nUsing and training a larger model like GPT-2 medium (355 million parameters) is more\\ncomputationally intensive than the smaller GPT-2 model (124 million parameters). If\\nyou encounter problems due to hardware limitations, you can switch to the smaller\\nmodel by changing CHOOSE_MODEL = \"gpt2-medium (355M)\" to CHOOSE_MODEL =\\n\"gpt2-small (124M)\" (see section 7.5). Alternatively, to speed up the model training,\\nconsider using a GPU. The following supplementary section in this book’s code repos-\\nitory lists several options for using cloud GPUs: https://mng.bz/EOEq.\\nThe following table provides reference run times for training each model on various\\ndevices, including CPUs and GPUs, for GPT-2. Running this code on a compatible GPU\\nrequires no code changes and can significantly speed up training. For the results\\nshown in this chapter, I used the GPT-2 medium model and trained it on an A100\\nGPU.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 252}, page_content='231\\n7.6\\nFine-tuning the LLM on instruction data\\nWith the model and data loaders prepared, we can now proceed to train the model.\\nThe code in listing 7.8 sets up the training process, including initializing the opti-\\nmizer, setting the number of epochs, and defining the evaluation frequency and start-\\ning context to evaluate generated LLM responses during training based on the first\\nvalidation set instruction (val_data[0]) we looked at in section 7.5.\\nimport time\\nstart_time = time.time()\\ntorch.manual_seed(123)\\noptimizer = torch.optim.AdamW(\\n    model.parameters(), lr=0.00005, weight_decay=0.1\\n)\\nnum_epochs = 2\\ntrain_losses, val_losses, tokens_seen = train_model_simple(\\n    model, train_loader, val_loader, optimizer, device,\\n    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\\n    start_context=format_input(val_data[0]), tokenizer=tokenizer\\n)\\nend_time = time.time()\\nexecution_time_minutes = (end_time - start_time) / 60\\nprint(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\\nThe following output displays the training progress over two epochs, where a steady\\ndecrease in losses indicates improving ability to follow instructions and generate\\nappropriate responses:\\nEp 1 (Step 000000): Train loss 2.637, Val loss 2.626\\nEp 1 (Step 000005): Train loss 1.174, Val loss 1.103\\nEp 1 (Step 000010): Train loss 0.872, Val loss 0.944\\nEp 1 (Step 000015): Train loss 0.857, Val loss 0.906\\n...\\nListing 7.8\\nInstruction fine-tuning the pretrained LLM\\nModel name\\nDevice\\nRun time for two epochs\\ngpt2-medium (355M)\\nCPU (M3 MacBook Air)\\n15.78 minutes\\ngpt2-medium (355M)\\nGPU (NVIDIA L4)\\n1.83 minutes\\ngpt2-medium (355M)\\nGPU (NVIDIA A100)\\n0.86 minutes\\ngpt2-small (124M)\\nCPU (M3 MacBook Air)\\n5.74 minutes\\ngpt2-small (124M)\\nGPU (NVIDIA L4)\\n0.69 minutes\\ngpt2-small (124M)\\nGPU (NVIDIA A100)\\n0.39 minutes'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 253}, page_content='232\\nCHAPTER 7\\nFine-tuning to follow instructions\\nEp 1 (Step 000115): Train loss 0.520, Val loss 0.665\\nBelow is an instruction that describes a task. Write a response that \\nappropriately completes the request.  ### Instruction: Convert the \\nactive sentence to passive: \\'The chef cooks the meal every day.\\' \\n### Response: The meal is prepared every day by the chef.<|endoftext|>\\nThe following is an instruction that describes a task. \\nWrite a response that appropriately completes the request.  \\n### Instruction: Convert the active sentence to passive:\\nEp 2 (Step 000120): Train loss 0.438, Val loss 0.670\\nEp 2 (Step 000125): Train loss 0.453, Val loss 0.685\\nEp 2 (Step 000130): Train loss 0.448, Val loss 0.681\\nEp 2 (Step 000135): Train loss 0.408, Val loss 0.677\\n...\\nEp 2 (Step 000230): Train loss 0.300, Val loss 0.657\\nBelow is an instruction that describes a task. Write a response \\nthat appropriately completes the request.  ### Instruction: \\nConvert the active sentence to passive: \\'The chef cooks the meal \\nevery day.\\'  ### Response: The meal is cooked every day by the \\nchef.<|endoftext|>The following is an instruction that describes \\na task. Write a response that appropriately completes the request.  \\n### Instruction: What is the capital of the United Kingdom\\nTraining completed in 0.87 minutes.\\nThe training output shows that the model is learning effectively, as we can tell based\\non the consistently decreasing training and validation loss values over the two epochs.\\nThis result suggests that the model is gradually improving its ability to understand and\\nfollow the provided instructions. (Since the model demonstrated effective learning\\nwithin these two epochs, extending the training to a third epoch or more is not essen-\\ntial and may even be counterproductive as it could lead to increased overfitting.)\\n Moreover, the generated responses at the end of each epoch let us inspect the\\nmodel’s progress in correctly executing the given task in the validation set example. In\\nthis case, the model successfully converts the active sentence \"The chef cooks the\\nmeal every day.\" into its passive voice counterpart: \"The meal is cooked every day by\\nthe chef.\"\\n We will revisit and evaluate the response quality of the model in more detail later.\\nFor now, let’s examine the training and validation loss curves to gain additional\\ninsights into the model’s learning process. For this, we use the same plot_losses\\nfunction we used for pretraining:\\nfrom chapter05 import plot_losses\\nepochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\\nplot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\\nFrom the loss plot shown in figure 7.17, we can see that the model’s performance on\\nboth the training and validation sets improves substantially over the course of train-\\ning. The rapid decrease in losses during the initial phase indicates that the model\\nquickly learns meaningful patterns and representations from the data. Then, as train-\\ning progresses to the second epoch, the losses continue to decrease but at a slower'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 254}, page_content='233\\n7.7\\nExtracting and saving responses\\nrate, suggesting that the model is fine-tuning its learned representations and converg-\\ning to a stable solution. \\nWhile the loss plot in figure 7.17 indicates that the model is training effectively, the\\nmost crucial aspect is its performance in terms of response quality and correctness.\\nSo, next, let’s extract the responses and store them in a format that allows us to evalu-\\nate and quantify the response quality.\\n7.7\\nExtracting and saving responses\\nHaving fine-tuned the LLM on the training portion of the instruction dataset, we are\\nnow ready to evaluate its performance on the held-out test set. First, we extract the\\nmodel-generated responses for each input in the test dataset and collect them for\\nmanual analysis, and then we evaluate the LLM to quantify the quality of the\\nresponses, as highlighted in figure 7.18.\\nExercise 7.3 Fine-tuning on the original Alpaca dataset \\nThe Alpaca dataset, by researchers at Stanford, is one of the earliest and most pop-\\nular openly shared instruction datasets, consisting of 52,002 entries. As an alterna-\\ntive to the instruction-data.json file we use here, consider fine-tuning an LLM on\\nthis dataset. The dataset is available at https://mng.bz/NBnE.\\nThis dataset contains 52,002 entries, which is approximately 50 times more than\\nthose we used here, and most entries are longer. Thus, I highly recommend using a\\nGPU to conduct the training, which will accelerate the fine-tuning process. If you\\nencounter out-of-memory errors, consider reducing the batch_size from 8 to 4, 2,\\nor even 1. Lowering the allowed_max_length from 1,024 to 512 or 256 can also\\nhelp manage memory problems.\\nFigure 7.17\\nThe training and validation loss trends over two \\nepochs. The solid line represents the training loss, showing a \\nsharp decrease before stabilizing, while the dotted line \\nrepresents the validation loss, which follows a similar pattern.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 255}, page_content='234\\nCHAPTER 7\\nFine-tuning to follow instructions\\nTo complete the response instruction step, we use the generate function. We then\\nprint the model responses alongside the expected test set answers for the first three\\ntest set entries, presenting them side by side for comparison:\\ntorch.manual_seed(123)\\nfor entry in test_data[:3]:     \\n    input_text = format_input(entry)\\n    token_ids = generate(              \\n        model=model,\\n        idx=text_to_token_ids(input_text, tokenizer).to(device),\\n        max_new_tokens=256,\\n        context_size=BASE_CONFIG[\"context_length\"],\\n        eos_id=50256\\n    )\\n    generated_text = token_ids_to_text(token_ids, tokenizer)\\n    \\n    response_text = (\\n        generated_text[len(input_text):]\\n        .replace(\"### Response:\", \"\")\\n        .strip()\\n    )\\nFigure 7.18\\nThe three-stage process for instruction fine-tuning the LLM. In the first \\ntwo steps of stage 3, we extract and collect the model responses on the held-out test \\ndataset for further analysis and then evaluate the model to quantify the performance of \\nthe instruction-fine-tuned LLM.\\n2) Batching the\\ndataset\\nStage 1:\\nPreparing the dataset\\n1) Dataset\\ndownload and\\nformatting\\n3) Creating\\ndata loaders\\n4) Loading a\\npretrained LLM\\n5) Instruction\\nﬁne-tuning the\\nLLM\\n9) Scoring the\\nresponses\\nStage 2:\\nFine-tuning the LLM\\nStage 3:\\nEvaluating the LLM\\n7) Extracting\\nresponses\\n6) Inspecting\\nthe modeling\\nloss\\n8) Qualitative\\nevaluation\\nThen, we compare the model’s\\nresponses with the correct\\nresponses in the test set.\\nIn this section, we extract\\nthe responses from our\\nﬁne-tuned LLM.\\nIterates over the \\nfirst three test set \\nsamples\\nUses the generate function \\nimported in section 7.5'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 256}, page_content='235\\n7.7\\nExtracting and saving responses\\n    print(input_text)\\n    print(f\"\\\\nCorrect response:\\\\n>> {entry[\\'output\\']}\")\\n    print(f\"\\\\nModel response:\\\\n>> {response_text.strip()}\")\\n    print(\"-------------------------------------\")\\nAs mentioned earlier, the generate function returns the combined input and output\\ntext, so we use slicing and the .replace() method on the generated_text contents to\\nextract the model’s response. The instructions, followed by the given test set response\\nand model response, are shown next.\\nBelow is an instruction that describes a task. Write a response that appropriately\\ncompletes the request.\\n### Instruction:\\nRewrite the sentence using a simile.\\n### Input:\\nThe car is very fast.\\nCorrect response:\\n>> The car is as fast as lightning.\\nModel response:\\n>> The car is as fast as a bullet.\\nBelow is an instruction that describes a task. Write a response that appropriately\\ncompletes the request.\\n### Instruction:\\nWhat type of cloud is typically associated with thunderstorms?\\nCorrect response:\\n>> The type of cloud typically associated with thunderstorms is cumulonimbus.\\nModel response:\\n>> The type of cloud associated with thunderstorms is a cumulus cloud.\\nBelow is an instruction that describes a task. Write a response that appropriately\\ncompletes the request.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 257}, page_content='236\\nCHAPTER 7\\nFine-tuning to follow instructions\\n### Instruction:\\nName the author of ‘Pride and Prejudice.’\\nCorrect response:\\n>> Jane Austen.\\nModel response:\\n>> The author of ‘Pride and Prejudice’ is Jane Austen.\\nAs we can see based on the test set instructions, given responses, and the model’s\\nresponses, the model performs relatively well. The answers to the first and last instruc-\\ntions are clearly correct, while the second answer is close but not entirely accurate.\\nThe model answers with “cumulus cloud” instead of “cumulonimbus,” although it’s\\nworth noting that cumulus clouds can develop into cumulonimbus clouds, which are\\ncapable of producing thunderstorms.\\n Most importantly, model evaluation is not as straightforward as it is for completion\\nfine-tuning, where we simply calculate the percentage of correct spam/non-spam class\\nlabels to obtain the classification’s accuracy. In practice, instruction-fine-tuned LLMs\\nsuch as chatbots are evaluated via multiple approaches:\\n\\uf0a1Short-answer and multiple-choice benchmarks, such as Measuring Massive Mul-\\ntitask Language Understanding (MMLU; https://arxiv.org/abs/2009.03300),\\nwhich test the general knowledge of a model.\\n\\uf0a1Human preference comparison to other LLMs, such as LMSYS chatbot arena\\n(https://arena.lmsys.org).\\n\\uf0a1Automated conversational benchmarks, where another LLM like GPT-4 is\\nused to evaluate the responses, such as AlpacaEval (https://tatsu-lab.github.io/\\nalpaca_eval/).\\nIn practice, it can be useful to consider all three types of evaluation methods: multiple-\\nchoice question answering, human evaluation, and automated metrics that measure\\nconversational performance. However, since we are primarily interested in assessing con-\\nversational performance rather than just the ability to answer multiple-choice ques-\\ntions, human evaluation and automated metrics may be more relevant.\\nConversational performance\\nConversational performance of LLMs refers to their ability to engage in human-like\\ncommunication by understanding context, nuance, and intent. It encompasses skills\\nsuch as providing relevant and coherent responses, maintaining consistency, and\\nadapting to different topics and styles of interaction.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 258}, page_content='237\\n7.7\\nExtracting and saving responses\\nHuman evaluation, while providing valuable insights, can be relatively laborious and\\ntime-consuming, especially when dealing with a large number of responses. For\\ninstance, reading and assigning ratings to all 1,100 responses would require a signifi-\\ncant amount of effort.\\n So, considering the scale of the task at hand, we will implement an approach simi-\\nlar to automated conversational benchmarks, which involves evaluating the responses\\nautomatically using another LLM. This method will allow us to efficiently assess the\\nquality of the generated responses without the need for extensive human involve-\\nment, thereby saving time and resources while still obtaining meaningful perfor-\\nmance indicators.\\n Let’s employ an approach inspired by AlpacaEval, using another LLM to evaluate\\nour fine-tuned model’s responses. However, instead of relying on a publicly available\\nbenchmark dataset, we use our own custom test set. This customization allows for a\\nmore targeted and relevant assessment of the model’s performance within the context\\nof our intended use cases, represented in our instruction dataset.\\n To prepare the responses for this evaluation process, we append the generated\\nmodel responses to the test_set dictionary and save the updated data as an\\n\"instruction-data-with-response.json\" file for record keeping. Additionally, by\\nsaving this file, we can easily load and analyze the responses in separate Python ses-\\nsions later on if needed.\\n The following code listing uses the generate method in the same manner as\\nbefore; however, we now iterate over the entire test_set. Also, instead of printing the\\nmodel responses, we add them to the test_set dictionary.\\nfrom tqdm import tqdm\\nfor i, entry in tqdm(enumerate(test_data), total=len(test_data)):\\n    input_text = format_input(entry)\\n    token_ids = generate(\\n        model=model,\\n        idx=text_to_token_ids(input_text, tokenizer).to(device),\\n        max_new_tokens=256,\\n        context_size=BASE_CONFIG[\"context_length\"],\\n        eos_id=50256\\n    )\\n    generated_text = token_ids_to_text(token_ids, tokenizer)\\n    \\n    response_text = (\\n        generated_text[len(input_text):]\\n        .replace(\"### Response:\", \"\")\\n        .strip()\\n    )\\n    test_data[i][\"model_response\"] = response_text\\nwith open(\"instruction-data-with-response.json\", \"w\") as file:\\n    json.dump(test_data, file, indent=4)        \\nListing 7.9\\nGenerating test set responses\\nindent for \\npretty-printing'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 259}, page_content='238\\nCHAPTER 7\\nFine-tuning to follow instructions\\nProcessing the dataset takes about 1 minute on an A100 GPU and 6 minutes on an M3\\nMacBook Air: \\n100%|██████████| 110/110 [01:05<00:00,  1.68it/s]\\nLet’s verify that the responses have been correctly added to the test_set dictionary\\nby examining one of the entries:\\nprint(test_data[0])\\nThe output shows that the model_response has been added correctly:\\n{\\'instruction\\': \\'Rewrite the sentence using a simile.\\', \\n \\'input\\': \\'The car is very fast.\\', \\n \\'output\\': \\'The car is as fast as lightning.\\', \\n \\'model_response\\': \\'The car is as fast as a bullet.\\'}\\nFinally, we save the model as gpt2-medium355M-sft.pth file to be able to reuse it in\\nfuture projects:\\nimport re\\nfile_name = f\"{re.sub(r\\'[ ()]\\', \\'\\', CHOOSE_MODEL) }-sft.pth\"     \\ntorch.save(model.state_dict(), file_name)\\nprint(f\"Model saved as {file_name}\")\\nThe saved model can then be loaded via model.load_state_dict(torch.load(\"gpt2\\n-medium355M-sft.pth\")).\\n7.8\\nEvaluating the fine-tuned LLM\\nPreviously, we judged the performance of an instruction-fine-tuned model by looking\\nat its responses on three examples of the test set. While this gives us a rough idea of\\nhow well the model performs, this method does not scale well to larger amounts of\\nresponses. So, we implement a method to automate the response evaluation of the\\nfine-tuned LLM using another, larger LLM, as highlighted in figure 7.19.\\n To evaluate test set responses in an automated fashion, we utilize an existing\\ninstruction-fine-tuned 8-billion-parameter Llama 3 model developed by Meta AI. This\\nmodel can be run locally using the open source Ollama application (https://ollama\\n.com).\\nNOTE\\nOllama is an efficient application for running LLMs on a laptop. It\\nserves as a wrapper around the open source llama.cpp library (https://github\\n.com/ggerganov/llama.cpp), which implements LLMs in pure C/C++ to\\nmaximize efficiency. However, Ollama is only a tool for generating text using\\nLLMs (inference) and does not support training or fine-tuning LLMs.\\nRemoves white spaces\\nand parentheses\\nfrom file name'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 260}, page_content='239\\n7.8\\nEvaluating the fine-tuned LLM\\nTo execute the following code, install Ollama by visiting https://ollama.com and fol-\\nlow the provided instructions for your operating system:\\n\\uf0a1For macOS and Windows users—Open the downloaded Ollama application. If\\nprompted to install command-line usage, select Yes.\\n\\uf0a1For Linux users—Use the installation command available on the Ollama website.\\nBefore implementing the model evaluation code, let’s first download the Llama 3\\nmodel and verify that Ollama is functioning correctly by using it from the command-\\nline terminal. To use Ollama from the command line, you must either start the Ollama\\napplication or run ollama serve in a separate terminal, as shown in figure 7.20.\\nUsing larger LLMs via web APIs \\nThe 8-billion-parameter Llama 3 model is a very capable LLM that runs locally. How-\\never, it’s not as capable as large proprietary LLMs such as GPT-4 offered by OpenAI.\\nFor readers interested in exploring how to utilize GPT-4 through the OpenAI API to\\nassess generated model responses, an optional code notebook is available within\\nthe supplementary materials accompanying this book at https://mng.bz/BgEv.\\nFigure 7.19\\nThe three-stage process for instruction fine-tuning the LLM. In this last \\nstep of the instruction-fine-tuning pipeline, we implement a method to quantify the \\nperformance of the fine-tuned model by scoring the responses it generated for the test.\\n2) Batching the\\ndataset\\nStage 1:\\nPreparing the dataset\\n1) Dataset\\ndownload and\\nformatting\\n3) Creating\\ndata loaders\\n4) Loading a\\npretrained LLM\\n5) Instruction\\nﬁne-tuning the\\nLLM\\n9) Scoring the\\nresponses\\nStage 2:\\nFine-tuning the LLM\\nStage 3:\\nEvaluating the LLM\\n7) Extracting\\nresponses\\n6) Inspecting\\nthe modeling\\nloss\\n8) Qualitative\\nevaluation\\nAfter extracting the responses by our\\nﬁne-tuned LLM, we use another LLM to\\nautomatically evaluate these responses.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 261}, page_content='240\\nCHAPTER 7\\nFine-tuning to follow instructions\\nWith the Ollama application or ollama serve running in a different terminal, execute\\nthe following command on the command line (not in a Python session) to try out the\\n8-billion-parameter Llama 3 model:\\nollama run llama3\\nThe first time you execute this command, this model, which takes up 4.7 GB of stor-\\nage space, will be automatically downloaded. The output looks like the following:\\npulling manifest\\npulling 6a0746a1ec1a... 100% |████████████████| 4.7 GB\\npulling 4fa551d4f938... 100% |████████████████|  12 KB\\npulling 8ab4849b038c... 100% |████████████████|  254 B\\npulling 577073ffcc6c... 100% |████████████████|  110 B\\npulling 3f8eb4da87fa... 100% |████████████████|  485 B\\nverifying sha256 digest\\nwriting manifest\\nremoving any unused layers\\nsuccess\\nFigure 7.20\\nTwo options for running Ollama. The left panel illustrates starting Ollama using ollama serve. \\nThe right panel shows a second option in macOS, running the Ollama application in the background instead of \\nusing the ollama serve command to start the application.\\nFirst option: make sure to start ollama in a separate\\nterminal via  the\\ncommand.\\nollama serve\\nThen run\\nto download\\nollama run llama3\\nand use the 8-billion-parameter Llama 3 model.\\nSecond option: if you are using macOS, you\\ncan also start the ollama application and\\nmake sure it is running in the background\\ninstead of running\\n.\\nollama serve'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 262}, page_content='241\\n7.8\\nEvaluating the fine-tuned LLM\\nOnce the model download is complete, we are presented with a command-line inter-\\nface that allows us to interact with the model. For example, try asking the model,\\n“What do llamas eat?”\\n>>> What do llamas eat?\\nLlamas are ruminant animals, which means they have a four-chambered\\nstomach and eat plants that are high in fiber. In the wild, \\nllamas typically feed on:\\n1. Grasses: They love to graze on various types of grasses, including tall\\ngrasses, wheat, oats, and barley.\\nNote that the response you see might differ since Ollama is not deterministic as of this\\nwriting.\\n You can end this ollama run llama3 session using the input /bye. However, make\\nsure to keep the ollama serve command or the Ollama application running for the\\nremainder of this chapter.\\n The following code verifies that the Ollama session is running properly before we\\nuse Ollama to evaluate the test set responses:\\nimport psutil\\ndef check_if_running(process_name):\\n    running = False\\n    for proc in psutil.process_iter([\"name\"]):\\n        if process_name in proc.info[\"name\"]:\\n            running = True\\n            break\\n    return running\\nollama_running = check_if_running(\"ollama\")\\nif not ollama_running:\\n    raise RuntimeError(\\n        \"Ollama not running. Launch ollama before proceeding.\"\\n)\\nprint(\"Ollama running:\", check_if_running(\"ollama\"))\\nAlternative Ollama models \\nThe llama3 in the ollama run llama3 command refers to the instruction-fine-tuned\\n8-billion-parameter Llama 3 model. Using Ollama with the llama3 model requires\\napproximately 16 GB of RAM. If your machine does not have sufficient RAM, you can\\ntry using a smaller model, such as the 3.8-billion-parameter phi3 model via ollama\\nrun llama3, which only requires around 8 GB of RAM.\\nFor more powerful computers, you can also use the larger 70-billion-parameter Llama\\n3 model by replacing llama3 with llama3:70b. However, this model requires signifi-\\ncantly more computational resources.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 263}, page_content='242\\nCHAPTER 7\\nFine-tuning to follow instructions\\nEnsure that the output from executing the previous code displays Ollama running:\\nTrue. If it shows False, verify that the ollama serve command or the Ollama applica-\\ntion is actively running.\\nAn alternative to the ollama run command for interacting with the model is through\\nits REST API using Python. The query_model function shown in the following listing\\ndemonstrates how to use the API.\\nimport urllib.request\\ndef query_model(\\n    prompt, \\n    model=\"llama3\", \\n    url=\"http://localhost:11434/api/chat\"\\n):\\n    data = {            \\n        \"model\": model,\\n        \"messages\": [\\n            {\"role\": \"user\", \"content\": prompt}\\n        ],\\n        \"options\": {        \\n            \"seed\": 123,\\nRunning the code in a new Python session \\nIf you already closed your Python session or if you prefer to execute the remaining\\ncode in a different Python session, use the following code, which loads the instruction\\nand response data file we previously created and redefines the format_input func-\\ntion we used earlier (the tqdm progress bar utility is used later): \\nimport json\\nfrom tqdm import tqdm\\nfile_path = \"instruction-data-with-response.json\"\\nwith open(file_path, \"r\") as file:\\n    test_data = json.load(file)\\ndef format_input(entry):\\n    instruction_text = (\\n        f\"Below is an instruction that describes a task. \"\\n        f\"Write a response that appropriately completes the request.\"\\n        f\"\\\\n\\\\n### Instruction:\\\\n{entry[\\'instruction\\']}\"\\n    )\\n    input_text = (\\n        f\"\\\\n\\\\n### Input:\\\\n{entry[\\'input\\']}\" if entry[\"input\"] else \"\"\\n    )\\n    return instruction_text + input_text\\nListing 7.10\\nQuerying a local Ollama model\\nCreates the data \\npayload as a dictionary\\nSettings for deterministic \\nresponses'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 264}, page_content='243\\n7.8\\nEvaluating the fine-tuned LLM\\n            \"temperature\": 0,\\n            \"num_ctx\": 2048\\n        }\\n    }\\n    payload = json.dumps(data).encode(\"utf-8\")   \\n    request = urllib.request.Request(                      \\n        url,                                               \\n        data=payload,                                      \\n        method=\"POST\"                                      \\n    )\\n    \\n    request.add_header(\"Content-Type\", \"application/json\")  \\n    response_data = \"\"\\n    with urllib.request.urlopen(request) as response:  \\n        while True:\\n            line = response.readline().decode(\"utf-8\")\\n            if not line:\\n                break\\n            response_json = json.loads(line)\\n            response_data += response_json[\"message\"][\"content\"]\\n    return response_data\\nBefore running the subsequent code cells in this notebook, ensure that Ollama is still\\nrunning. The previous code cells should print \"Ollama running: True\" to confirm\\nthat the model is active and ready to receive requests.\\n The following is an example of how to use the query_model function we just\\nimplemented:\\nmodel = \"llama3\"\\nresult = query_model(\"What do Llamas eat?\", model)\\nprint(result)\\nThe resulting response is as follows:\\nLlamas are ruminant animals, which means they have a four-chambered \\nstomach that allows them to digest plant-based foods. Their diet \\ntypically consists of:\\n1. Grasses: Llamas love to graze on grasses, including tall grasses, \\nshort grasses, and even weeds.\\n...\\nUsing the query_model function defined earlier, we can evaluate the responses gen-\\nerated by our fine-tuned model that prompts the Llama 3 model to rate our fine-\\ntuned model’s responses on a scale from 0 to 100 based on the given test set\\nresponse as reference. \\nConverts the\\ndictionary to a JSON-\\nformatted string and\\nencodes it to bytes\\nCreates a request \\nobject, setting the \\nmethod to POST and \\nadding necessary \\nheaders\\nSends the \\nrequest and \\ncaptures the \\nresponse'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 265}, page_content='244\\nCHAPTER 7\\nFine-tuning to follow instructions\\n First, we apply this approach to the first three examples from the test set that we\\npreviously examined:\\nfor entry in test_data[:3]:\\n    prompt = (\\n        f\"Given the input `{format_input(entry)}` \"\\n        f\"and correct output `{entry[\\'output\\']}`, \"\\n        f\"score the model response `{entry[\\'model_response\\']}`\"\\n        f\" on a scale from 0 to 100, where 100 is the best score. \"\\n    )\\n    print(\"\\\\nDataset response:\")\\n    print(\">>\", entry[\\'output\\'])\\n    print(\"\\\\nModel response:\")\\n    print(\">>\", entry[\"model_response\"])\\n    print(\"\\\\nScore:\")\\n    print(\">>\", query_model(prompt))\\n    print(\"\\\\n-------------------------\")\\nThis code prints outputs similar to the following (as of this writing, Ollama is not fully\\ndeterministic, so the generated texts may vary):\\nDataset response:\\n>> The car is as fast as lightning.\\nModel response:\\n>> The car is as fast as a bullet.\\nScore:\\n>> I’d rate the model response “The car is as fast as a bullet.” an 85 out of 100.\\nHere’s why:\\nThe response uses a simile correctly, comparing the speed of the car to something\\nelse (in this case, a bullet).\\nThe comparison is relevant and makes sense, as bullets are known for their high\\nvelocity.\\nThe phrase “as fast as” is used correctly to introduce the simile.\\nThe only reason I wouldn’t give it a perfect score is that some people might find the\\ncomparison slightly less vivid or evocative than others. For example, comparing\\nsomething to lightning (as in the original response) can be more dramatic and atten-\\ntion grabbing. However, “as fast as a bullet” is still a strong and effective simile that\\neffectively conveys the idea of the car’s speed.\\nOverall, I think the model did a great job!'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 266}, page_content='245\\n7.8\\nEvaluating the fine-tuned LLM\\nDataset response:\\n>> The type of cloud typically associated with thunderstorms is cumulonimbus.\\nModel response:\\n>> The type of cloud associated with thunderstorms is a cumulus cloud.\\nScore:\\n>> I’d score this model response as 40 out of 100.\\nHere’s why:\\nThe model correctly identifies that thunderstorms are related to clouds (correctly\\nidentifying the type of phenomenon).\\nHowever, it incorrectly specifies the type of cloud associated with thunderstorms.\\nCumulus clouds are not typically associated with thunderstorms; cumulonimbus\\nclouds are.\\nThe response lacks precision and accuracy in its description.\\nOverall, while the model attempts to address the instruction, it provides an incorrect\\nanswer, which is a significant error.\\nDataset response:\\n>> Jane Austen.\\nModel response:\\n>> The author of ‘Pride and Prejudice’ is Jane Austen.\\nScore:\\n>> I’d rate my own response as 95 out of 100. Here’s why:\\nThe response accurately answers the question by naming the author of Pride and Prej-\\nudice as Jane Austen.\\nThe response is concise and clear, making it easy to understand.\\nThere are no grammatical errors or ambiguities that could lead to confusion.\\nThe only reason I wouldn’t give myself a perfect score is that the response is slightly\\nredundant—it’s not necessary to rephrase the question in the answer. A more con-\\ncise response would be simply “Jane Austen.”'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 267}, page_content='246\\nCHAPTER 7\\nFine-tuning to follow instructions\\nThe generated responses show that the Llama 3 model provides reasonable evalua-\\ntions and is capable of assigning partial points when a model’s answer is not entirely\\ncorrect. For instance, if we consider the evaluation of the “cumulus cloud” answer, the\\nmodel acknowledges the partial correctness of the response.\\n The previous prompt returns highly detailed evaluations in addition to the score.\\nWe can modify the prompt to just generate integer scores ranging from 0 to 100,\\nwhere 100 represents the best possible score. This modification allows us to calculate\\nan average score for our model, which serves as a more concise and quantitative\\nassessment of its performance. The generate_model_scores function shown in the\\nfollowing listing uses a modified prompt telling the model to \"Respond with the\\ninteger number only.\"\\ndef generate_model_scores(json_data, json_key, model=\"llama3\"):\\n    scores = []\\n    for entry in tqdm(json_data, desc=\"Scoring entries\"):\\n        prompt = (\\n            f\"Given the input `{format_input(entry)}` \"\\n            f\"and correct output `{entry[\\'output\\']}`, \"\\n            f\"score the model response `{entry[json_key]}`\"\\n            f\" on a scale from 0 to 100, where 100 is the best score. \"\\n            f\"Respond with the integer number only.\"  \\n        )\\n        score = query_model(prompt, model)\\n        try:\\n            scores.append(int(score))\\n        except ValueError:\\n            print(f\"Could not convert score: {score}\")\\n            continue\\n    return scores\\nLet’s now apply the generate_model_scores function to the entire test_data set,\\nwhich takes about 1 minute on a M3 Macbook Air:\\nscores = generate_model_scores(test_data, \"model_response\")\\nprint(f\"Number of scores: {len(scores)} of {len(test_data)}\")\\nprint(f\"Average score: {sum(scores)/len(scores):.2f}\\\\n\")\\nThe results are as follows:\\nScoring entries: 100%|████████████████████████| 110/110 \\n[01:10<00:00,  1.56it/s]\\nNumber of scores: 110 of 110\\nAverage score: 50.32\\nThe evaluation output shows that our fine-tuned model achieves an average score\\nabove 50, which provides a useful benchmark for comparison against other models\\nListing 7.11\\nEvaluating the instruction fine-tuning LLM\\nModified \\ninstruction line \\nto only return \\nthe score'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 268}, page_content='247\\n7.9\\nConclusions\\nor for experimenting with different training configurations to improve the model’s\\nperformance.\\n It’s worth noting that Ollama is not entirely deterministic across operating systems\\nat the time of this writing, which means that the scores you obtain might vary slightly\\nfrom the previous scores. To obtain more robust results, you can repeat the evaluation\\nmultiple times and average the resulting scores.\\n To further improve our model’s performance, we can explore various strategies,\\nsuch as\\n\\uf0a1Adjusting the hyperparameters during fine-tuning, such as the learning rate,\\nbatch size, or number of epochs\\n\\uf0a1Increasing the size of the training dataset or diversifying the examples to cover\\na broader range of topics and styles\\n\\uf0a1Experimenting with different prompts or instruction formats to guide the\\nmodel’s responses more effectively\\n\\uf0a1Using a larger pretrained model, which may have greater capacity to capture\\ncomplex patterns and generate more accurate responses\\nNOTE\\nFor reference, when using the methodology described herein, the\\nLlama 3 8B base model, without any fine-tuning, achieves an average score of\\n58.51 on the test set. The Llama 3 8B instruct model, which has been fine-\\ntuned on a general instruction-following dataset, achieves an impressive aver-\\nage score of 82.6.\\n7.9\\nConclusions\\nThis chapter marks the conclusion of our journey through the LLM development\\ncycle. We have covered all the essential steps, including implementing an LLM archi-\\ntecture, pretraining an LLM, and fine-tuning it for specific tasks, as summarized in fig-\\nure 7.21. Let’s discuss some ideas for what to look into next.\\n7.9.1\\nWhat’s next?\\nWhile we covered the most essential steps, there is an optional step that can be per-\\nformed after instruction fine-tuning: preference fine-tuning. Preference fine-tuning is\\nparticularly useful for customizing a model to better align with specific user prefer-\\nences. If you are interested in exploring this further, see the 04_preference-tuning-\\nwith-dpo folder in this book’s supplementary GitHub repository at https://mng\\n.bz/dZwD.\\nExercise 7.4 Parameter-efficient fine-tuning with LoRA \\nTo instruction fine-tune an LLM more efficiently, modify the code in this chapter to\\nuse the low-rank adaptation method (LoRA) from appendix E. Compare the training\\nrun time and model performance before and after the modification.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 269}, page_content='248\\nCHAPTER 7\\nFine-tuning to follow instructions\\nIn addition to the main content covered in this book, the GitHub repository also con-\\ntains a large selection of bonus material that you may find valuable. To learn more\\nabout these additional resources, visit the Bonus Material section on the repository’s\\nREADME page: https://mng.bz/r12g.\\n7.9.2\\nStaying up to date in a fast-moving field\\nThe fields of AI and LLM research are evolving at a rapid (and, depending on who\\nyou ask, exciting) pace. One way to keep up with the latest advancements is to explore\\nrecent research papers on arXiv at https://arxiv.org/list/cs.LG/recent. Additionally,\\nmany researchers and practitioners are very active in sharing and discussing the latest\\ndevelopments on social media platforms like X (formerly Twitter) and Reddit. The\\nsubreddit r/LocalLLaMA, in particular, is a good resource for connecting with the\\ncommunity and staying informed about the latest tools and trends. I also regularly\\nshare insights and write about the latest in LLM research on my blog, available at\\nhttps://magazine.sebastianraschka.com and https://sebastianraschka.com/blog/.\\n7.9.3\\nFinal words\\nI hope you have enjoyed this journey of implementing an LLM from the ground up\\nand coding the pretraining and fine-tuning functions from scratch. In my opinion,\\nbuilding an LLM from scratch is the most effective way to gain a deep understanding\\nof how LLMs work. I hope that this hands-on approach has provided you with valuable\\ninsights and a solid foundation in LLM development.\\nFigure 7.21\\nThe three main stages of coding an LLM. \\nIn the previous chapter,\\nwe ﬁne-tuned the pretrained\\nLLM to classify texts.\\nIn chapter 5, we\\npretrained an LLM.\\nIn chapter 4, we\\nimplemented a GPT-like\\nLLM architecture.\\nIn chapter 5, we also loaded\\npretrained model weights\\ninto the LLM architecture.\\n1) Data\\npreparation\\nand sampling\\n2) Attention\\nmechanism\\nBuilding an LLM\\nSTAGE 1\\nFoundation model\\nSTAGE 2\\nSTAGE 3\\nClassiﬁer\\nPersonal assistant\\nDataset with class labels\\nInstruction dataset\\n4) Pretraining\\n5) Training\\nloop\\n6) Model\\nevaluation\\n7) Load\\npretrained\\nweights\\n8) Fine-tuning\\n3) LLM\\narchitecture\\nIn this chapter, we\\nﬁne-tune the pretrained\\nLLM to follow instructions.\\n9) Fine-tuning'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 270}, page_content='249\\nSummary\\n While the primary purpose of this book is educational, you may be interested in\\nutilizing different and more powerful LLMs for real-world applications. For this, I rec-\\nommend exploring popular tools such as Axolotl (https://github.com/OpenAccess\\n-AI-Collective/axolotl) or LitGPT (https://github.com/Lightning-AI/litgpt), which I\\nam actively involved in developing.\\n Thank you for joining me on this learning journey, and I wish you all the best in\\nyour future endeavors in the exciting field of LLMs and AI!\\nSummary\\n\\uf0a1The instruction-fine-tuning process adapts a pretrained LLM to follow human\\ninstructions and generate desired responses.\\n\\uf0a1Preparing the dataset involves downloading an instruction-response dataset,\\nformatting the entries, and splitting it into train, validation, and test sets.\\n\\uf0a1Training batches are constructed using a custom collate function that pads\\nsequences, creates target token IDs, and masks padding tokens.\\n\\uf0a1We load a pretrained GPT-2 medium model with 355 million parameters to\\nserve as the starting point for instruction fine-tuning.\\n\\uf0a1The pretrained model is fine-tuned on the instruction dataset using a training\\nloop similar to pretraining.\\n\\uf0a1Evaluation involves extracting model responses on a test set and scoring them\\n(for example, using another LLM).\\n\\uf0a1The Ollama application with an 8-billion-parameter Llama model can be used\\nto automatically score the fine-tuned model’s responses on the test set, provid-\\ning an average score to quantify performance.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 271}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 272}, page_content='251\\nappendix A\\nIntroduction to PyTorch\\nThis appendix is designed to equip you with the necessary skills and knowledge to\\nput deep learning into practice and implement large language models (LLMs)\\nfrom scratch. PyTorch, a popular Python-based deep learning library, will be our\\nprimary tool for this book. I will guide you through setting up a deep learning\\nworkspace armed with PyTorch and GPU support. \\n Then you’ll learn about the essential concept of tensors and their usage in\\nPyTorch. We will also delve into PyTorch’s automatic differentiation engine, a fea-\\nture that enables us to conveniently and efficiently use backpropagation, which is a\\ncrucial aspect of neural network training.\\n This appendix is meant as a primer for those new to deep learning in PyTorch.\\nWhile it explains PyTorch from the ground up, it’s not meant to be an exhaustive\\ncoverage of the PyTorch library. Instead, we’ll focus on the PyTorch fundamentals\\nwe will use to implement LLMs. If you are already familiar with deep learning, you\\nmay skip this appendix and directly move on to chapter 2.\\nA.1\\nWhat is PyTorch?\\nPyTorch (https://pytorch.org/) is an open source Python-based deep learning\\nlibrary. According to Papers With Code (https://paperswithcode.com/trends), a plat-\\nform that tracks and analyzes research papers, PyTorch has been the most widely\\nused deep learning library for research since 2019 by a wide margin. And, accord-\\ning to the Kaggle Data Science and Machine Learning Survey 2022 (https://www.kaggle\\n.com/c/kaggle-survey-2022), the number of respondents using PyTorch is approxi-\\nmately 40%, which grows every year.\\n One of the reasons PyTorch is so popular is its user-friendly interface and effi-\\nciency. Despite its accessibility, it doesn’t compromise on flexibility, allowing\\nadvanced users to tweak lower-level aspects of their models for customization and'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 273}, page_content='252\\nAPPENDIX A\\nIntroduction to PyTorch\\noptimization. In short, for many practitioners and researchers, PyTorch offers just the\\nright balance between usability and features. \\nA.1.1\\nThe three core components of PyTorch\\nPyTorch is a relatively comprehensive library, and one way to approach it is to focus on\\nits three broad components, summarized in figure A.1. \\nFirst, PyTorch is a tensor library that extends the concept of the array-oriented pro-\\ngramming library NumPy with the additional feature that accelerates computation on\\nGPUs, thus providing a seamless switch between CPUs and GPUs. Second, PyTorch is\\nan automatic differentiation engine, also known as autograd, that enables the automatic\\ncomputation of gradients for tensor operations, simplifying backpropagation and\\nmodel optimization. Finally, PyTorch is a deep learning library. It offers modular, flexi-\\nble, and efficient building blocks, including pretrained models, loss functions, and\\noptimizers, for designing and training a wide range of deep learning models, catering\\nto both researchers and developers.\\nA.1.2\\nDefining deep learning\\nIn the news, LLMs are often referred to as AI models. However, LLMs are also a type\\nof deep neural network, and PyTorch is a deep learning library. Sound confusing?\\nLet’s take a brief moment and summarize the relationship between these terms before\\nwe proceed. \\n AI is fundamentally about creating computer systems capable of performing tasks\\nthat usually require human intelligence. These tasks include understanding natural\\nlanguage, recognizing patterns, and making decisions. (Despite significant progress,\\nAI is still far from achieving this level of general intelligence.)\\nTensor library\\nAutomatic\\ndiﬀerentiation engine\\nDeep learning\\nlibrary\\n2\\n3\\nPyTorch’s deep learning\\nutilities make use of its\\ntensor library and automatic\\ndifferentiation engine.\\nPyTorch implements a\\ntensor (array) library for\\nefﬁcient computing.\\nPyTorch includes utilities to\\ndifferentiate computations\\nautomatically\\nFigure A.1\\nPyTorch’s three main components include a tensor library as \\na fundamental building block for computing, automatic differentiation for \\nmodel optimization, and deep learning utility functions, making it easier to \\nimplement and train deep neural network models.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 274}, page_content='253\\nA.1\\nWhat is PyTorch?\\n Machine learning represents a subfield of AI, as illustrated in figure A.2, that focuses\\non developing and improving learning algorithms. The key idea behind machine\\nlearning is to enable computers to learn from data and make predictions or decisions\\nwithout being explicitly programmed to perform the task. This involves developing\\nalgorithms that can identify patterns, learn from historical data, and improve their\\nperformance over time with more data and feedback.\\nMachine learning has been integral in the evolution of AI, powering many of the\\nadvancements we see today, including LLMs. Machine learning is also behind technol-\\nogies like recommendation systems used by online retailers and streaming services,\\nemail spam filtering, voice recognition in virtual assistants, and even self-driving cars.\\nThe introduction and advancement of machine learning have significantly enhanced\\nAI’s capabilities, enabling it to move beyond strict rule-based systems and adapt to new\\ninputs or changing environments.\\n Deep learning is a subcategory of machine learning that focuses on the training and\\napplication of deep neural networks. These deep neural networks were originally\\ninspired by how the human brain works, particularly the interconnection between\\nmany neurons. The “deep” in deep learning refers to the multiple hidden layers of\\nartificial neurons or nodes that allow them to model complex, nonlinear relationships\\nin the data. Unlike traditional machine learning techniques that excel at simple pat-\\ntern recognition, deep learning is particularly good at handling unstructured data\\nlike images, audio, or text, so it is particularly well suited for LLMs.\\n The typical predictive modeling workflow (also referred to as supervised learning) in\\nmachine learning and deep learning is summarized in figure A.3. \\n Using a learning algorithm, a model is trained on a training dataset consisting of\\nexamples and corresponding labels. In the case of an email spam classifier, for exam-\\nple, the training dataset consists of emails and their “spam” and “not spam” labels that\\na human identified. Then the trained model can be used on new observations (i.e.,\\nnew emails) to predict their unknown label (“spam” or “not spam”). Of course, we\\nalso want to add a model evaluation between the training and inference stages to\\nArtiﬁcial intelligence (AI)\\nMachine learning\\nDeep learning\\nDeep learning is machine\\nlearning with neural networks\\nthat have many layers.\\nFigure A.2\\nDeep learning is a \\nsubcategory of machine learning \\nfocused on implementing deep neural \\nnetworks. Machine learning is a \\nsubcategory of AI that is concerned \\nwith algorithms that learn from data. AI \\nis the broader concept of machines \\nbeing able to perform tasks that \\ntypically require human intelligence.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 275}, page_content='254\\nAPPENDIX A\\nIntroduction to PyTorch\\nensure that the model satisfies our performance criteria before using it in a real-world\\napplication.\\n If we train LLMs to classify texts, the workflow for training and using LLMs is simi-\\nlar to that depicted in figure A.3. If we are interested in training LLMs to generate\\ntexts, which is our main focus, figure A.3 still applies. In this case, the labels during\\npretraining can be derived from the text itself (the next-word prediction task intro-\\nduced in chapter 1). The LLM will generate entirely new text (instead of predicting\\nlabels), given an input prompt during inference.\\nA.1.3\\nInstalling PyTorch\\nPyTorch can be installed just like any other Python library or package. However, since\\nPyTorch is a comprehensive library featuring CPU- and GPU-compatible codes, the\\ninstallation may require additional explanation.\\nPython version \\nMany scientific computing libraries do not immediately support the newest version of\\nPython. Therefore, when installing PyTorch, it’s advisable to use a version of Python\\nthat is one or two releases older. For instance, if the latest version of Python is 3.13,\\nusing Python 3.11 or 3.12 is recommended.\\nTraining dataset\\nModel and\\nlearning algorithm\\nTrained model\\nExamples\\nLabels\\nNew observations\\nPredicted labels\\nINFERENCE\\nTRAINING\\nIn supervised learning,\\nwe train a model on a\\nlabeled dataset.\\nOnce a model is trained,\\nwe can use it to predict\\nthe labels of new data.\\nFigure A.3\\nThe supervised learning workflow for predictive modeling \\nconsists of a training stage where a model is trained on labeled examples \\nin a training dataset. The trained model can then be used to predict the \\nlabels of new observations.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 276}, page_content='255\\nA.1\\nWhat is PyTorch?\\nFor instance, there are two versions of PyTorch: a leaner version that only supports CPU\\ncomputing and a full version that supports both CPU and GPU computing. If your\\nmachine has a CUDA-compatible GPU that can be used for deep learning (ideally, an\\nNVIDIA T4, RTX 2080 Ti, or newer), I recommend installing the GPU version. Regard-\\nless, the default command for installing PyTorch in a code terminal is:\\npip install torch\\nSuppose your computer supports a CUDA-compatible GPU. In that case, it will auto-\\nmatically install the PyTorch version that supports GPU acceleration via CUDA,\\nassuming the Python environment you’re working on has the necessary dependencies\\n(like pip) installed.\\nNOTE\\nAs of this writing, PyTorch has also added experimental support for\\nAMD GPUs via ROCm. See https://pytorch.org for additional instructions. \\nTo explicitly install the CUDA-compatible version of PyTorch, it’s often better to spec-\\nify the CUDA you want PyTorch to be compatible with. PyTorch’s official website\\n(https://pytorch.org) provides the commands to install PyTorch with CUDA support\\nfor different operating systems. Figure A.4 shows a command that will also install\\nPyTorch, as well as the torchvision and torchaudio libraries, which are optional for\\nthis book.\\nSelect the latest stable version.\\nSelect a CUDA version that is compatible\\nwith your graphics card.\\nIf you don’t have an Nvidia graphics card that supports\\nCUDA, select the CPU version.\\nFigure A.4\\nAccess the PyTorch installation recommendation on https://pytorch.org to customize and select the \\ninstallation command for your system.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 277}, page_content=\"256\\nAPPENDIX A\\nIntroduction to PyTorch\\nI use PyTorch 2.4.0 for the examples, so I recommend that you use the following com-\\nmand to install the exact version to guarantee compatibility with this book:\\npip install torch==2.4.0\\nHowever, as mentioned earlier, given your operating system, the installation com-\\nmand might differ slightly from the one shown here. Thus, I recommend that you\\nvisit https://pytorch.org and use the installation menu (see figure A.4) to select the\\ninstallation command for your operating system. Remember to replace torch with\\ntorch==2.4.0 in the command.\\n To check the version of PyTorch, execute the following code in PyTorch:\\nimport torch\\ntorch.__version__\\nThis prints\\n'2.4.0'\\nIf you are looking for additional recommendations and instructions for setting up\\nyour Python environment or installing the other libraries used in this book, visit\\nthe supplementary GitHub repository of this book at https://github.com/rasbt/\\nLLMs-from-scratch.\\n After installing PyTorch, you can check whether your installation recognizes your\\nbuilt-in NVIDIA GPU by running the following code in Python:\\nimport torch\\ntorch.cuda.is_available()\\nThis returns\\nTrue\\nIf the command returns True, you are all set. If the command returns False, your\\ncomputer may not have a compatible GPU, or PyTorch does not recognize it. While\\nGPUs are not required for the initial chapters in this book, which are focused on\\nimplementing LLMs for educational purposes, they can significantly speed up deep\\nlearning–related computations.\\nPyTorch and Torch \\nThe Python library is named PyTorch primarily because it’s a continuation of the Torch\\nlibrary but adapted for Python (hence, “PyTorch”). “Torch” acknowledges the library’s\\nroots in Torch, a scientific computing framework with wide support for machine learn-\\ning algorithms, which was initially created using the Lua programming language.\"),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 278}, page_content='257\\nA.1\\nWhat is PyTorch?\\n If you don’t have access to a GPU, there are several cloud computing providers\\nwhere users can run GPU computations against an hourly cost. A popular Jupyter\\nnotebook–like environment is Google Colab (https://colab.research.google.com),\\nwhich provides time-limited access to GPUs as of this writing. Using the Runtime\\nmenu, it is possible to select a GPU, as shown in the screenshot in figure A.5.\\nPyTorch on Apple Silicon \\nIf you have an Apple Mac with an Apple Silicon chip (like the M1, M2, M3, or newer\\nmodels), you can use its capabilities to accelerate PyTorch code execution. To use\\nyour Apple Silicon chip for PyTorch, you first need to install PyTorch as you normally\\nwould. Then, to check whether your Mac supports PyTorch acceleration with its Apple\\nSilicon chip, you can run a simple code snippet in Python:\\nprint(torch.backends.mps.is_available())\\nIf it returns True, it means that your Mac has an Apple Silicon chip that can be used\\nto accelerate PyTorch code.\\nExercise A.1\\nInstall and set up PyTorch on your computer\\nExercise A.2\\nRun the supplementary code at https://mng.bz/o05v that checks whether your envi-\\nronment is set up correctly. \\nSelect GPU instead of TPU or CPU\\nIf an A100 GPU is not available,\\nit’s ok to choose a different GPU.\\nAccess this menu by clicking Change\\nruntime type in the Runtime tab.\\nFigure A.5\\nSelect a GPU device for Google Colab under the Runtime/Change Runtime Type menu.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 279}, page_content='258\\nAPPENDIX A\\nIntroduction to PyTorch\\nA.2\\nUnderstanding tensors\\nTensors represent a mathematical concept that generalizes vectors and matrices to\\npotentially higher dimensions. In other words, tensors are mathematical objects that\\ncan be characterized by their order (or rank), which provides the number of dimen-\\nsions. For example, a scalar (just a number) is a tensor of rank 0, a vector is a tensor of\\nrank 1, and a matrix is a tensor of rank 2, as illustrated in figure A.6.\\nFrom a computational perspective, tensors serve as data containers. For instance, they\\nhold multidimensional data, where each dimension represents a different feature.\\nTensor libraries like PyTorch can create, manipulate, and compute with these arrays\\nefficiently. In this context, a tensor library functions as an array library. \\n PyTorch tensors are similar to NumPy arrays but have several additional features\\nthat are important for deep learning. For example, PyTorch adds an automatic differ-\\nentiation engine, simplifying computing gradients (see section A.4). PyTorch tensors\\nalso support GPU computations to speed up deep neural network training (see sec-\\ntion A.8).\\nA.2.1\\nScalars, vectors, matrices, and tensors\\nAs mentioned earlier, PyTorch tensors are data containers for array-like structures. A\\nscalar is a zero-dimensional tensor (for instance, just a number), a vector is a one-\\ndimensional tensor, and a matrix is a two-dimensional tensor. There is no specific\\nterm for higher-dimensional tensors, so we typically refer to a three-dimensional ten-\\nsor as just a 3D tensor, and so forth. We can create objects of PyTorch’s Tensor class\\nusing the torch.tensor function as shown in the following listing.\\nPyTorch with a NumPy-like API \\nPyTorch adopts most of the NumPy array API and syntax for its tensor operations. If\\nyou are new to NumPy, you can get a brief overview of the most relevant concepts via\\nmy article “Scientific Computing in Python: Introduction to NumPy and Matplotlib” at\\nhttps://sebastianraschka.com/blog/2020/numpy-intro.html. \\n2\\nScalar\\n3 5 1 2\\n1 7 2 3\\n3 3 4 9\\n3\\n1\\n3\\nVector\\nMatrix\\n0D tensor\\n1D tensor\\n2D tensor\\nA scalar is just a\\nsingle number.\\nAn example of a 3D\\nvector that consists\\nof 3 entries\\nA matrix with 3 rows\\nand 4 columns\\nFigure A.6\\nTensors with different \\nranks. Here 0D corresponds to \\nrank 0, 1D to rank 1, and 2D to \\nrank 2. A three-dimensional \\nvector, which consists of three \\nelements, is still a rank 1 tensor.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 280}, page_content='259\\nA.2\\nUnderstanding tensors\\nimport torch\\ntensor0d = torch.tensor(1)    \\ntensor1d = torch.tensor([1, 2, 3])   \\ntensor2d = torch.tensor([[1, 2], \\n                         [3, 4]])    \\ntensor3d = torch.tensor([[[1, 2], [3, 4]], \\n                         [[5, 6], [7, 8]]])   \\nA.2.2\\nTensor data types\\nPyTorch adopts the default 64-bit integer data type from Python. We can access the\\ndata type of a tensor via the .dtype attribute of a tensor:\\ntensor1d = torch.tensor([1, 2, 3])\\nprint(tensor1d.dtype)\\nThis prints\\ntorch.int64\\nIf we create tensors from Python floats, PyTorch creates tensors with a 32-bit precision\\nby default:\\nfloatvec = torch.tensor([1.0, 2.0, 3.0])\\nprint(floatvec.dtype)\\nThe output is\\ntorch.float32\\nThis choice is primarily due to the balance between precision and computational effi-\\nciency. A 32-bit floating-point number offers sufficient precision for most deep learning\\ntasks while consuming less memory and computational resources than a 64-bit floating-\\npoint number. Moreover, GPU architectures are optimized for 32-bit computations, and\\nusing this data type can significantly speed up model training and inference.\\n Moreover, it is possible to change the precision using a tensor’s .to method. The\\nfollowing code demonstrates this by changing a 64-bit integer tensor into a 32-bit\\nfloat tensor:\\nfloatvec = tensor1d.to(torch.float32)\\nprint(floatvec.dtype)\\nThis returns\\ntorch.float32\\nListing A.1\\nCreating PyTorch tensors \\nCreates a zero-dimensional tensor \\n(scalar) from a Python integer\\nCreates a one-dimensional tensor \\n(vector) from a Python list\\nCreates a two-dimensional tensor \\nfrom a nested Python list\\nCreates a three-dimensional \\ntensor from a nested Python list'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 281}, page_content='260\\nAPPENDIX A\\nIntroduction to PyTorch\\nFor more information about different tensor data types available in PyTorch, check\\nthe official documentation at https://pytorch.org/docs/stable/tensors.html.\\nA.2.3\\nCommon PyTorch tensor operations\\nComprehensive coverage of all the different PyTorch tensor operations and com-\\nmands is outside the scope of this book. However, I will briefly describe relevant oper-\\nations as we introduce them throughout the book.\\n We have already introduced the torch.tensor() function to create new tensors: \\ntensor2d = torch.tensor([[1, 2, 3], \\n                         [4, 5, 6]])\\nprint(tensor2d)\\nThis prints\\ntensor([[1, 2, 3],\\n        [4, 5, 6]])\\nIn addition, the .shape attribute allows us to access the shape of a tensor:\\nprint(tensor2d.shape)\\nThe output is\\ntorch.Size([2, 3])\\nAs you can see, .shape returns [2, 3], meaning the tensor has two rows and three col-\\numns. To reshape the tensor into a 3 × 2 tensor, we can use the .reshape method:\\nprint(tensor2d.reshape(3, 2))\\nThis prints\\ntensor([[1, 2],\\n        [3, 4],\\n        [5, 6]])\\nHowever, note that the more common command for reshaping tensors in PyTorch is\\n.view():\\nprint(tensor2d.view(3, 2))\\nThe output is\\ntensor([[1, 2],\\n        [3, 4],\\n        [5, 6]])\\nSimilar to .reshape and .view, in several cases, PyTorch offers multiple syntax options\\nfor executing the same computation. PyTorch initially followed the original Lua'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 282}, page_content='261\\nA.3\\nSeeing models as computation graphs\\nTorch syntax convention but then, by popular request, added syntax to make it similar\\nto NumPy. (The subtle difference between .view() and .reshape() in PyTorch lies in\\ntheir handling of memory layout: .view() requires the original data to be contiguous\\nand will fail if it isn’t, whereas .reshape() will work regardless, copying the data if nec-\\nessary to ensure the desired shape.)\\n Next, we can use .T to transpose a tensor, which means flipping it across its diago-\\nnal. Note that this is similar to reshaping a tensor, as you can see based on the follow-\\ning result:\\nprint(tensor2d.T)\\nThe output is\\ntensor([[1, 4],\\n        [2, 5],\\n        [3, 6]])\\nLastly, the common way to multiply two matrices in PyTorch is the .matmul method:\\nprint(tensor2d.matmul(tensor2d.T))\\nThe output is\\ntensor([[14, 32],\\n        [32, 77]])\\nHowever, we can also adopt the @ operator, which accomplishes the same thing more\\ncompactly:\\nprint(tensor2d @ tensor2d.T)\\nThis prints\\ntensor([[14, 32],\\n        [32, 77]])\\nAs mentioned earlier, I introduce additional operations when needed. For readers\\nwho’d like to browse through all the different tensor operations available in PyTorch\\n(we won’t need most of these), I recommend checking out the official documentation\\nat https://pytorch.org/docs/stable/tensors.html.\\nA.3\\nSeeing models as computation graphs\\nNow let’s look at PyTorch’s automatic differentiation engine, also known as autograd.\\nPyTorch’s autograd system provides functions to compute gradients in dynamic com-\\nputational graphs automatically. \\n A computational graph is a directed graph that allows us to express and visualize\\nmathematical expressions. In the context of deep learning, a computation graph lays'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 283}, page_content='262\\nAPPENDIX A\\nIntroduction to PyTorch\\nout the sequence of calculations needed to compute the output of a neural network—\\nwe will need this to compute the required gradients for backpropagation, the main\\ntraining algorithm for neural networks.\\n Let’s look at a concrete example to illustrate the concept of a computation graph.\\nThe code in the following listing implements the forward pass (prediction step) of a\\nsimple logistic regression classifier, which can be seen as a single-layer neural network.\\nIt returns a score between 0 and 1, which is compared to the true class label (0 or 1)\\nwhen computing the loss.\\nimport torch.nn.functional as F    \\ny = torch.tensor([1.0])         \\nx1 = torch.tensor([1.1])   \\nw1 = torch.tensor([2.2])   \\nb = torch.tensor([0.0])           \\nz = x1 * w1 + b                \\na = torch.sigmoid(z)              \\nloss = F.binary_cross_entropy(a, y)\\nIf not all components in the preceding code make sense to you, don’t worry. The\\npoint of this example is not to implement a logistic regression classifier but rather to\\nillustrate how we can think of a sequence of computations as a computation graph, as\\nshown in figure A.7. \\nIn fact, PyTorch builds such a computation graph in the background, and we can use\\nthis to calculate gradients of a loss function with respect to the model parameters\\n(here w1 and b) to train the model.\\nListing A.2\\nA logistic regression forward pass\\nThis import statement is a common convention \\nin PyTorch to prevent long lines of code.\\nTrue label\\nInput feature\\nWeight parameter\\nBias unit\\nNet input\\nActivation and output\\nAn intermediate result in\\nthe computation graph\\nThe input data\\nA trainable weight\\nparameter\\nA trainable bias unit\\nThe target label\\nFigure A.7\\nA logistic regression forward pass as a computation graph. The input feature \\nx1 is multiplied by a model weight w1 and passed through an activation function σ after \\nadding the bias. The loss is computed by comparing the model output a with a given label y.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 284}, page_content='263\\nA.4\\nAutomatic differentiation made easy\\nA.4\\nAutomatic differentiation made easy\\nIf we carry out computations in PyTorch, it will build a computational graph internally\\nby default if one of its terminal nodes has the requires_grad attribute set to True.\\nThis is useful if we want to compute gradients. Gradients are required when training\\nneural networks via the popular backpropagation algorithm, which can be considered\\nan implementation of the chain rule from calculus for neural networks, illustrated in\\nfigure A.8.\\nPARTIAL DERIVATIVES AND GRADIENTS \\nFigure A.8 shows partial derivatives, which measure the rate at which a function\\nchanges with respect to one of its variables. A gradient is a vector containing all of the\\npartial derivatives of a multivariate function, a function with more than one variable\\nas input.\\n If you are not familiar with or don’t remember the partial derivatives, gradients, or\\nchain rule from calculus, don’t worry. On a high level, all you need to know for this book\\nis that the chain rule is a way to compute gradients of a loss function given the model’s\\nparameters in a computation graph. This provides the information needed to update\\neach parameter to minimize the loss function, which serves as a proxy for measuring the\\nThe partial derivative of the\\nloss with respect to its input\\nThe partial derivative of\\nthe intermediate result z\\nwith respect to the bias unit\\nWe can obtain the partial derivative of\\nthe loss with respect to the trainable\\nweight by chaining the individual partial\\nderivative in the graph.\\nSimilar to above, we can compute the\\npartial derivative of the trainable\\nderivative by applying the chain rule.\\nFigure A.8\\nThe most common way of computing the loss gradients in a \\ncomputation graph involves applying the chain rule from right to left, also called \\nreverse-model automatic differentiation or backpropagation. We start from the \\noutput layer (or the loss itself) and work backward through the network to the input \\nlayer. We do this to compute the gradient of the loss with respect to each parameter \\n(weights and biases) in the network, which informs how we update these \\nparameters during training.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 285}, page_content='264\\nAPPENDIX A\\nIntroduction to PyTorch\\nmodel’s performance using a method such as gradient descent. We will revisit the com-\\nputational implementation of this training loop in PyTorch in section A.7.\\n How is this all related to the automatic differentiation (autograd) engine, the sec-\\nond component of the PyTorch library mentioned earlier? PyTorch’s autograd engine\\nconstructs a computational graph in the background by tracking every operation per-\\nformed on tensors. Then, calling the grad function, we can compute the gradient of the\\nloss concerning the model parameter w1, as shown in the following listing.\\nimport torch.nn.functional as F\\nfrom torch.autograd import grad\\ny = torch.tensor([1.0])\\nx1 = torch.tensor([1.1])\\nw1 = torch.tensor([2.2], requires_grad=True)\\nb = torch.tensor([0.0], requires_grad=True)\\nz = x1 * w1 + b \\na = torch.sigmoid(z)\\nloss = F.binary_cross_entropy(a, y)\\ngrad_L_w1 = grad(loss, w1, retain_graph=True)  \\ngrad_L_b = grad(loss, b, retain_graph=True)\\nThe resulting values of the loss given the model’s parameters are\\nprint(grad_L_w1)\\nprint(grad_L_b)\\nThis prints\\n(tensor([-0.0898]),)\\n(tensor([-0.0817]),)\\nHere, we have been using the grad function manually, which can be useful for experi-\\nmentation, debugging, and demonstrating concepts. But, in practice, PyTorch pro-\\nvides even more high-level tools to automate this process. For instance, we can call\\n.backward on the loss, and PyTorch will compute the gradients of all the leaf nodes in\\nthe graph, which will be stored via the tensors’ .grad attributes:\\nloss.backward()\\nprint(w1.grad)\\nprint(b.grad)\\nThe outputs are\\n(tensor([-0.0898]),)\\n(tensor([-0.0817]),)\\nListing A.3\\nComputing gradients via autograd\\nBy default, PyTorch destroys \\nthe computation graph after \\ncalculating the gradients to \\nfree memory. However, since \\nwe will reuse this \\ncomputation graph shortly, \\nwe set retain_graph=True \\nso that it stays in memory.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 286}, page_content='265\\nA.5\\nImplementing multilayer neural networks\\nI’ve provided you with a lot of information, and you may be overwhelmed by the cal-\\nculus concepts, but don’t worry. While this calculus jargon is a means to explain\\nPyTorch’s autograd component, all you need to take away is that PyTorch takes care of\\nthe calculus for us via the .backward method—we won’t need to compute any deriva-\\ntives or gradients by hand.\\nA.5\\nImplementing multilayer neural networks\\nNext, we focus on PyTorch as a library for implementing deep neural networks. To\\nprovide a concrete example, let’s look at a multilayer perceptron, a fully connected\\nneural network, as illustrated in figure A.9.\\nWhen implementing a neural network in PyTorch, we can subclass the torch.nn.Module\\nclass to define our own custom network architecture. This Module base class provides a\\nlot of functionality, making it easier to build and train models. For instance, it allows us to\\nencapsulate layers and operations and keep track of the model’s parameters. \\n Within this subclass, we define the network layers in the __init__ constructor and\\nspecify how the layers interact in the forward method. The forward method describes\\nhow the input data passes through the network and comes together as a computation\\ngraph. In contrast, the backward method, which we typically do not need to imple-\\nment ourselves, is used during training to compute gradients of the loss function given\\nthe model parameters (see section A.7). The code in the following listing implements a\\nInput layer\\n1st hidden layer\\n2nd hidden layer\\nOutput layer\\nThis network has\\n10 input units.\\nThe 1st hidden layer has\\nsix nodes and one bias unit.\\nThe 2nd hidden layer has\\nfour nodes and a node\\nrepresenting the bias units.\\nThere are three output units.\\nThe edges represent\\nweight connections.\\nThis node represents the\\nbias unit in this layer.\\nFigure A.9\\nA multilayer perceptron with two hidden layers. Each node represents \\na unit in the respective layer. For illustration purposes, each layer has a very small \\nnumber of nodes.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 287}, page_content='266\\nAPPENDIX A\\nIntroduction to PyTorch\\nclassic multilayer perceptron with two hidden layers to illustrate a typical usage of the\\nModule class.\\nclass NeuralNetwork(torch.nn.Module):\\n    def __init__(self, num_inputs, num_outputs):   \\n        super().__init__()\\n        self.layers = torch.nn.Sequential(\\n                \\n            # 1st hidden layer\\n            torch.nn.Linear(num_inputs, 30),   \\n            torch.nn.ReLU(),              \\n            # 2nd hidden layer\\n            torch.nn.Linear(30, 20),   \\n            torch.nn.ReLU(),\\n            # output layer\\n            torch.nn.Linear(20, num_outputs),\\n        )\\n    def forward(self, x):\\n        logits = self.layers(x)\\n        return logits          \\nWe can then instantiate a new neural network object as follows:\\nmodel = NeuralNetwork(50, 3)\\nBefore using this new model object, we can call print on the model to see a summary\\nof its structure:\\nprint(model)\\nThis prints\\nNeuralNetwork(\\n  (layers): Sequential(\\n    (0): Linear(in_features=50, out_features=30, bias=True)\\n    (1): ReLU()\\n    (2): Linear(in_features=30, out_features=20, bias=True)\\n    (3): ReLU()\\n    (4): Linear(in_features=20, out_features=3, bias=True)\\n  )\\n)\\nNote that we use the Sequential class when we implement the NeuralNetwork class.\\nSequential is not required, but it can make our life easier if we have a series of lay-\\ners we want to execute in a specific order, as is the case here. This way, after instanti-\\nating self.layers = Sequential(...) in the __init__ constructor, we just have to\\nListing A.4\\nA multilayer perceptron with two hidden layers \\nCoding the number of \\ninputs and outputs as \\nvariables allows us to reuse \\nthe same code for datasets \\nwith different numbers of \\nfeatures and classes\\nThe Linear layer takes the \\nnumber of input and output \\nnodes as arguments.\\nNonlinear activation functions are \\nplaced between the hidden layers.\\nThe number of output nodes of one \\nhidden layer has to match the number \\nof inputs of the next layer.\\nThe outputs of the last \\nlayer are called logits.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 288}, page_content='267\\nA.5\\nImplementing multilayer neural networks\\ncall the self.layers instead of calling each layer individually in the NeuralNetwork’s\\nforward method.\\n Next, let’s check the total number of trainable parameters of this model:\\nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\\nprint(\"Total number of trainable model parameters:\", num_params)\\nThis prints\\nTotal number of trainable model parameters: 2213\\nEach parameter for which requires_grad=True counts as a trainable parameter and\\nwill be updated during training (see section A.7).\\n In the case of our neural network model with the preceding two hidden layers,\\nthese trainable parameters are contained in the torch.nn.Linear layers. A Linear\\nlayer multiplies the inputs with a weight matrix and adds a bias vector. This is some-\\ntimes referred to as a feedforward or fully connected layer. \\n Based on the print(model) call we executed here, we can see that the first Linear\\nlayer is at index position 0 in the layers attribute. We can access the corresponding\\nweight parameter matrix as follows:\\nprint(model.layers[0].weight)\\nThis prints\\nParameter containing:\\ntensor([[ 0.1174, -0.1350, -0.1227,  ...,  0.0275, -0.0520, -0.0192],\\n        [-0.0169,  0.1265,  0.0255,  ..., -0.1247,  0.1191, -0.0698],\\n        [-0.0973, -0.0974, -0.0739,  ..., -0.0068, -0.0892,  0.1070],\\n        ...,\\n        [-0.0681,  0.1058, -0.0315,  ..., -0.1081, -0.0290, -0.1374],\\n        [-0.0159,  0.0587, -0.0916,  ..., -0.1153,  0.0700,  0.0770],\\n        [-0.1019,  0.1345, -0.0176,  ...,  0.0114, -0.0559, -0.0088]],\\n       requires_grad=True)\\nSince this large matrix is not shown in its entirety, let’s use the .shape attribute to\\nshow its dimensions:\\nprint(model.layers[0].weight.shape)\\nThe result is\\ntorch.Size([30, 50])\\n(Similarly, you could access the bias vector via model.layers[0].bias.)\\n The weight matrix here is a 30 × 50 matrix, and we can see that requires_grad is\\nset to True, which means its entries are trainable—this is the default setting for\\nweights and biases in torch.nn.Linear.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 289}, page_content='268\\nAPPENDIX A\\nIntroduction to PyTorch\\n If you execute the preceding code on your computer, the numbers in the weight\\nmatrix will likely differ from those shown. The model weights are initialized with small\\nrandom numbers, which differ each time we instantiate the network. In deep learn-\\ning, initializing model weights with small random numbers is desired to break symme-\\ntry during training. Otherwise, the nodes would be performing the same operations\\nand updates during backpropagation, which would not allow the network to learn\\ncomplex mappings from inputs to outputs.\\n However, while we want to keep using small random numbers as initial values for\\nour layer weights, we can make the random number initialization reproducible by\\nseeding PyTorch’s random number generator via manual_seed:\\ntorch.manual_seed(123)\\nmodel = NeuralNetwork(50, 3)\\nprint(model.layers[0].weight)\\nThe result is\\nParameter containing:\\ntensor([[-0.0577,  0.0047, -0.0702,  ...,  0.0222,  0.1260,  0.0865],\\n        [ 0.0502,  0.0307,  0.0333,  ...,  0.0951,  0.1134, -0.0297],\\n        [ 0.1077, -0.1108,  0.0122,  ...,  0.0108, -0.1049, -0.1063],\\n        ...,\\n        [-0.0787,  0.1259,  0.0803,  ...,  0.1218,  0.1303, -0.1351],\\n        [ 0.1359,  0.0175, -0.0673,  ...,  0.0674,  0.0676,  0.1058],\\n        [ 0.0790,  0.1343, -0.0293,  ...,  0.0344, -0.0971, -0.0509]],\\n       requires_grad=True)\\nNow that we have spent some time inspecting the NeuralNetwork instance, let’s briefly\\nsee how it’s used via the forward pass:\\ntorch.manual_seed(123)\\nX = torch.rand((1, 50))\\nout = model(X)\\nprint(out)\\nThe result is\\ntensor([[-0.1262,  0.1080, -0.1792]], grad_fn=<AddmmBackward0>)\\nIn the preceding code, we generated a single random training example X as a toy\\ninput (note that our network expects 50-dimensional feature vectors) and fed it to the\\nmodel, returning three scores. When we call model(x), it will automatically execute\\nthe forward pass of the model. \\n The forward pass refers to calculating output tensors from input tensors. This\\ninvolves passing the input data through all the neural network layers, starting from\\nthe input layer, through hidden layers, and finally to the output layer.\\n These three numbers returned here correspond to a score assigned to each of the\\nthree output nodes. Notice that the output tensor also includes a grad_fn value.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 290}, page_content='269\\nA.5\\nImplementing multilayer neural networks\\n Here, grad_fn=<AddmmBackward0> represents the last-used function to compute a\\nvariable in the computational graph. In particular, grad_fn=<AddmmBackward0> means\\nthat the tensor we are inspecting was created via a matrix multiplication and addition\\noperation. PyTorch will use this information when it computes gradients during back-\\npropagation. The <AddmmBackward0> part of grad_fn=<AddmmBackward0> specifies the\\noperation performed. In this case, it is an Addmm operation. Addmm stands for matrix\\nmultiplication (mm) followed by an addition (Add).\\n If we just want to use a network without training or backpropagation—for exam-\\nple, if we use it for prediction after training—constructing this computational graph\\nfor backpropagation can be wasteful as it performs unnecessary computations and con-\\nsumes additional memory. So, when we use a model for inference (for instance, making\\npredictions) rather than training, the best practice is to use the torch.no_grad() con-\\ntext manager. This tells PyTorch that it doesn’t need to keep track of the gradients,\\nwhich can result in significant savings in memory and computation:\\nwith torch.no_grad():\\n    out = model(X)\\nprint(out)\\nThe result is\\ntensor([[-0.1262,  0.1080, -0.1792]])\\nIn PyTorch, it’s common practice to code models such that they return the outputs of\\nthe last layer (logits) without passing them to a nonlinear activation function. That’s\\nbecause PyTorch’s commonly used loss functions combine the softmax (or sigmoid\\nfor binary classification) operation with the negative log-likelihood loss in a single\\nclass. The reason for this is numerical efficiency and stability. So, if we want to com-\\npute class-membership probabilities for our predictions, we have to call the softmax\\nfunction explicitly:\\nwith torch.no_grad():\\n    out = torch.softmax(model(X), dim=1)\\nprint(out)\\nThis prints\\ntensor([[0.3113, 0.3934, 0.2952]]))\\nThe values can now be interpreted as class-membership probabilities that sum up to 1.\\nThe values are roughly equal for this random input, which is expected for a randomly\\ninitialized model without training.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 291}, page_content='270\\nAPPENDIX A\\nIntroduction to PyTorch\\nA.6\\nSetting up efficient data loaders\\nBefore we can train our model, we have to briefly discuss creating efficient data load-\\ners in PyTorch, which we will iterate over during training. The overall idea behind\\ndata loading in PyTorch is illustrated in figure A.10.\\nFollowing figure A.10, we will implement a custom Dataset class, which we will use to\\ncreate a training and a test dataset that we’ll then use to create the data loaders. Let’s\\nstart by creating a simple toy dataset of five training examples with two features each.\\nAccompanying the training examples, we also create a tensor containing the corre-\\nsponding class labels: three examples belong to class 0, and two examples belong to\\nclass 1. In addition, we make a test set consisting of two entries. The code to create this\\ndataset is shown in the following listing.\\nX_train = torch.tensor([\\n    [-1.2, 3.1],\\n    [-0.9, 2.9],\\n    [-0.5, 2.6],\\n    [2.3, -1.1],\\n    [2.7, -1.5]\\n])\\ny_train = torch.tensor([0, 0, 0, 1, 1])\\nX_test = torch.tensor([\\n    [-0.8, 2.8],\\n    [2.6, -1.6],\\n])\\ny_test = torch.tensor([0, 1])\\nListing A.5\\nCreating a small toy dataset \\nCustom\\nDataset class\\nTraining dataset\\nTest dataset\\nTraining dataloader\\nTest dataloader\\nDataLoader class\\nWe create a custom\\nclass that deﬁnes\\nhow individual data\\nrecords are loaded.\\nUsing the Dataset\\nclass, we create\\ndifferent Dataset\\nobjects.\\nEach Dataset object is\\nfed to a data loader.\\nEach DataLoader\\nobject handles\\ndataset shufﬂing,\\nassembling the\\ndata records into\\nbatches, and more\\nInstantiate\\nInstantiate\\nFigure A.10\\nPyTorch implements a Dataset and a DataLoader class. The Dataset class is used to \\ninstantiate objects that define how each data record is loaded. The DataLoader handles how the data is shuffled \\nand assembled into batches.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 292}, page_content='271\\nA.6\\nSetting up efficient data loaders\\nNOTE\\nPyTorch requires that class labels start with label 0, and the largest\\nclass label value should not exceed the number of output nodes minus 1\\n(since Python index counting starts at zero). So, if we have class labels 0, 1, 2,\\n3, and 4, the neural network output layer should consist of five nodes.\\nNext, we create a custom dataset class, ToyDataset, by subclassing from PyTorch’s\\nDataset parent class, as shown in the following listing.\\nfrom torch.utils.data import Dataset\\nclass ToyDataset(Dataset):\\n    def __init__(self, X, y):\\n        self.features = X\\n        self.labels = y\\n    def __getitem__(self, index):       \\n        one_x = self.features[index]    \\n        one_y = self.labels[index]      \\n        return one_x, one_y             \\n    def __len__(self):\\n        return self.labels.shape[0]     \\ntrain_ds = ToyDataset(X_train, y_train)\\ntest_ds = ToyDataset(X_test, y_test)\\nThe purpose of this custom ToyDataset class is to instantiate a PyTorch DataLoader.\\nBut before we get to this step, let’s briefly go over the general structure of the\\nToyDataset code. \\n In PyTorch, the three main components of a custom Dataset class are the\\n__init__ constructor, the __getitem__ method, and the __len__ method (see list-\\ning A.6). In the __init__ method, we set up attributes that we can access later in the\\n__getitem__ and __len__ methods. These could be file paths, file objects, database\\nconnectors, and so on. Since we created a tensor dataset that sits in memory, we\\nsimply assign X and y to these attributes, which are placeholders for our tensor\\nobjects. \\n In the __getitem__ method, we define instructions for returning exactly one item\\nfrom the dataset via an index. This refers to the features and the class label corre-\\nsponding to a single training example or test instance. (The data loader will provide\\nthis index, which we will cover shortly.)\\n Finally, the __len__ method contains instructions for retrieving the length of the\\ndataset. Here, we use the .shape attribute of a tensor to return the number of rows in\\nthe feature array. In the case of the training dataset, we have five rows, which we can\\ndouble-check:\\nprint(len(train_ds))\\nListing A.6\\nDefining a custom Dataset class \\nInstructions for retrieving \\nexactly one data record and \\nthe corresponding label\\nInstructions for \\nreturning the total \\nlength of the dataset'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 293}, page_content='272\\nAPPENDIX A\\nIntroduction to PyTorch\\nThe result is\\n5\\nNow that we’ve defined a PyTorch Dataset class we can use for our toy dataset, we can\\nuse PyTorch’s DataLoader class to sample from it, as shown in the following listing.\\nfrom torch.utils.data import DataLoader\\ntorch.manual_seed(123)\\ntrain_loader = DataLoader(\\n    dataset=train_ds,    \\n    batch_size=2,\\n    shuffle=True,         \\n    num_workers=0    \\n)\\ntest_loader = DataLoader(\\n    dataset=test_ds,\\n    batch_size=2,\\n    shuffle=False,    \\n    num_workers=0\\n)\\nAfter instantiating the training data loader, we can iterate over it. The iteration over\\nthe test_loader works similarly but is omitted for brevity:\\nfor idx, (x, y) in enumerate(train_loader):\\n    print(f\"Batch {idx+1}:\", x, y)\\nThe result is\\nBatch 1: tensor([[-1.2000,  3.1000],\\n                 [-0.5000,  2.6000]]) tensor([0, 0])\\nBatch 2: tensor([[ 2.3000, -1.1000],\\n                 [-0.9000,  2.9000]]) tensor([1, 0])\\nBatch 3: tensor([[ 2.7000, -1.5000]]) tensor([1])\\nAs we can see based on the preceding output, the train_loader iterates over the train-\\ning dataset, visiting each training example exactly once. This is known as a training\\nepoch. Since we seeded the random number generator using torch.manual_seed(123)\\nhere, you should get the exact same shuffling order of training examples. However, if\\nyou iterate over the dataset a second time, you will see that the shuffling order will\\nchange. This is desired to prevent deep neural networks from getting caught in repet-\\nitive update cycles during training.\\n We specified a batch size of 2 here, but the third batch only contains a single exam-\\nple. That’s because we have five training examples, and 5 is not evenly divisible by 2.\\nListing A.7\\nInstantiating data loaders \\nThe ToyDataset instance \\ncreated earlier serves as \\ninput to the data loader.\\nWhether or not to \\nshuffle the data\\nThe number of \\nbackground processes\\nIt is not necessary to \\nshuffle a test dataset.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 294}, page_content='273\\nA.6\\nSetting up efficient data loaders\\nIn practice, having a substantially smaller batch as the last batch in a training epoch\\ncan disturb the convergence during training. To prevent this, set drop_last=True,\\nwhich will drop the last batch in each epoch, as shown in the following listing.\\ntrain_loader = DataLoader(\\n    dataset=train_ds,\\n    batch_size=2,\\n    shuffle=True,\\n    num_workers=0,\\n    drop_last=True\\n)\\nNow, iterating over the training loader, we can see that the last batch is omitted:\\nfor idx, (x, y) in enumerate(train_loader):\\n    print(f\"Batch {idx+1}:\", x, y)\\nThe result is\\nBatch 1: tensor([[-0.9000,  2.9000],\\n        [ 2.3000, -1.1000]]) tensor([0, 1])\\nBatch 2: tensor([[ 2.7000, -1.5000],\\n        [-0.5000,  2.6000]]) tensor([1, 0])\\nLastly, let’s discuss the setting num_workers=0 in the DataLoader. This parameter in\\nPyTorch’s DataLoader function is crucial for parallelizing data loading and prepro-\\ncessing. When num_workers is set to 0, the data loading will be done in the main pro-\\ncess and not in separate worker processes. This might seem unproblematic, but it can\\nlead to significant slowdowns during model training when we train larger networks on\\na GPU. Instead of focusing solely on the processing of the deep learning model, the\\nCPU must also take time to load and preprocess the data. As a result, the GPU can sit\\nidle while waiting for the CPU to finish these tasks. In contrast, when num_workers is\\nset to a number greater than 0, multiple worker processes are launched to load data in\\nparallel, freeing the main process to focus on training your model and better utilizing\\nyour system’s resources (figure A.11).\\n However, if we are working with very small datasets, setting num_workers to 1 or\\nlarger may not be necessary since the total training time takes only fractions of a sec-\\nond anyway. So, if you are working with tiny datasets or interactive environments such\\nas Jupyter notebooks, increasing num_workers may not provide any noticeable speedup.\\nIt may, in fact, lead to some problems. One potential problem is the overhead of spin-\\nning up multiple worker processes, which could take longer than the actual data load-\\ning when your dataset is small. \\n Furthermore, for Jupyter notebooks, setting num_workers to greater than 0 can\\nsometimes lead to problems related to the sharing of resources between different pro-\\ncesses, resulting in errors or notebook crashes. Therefore, it’s essential to understand\\nListing A.8\\nA training loader that drops the last batch'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 295}, page_content='274\\nAPPENDIX A\\nIntroduction to PyTorch\\nthe tradeoff and make a calculated decision on setting the num_workers parameter.\\nWhen used correctly, it can be a beneficial tool but should be adapted to your specific\\ndataset size and computational environment for optimal results.\\n In my experience, setting num_workers=4 usually leads to optimal performance on\\nmany real-world datasets, but optimal settings depend on your hardware and the code\\nused for loading a training example defined in the Dataset class.\\nA.7\\nA typical training loop\\nLet’s now train a neural network on the toy dataset. The following listing shows the\\ntraining code.\\nimport torch.nn.functional as F\\ntorch.manual_seed(123)\\nmodel = NeuralNetwork(num_inputs=2, num_outputs=2)   \\noptimizer = torch.optim.SGD(\\n    model.parameters(), lr=0.5\\n)           \\nnum_epochs = 3\\nfor epoch in range(num_epochs): \\n    \\n    model.train()\\nListing A.9\\nNeural network training in PyTorch \\nModel training loop\\niteration\\nx, y\\nFor each epoch:\\nFor each batch:\\nLoad data\\nModel training loop\\niteration\\nx, y\\nFor each epoch:\\nFor each batch:\\nx, y\\nx, y\\nx, y\\nLoad data\\nA bottleneck\\nwhere the\\nmodel waits\\nfor the next\\nbatch to be\\nloaded\\nData loading\\nmultiple workers\\nwithout\\nModel predicts the labels,\\nthe loss is computed, and the\\nmodel weights are updated.\\nContinue with\\nthe next batch\\nWith multiple workers\\nenabled, the data loader\\ncan prepare the next data\\nbatches in the background.\\nThe next batch is taken\\nfrom the loaded batches\\nthe data loader already\\nqueued up in the\\nbackground.\\nData loading\\nmultiple workers\\nwith\\nFigure A.11\\nLoading data without multiple workers (setting num_workers=0) will create a data loading \\nbottleneck where the model sits idle until the next batch is loaded (left). If multiple workers are enabled, the data \\nloader can queue up the next batch in the background (right).\\nThe dataset has two \\nfeatures and two \\nclasses.\\nThe optimizer needs to \\nknow which parameters \\nto optimize.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 296}, page_content='275\\nA.7\\nA typical training loop\\n    for batch_idx, (features, labels) in enumerate(train_loader):\\n        logits = model(features)\\n       \\n        loss = F.cross_entropy(logits, labels)\\n        \\n        optimizer.zero_grad()           \\n        loss.backward()        \\n        optimizer.step()       \\n    \\n        ### LOGGING\\n        print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\\n              f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\\n              f\" | Train Loss: {loss:.2f}\")\\n    model.eval()\\n    # Insert optional model evaluation code\\nRunning this code yields the following outputs:\\nEpoch: 001/003 | Batch 000/002 | Train Loss: 0.75\\nEpoch: 001/003 | Batch 001/002 | Train Loss: 0.65\\nEpoch: 002/003 | Batch 000/002 | Train Loss: 0.44\\nEpoch: 002/003 | Batch 001/002 | Trainl Loss: 0.13\\nEpoch: 003/003 | Batch 000/002 | Train Loss: 0.03\\nEpoch: 003/003 | Batch 001/002 | Train Loss: 0.00\\nAs we can see, the loss reaches 0 after three epochs, a sign that the model converged\\non the training set. Here, we initialize a model with two inputs and two outputs\\nbecause our toy dataset has two input features and two class labels to predict. We used\\na stochastic gradient descent (SGD) optimizer with a learning rate (lr) of 0.5. The\\nlearning rate is a hyperparameter, meaning it’s a tunable setting that we must experi-\\nment with based on observing the loss. Ideally, we want to choose a learning rate such\\nthat the loss converges after a certain number of epochs—the number of epochs is\\nanother hyperparameter to choose. \\nIn practice, we often use a third dataset, a so-called validation dataset, to find the opti-\\nmal hyperparameter settings. A validation dataset is similar to a test set. However,\\nwhile we only want to use a test set precisely once to avoid biasing the evaluation, we\\nusually use the validation set multiple times to tweak the model settings. \\n We also introduced new settings called model.train() and model.eval(). As these\\nnames imply, these settings are used to put the model into a training and an evalua-\\ntion mode. This is necessary for components that behave differently during training\\nand inference, such as dropout or batch normalization layers. Since we don’t have dropout\\nExercise A.3\\nHow many parameters does the neural network introduced in listing A.9 have?\\nSets the gradients from the previous \\nround to 0 to prevent unintended \\ngradient accumulation\\nComputes \\nthe gradients \\nof the loss \\ngiven the \\nmodel \\nparameters\\nThe optimizer uses the gradients \\nto update the model parameters.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 297}, page_content='276\\nAPPENDIX A\\nIntroduction to PyTorch\\nor other components in our NeuralNetwork class that are affected by these settings,\\nusing model.train() and model.eval() is redundant in our preceding code. How-\\never, it’s best practice to include them anyway to avoid unexpected behaviors when we\\nchange the model architecture or reuse the code to train a different model.\\n As discussed earlier, we pass the logits directly into the cross_entropy loss func-\\ntion, which will apply the softmax function internally for efficiency and numerical\\nstability reasons. Then, calling loss.backward() will calculate the gradients in the com-\\nputation graph that PyTorch constructed in the background. The optimizer.step()\\nmethod will use the gradients to update the model parameters to minimize the loss.\\nIn the case of the SGD optimizer, this means multiplying the gradients with the learn-\\ning rate and adding the scaled negative gradient to the parameters. \\nNOTE\\nTo prevent undesired gradient accumulation, it is important to include\\nan optimizer.zero_grad() call in each update round to reset the gradients to\\n0. Otherwise, the gradients will accumulate, which may be undesired.\\nAfter we have trained the model, we can use it to make predictions:\\nmodel.eval()\\nwith torch.no_grad():\\n    outputs = model(X_train)\\nprint(outputs)\\nThe results are \\ntensor([[ 2.8569, -4.1618],\\n        [ 2.5382, -3.7548],\\n        [ 2.0944, -3.1820],\\n        [-1.4814,  1.4816],\\n        [-1.7176,  1.7342]])\\nTo obtain the class membership probabilities, we can then use PyTorch’s softmax\\nfunction:\\ntorch.set_printoptions(sci_mode=False)\\nprobas = torch.softmax(outputs, dim=1)\\nprint(probas)\\nThis outputs\\ntensor([[    0.9991,     0.0009],\\n        [    0.9982,     0.0018],\\n        [    0.9949,     0.0051],\\n        [    0.0491,     0.9509],\\n        [    0.0307,     0.9693]])\\nLet’s consider the first row in the preceding code output. Here, the first value (col-\\numn) means that the training example has a 99.91% probability of belonging to class'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 298}, page_content='277\\nA.7\\nA typical training loop\\n0 and a 0.09% probability of belonging to class 1. (The set_printoptions call is used\\nhere to make the outputs more legible.)\\n We can convert these values into class label predictions using PyTorch’s argmax\\nfunction, which returns the index position of the highest value in each row if we set\\ndim=1 (setting dim=0 would return the highest value in each column instead):\\npredictions = torch.argmax(probas, dim=1)\\nprint(predictions)\\nThis prints\\ntensor([0, 0, 0, 1, 1])\\nNote that it is unnecessary to compute softmax probabilities to obtain the class labels.\\nWe could also apply the argmax function to the logits (outputs) directly:\\npredictions = torch.argmax(outputs, dim=1)\\nprint(predictions)\\nThe output is\\ntensor([0, 0, 0, 1, 1])\\nHere, we computed the predicted labels for the training dataset. Since the training\\ndataset is relatively small, we could compare it to the true training labels by eye and\\nsee that the model is 100% correct. We can double-check this using the == comparison\\noperator:\\npredictions == y_train\\nThe results are\\ntensor([True, True, True, True, True])\\nUsing torch.sum, we can count the number of correct predictions:\\ntorch.sum(predictions == y_train)\\nThe output is\\n5\\nSince the dataset consists of five training examples, we have five out of five predictions\\nthat are correct, which has 5/5 × 100% = 100% prediction accuracy.\\n To generalize the computation of the prediction accuracy, let’s implement a\\ncompute_accuracy function, as shown in the following listing.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 299}, page_content='278\\nAPPENDIX A\\nIntroduction to PyTorch\\ndef compute_accuracy(model, dataloader):\\n    model = model.eval()\\n    correct = 0.0\\n    total_examples = 0\\n    \\n    for idx, (features, labels) in enumerate(dataloader):\\n       \\n        with torch.no_grad():\\n            logits = model(features)\\n        \\n        predictions = torch.argmax(logits, dim=1)\\n        compare = labels == predictions      \\n        correct += torch.sum(compare)     \\n        total_examples += len(compare)\\n    return (correct / total_examples).item()   \\nThe code iterates over a data loader to compute the number and fraction of the cor-\\nrect predictions. When we work with large datasets, we typically can only call the model\\non a small part of the dataset due to memory limitations. The compute_accuracy func-\\ntion here is a general method that scales to datasets of arbitrary size since, in each iter-\\nation, the dataset chunk that the model receives is the same size as the batch size seen\\nduring training. The internals of the compute_accuracy function are similar to what\\nwe used before when we converted the logits to the class labels. \\n We can then apply the function to the training:\\nprint(compute_accuracy(model, train_loader))\\nThe result is\\n1.0\\nSimilarly, we can apply the function to the test set:\\nprint(compute_accuracy(model, test_loader))\\nThis prints\\n1.0\\nA.8\\nSaving and loading models\\nNow that we’ve trained our model, let’s see how to save it so we can reuse it later.\\nHere’s the recommended way how we can save and load models in PyTorch:\\ntorch.save(model.state_dict(), \"model.pth\")\\nListing A.10\\nA function to compute the prediction accuracy \\nReturns a tensor of True/\\nFalse values depending on \\nwhether the labels match\\nThe sum operation counts \\nthe number of True values.\\nThe fraction of correct prediction, \\na value between 0 and 1. .item() \\nreturns the value of the tensor as \\na Python float.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 300}, page_content='279\\nA.9\\nOptimizing training performance with GPUs\\nThe model’s state_dict is a Python dictionary object that maps each layer in the\\nmodel to its trainable parameters (weights and biases). \"model.pth\" is an arbitrary\\nfilename for the model file saved to disk. We can give it any name and file ending we\\nlike; however, .pth and .pt are the most common conventions.\\n Once we saved the model, we can restore it from disk:\\nmodel = NeuralNetwork(2, 2) \\nmodel.load_state_dict(torch.load(\"model.pth\"))\\nThe torch.load(\"model.pth\") function reads the file \"model.pth\" and recon-\\nstructs the Python dictionary object containing the model’s parameters while\\nmodel.load_state_dict() applies these parameters to the model, effectively restor-\\ning its learned state from when we saved it.\\n The line model = NeuralNetwork(2, 2) is not strictly necessary if you execute this\\ncode in the same session where you saved a model. However, I included it here to\\nillustrate that we need an instance of the model in memory to apply the saved\\nparameters. Here, the NeuralNetwork(2, 2) architecture needs to match the origi-\\nnal saved model exactly.\\nA.9\\nOptimizing training performance with GPUs\\nNext, let’s examine how to utilize GPUs, which accelerate deep neural network train-\\ning compared to regular CPUs. First, we’ll look at the main concepts behind GPU\\ncomputing in PyTorch. Then we will train a model on a single GPU. Finally, we’ll look\\nat distributed training using multiple GPUs.\\nA.9.1\\nPyTorch computations on GPU devices\\nModifying the training loop to run optionally on a GPU is relatively simple and only\\nrequires changing three lines of code (see section A.7). Before we make the modifica-\\ntions, it’s crucial to understand the main concept behind GPU computations within\\nPyTorch. In PyTorch, a device is where computations occur and data resides. The CPU\\nand the GPU are examples of devices. A PyTorch tensor resides in a device, and its\\noperations are executed on the same device.\\n Let’s see how this works in action. Assuming that you installed a GPU-compatible\\nversion of PyTorch (see section A.1.3), we can double-check that our runtime indeed\\nsupports GPU computing via the following code:\\nprint(torch.cuda.is_available())\\nThe result is\\nTrue\\nNow, suppose we have two tensors that we can add; this computation will be carried\\nout on the CPU by default:'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 301}, page_content='280\\nAPPENDIX A\\nIntroduction to PyTorch\\ntensor_1 = torch.tensor([1., 2., 3.])\\ntensor_2 = torch.tensor([4., 5., 6.])\\nprint(tensor_1 + tensor_2)\\nThis outputs\\ntensor([5., 7., 9.])\\nWe can now use the .to() method. This method is the same as the one we use to\\nchange a tensor’s datatype (see 2.2.2) to transfer these tensors onto a GPU and per-\\nform the addition there: \\ntensor_1 = tensor_1.to(\"cuda\")\\ntensor_2 = tensor_2.to(\"cuda\")\\nprint(tensor_1 + tensor_2)\\nThe output is\\ntensor([5., 7., 9.], device=\\'cuda:0\\')\\nThe resulting tensor now includes the device information, device=\\'cuda:0\\', which\\nmeans that the tensors reside on the first GPU. If your machine hosts multiple GPUs,\\nyou can specify which GPU you’d like to transfer the tensors to. You do so by indicat-\\ning the device ID in the transfer command. For instance, you can use .to(\"cuda:0\"),\\n.to(\"cuda:1\"), and so on.\\n However, all tensors must be on the same device. Otherwise, the computation will\\nfail, where one tensor resides on the CPU and the other on the GPU:\\ntensor_1 = tensor_1.to(\"cpu\")\\nprint(tensor_1 + tensor_2)\\nThe results are\\nRuntimeError      Traceback (most recent call last)\\n<ipython-input-7-4ff3c4d20fc3> in <cell line: 2>()\\n      1 tensor_1 = tensor_1.to(\"cpu\")\\n----> 2 print(tensor_1 + tensor_2)\\nRuntimeError: Expected all tensors to be on the same device, but found at\\nleast two devices, cuda:0 and cpu!\\nIn sum, we only need to transfer the tensors onto the same GPU device, and PyTorch\\nwill handle the rest. \\nA.9.2\\nSingle-GPU training\\nNow that we are familiar with transferring tensors to the GPU, we can modify the\\ntraining loop to run on a GPU. This step requires only changing three lines of code,\\nas shown in the following listing.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 302}, page_content='281\\nA.9\\nOptimizing training performance with GPUs\\ntorch.manual_seed(123)\\nmodel = NeuralNetwork(num_inputs=2, num_outputs=2)\\ndevice = torch.device(\"cuda\")     \\nmodel = model.to(device)         \\noptimizer = torch.optim.SGD(model.parameters(), lr=0.5)\\nnum_epochs = 3\\nfor epoch in range(num_epochs):\\n    \\n    model.train()\\n    for batch_idx, (features, labels) in enumerate(train_loader):\\n        features, labels = features.to(device), labels.to(device)  \\n        logits = model(features)\\n        loss = F.cross_entropy(logits, labels) # Loss function\\n        \\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n    \\n        ### LOGGING\\n        print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\\n              f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\\n              f\" | Train/Val Loss: {loss:.2f}\")\\n    model.eval()\\n    # Insert optional model evaluation code\\nRunning the preceding code will output the following, similar to the results obtained\\non the CPU (section A.7):\\nEpoch: 001/003 | Batch 000/002 | Train/Val Loss: 0.75\\nEpoch: 001/003 | Batch 001/002 | Train/Val Loss: 0.65\\nEpoch: 002/003 | Batch 000/002 | Train/Val Loss: 0.44\\nEpoch: 002/003 | Batch 001/002 | Train/Val Loss: 0.13\\nEpoch: 003/003 | Batch 000/002 | Train/Val Loss: 0.03\\nEpoch: 003/003 | Batch 001/002 | Train/Val Loss: 0.00\\nWe can use .to(\"cuda\") instead of device = torch.device(\"cuda\"). Transferring a\\ntensor to \"cuda\" instead of torch.device(\"cuda\") works as well and is shorter (see\\nsection A.9.1). We can also modify the statement, which will make the same code exe-\\ncutable on a CPU if a GPU is not available. This is considered best practice when shar-\\ning PyTorch code:\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\nIn the case of the modified training loop here, we probably won’t see a speedup due\\nto the memory transfer cost from CPU to GPU. However, we can expect a significant\\nspeedup when training deep neural networks, especially LLMs. \\nListing A.11\\nA training loop on a GPU\\nDefines a device variable \\nthat defaults to a GPU\\nTransfers the model \\nonto the GPU\\nTransfers the data\\nonto the GPU'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 303}, page_content='282\\nAPPENDIX A\\nIntroduction to PyTorch\\nA.9.3\\nTraining with multiple GPUs\\nDistributed training is the concept of dividing the model training across multiple\\nGPUs and machines. Why do we need this? Even when it is possible to train a model\\non a single GPU or machine, the process could be exceedingly time-consuming. The\\ntraining time can be significantly reduced by distributing the training process across\\nmultiple machines, each with potentially multiple GPUs. This is particularly crucial in\\nthe experimental stages of model development, where numerous training iterations\\nmight be necessary to fine-tune the model parameters and architecture. \\nNOTE\\nFor this book, access to or use of multiple GPUs is not required. This\\nsection is included for those interested in how multi-GPU computing works in\\nPyTorch.\\nLet’s begin with the most basic case of distributed training: PyTorch’s Distributed-\\nDataParallel (DDP) strategy. DDP enables parallelism by splitting the input data\\nacross the available devices and processing these data subsets simultaneously.\\n How does this work? PyTorch launches a separate process on each GPU, and each\\nprocess receives and keeps a copy of the model; these copies will be synchronized\\nduring training. To illustrate this, suppose we have two GPUs that we want to use to\\ntrain a neural network, as shown in figure A.12. \\n Each of the two GPUs will receive a copy of the model. Then, in every training iter-\\nation, each model will receive a minibatch (or just “batch”) from the data loader. We\\nPyTorch on macOS \\nOn an Apple Mac with an Apple Silicon chip (like the M1, M2, M3, or newer models)\\ninstead of a computer with an Nvidia GPU, you can change \\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\nto\\ndevice = torch.device(\\n    \"mps\" if torch.backends.mps.is_available() else \"cpu\"\\n)\\nto take advantage of this chip. \\nExercise A.4\\nCompare the run time of matrix multiplication on a CPU to a GPU. At what matrix size\\ndo you begin to see the matrix multiplication on the GPU being faster than on the\\nCPU? Hint: use the %timeit command in Jupyter to compare the run time. For exam-\\nple, given matrices a and b, run the command %timeit a @ b in a new notebook cell.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 304}, page_content='283\\nA.9\\nOptimizing training performance with GPUs\\ncan use a DistributedSampler to ensure that each GPU will receive a different, non-\\noverlapping batch when using DDP. \\n Since each model copy will see a different sample of the training data, the model\\ncopies will return different logits as outputs and compute different gradients during\\nthe backward pass. These gradients are then averaged and synchronized during train-\\ning to update the models. This way, we ensure that the models don’t diverge, as illus-\\ntrated in figure A.13.\\nThe benefit of using DDP is the enhanced speed it offers for processing the dataset com-\\npared to a single GPU. Barring a minor communication overhead between devices that\\nThe model is initialized\\non the CPU.\\nThe model is initialized\\non the CPU.\\nThe ﬁrst minibatch\\nFigure A.12\\nThe model and data transfer in DDP involves two key steps. First, we create a \\ncopy of the model on each of the GPUs. Then we divide the input data into unique \\nminibatches that we pass on to each model copy.\\nEach GPU computes\\nthe outputs (logits)\\nindependently.\\nThe gradients are\\nsynced across the\\nGPUs to compute\\nthe weight updates\\nfor each GPU.\\nFigure A.13\\nThe forward and backward passes in DDP are executed independently on each GPU with \\nits corresponding data subset. Once the forward and backward passes are completed, gradients from \\neach model replica (on each GPU) are synchronized across all GPUs. This ensures that every model \\nreplica has the same updated weights.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 305}, page_content='284\\nAPPENDIX A\\nIntroduction to PyTorch\\ncomes with DDP use, it can theoretically process a training epoch in half the time with\\ntwo GPUs compared to just one. The time efficiency scales up with the number of GPUs,\\nallowing us to process an epoch eight times faster if we have eight GPUs, and so on.\\nNOTE\\nDDP does not function properly within interactive Python environ-\\nments like Jupyter notebooks, which don’t handle multiprocessing in the same\\nway a standalone Python script does. Therefore, the following code should be\\nexecuted as a script, not within a notebook interface like Jupyter. DDP needs\\nto spawn multiple processes, and each process should have its own Python\\ninterpreter instance.\\nLet’s now see how this works in practice. For brevity, I focus on the core parts of the\\ncode that need to be adjusted for DDP training. However, readers who want to run the\\ncode on their own multi-GPU machine or a cloud instance of their choice should use\\nthe standalone script provided in this book’s GitHub repository at https://github\\n.com/rasbt/LLMs-from-scratch. \\n First, we import a few additional submodules, classes, and functions for distributed\\ntraining PyTorch, as shown in the following listing.\\nimport torch.multiprocessing as mp\\nfrom torch.utils.data.distributed import DistributedSampler\\nfrom torch.nn.parallel import DistributedDataParallel as DDP\\nfrom torch.distributed import init_process_group, destroy_process_group\\nBefore we dive deeper into the changes to make the training compatible with DDP,\\nlet’s briefly go over the rationale and usage for these newly imported utilities that we\\nneed alongside the DistributedDataParallel class.\\n PyTorch’s multiprocessing submodule contains functions such as multiprocessing\\n.spawn, which we will use to spawn multiple processes and apply a function to multi-\\nple inputs in parallel. We will use it to spawn one training process per GPU. If we\\nspawn multiple processes for training, we will need a way to divide the dataset among\\nthese different processes. For this, we will use the DistributedSampler.\\n init_process_group and destroy_process_group are used to initialize and quit\\nthe distributed training mods. The init_process_group function should be called\\nat the beginning of the training script to initialize a process group for each process in\\nthe distributed setup, and destroy_process_group should be called at the end of the\\ntraining script to destroy a given process group and release its resources. The code in\\nthe following listing illustrates how these new components are used to implement\\nDDP training for the NeuralNetwork model we implemented earlier.\\ndef ddp_setup(rank, world_size):\\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"   \\nListing A.12\\nPyTorch utilities for distributed training\\nListing A.13\\nModel training with the DistributedDataParallel strategy\\nAddress of the \\nmain node'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 306}, page_content='285\\nA.9\\nOptimizing training performance with GPUs\\n    os.environ[\"MASTER_PORT\"] = \"12345\"     \\n    init_process_group(\\n        backend=\"nccl\",             \\n        rank=rank,                        \\n        world_size=world_size           \\n    )\\n    torch.cuda.set_device(rank)       \\ndef prepare_dataset():\\n    # insert dataset preparation code \\n    train_loader = DataLoader(\\n        dataset=train_ds,\\n        batch_size=2,\\n        shuffle=False,            \\n        pin_memory=True,          \\n        drop_last=True,\\n        sampler=DistributedSampler(train_ds)   \\n    )    \\n    return train_loader, test_loader\\ndef main(rank, world_size, num_epochs):      \\n    ddp_setup(rank, world_size)\\n    train_loader, test_loader = prepare_dataset()\\n    model = NeuralNetwork(num_inputs=2, num_outputs=2)\\n    model.to(rank)\\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\\n    model = DDP(model, device_ids=[rank])\\n    for epoch in range(num_epochs):\\n    for features, labels in train_loader:\\n            features, labels = features.to(rank), labels.to(rank)     \\n            # insert model prediction and backpropagation code \\n            print(f\"[GPU{rank}] Epoch: {epoch+1:03d}/{num_epochs:03d}\"\\n                  f\" | Batchsize {labels.shape[0]:03d}\"\\n                  f\" | Train/Val Loss: {loss:.2f}\")\\n    \\n    model.eval()\\n    train_acc = compute_accuracy(model, train_loader, device=rank)\\n    print(f\"[GPU{rank}] Training accuracy\", train_acc)\\n    test_acc = compute_accuracy(model, test_loader, device=rank)\\n    print(f\"[GPU{rank}] Test accuracy\", test_acc)\\n    destroy_process_group()                     \\nif __name__ == \"__main__\":\\n    print(\"Number of GPUs available:\", torch.cuda.device_count())\\n    torch.manual_seed(123)\\n    num_epochs = 3\\n    world_size = torch.cuda.device_count()\\n    mp.spawn(main, args=(world_size, num_epochs), nprocs=world_size) \\nBefore we run this code, let’s summarize how it works in addition to the preceding\\nannotations. We have a __name__ == \"__main__\" clause at the bottom containing code\\nexecuted when we run the code as a Python script instead of importing it as a module.\\nAny free port \\non the machine\\nnccl stands for NVIDIA Collective \\nCommunication Library.\\nrank refers to the index of \\nthe GPU we want to use.\\nworld_size\\nis the\\nnumber of\\nGPUs to\\nuse.\\nSets the current GPU device on \\nwhich tensors will be allocated and \\noperations will be performed\\nDistibuted-\\nSampler\\ntakes care of\\nthe shuffling\\nnow.\\nEnables faster memory transfer \\nwhen training on GPU\\nSplits the dataset into distinct, \\nnon-overlapping subsets for \\neach process (GPU)\\nThe main function \\nrunning the model \\ntraining\\nrank is the\\nGPU ID\\nCleans up resource \\nallocation\\nLaunches the main function using multiple processes, where\\nnprocs=world_size means one process per GPU.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 307}, page_content='286\\nAPPENDIX A\\nIntroduction to PyTorch\\nThis code first prints the number of available GPUs using torch.cuda.device_count(),\\nsets a random seed for reproducibility, and then spawns new processes using PyTorch’s\\nmultiprocessesing.spawn function. Here, the spawn function launches one process per\\nGPU setting nproces=world_size, where the world size is the number of available GPUs.\\nThis spawn function launches the code in the main function we define in the same script\\nwith some additional arguments provided via args. Note that the main function has a\\nrank argument that we don’t include in the mp.spawn() call. That’s because the rank,\\nwhich refers to the process ID we use as the GPU ID, is already passed automatically.\\n The main function sets up the distributed environment via ddp_setup—another\\nfunction we defined—loads the training and test sets, sets up the model, and carries\\nout the training. Compared to the single-GPU training (section A.9.2), we now trans-\\nfer the model and data to the target device via .to(rank), which we use to refer to the\\nGPU device ID. Also, we wrap the model via DDP, which enables the synchronization of\\nthe gradients between the different GPUs during training. After the training finishes\\nand we evaluate the models, we use destroy_process_group() to cleanly exit the dis-\\ntributed training and free up the allocated resources.\\n Earlier I mentioned that each GPU will receive a different subsample of the train-\\ning data. To ensure this, we set sampler=DistributedSampler(train_ds) in the train-\\ning loader.\\n The last function to discuss is ddp_setup. It sets the main node’s address and port\\nto allow for communication between the different processes, initializes the process\\ngroup with the NCCL backend (designed for GPU-to-GPU communication), and sets\\nthe rank (process identifier) and world size (total number of processes). Finally, it\\nspecifies the GPU device corresponding to the current model training process rank.\\nSELECTING AVAILABLE GPUS ON A MULTI-GPU MACHINE  \\nIf you wish to restrict the number of GPUs used for training on a multi-GPU machine,\\nthe simplest way is to use the CUDA_VISIBLE_DEVICES environment variable. To illus-\\ntrate this, suppose your machine has multiple GPUs, and you only want to use one\\nGPU—for example, the GPU with index 0. Instead of python some_script.py, you can\\nrun the following code from the terminal:\\nCUDA_VISIBLE_DEVICES=0 python some_script.py\\nOr, if your machine has four GPUs and you only want to use the first and third GPU,\\nyou can use\\nCUDA_VISIBLE_DEVICES=0,2 python some_script.py\\nSetting CUDA_VISIBLE_DEVICES in this way is a simple and effective way to manage\\nGPU allocation without modifying your PyTorch scripts.\\n Let’s now run this code and see how it works in practice by launching the code as a\\nscript from the terminal:\\npython ch02-DDP-script.py'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 308}, page_content='287\\nA.9\\nOptimizing training performance with GPUs\\nNote that it should work on both single and multi-GPU machines. If we run this code\\non a single GPU, we should see the following output:\\nPyTorch version: 2.2.1+cu117\\nCUDA available: True\\nNumber of GPUs available: 1\\n[GPU0] Epoch: 001/003 | Batchsize 002 | Train/Val Loss: 0.62\\n[GPU0] Epoch: 001/003 | Batchsize 002 | Train/Val Loss: 0.32\\n[GPU0] Epoch: 002/003 | Batchsize 002 | Train/Val Loss: 0.11\\n[GPU0] Epoch: 002/003 | Batchsize 002 | Train/Val Loss: 0.07\\n[GPU0] Epoch: 003/003 | Batchsize 002 | Train/Val Loss: 0.02\\n[GPU0] Epoch: 003/003 | Batchsize 002 | Train/Val Loss: 0.03\\n[GPU0] Training accuracy 1.0\\n[GPU0] Test accuracy 1.0\\nThe code output looks similar to that using a single GPU (section A.9.2), which is a\\ngood sanity check. \\n Now, if we run the same command and code on a machine with two GPUs, we\\nshould see the following:\\nPyTorch version: 2.2.1+cu117\\nCUDA available: True\\nNumber of GPUs available: 2\\n[GPU1] Epoch: 001/003 | Batchsize 002 | Train/Val Loss: 0.60\\n[GPU0] Epoch: 001/003 | Batchsize 002 | Train/Val Loss: 0.59\\n[GPU0] Epoch: 002/003 | Batchsize 002 | Train/Val Loss: 0.16\\n[GPU1] Epoch: 002/003 | Batchsize 002 | Train/Val Loss: 0.17\\n[GPU0] Epoch: 003/003 | Batchsize 002 | Train/Val Loss: 0.05\\n[GPU1] Epoch: 003/003 | Batchsize 002 | Train/Val Loss: 0.05\\n[GPU1] Training accuracy 1.0\\n[GPU0] Training accuracy 1.0\\n[GPU1] Test accuracy 1.0\\n[GPU0] Test accuracy 1.0\\nAs expected, we can see that some batches are processed on the first GPU (GPU0) and\\nothers on the second (GPU1). However, we see duplicated output lines when printing\\nthe training and test accuracies. Each process (in other words, each GPU) prints the\\ntest accuracy independently. Since DDP replicates the model onto each GPU and\\neach process runs independently, if you have a print statement inside your testing\\nloop, each process will execute it, leading to repeated output lines. If this bothers you,\\nyou can fix it using the rank of each process to control your print statements: \\nif rank == 0:                 \\n    print(\"Test accuracy: \", accuracy)\\nThis is, in a nutshell, how distributed training via DDP works. If you are interested in\\nadditional details, I recommend checking the official API documentation at https://\\nmng.bz/9dPr.\\nOnly print in the \\nfirst process'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 309}, page_content='288\\nAPPENDIX A\\nIntroduction to PyTorch\\nSummary\\n\\uf0a1PyTorch is an open source library with three core components: a tensor library,\\nautomatic differentiation functions, and deep learning utilities.\\n\\uf0a1PyTorch’s tensor library is similar to array libraries like NumPy.\\n\\uf0a1In the context of PyTorch, tensors are array-like data structures representing\\nscalars, vectors, matrices, and higher-dimensional arrays.\\n\\uf0a1PyTorch tensors can be executed on the CPU, but one major advantage of\\nPyTorch’s tensor format is its GPU support to accelerate computations.\\n\\uf0a1The automatic differentiation (autograd) capabilities in PyTorch allow us to\\nconveniently train neural networks using backpropagation without manually\\nderiving gradients.\\n\\uf0a1The deep learning utilities in PyTorch provide building blocks for creating cus-\\ntom deep neural networks.\\n\\uf0a1PyTorch includes Dataset and DataLoader classes to set up efficient data-load-\\ning pipelines.\\n\\uf0a1It’s easiest to train models on a CPU or single GPU. \\n\\uf0a1Using DistributedDataParallel is the simplest way in PyTorch to accelerate\\nthe training if multiple GPUs are available.\\nAlternative PyTorch APIs for multi-GPU training \\nIf you prefer a more straightforward way to use multiple GPUs in PyTorch, you can con-\\nsider add-on APIs like the open-source Fabric library. I wrote about it in “Accelerating\\nPyTorch Model Training: Using Mixed-Precision and Fully Sharded Data Parallelism”\\n(https://mng.bz/jXle).'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 310}, page_content='289\\nappendix B\\nReferences and\\nfurther reading\\nChapter 1\\nCustom-built LLMs are able to outperform general-purpose LLMs as a team at\\nBloomberg showed via a version of GPT pretrained on finance data from scratch.\\nThe custom LLM outperformed ChatGPT on financial tasks while maintaining\\ngood performance on general LLM benchmarks:\\n\\uf0a1“BloombergGPT: A Large Language Model for Finance” (2023) by Wu et al.,\\nhttps://arxiv.org/abs/2303.17564\\nExisting LLMs can be adapted and fine-tuned to outperform general LLMs as well,\\nwhich teams from Google Research and Google DeepMind showed in a medical\\ncontext:\\n\\uf0a1“Towards Expert-Level Medical Question Answering with Large Language\\nModels” (2023) by Singhal et al., https://arxiv.org/abs/2305.09617\\nThe following paper proposed the original transformer architecture:\\n\\uf0a1“Attention Is All You Need” (2017) by Vaswani et al., https://arxiv.org/abs/\\n1706.03762\\nOn the original encoder-style transformer, called BERT, see\\n\\uf0a1“BERT: Pre-training of Deep Bidirectional Transformers for Language Under-\\nstanding” (2018) by Devlin et al., https://arxiv.org/abs/1810.04805\\nThe paper describing the decoder-style GPT-3 model, which inspired modern LLMs\\nand will be used as a template for implementing an LLM from scratch in this book, is'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 311}, page_content='290\\nAPPENDIX B\\nReferences and further reading\\n\\uf0a1“Language Models are Few-Shot Learners” (2020) by Brown et al., https://\\narxiv.org/abs/2005.14165\\nThe following covers the original vision transformer for classifying images, which illus-\\ntrates that transformer architectures are not only restricted to text inputs:\\n\\uf0a1“An Image is Worth 16x16 Words: Transformers for Image Recognition at\\nScale” (2020) by Dosovitskiy et al., https://arxiv.org/abs/2010.11929 \\nThe following experimental (but less popular) LLM architectures serve as examples\\nthat not all LLMs need to be based on the transformer architecture:\\n\\uf0a1“RWKV: Reinventing RNNs for the Transformer Era” (2023) by Peng et al.,\\nhttps://arxiv.org/abs/2305.13048\\n\\uf0a1“Hyena Hierarchy: Towards Larger Convolutional Language Models” (2023) by\\nPoli et al., https://arxiv.org/abs/2302.10866\\n\\uf0a1“Mamba: Linear-Time Sequence Modeling with Selective State Spaces” (2023)\\nby Gu and Dao, https://arxiv.org/abs/2312.00752\\nMeta AI’s model is a popular implementation of a GPT-like model that is openly avail-\\nable in contrast to GPT-3 and ChatGPT:\\n\\uf0a1“Llama 2: Open Foundation and Fine-Tuned Chat Models” (2023) by Touvron\\net al., https://arxiv.org/abs/2307.092881\\nFor readers interested in additional details about the dataset references in section 1.5,\\nthis paper describes the publicly available The Pile dataset curated by Eleuther AI:\\n\\uf0a1“The Pile: An 800GB Dataset of Diverse Text for Language Modeling” (2020) by\\nGao et al., https://arxiv.org/abs/2101.00027\\nThe following paper provides the reference for InstructGPT for fine-tuning GPT-3,\\nwhich was mentioned in section 1.6 and will be discussed in more detail in chapter 7:\\n\\uf0a1“Training Language Models to Follow Instructions with Human Feedback”\\n(2022) by Ouyang et al., https://arxiv.org/abs/2203.02155\\nChapter 2\\nReaders who are interested in discussion and comparison of embedding spaces with\\nlatent spaces and the general notion of vector representations can find more informa-\\ntion in the first chapter of my book:\\n\\uf0a1Machine Learning Q and AI (2023) by Sebastian Raschka, https://leanpub.com/\\nmachine-learning-q-and-ai\\nThe following paper provides more in-depth discussions of how byte pair encoding is\\nused as a tokenization method:\\n\\uf0a1“Neural Machine Translation of Rare Words with Subword Units” (2015) by\\nSennrich et al., https://arxiv.org/abs/1508.07909'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 312}, page_content='291\\nChapter 3\\nThe code for the byte pair encoding tokenizer used to train GPT-2 was open-sourced\\nby OpenAI:\\n\\uf0a1https://github.com/openai/gpt-2/blob/master/src/encoder.py\\nOpenAI provides an interactive web UI to illustrate how the byte pair tokenizer in\\nGPT models works:\\n\\uf0a1https://platform.openai.com/tokenizer\\nFor readers interested in coding and training a BPE tokenizer from the ground\\nup, Andrej Karpathy’s GitHub repository minbpe offers a minimal and readable\\nimplementation:\\n\\uf0a1“A Minimal Implementation of a BPE Tokenizer,” https://github.com/karpa-\\nthy/minbpe\\nReaders who are interested in studying alternative tokenization schemes that are used\\nby some other popular LLMs can find more information in the SentencePiece and\\nWordPiece papers:\\n\\uf0a1“SentencePiece: A Simple and Language Independent Subword Tokenizer and\\nDetokenizer for Neural Text Processing” (2018) by Kudo and Richardson,\\nhttps://aclanthology.org/D18-2012/\\n\\uf0a1“Fast WordPiece Tokenization” (2020) by Song et al., https://arxiv.org/abs/\\n2012.15524\\nChapter 3\\nReaders interested in learning more about Bahdanau attention for RNN and lan-\\nguage translation can find detailed insights in the following paper:\\n\\uf0a1“Neural Machine Translation by Jointly Learning to Align and Translate”\\n(2014) by Bahdanau, Cho, and Bengio, https://arxiv.org/abs/1409.0473\\nThe concept of self-attention as scaled dot-product attention was introduced in the\\noriginal transformer paper:\\n\\uf0a1“Attention Is All You Need” (2017) by Vaswani et al., https://arxiv.org/abs/\\n1706.03762\\nFlashAttention is a highly efficient implementation of a self-attention mechanism,\\nwhich accelerates the computation process by optimizing memory access patterns.\\nFlashAttention is mathematically the same as the standard self-attention mechanism\\nbut optimizes the computational process for efficiency:\\n\\uf0a1“FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness”\\n(2022) by Dao et al., https://arxiv.org/abs/2205.14135\\n\\uf0a1“FlashAttention-2: Faster Attention with Better Parallelism and Work Partition-\\ning” (2023) by Dao, https://arxiv.org/abs/2307.08691'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 313}, page_content='292\\nAPPENDIX B\\nReferences and further reading\\nPyTorch implements a function for self-attention and causal attention that supports\\nFlashAttention for efficiency. This function is beta and subject to change:\\n\\uf0a1\\nscaled_dot_product_attention documentation: https://mng.bz/NRJd\\nPyTorch also implements an efficient MultiHeadAttention class based on the scaled_\\ndot_product function:\\n\\uf0a1\\nMultiHeadAttention documentation: https://mng.bz/DdJV\\nDropout is a regularization technique used in neural networks to prevent overfitting\\nby randomly dropping units (along with their connections) from the neural network\\nduring training:\\n\\uf0a1“Dropout: A Simple Way to Prevent Neural Networks from Overfitting” (2014)\\nby Srivastava et al., https://jmlr.org/papers/v15/srivastava14a.html\\nWhile using the multi-head attention based on scaled-dot product attention remains\\nthe most common variant of self-attention in practice, authors have found that it’s\\npossible to also achieve good performance without the value weight matrix and pro-\\njection layer:\\n\\uf0a1“Simplifying Transformer Blocks” (2023) by He and Hofmann, https://arxiv\\n.org/abs/2311.01906\\nChapter 4\\nThe following paper introduces a technique that stabilizes the hidden state dynamics\\nneural networks by normalizing the summed inputs to the neurons within a hidden\\nlayer, significantly reducing training time compared to previously published methods:\\n\\uf0a1“Layer Normalization” (2016) by Ba, Kiros, and Hinton, https://arxiv.org/abs/\\n1607.06450\\nPost-LayerNorm, used in the original transformer model, applies layer normalization\\nafter the self-attention and feed forward networks. In contrast, Pre-LayerNorm, as\\nadopted in models like GPT-2 and newer LLMs, applies layer normalization before\\nthese components, which can lead to more stable training dynamics and has been\\nshown to improve performance in some cases, as discussed in the following papers:\\n\\uf0a1“On Layer Normalization in the Transformer Architecture” (2020) by Xiong et\\nal., https://arxiv.org/abs/2002.04745\\n\\uf0a1“ResiDual: Transformer with Dual Residual Connections” (2023) by Tie et al.,\\nhttps://arxiv.org/abs/2304.14802\\nA popular variant of LayerNorm used in modern LLMs is RMSNorm due to its\\nimproved computing efficiency. This variant simplifies the normalization process by\\nnormalizing the inputs using only the root mean square of the inputs, without sub-\\ntracting the mean before squaring. This means it does not center the data before com-\\nputing the scale. RMSNorm is described in more detail in'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 314}, page_content='293\\nChapter 5\\n\\uf0a1“Root Mean Square Layer Normalization” (2019) by Zhang and Sennrich,\\nhttps://arxiv.org/abs/1910.07467\\nThe Gaussian Error Linear Unit (GELU) activation function combines the properties\\nof both the classic ReLU activation function and the normal distribution’s cumulative\\ndistribution function to model layer outputs, allowing for stochastic regularization\\nand nonlinearities in deep learning models:\\n\\uf0a1“Gaussian Error Linear Units (GELUs)” (2016) by Hendricks and Gimpel,\\nhttps://arxiv.org/abs/1606.08415\\nThe GPT-2 paper introduced a series of transformer-based LLMs with varying sizes—\\n124 million, 355 million, 774 million, and 1.5 billion parameters:\\n\\uf0a1“Language Models Are Unsupervised Multitask Learners” (2019) by Radford et\\nal., https://mng.bz/lMgo\\nOpenAI’s GPT-3 uses fundamentally the same architecture as GPT-2, except that the\\nlargest version (175 billion) is 100x larger than the largest GPT-2 model and has been\\ntrained on much more data. Interested readers can refer to the official GPT-3 paper\\nby OpenAI and the technical overview by Lambda Labs, which calculates that training\\nGPT-3 on a single RTX 8000 consumer GPU would take 665 years:\\n\\uf0a1“Language Models are Few-Shot Learners” (2023) by Brown et al., https://\\narxiv.org/abs/2005.14165\\n\\uf0a1“OpenAI’s GPT-3 Language Model: A Technical Overview,” https://lambdalabs\\n.com/blog/demystifying-gpt-3\\nNanoGPT is a code repository with a minimalist yet efficient implementation of a\\nGPT-2 model, similar to the model implemented in this book. While the code in this\\nbook is different from nanoGPT, this repository inspired the reorganization of a large\\nGPT Python parent class implementation into smaller submodules:\\n\\uf0a1“NanoGPT, a Repository for Training Medium-Sized GPTs, https://github.com/\\nkarpathy/nanoGPT\\nAn informative blog post showing that most of the computation in LLMs is spent in\\nthe feed forward layers rather than attention layers when the context size is smaller\\nthan 32,000 tokens is:\\n\\uf0a1“In the Long (Context) Run” by Harm de Vries, https://www.harmdevries.com/\\npost/context-length/\\nChapter 5\\nFor information on detailing the loss function and applying a log transformation to\\nmake it easier to handle for mathematical optimization, see my lecture video:\\n\\uf0a1L8.2 Logistic Regression Loss Function, https://www.youtube.com/watch?v=\\nGxJe0DZvydM'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 315}, page_content='294\\nAPPENDIX B\\nReferences and further reading\\nThe following lecture and code example by the author explain how PyTorch’s cross-\\nentropy functions works under the hood:\\n\\uf0a1L8.7.1 OneHot Encoding and Multi-category Cross Entropy, https://www\\n.youtube.com/watch?v=4n71-tZ94yk\\n\\uf0a1Understanding Onehot Encoding and Cross Entropy in PyTorch, https://\\nmng.bz/o05v\\nThe following two papers detail the dataset, hyperparameter, and architecture details\\nused for pretraining LLMs:\\n\\uf0a1“Pythia: A Suite for Analyzing Large Language Models Across Training and\\nScaling” (2023) by Biderman et al., https://arxiv.org/abs/2304.01373\\n\\uf0a1“OLMo: Accelerating the Science of Language Models” (2024) by Groeneveld\\net al., https://arxiv.org/abs/2402.00838\\nThe following supplementary code available for this book contains instructions for\\npreparing 60,000 public domain books from Project Gutenberg for LLM training:\\n\\uf0a1Pretraining GPT on the Project Gutenberg Dataset, https://mng.bz/Bdw2\\nChapter 5 discusses the pretraining of LLMs, and appendix D covers more advanced\\ntraining functions, such as linear warmup and cosine annealing. The following paper\\nfinds that similar techniques can be successfully applied to continue pretraining\\nalready pretrained LLMs, along with additional tips and insights:\\n\\uf0a1“Simple and Scalable Strategies to Continually Pre-train Large Language Mod-\\nels” (2024) by Ibrahim et al., https://arxiv.org/abs/2403.08763\\nBloombergGPT is an example of a domain-specific LLM created by training on both\\ngeneral and domain-specific text corpora, specifically in the field of finance:\\n\\uf0a1“BloombergGPT: A Large Language Model for Finance” (2023) by Wu et al.,\\nhttps://arxiv.org/abs/2303.17564\\nGaLore is a recent research project that aims to make LLM pretraining more efficient.\\nThe required code change boils down to just replacing PyTorch’s AdamW optimizer in\\nthe training function with the GaLoreAdamW optimizer provided by the galore-torch\\nPython package:\\n\\uf0a1“GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection”\\n(2024) by Zhao et al., https://arxiv.org/abs/2403.03507\\n\\uf0a1GaLore code repository, https://github.com/jiaweizzhao/GaLore\\nThe following papers and resources share openly available, large-scale pretraining\\ndatasets for LLMs that consist of hundreds of gigabytes to terabytes of text data:\\n\\uf0a1“Dolma: An Open Corpus of Three Trillion Tokens for LLM Pretraining\\nResearch” (2024) by Soldaini et al., https://arxiv.org/abs/2402.00159'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 316}, page_content='295\\nChapter 6\\n\\uf0a1“The Pile: An 800GB Dataset of Diverse Text for Language Modeling” (2020) by\\nGao et al., https://arxiv.org/abs/2101.00027\\n\\uf0a1“The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora\\nwith Web Data, and Web Data Only,” (2023) by Penedo et al., https://arxiv.org/\\nabs/2306.01116\\n\\uf0a1“RedPajama,” by Together AI, https://mng.bz/d6nw\\n\\uf0a1The FineWeb Dataset, which includes more than 15 trillion tokens of cleaned\\nand deduplicated English web data sourced from CommonCrawl, https://\\nmng.bz/rVzy\\nThe paper that originally introduced top-k sampling is\\n\\uf0a1“Hierarchical Neural Story Generation” (2018) by Fan et al., https://arxiv.org/\\nabs/1805.04833\\nAn alternative to top-k sampling is top-p sampling (not covered in chapter 5), which\\nselects from the smallest set of top tokens whose cumulative probability exceeds a\\nthreshold p, while top-k sampling picks from the top k tokens by probability:\\n\\uf0a1Top-p sampling, https://en.wikipedia.org/wiki/Top-p_sampling\\nBeam search (not covered in chapter 5) is an alternative decoding algorithm that gen-\\nerates output sequences by keeping only the top-scoring partial sequences at each step\\nto balance efficiency and quality:\\n\\uf0a1“Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence\\nModels” (2016) by Vijayakumar et al., https://arxiv.org/abs/1610.02424\\nChapter 6\\nAdditional resources that discuss the different types of fine-tuning are\\n\\uf0a1“Using and Finetuning Pretrained Transformers,” https://mng.bz/VxJG\\n\\uf0a1“Finetuning Large Language Models,” https://mng.bz/x28X\\nAdditional experiments, including a comparison of fine-tuning the first output token\\nversus the last output token, can be found in the supplementary code material on\\nGitHub:\\n\\uf0a1Additional spam classification experiments, https://mng.bz/AdJx\\nFor a binary classification task, such as spam classification, it is technically possible\\nto use only a single output node instead of two output nodes, as I discuss in the fol-\\nlowing article:\\n\\uf0a1“Losses Learned—Optimizing Negative Log-Likelihood and Cross-Entropy in\\nPyTorch,” https://mng.bz/ZEJA'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 317}, page_content='296\\nAPPENDIX B\\nReferences and further reading\\nYou can find additional experiments on fine-tuning different layers of an LLM in the\\nfollowing article, which shows that fine-tuning the last transformer block, in addition\\nto the output layer, improves the predictive performance substantially:\\n\\uf0a1“Finetuning Large Language Models,” https://mng.bz/RZJv\\nReaders can find additional resources and information for dealing with imbalanced\\nclassification datasets in the imbalanced-learn documentation:\\n\\uf0a1“Imbalanced-Learn User Guide,” https://mng.bz/2KNa\\nFor readers interested in classifying spam emails rather than spam text messages, the\\nfollowing resource provides a large email spam classification dataset in a convenient\\nCSV format similar to the dataset format used in chapter 6:\\n\\uf0a1Email Spam Classification Dataset, https://mng.bz/1GEq\\nGPT-2 is a model based on the decoder module of the transformer architecture, and\\nits primary purpose is to generate new text. As an alternative, encoder-based models\\nsuch as BERT and RoBERTa can be effective for classification tasks:\\n\\uf0a1“BERT: Pre-training of Deep Bidirectional Transformers for Language Under-\\nstanding” (2018) by Devlin et al., https://arxiv.org/abs/1810.04805\\n\\uf0a1“RoBERTa: A Robustly Optimized BERT Pretraining Approach” (2019) by Liu\\net al., https://arxiv.org/abs/1907.11692\\n\\uf0a1“Additional Experiments Classifying the Sentiment of 50k IMDB Movie Reviews,”\\nhttps://mng.bz/PZJR\\nRecent papers are showing that the classification performance can be further\\nimproved by removing the causal mask during classification fine-tuning alongside\\nother modifications:\\n\\uf0a1“Label Supervised LLaMA Finetuning” (2023) by Li et al., https://arxiv.org/\\nabs/2310.01208\\n\\uf0a1“LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders”\\n(2024) by BehnamGhader et al., https://arxiv.org/abs/2404.05961\\nChapter 7\\nThe Alpaca dataset for instruction fine-tuning contains 52,000 instruction–response\\npairs and is one of the first and most popular publicly available datasets for instruction\\nfine-tuning:\\n\\uf0a1“Stanford Alpaca: An Instruction-Following Llama Model,” https://github\\n.com/tatsu-lab/stanford_alpaca\\nAdditional publicly accessible datasets suitable for instruction fine-tuning include\\n\\uf0a1LIMA, https://huggingface.co/datasets/GAIR/lima \\n– For more information, see “LIMA: Less Is More for Alignment,” Zhou et al.,\\nhttps://arxiv.org/abs/2305.11206'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 318}, page_content='297\\nChapter 7\\n\\uf0a1UltraChat, https://huggingface.co/datasets/openchat/ultrachat-sharegpt\\n– A large-scale dataset consisting of 805,000 instruction–response pairs; for\\nmore information, see “Enhancing Chat Language Models by Scaling High-\\nquality Instructional Conversations,” by Ding et al., https://arxiv.org/abs/\\n2305.14233\\n\\uf0a1Alpaca GPT4, https://mng.bz/Aa0p \\n– An Alpaca-like dataset with 52,000 instruction–response pairs generated with\\nGPT-4 instead of GPT-3.5\\nPhi-3 is a 3.8-billion-parameter model with an instruction-fine-tuned variant that is\\nreported to be comparable to much larger proprietary models, such as GPT-3.5:\\n\\uf0a1“Phi-3 Technical Report: A Highly Capable Language Model Locally on Your\\nPhone” (2024) by Abdin et al., https://arxiv.org/abs/2404.14219\\nResearchers propose a synthetic instruction data generation method that generates\\n300,000 high-quality instruction-response pairs from an instruction fine-tuned Llama-\\n3 model. A pretrained Llama 3 base model fine-tuned on these instruction examples\\nperforms comparably to the original instruction fine-tuned Llama-3 model:\\n\\uf0a1“Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs\\nwith Nothing” (2024) by Xu et al., https://arxiv.org/abs/2406.08464\\nResearch has shown that not masking the instructions and inputs in instruction fine-\\ntuning effectively improves performance on various NLP tasks and open-ended gener-\\nation benchmarks, particularly when trained on datasets with lengthy instructions and\\nbrief outputs or when using a small number of training examples:\\n\\uf0a1“Instruction Tuning with Loss Over Instructions” (2024) by Shi, https://\\narxiv.org/abs/2405.14394\\nPrometheus and PHUDGE are openly available LLMs that match GPT-4 in evaluating\\nlong-form responses with customizable criteria. We don’t use these because at the\\ntime of this writing, they are not supported by Ollama and thus cannot be executed\\nefficiently on a laptop:\\n\\uf0a1“Prometheus: Inducing Finegrained Evaluation Capability in Language Mod-\\nels” (2023) by Kim et al., https://arxiv.org/abs/2310.08491\\n\\uf0a1“PHUDGE: Phi-3 as Scalable Judge” (2024) by Deshwal and Chawla, “https://\\narxiv.org/abs/2405.08029\\n\\uf0a1“Prometheus 2: An Open Source Language Model Specialized in Evaluating\\nOther Language Models” (2024), by Kim et al., https://arxiv.org/abs/2405\\n.01535\\nThe results in the following report support the view that large language models pri-\\nmarily acquire factual knowledge during pretraining and that fine-tuning mainly\\nenhances their efficiency in using this knowledge. Furthermore, this study explores'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 319}, page_content='298\\nAPPENDIX B\\nReferences and further reading\\nhow fine-tuning large language models with new factual information affects their abil-\\nity to use preexisting knowledge, revealing that models learn new facts more slowly\\nand their introduction during fine-tuning increases the model’s tendency to generate\\nincorrect information:\\n\\uf0a1“Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?” (2024)\\nby Gekhman, https://arxiv.org/abs/2405.05904\\nPreference fine-tuning is an optional step after instruction fine-tuning to align the\\nLLM more closely with human preferences. The following articles by the author pro-\\nvide more information about this process:\\n\\uf0a1“LLM Training: RLHF and Its Alternatives,” https://mng.bz/ZVPm\\n\\uf0a1“Tips for LLM Pretraining and Evaluating Reward Models,” https://mng.bz/\\nRNXj\\nAppendix A\\nWhile appendix A should be sufficient to get you up to speed, if you are looking for\\nmore comprehensive introductions to deep learning, I recommend the following\\nbooks:\\n\\uf0a1Machine Learning with PyTorch and Scikit-Learn (2022) by Sebastian Raschka,\\nHayden Liu, and Vahid Mirjalili. ISBN 978-1801819312\\n\\uf0a1Deep Learning with PyTorch (2021) by Eli Stevens, Luca Antiga, and Thomas Vieh-\\nmann. ISBN 978-1617295263\\nFor a more thorough introduction to the concepts of tensors, readers can find a 15-\\nminute video tutorial that I recorded:\\n\\uf0a1“Lecture 4.1: Tensors in Deep Learning,” https://www.youtube.com/watch?v=\\nJXfDlgrfOBY\\nIf you want to learn more about model evaluation in machine learning, I recommend\\nmy article\\n\\uf0a1“Model Evaluation, Model Selection, and Algorithm Selection in Machine\\nLearning” (2018) by Sebastian Raschka, https://arxiv.org/abs/1811.12808\\nFor readers who are interested in a refresher or gentle introduction to calculus, I’ve\\nwritten a chapter on calculus that is freely available on my website:\\n\\uf0a1“Introduction to Calculus,” by Sebastian Raschka, https://mng.bz/WEyW\\nWhy does PyTorch not call optimizer.zero_grad() automatically for us in the back-\\nground? In some instances, it may be desirable to accumulate the gradients, and\\nPyTorch will leave this as an option for us. If you want to learn more about gradient\\naccumulation, please see the following article:\\n\\uf0a1“Finetuning Large Language Models on a Single GPU Using Gradient Accumu-\\nlation” by Sebastian Raschka, https://mng.bz/8wPD'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 320}, page_content='299\\nAppendix A\\nThis appendix covers DDP, which is a popular approach for training deep learning\\nmodels across multiple GPUs. For more advanced use cases where a single model\\ndoesn’t fit onto the GPU, you may also consider PyTorch’s Fully Sharded Data Parallel\\n(FSDP) method, which performs distributed data parallelism and distributes large lay-\\ners across different GPUs. For more information, see this overview with further links\\nto the API documentation: \\n\\uf0a1“Introducing PyTorch Fully Sharded Data Parallel (FSDP) API,” https://mng\\n.bz/EZJR'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 321}, page_content='300\\nappendix C\\nExercise solutions\\nThe complete code examples for the exercises’ answers can be found in the supple-\\nmentary GitHub repository at https://github.com/rasbt/LLMs-from-scratch.\\nChapter 2\\nExercise 2.1\\nYou can obtain the individual token IDs by prompting the encoder with one string\\nat a time:\\nprint(tokenizer.encode(\"Ak\"))\\nprint(tokenizer.encode(\"w\"))\\n# ...\\nThis prints\\n[33901]\\n[86]\\n# ...\\nYou can then use the following code to assemble the original string:\\nprint(tokenizer.decode([33901, 86, 343, 86, 220, 959]))\\nThis returns\\n\\'Akwirw ier\\''),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 322}, page_content='301\\nChapter 3\\nExercise 2.2\\nThe code for the data loader with max_length=2 and stride=2:\\ndataloader = create_dataloader(\\n    raw_text, batch_size=4, max_length=2, stride=2\\n)\\nIt produces batches of the following format:\\ntensor([[  40,  367],\\n        [2885, 1464],\\n        [1807, 3619],\\n        [ 402,  271]])\\nThe code of the second data loader with max_length=8 and stride=2:\\ndataloader = create_dataloader(\\n    raw_text, batch_size=4, max_length=8, stride=2\\n)\\nAn example batch looks like\\ntensor([[   40,   367,  2885,  1464,  1807,  3619,   402,   271],\\n        [ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138],\\n        [ 1807,  3619,   402,   271, 10899,  2138,   257,  7026],\\n        [  402,   271, 10899,  2138,   257,  7026, 15632,   438]])\\nChapter 3\\nExercise 3.1\\nThe correct weight assignment is\\nsa_v1.W_query = torch.nn.Parameter(sa_v2.W_query.weight.T)\\nsa_v1.W_key = torch.nn.Parameter(sa_v2.W_key.weight.T)\\nsa_v1.W_value = torch.nn.Parameter(sa_v2.W_value.weight.T)\\nExercise 3.2\\nTo achieve an output dimension of 2, similar to what we had in single-head attention,\\nwe need to change the projection dimension d_out to 1.\\nd_out = 1\\nmha = MultiHeadAttentionWrapper(d_in, d_out, block_size, 0.0, num_heads=2)\\nExercise 3.3\\nThe initialization for the smallest GPT-2 model is\\nblock_size = 1024\\nd_in, d_out = 768, 768\\nnum_heads = 12\\nmha = MultiHeadAttention(d_in, d_out, block_size, 0.0, num_heads)'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 323}, page_content='302\\nAPPENDIX C\\nExercise solutions\\nChapter 4\\nExercise 4.1\\nWe can calculate the number of parameters in the feed forward and attention mod-\\nules as follows:\\nblock = TransformerBlock(GPT_CONFIG_124M)\\ntotal_params = sum(p.numel() for p in block.ff.parameters())\\nprint(f\"Total number of parameters in feed forward module: {total_params:,}\")\\ntotal_params = sum(p.numel() for p in block.att.parameters())\\nprint(f\"Total number of parameters in attention module: {total_params:,}\")\\nAs we can see, the feed forward module contains approximately twice as many param-\\neters as the attention module:\\nTotal number of parameters in feed forward module: 4,722,432\\nTotal number of parameters in attention module: 2,360,064\\nExercise 4.2\\nTo instantiate the other GPT model sizes, we can modify the configuration dictionary\\nas follows (here shown for GPT-2 XL):\\nGPT_CONFIG = GPT_CONFIG_124M.copy()\\nGPT_CONFIG[\"emb_dim\"] = 1600\\nGPT_CONFIG[\"n_layers\"] = 48\\nGPT_CONFIG[\"n_heads\"] = 25\\nmodel = GPTModel(GPT_CONFIG)\\nThen, reusing the code from section 4.6 to calculate the number of parameters and\\nRAM requirements, we find\\ngpt2-xl:\\nTotal number of parameters: 1,637,792,000\\nNumber of trainable parameters considering weight tying: 1,557,380,800\\nTotal size of the model: 6247.68 MB\\nExercise 4.3\\nThere are three distinct places in chapter 4 where we used dropout layers: the embed-\\nding layer, shortcut layer, and multi-head attention module. We can control the drop-\\nout rates for each of the layers by coding them separately in the config file and then\\nmodifying the code implementation accordingly. \\n The modified configuration is as follows:\\nGPT_CONFIG_124M = {\\n    \"vocab_size\": 50257,\\n    \"context_length\": 1024,\\n    \"emb_dim\": 768,'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 324}, page_content='303\\nChapter 4\\n    \"n_heads\": 12,\\n    \"n_layers\": 12,\\n    \"drop_rate_attn\": 0.1,     \\n    \"drop_rate_shortcut\": 0.1,     \\n    \"drop_rate_emb\": 0.1,     \\n    \"qkv_bias\": False\\n}\\nThe modified TransformerBlock and GPTModel look like\\nclass TransformerBlock(nn.Module):\\n    def __init__(self, cfg):\\n        super().__init__()\\n        self.att = MultiHeadAttention(\\n            d_in=cfg[\"emb_dim\"],\\n            d_out=cfg[\"emb_dim\"],\\n            context_length=cfg[\"context_length\"],\\n            num_heads=cfg[\"n_heads\"], \\n            dropout=cfg[\"drop_rate_attn\"],     \\n            qkv_bias=cfg[\"qkv_bias\"])\\n        self.ff = FeedForward(cfg)\\n        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\\n        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\\n        self.drop_shortcut = nn.Dropout(       \\n            cfg[\"drop_rate_shortcut\"]          \\n        )                                      \\n    def forward(self, x):\\n        shortcut = x\\n        x = self.norm1(x)\\n        x = self.att(x)\\n        x = self.drop_shortcut(x)\\n        x = x + shortcut\\n        shortcut = x\\n        x = self.norm2(x)\\n        x = self.ff(x)\\n        x = self.drop_shortcut(x)\\n        x = x + shortcut\\n        return x\\nclass GPTModel(nn.Module):\\n    def __init__(self, cfg):\\n        super().__init__()\\n        self.tok_emb = nn.Embedding(\\n            cfg[\"vocab_size\"], cfg[\"emb_dim\"]\\n        )\\n        self.pos_emb = nn.Embedding(\\n            cfg[\"context_length\"], cfg[\"emb_dim\"]\\n        )\\n        self.drop_emb = nn.Dropout(cfg[\"drop_rate_emb\"])   \\nDropout for multi-\\nhead attention\\nDropout for shortcut \\nconnections\\nDropout for \\nembedding layer\\nDropout for multi-\\nhead attention\\nDropout for shortcut \\nconnections\\nDropout for \\nembedding \\nlayer'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 325}, page_content='304\\nAPPENDIX C\\nExercise solutions\\n        self.trf_blocks = nn.Sequential(\\n            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\\n        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\\n        self.out_head = nn.Linear(\\n            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\\n        )\\n    def forward(self, in_idx):\\n        batch_size, seq_len = in_idx.shape\\n        tok_embeds = self.tok_emb(in_idx)\\n        pos_embeds = self.pos_emb(\\n            torch.arange(seq_len, device=in_idx.device)\\n        )\\n        x = tok_embeds + pos_embeds\\n        x = self.drop_emb(x)\\n        x = self.trf_blocks(x)\\n        x = self.final_norm(x)\\n        logits = self.out_head(x)\\n        return logitss\\nChapter 5\\nExercise 5.1\\nWe can print the number of times the token (or word) “pizza” is sampled using the\\nprint_sampled_tokens function we defined in this section. Let’s start with the code\\nwe defined in section 5.3.1.\\n The “pizza” token is sampled 0x if the temperature is 0 or 0.1, and it is sampled 32× if\\nthe temperature is scaled up to 5. The estimated probability is 32/1000 × 100% = 3.2%.\\n The actual probability is 4.3% and is contained in the rescaled softmax probability\\ntensor (scaled_probas[2][6]).\\nExercise 5.2\\nTop-k sampling and temperature scaling are settings that have to be adjusted based on\\nthe LLM and the desired degree of diversity and randomness in the output. \\n When using relatively small top-k values (e.g., smaller than 10) and when the tem-\\nperature is set below 1, the model’s output becomes less random and more determin-\\nistic. This setting is useful when we need the generated text to be more predictable,\\ncoherent, and closer to the most likely outcomes based on the training data. \\n Applications for such low k and temperature settings include generating formal\\ndocuments or reports where clarity and accuracy are most important. Other examples\\nof applications include technical analysis or code-generation tasks, where precision is\\ncrucial. Also, question answering and educational content require accurate answers\\nwhere a temperature below 1 is helpful.\\n On the other hand, larger top-k values (e.g., values in the range of 20 to 40) and\\ntemperature values above 1 are useful when using LLMs for brainstorming or generat-\\ning creative content, such as fiction.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 326}, page_content='305\\nChapter 5\\nExercise 5.3\\nThere are multiple ways to force deterministic behavior with the generate function:\\n1\\nSetting to top_k=None and applying no temperature scaling\\n2\\nSetting top_k=1\\nExercise 5.4\\nIn essence, we have to load the model and optimizer that we saved in the main chapter:\\ncheckpoint = torch.load(\"model_and_optimizer.pth\")\\nmodel = GPTModel(GPT_CONFIG_124M)\\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\\noptimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\\nThen, call the train_simple_function with num_epochs=1 to train the model for\\nanother epoch.\\nExercise 5.5\\nWe can use the following code to calculate the training and validation set losses of the\\nGPT model:\\ntrain_loss = calc_loss_loader(train_loader, gpt, device)\\nval_loss = calc_loss_loader(val_loader, gpt, device)\\nThe resulting losses for the 124-million parameter are as follows:\\nTraining loss: 3.754748503367106\\nValidation loss: 3.559617757797241\\nThe main observation is that the training and validation set performances are in the\\nsame ballpark. This can have multiple explanations:\\n1\\n“The Verdict” was not part of the pretraining dataset when OpenAI trained\\nGPT-2. Hence, the model is not explicitly overfitting to the training set and per-\\nforms similarly well on the training and validation set portions of “The Verdict.”\\n(The validation set loss is slightly lower than the training set loss, which is\\nunusual in deep learning. However, it’s likely due to random noise since the\\ndataset is relatively small. In practice, if there is no overfitting, the training and\\nvalidation set performances are expected to be roughly identical).\\n2\\n“The Verdict” was part of GPT-2’s training dataset. In this case, we can’t tell\\nwhether the model is overfitting the training data because the validation set\\nwould have been used for training as well. To evaluate the degree of overfitting,\\nwe’d need a new dataset generated after OpenAI finished training GPT-2 to\\nmake sure that it couldn’t have been part of the pretraining.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 327}, page_content='306\\nAPPENDIX C\\nExercise solutions\\nExercise 5.6\\nIn the main chapter, we experimented with the smallest GPT-2 model, which has only\\n124-million parameters. The reason was to keep the resource requirements as low as\\npossible. However, you can easily experiment with larger models with minimal code\\nchanges. For example, instead of loading the 1,558 million instead of 124 million\\nmodel weights in chapter 5, the only two lines of code that we have to change are the\\nfollowing:\\nhparams, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\\nmodel_name = \"gpt2-small (124M)\"\\nThe updated code is\\nhparams, params = download_and_load_gpt2(model_size=\"1558M\", models_dir=\"gpt2\")\\nmodel_name = \"gpt2-xl (1558M)\"\\nChapter 6\\nExercise 6.1\\nWe can pad the inputs to the maximum number of tokens the model supports by set-\\nting the max length to max_length = 1024 when initializing the datasets:\\ntrain_dataset = SpamDataset(..., max_length=1024, ...)\\nval_dataset = SpamDataset(..., max_length=1024, ...)\\ntest_dataset = SpamDataset(..., max_length=1024, ...)\\nHowever, the additional padding results in a substantially worse test accuracy of\\n78.33% (vs. the 95.67% in the main chapter).\\nExercise 6.2\\nInstead of fine-tuning just the final transformer block, we can fine-tune the entire\\nmodel by removing the following lines from the code:\\nfor param in model.parameters():\\n    param.requires_grad = False\\nThis modification results in a 1% improved test accuracy of 96.67% (vs. the 95.67% in\\nthe main chapter).\\nExercise 6.3\\nRather than fine-tuning the last output token, we can fine-tune the first output token\\nby changing model(input_batch)[:, -1, :] to model(input_batch)[:, 0, :] every-\\nwhere in the code.\\n As expected, since the first token contains less information than the last token, this\\nchange results in a substantially worse test accuracy of 75.00% (vs. the 95.67% in the\\nmain chapter).'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 328}, page_content='307\\nChapter 7\\nChapter 7\\nExercise 7.1\\nThe Phi-3 prompt format, which is shown in figure 7.4, looks like the following for a\\ngiven example input:\\n<user>\\nIdentify the correct spelling of the following word: \\'Occasion\\'\\n<assistant>\\nThe correct spelling is \\'Occasion\\'.\\nTo use this template, we can modify the format_input function as follows:\\ndef format_input(entry):\\n    instruction_text = (\\n        f\"<|user|>\\\\n{entry[\\'instruction\\']}\"\\n    )\\n    input_text = f\"\\\\n{entry[\\'input\\']}\" if entry[\"input\"] else \"\"\\n    return instruction_text + input_text\\nLastly, we also have to update the way we extract the generated response when we col-\\nlect the test set responses:\\nfor i, entry in tqdm(enumerate(test_data), total=len(test_data)):\\n    input_text = format_input(entry)\\n    tokenizer=tokenizer\\n    token_ids = generate(\\n        model=model,\\n        idx=text_to_token_ids(input_text, tokenizer).to(device),\\n        max_new_tokens=256,\\n        context_size=BASE_CONFIG[\"context_length\"],\\n        eos_id=50256\\n    )\\n    generated_text = token_ids_to_text(token_ids, tokenizer)\\n    response_text = (                      \\n        generated_text[len(input_text):]\\n        .replace(\"<|assistant|>:\", \"\")\\n        .strip()\\n    )\\n    test_data[i][\"model_response\"] = response_text\\nFine-tuning the model with the Phi-3 template is approximately 17% faster since it\\nresults in shorter model inputs. The score is close to 50, which is in the same ballpark\\nas the score we previously achieved with the Alpaca-style prompts.\\nExercise 7.2\\nTo mask out the instructions as shown in figure 7.13, we need to make slight modifica-\\ntions to the InstructionDataset class and custom_collate_fn function. We can\\nmodify the InstructionDataset class to collect the lengths of the instructions, which\\nNew: Adjust \\n###Response to \\n<|assistant|>'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 329}, page_content='308\\nAPPENDIX C\\nExercise solutions\\nwe will use in the collate function to locate the instruction content positions in the tar-\\ngets when we code the collate function, as follows:\\nclass InstructionDataset(Dataset):\\n    def __init__(self, data, tokenizer):\\n        self.data = data\\n        self.instruction_lengths = []    \\n        self.encoded_texts = []\\n        \\n        for entry in data:\\n            instruction_plus_input = format_input(entry)\\n            response_text = f\"\\\\n\\\\n### Response:\\\\n{entry[\\'output\\']}\"\\n            full_text = instruction_plus_input + response_text\\n            \\n            self.encoded_texts.append(\\n                tokenizer.encode(full_text)\\n            )\\n            instruction_length = ( \\n                len(tokenizer.encode(instruction_plus_input)\\n            )\\n            self.instruction_lengths.append(instruction_length)     \\n            \\n    def __getitem__(self, index):   \\n        return self.instruction_lengths[index], self.encoded_texts[index]\\n    def __len__(self):\\n        return len(self.data)\\nNext, we update the custom_collate_fn where each batch is now a tuple contain-\\ning (instruction_length, item) instead of just item due to the changes in the\\nInstructionDataset dataset. In addition, we now mask the corresponding instruc-\\ntion tokens in the target ID list:\\ndef custom_collate_fn(\\n    batch,\\n    pad_token_id=50256,\\n    ignore_index=-100,\\n    allowed_max_length=None,\\n    device=\"cpu\"\\n):\\n    batch_max_length = max(len(item)+1 for instruction_length, item in batch)\\n    inputs_lst, targets_lst = [], []         \\n    for instruction_length, item in batch:   \\n        new_item = item.copy()\\n        new_item += [pad_token_id]\\n        padded = (\\n            new_item + [pad_token_id] * (batch_max_length - len(new_item)\\n        )\\n        inputs = torch.tensor(padded[:-1])\\n        targets = torch.tensor(padded[1:])\\n        mask = targets == pad_token_id\\nSeparate list \\nfor instruction \\nlengths\\nCollects\\ninstruction\\nlengths\\nReturns both instruction\\nlengths and texts separately\\nbatch is now \\na tuple.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 330}, page_content='309\\nChapter 7\\n        indices = torch.nonzero(mask).squeeze()\\n        if indices.numel() > 1:\\n            targets[indices[1:]] = ignore_index\\n        targets[:instruction_length-1] = -100      \\n        \\n        if allowed_max_length is not None:\\n            inputs = inputs[:allowed_max_length]\\n            targets = targets[:allowed_max_length]\\n        \\n        inputs_lst.append(inputs)\\n        targets_lst.append(targets)\\n    inputs_tensor = torch.stack(inputs_lst).to(device)\\n    targets_tensor = torch.stack(targets_lst).to(device)\\n    return inputs_tensor, targets_tensor\\nWhen evaluating a model fine-tuned with this instruction masking method, it per-\\nforms slightly worse (approximately 4 points using the Ollama Llama 3 method from\\nchapter 7). This is consistent with observations in the “Instruction Tuning With Loss\\nOver Instructions” paper (https://arxiv.org/abs/2405.14394).\\nExercise 7.3\\nTo fine-tune the model on the original Stanford Alpaca dataset (https://github.com/\\ntatsu-lab/stanford_alpaca), we just have to change the file URL from\\nurl = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/\\n01_main-chapter-code/instruction-data.json\"\\nto\\nurl = \"https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/\\nalpaca_data.json\"\\nNote that the dataset contains 52,000 entries (50x more than in chapter 7), and the\\nentries are longer than the ones we worked with in chapter 7.\\n Thus, it’s highly recommended that the training be run on a GPU.\\n If you encounter out-of-memory errors, consider reducing the batch size from 8 to\\n4, 2, or 1. In addition to lowering the batch size, you may also want to consider lower-\\ning the allowed_max_length from 1024 to 512 or 256.\\n Below are a few examples from the Alpaca dataset, including the generated model\\nresponses:\\nExercise 7.4\\nTo instruction fine-tune the model using LoRA, use the relevant classes and functions\\nfrom appendix E:\\nfrom appendix_E import LoRALayer, LinearWithLoRA, replace_linear_with_lora\\nMasks all input and \\ninstruction tokens \\nin the targets'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 331}, page_content='310\\nAPPENDIX C\\nExercise solutions\\nNext, add the following lines of code below the model loading code in section 7.5:\\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\\nprint(f\"Total trainable parameters before: {total_params:,}\")\\nfor param in model.parameters():\\n    param.requires_grad = False\\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\\nprint(f\"Total trainable parameters after: {total_params:,}\")\\nreplace_linear_with_lora(model, rank=16, alpha=16)\\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\\nprint(f\"Total trainable LoRA parameters: {total_params:,}\")\\nmodel.to(device)\\nNote that, on an Nvidia L4 GPU, the fine-tuning with LoRA takes 1.30 min to run on\\nan L4. On the same GPU, the original code takes 1.80 minutes to run. So, LoRA is\\napproximately 28% faster in this case. The score, evaluated with the Ollama Llama 3\\nmethod from chapter 7, is around 50, which is in the same ballpark as the original\\nmodel.\\nAppendix A\\nExercise A.1\\nThe network has two inputs and two outputs. In addition, there are two hidden layers\\nwith 30 and 20 nodes, respectively. Programmatically, we can calculate the number of\\nparameters as follows:\\nmodel = NeuralNetwork(2, 2)\\nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\\nprint(\"Total number of trainable model parameters:\", num_params)\\nThis returns\\n752\\nWe can also calculate this manually:\\n\\uf0a1First hidden layer—2 inputs times 30 hidden units plus 30 bias units\\n\\uf0a1Second hidden layer—30 incoming units times 20 nodes plus 20 bias units\\n\\uf0a1Output layer—20 incoming nodes times 2 output nodes plus 2 bias units\\nThen, adding all the parameters in each layer results in 2 × 30 + 30 + 30 × 20 + 20 + 20\\n× 2 + 2 = 752.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 332}, page_content='311\\nAppendix A\\nExercise A.2\\nThe exact run-time results will be specific to the hardware used for this experiment. In\\nmy experiments, I observed significant speedups even for small matrix multiplications\\nas the following one when using a Google Colab instance connected to a V100 GPU:\\na = torch.rand(100, 200)\\nb = torch.rand(200, 300)\\n%timeit a@b\\nOn the CPU, this resulted in\\n63.8 μs ± 8.7 μs per loop\\nWhen executed on a GPU,\\na, b = a.to(\"cuda\"), b.to(\"cuda\")\\n%timeit a @ b\\nthe result was\\n13.8 μs ± 425 ns per loop\\nIn this case, on a V100, the computation was approximately four times faster.\\nExercise A.3\\nThe network has two inputs and two outputs. In addition, there are 2 hidden layers\\nwith 30 and 20 nodes, respectively. Programmatically, we can calculate the number of\\nparameters as follows:\\nmodel = NeuralNetwork(2, 2)\\nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\\nprint(\"Total number of trainable model parameters:\", num_params)\\nThis returns\\n752\\nWe can also calculate this manually as follows:\\n\\uf0a1First hidden layer: 2 inputs times 30 hidden units plus 30 bias units\\n\\uf0a1Second hidden layer: 30 incoming units times 20 nodes plus 20 bias units\\n\\uf0a1Output layer: 20 incoming nodes times 2 output nodes plus 2 bias units\\nThen, adding all the parameters in each layer results in 2×30+30 + 30×20+20 +\\n20×2+2 = 752.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 333}, page_content='312\\nAPPENDIX C\\nExercise solutions\\nExercise A.4\\nThe exact run-time results will be specific to the hardware used for this experiment. In\\nmy experiments, I observed significant speed-ups even for small matrix multiplications\\nwhen using a Google Colab instance connected to a V100 GPU:\\na = torch.rand(100, 200)\\nb = torch.rand(200, 300)\\n%timeit a@b\\nOn the CPU this resulted in\\n63.8 μs ± 8.7 μs per loop\\nWhen executed on a GPU\\na, b = a.to(\"cuda\"), b.to(\"cuda\")\\n%timeit a @ b\\nThe result was\\n13.8 μs ± 425 ns per loop\\nIn this case, on a V100, the computation was approximately four times faster.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 334}, page_content='313\\nappendix D\\nAdding bells and whistles\\nto the training loop\\nIn this appendix, we enhance the training function for the pretraining and fine-\\ntuning processes covered in chapters 5 to 7. In particular, it covers learning rate war-\\nmup, cosine decay, and gradient clipping. We then incorporate these techniques into\\nthe training function and pretrain an LLM. \\n To make the code self-contained, we reinitialize the model we trained in\\nchapter 5:\\nimport torch\\nfrom chapter04 import GPTModel\\nGPT_CONFIG_124M = {\\n    \"vocab_size\": 50257,         \\n    \"context_length\": 256,      \\n    \"emb_dim\": 768,          \\n    \"n_heads\": 12,           \\n    \"n_layers\": 12,          \\n    \"drop_rate\": 0.1,        \\n    \"qkv_bias\": False        \\n}\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\ntorch.manual_seed(123)\\nmodel = GPTModel(GPT_CONFIG_124M)\\nmodel.to(device)\\nmodel.eval()\\nAfter initializing the model, we need to initialize the data loaders. First, we load the\\n“The Verdict” short story:\\nVocabulary size\\nShortened context \\nlength (orig: 1024)\\nEmbedding dimension\\nNumber of attention heads\\nNumber of layers\\nDropout rate\\nQuery-key-value bias'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 335}, page_content='314\\nAPPENDIX D\\nAdding bells and whistles to the training loop\\nimport os\\nimport urllib.request\\nfile_path = \"the-verdict.txt\"\\nurl = (\\n    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/\"\\n    \"main/ch02/01_main-chapter-code/the-verdict.txt\"\\n)\\nif not os.path.exists(file_path):\\n    with urllib.request.urlopen(url) as response:\\n        text_data = response.read().decode(\\'utf-8\\')\\n    with open(file_path, \"w\", encoding=\"utf-8\") as file:\\n        file.write(text_data)\\nelse:\\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\\n        text_data = file.read()\\nNext, we load the text_data into the data loaders:\\nfrom previous_chapters import create_dataloader_v1\\ntrain_ratio = 0.90\\nsplit_idx = int(train_ratio * len(text_data))\\ntorch.manual_seed(123)\\ntrain_loader = create_dataloader_v1(\\n    text_data[:split_idx],\\n    batch_size=2,\\n    max_length=GPT_CONFIG_124M[\"context_length\"],\\n    stride=GPT_CONFIG_124M[\"context_length\"],\\n    drop_last=True,\\n    shuffle=True,\\n    num_workers=0\\n)\\nval_loader = create_dataloader_v1(\\n    text_data[split_idx:],\\n    batch_size=2,\\n    max_length=GPT_CONFIG_124M[\"context_length\"],\\n    stride=GPT_CONFIG_124M[\"context_length\"],\\n    drop_last=False,\\n    shuffle=False,\\n    num_workers=0\\n)\\nD.1\\nLearning rate warmup\\nImplementing a learning rate warmup can stabilize the training of complex models\\nsuch as LLMs. This process involves gradually increasing the learning rate from a very\\nlow initial value (initial_lr) to a maximum value specified by the user (peak_lr).\\nStarting the training with smaller weight updates decreases the risk of the model\\nencountering large, destabilizing updates during its training phase.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 336}, page_content='315\\nD.1\\nLearning rate warmup\\n Suppose we plan to train an LLM for 15 epochs, starting with an initial learning\\nrate of 0.0001 and increasing it to a maximum learning rate of 0.01: \\nn_epochs = 15\\ninitial_lr = 0.0001\\npeak_lr = 0.01\\nwarmup_steps = 20\\nThe number of warmup steps is usually set between 0.1% and 20% of the total num-\\nber of steps, which we can calculate as follows:\\ntotal_steps = len(train_loader) * n_epochs\\nwarmup_steps = int(0.2 * total_steps)      \\nprint(warmup_steps)\\nThis prints 27, meaning that we have 20 warmup steps to increase the initial learning\\nrate from 0.0001 to 0.01 in the first 27 training steps.\\n Next, we implement a simple training loop template to illustrate this warmup process:\\noptimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.1)\\nlr_increment = (peak_lr - initial_lr) / warmup_steps   \\nglobal_step = -1\\ntrack_lrs = []\\nfor epoch in range(n_epochs):   \\n    for input_batch, target_batch in train_loader:\\n        optimizer.zero_grad()\\n        global_step += 1\\n    \\n        if global_step < warmup_steps:            \\n            lr = initial_lr + global_step * lr_increment\\n        else:\\n            lr = peak_lr\\n        \\n        for param_group in optimizer.param_groups:   \\n            param_group[\"lr\"] = lr\\n        track_lrs.append(optimizer.param_groups[0][\"lr\"])  \\nAfter running the preceding code, we visualize how the learning rate was changed by\\nthe training loop to verify that the learning rate warmup works as intended:\\nimport matplotlib.pyplot as plt\\nplt.ylabel(\"Learning rate\")\\nplt.xlabel(\"Step\")\\ntotal_training_steps = len(train_loader) * n_epochs\\nplt.plot(range(total_training_steps), track_lrs);\\nplt.show()\\n20% warmup\\nThis increment is \\ndetermined by how \\nmuch we increase the \\ninital_lr in each of the \\n20 warmup steps.\\nExecutes a typical \\ntraining loop iterating \\nover the batches in the \\ntraining loader in each \\nepoch\\nUpdates the learning \\nrate if we are still in \\nthe warmup phase\\nApplies the\\ncalculated\\nlearning\\nrate to the\\noptimizer\\nIn a complete training loop, the loss and the model updates\\nwould be calculated, which are omitted here for simplicity.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 337}, page_content='316\\nAPPENDIX D\\nAdding bells and whistles to the training loop\\nThe resulting plot shows that the learning rate starts with a low value and increases for\\n20 steps until it reaches the maximum value after 20 steps (figure D.1).\\nNext, we will modify the learning rate further so that it decreases after reaching the\\nmaximum learning rate, which further helps improve the model training.\\nD.2\\nCosine decay\\nAnother widely adopted technique for training complex deep neural networks and\\nLLMs is cosine decay. This method modulates the learning rate throughout the training\\nepochs, making it follow a cosine curve after the warmup stage. \\n In its popular variant, cosine decay reduces (or decays) the learning rate to nearly\\nzero, mimicking the trajectory of a half-cosine cycle. The gradual learning decrease in\\ncosine decay aims to decelerate the pace at which the model updates its weights. This\\nis particularly important because it helps minimize the risk of overshooting the loss\\nminima during the training process, which is essential for ensuring the stability of the\\ntraining during its later phases.\\n We can modify the training loop template by adding cosine decay:\\nimport math\\nmin_lr = 0.1 * initial_lr\\ntrack_lrs = []\\nlr_increment = (peak_lr - initial_lr) / warmup_steps\\nglobal_step = -1\\nfor epoch in range(n_epochs):\\n    for input_batch, target_batch in train_loader:\\n        optimizer.zero_grad()\\n        global_step += 1\\n        if global_step < warmup_steps:                    \\n            lr = initial_lr + global_step * lr_increment  \\n        else:                                               \\n            progress = ((global_step - warmup_steps) / \\n                        (total_training_steps - warmup_steps))\\nFigure D.1\\nThe learning rate warmup \\nincreases the learning rate for the first \\n20 training steps. After 20 steps, the \\nlearning rate reaches the peak of 0.01 \\nand remains constant for the rest of \\nthe training.\\nApplies linear \\nwarmup\\nUses cosine \\nannealing \\nafter warmup'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 338}, page_content='317\\nD.3\\nGradient clipping\\n            lr = min_lr + (peak_lr - min_lr) * 0.5 * (\\n                1 + math.cos(math.pi * progress)\\n            )\\n        \\n        for param_group in optimizer.param_groups:\\n            param_group[\"lr\"] = lr\\n        track_lrs.append(optimizer.param_groups[0][\"lr\"])\\nAgain, to verify that the learning rate has changed as intended, we plot the learning rate:\\nplt.ylabel(\"Learning rate\")\\nplt.xlabel(\"Step\")\\nplt.plot(range(total_training_steps), track_lrs)\\nplt.show()\\nThe resulting learning rate plot shows that the learning rate starts with a linear warmup\\nphase, which increases for 20 steps until it reaches the maximum value after 20 steps.\\nAfter the 20 steps of linear warmup, cosine decay kicks in, reducing the learning rate\\ngradually until it reaches its minimum (figure D.2).\\nD.3\\nGradient clipping\\nGradient clipping is another important technique for enhancing stability during LLM\\ntraining. This method involves setting a threshold above which gradients are down-\\nscaled to a predetermined maximum magnitude. This process ensures that the updates\\nto the model’s parameters during backpropagation stay within a manageable range. \\n For example, applying the max_norm=1.0 setting within PyTorch’s clip_grad_\\nnorm_ function ensures that the norm of the gradients does not surpass 1.0. Here, the\\nterm “norm” signifies the measure of the gradient vector’s length, or magnitude,\\nwithin the model’s parameter space, specifically referring to the L2 norm, also known\\nas the Euclidean norm.\\n In mathematical terms, for a vector v composed of components v = [v1, v2, ..., vn],\\nthe L2 norm is \\nFigure D.2\\nThe first 20 steps of \\nlinear learning rate warmup are \\nfollowed by a cosine decay, which \\nreduces the learning rate in a half-\\ncosine cycle until it reaches its \\nminimum point at the end of training.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 339}, page_content=\"318\\nAPPENDIX D\\nAdding bells and whistles to the training loop\\nThis calculation method is also applied to matrices. For instance, consider a gradient\\nmatrix given by\\nIf we want to clip these gradients to a max_norm of 1, we first compute the L2 norm of\\nthese gradients, which is\\nGiven that |G|2 = 5 exceeds our max_norm of 1, we scale down the gradients to ensure\\ntheir norm equals exactly 1. This is achieved through a scaling factor, calculated as\\nmax_norm/|G|2 = 1/5. Consequently, the adjusted gradient matrix G' becomes\\nTo illustrate this gradient clipping process, we begin by initializing a new model and\\ncalculating the loss for a training batch, similar to the procedure in a standard train-\\ning loop:\\nfrom chapter05 import calc_loss_batch\\ntorch.manual_seed(123)\\nmodel = GPTModel(GPT_CONFIG_124M)\\nmodel.to(device)\\nloss = calc_loss_batch(input_batch, target_batch, model, device)\\nloss.backward()\\nUpon calling the .backward() method, PyTorch calculates the loss gradients and\\nstores them in a .grad attribute for each model weight (parameter) tensor.\\n To clarify the point, we can define the following find_highest_gradient utility\\nfunction to identify the highest gradient value by scanning all the .grad attributes of\\nthe model’s weight tensors after calling .backward():\\ndef find_highest_gradient(model):\\n    max_grad = None\\n    for param in model.parameters():\\n        if param.grad is not None:\\n            grad_values = param.grad.data.flatten()\\n            max_grad_param = grad_values.max()\\n            if max_grad is None or max_grad_param > max_grad:\\n                max_grad = max_grad_param\\n    return max_grad\\nprint(find_highest_gradient(model))\"),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 340}, page_content='319\\nD.4\\nThe modified training function\\nThe largest gradient value identified by the preceding code is \\ntensor(0.0411)\\nLet’s now apply gradient clipping and see how this affects the largest gradient value:\\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\\nprint(find_highest_gradient(model))\\nThe largest gradient value after applying the gradient clipping with the max norm of 1\\nis substantially smaller than before:\\ntensor(0.0185)\\nD.4\\nThe modified training function\\nFinally, we improve the train_model_simple training function (see chapter 5) by add-\\ning the three concepts introduced herein: linear warmup, cosine decay, and gradient\\nclipping. Together, these methods help stabilize LLM training.\\n The code, with the changes compared to the train_model_simple annotated, is as\\nfollows:\\nfrom chapter05 import evaluate_model, generate_and_print_sample\\ndef train_model(model, train_loader, val_loader, optimizer, device,\\n                n_epochs, eval_freq, eval_iter, start_context, tokenizer,\\n                warmup_steps, initial_lr=3e-05, min_lr=1e-6):\\n    train_losses, val_losses, track_tokens_seen, track_lrs = [], [], [], []\\n    tokens_seen, global_step = 0, -1\\n    peak_lr = optimizer.param_groups[0][\"lr\"]  \\n    total_training_steps = len(train_loader) * n_epochs    \\n    lr_increment = (peak_lr - initial_lr) / warmup_steps   \\n    for epoch in range(n_epochs):\\n        model.train()\\n        for input_batch, target_batch in train_loader:\\n            optimizer.zero_grad()\\n            global_step += 1\\n            if global_step < warmup_steps:  \\n                lr = initial_lr + global_step * lr_increment  \\n            else:\\n                progress = ((global_step - warmup_steps) / \\n                            (total_training_steps - warmup_steps))\\n                lr = min_lr + (peak_lr - min_lr) * 0.5 * (\\n                    1 + math.cos(math.pi * progress))\\nRetrieves the initial learning rate from the optimizer,\\nassuming we use it as the peak learning rate\\nCalculates the \\ntotal number of \\niterations in the \\ntraining process\\nCalculates the learning \\nrate increment during \\nthe warmup phase\\nAdjusts the \\nlearning rate \\nbased on the \\ncurrent phase \\n(warmup or \\ncosine \\nannealing)'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 341}, page_content='320\\nAPPENDIX D\\nAdding bells and whistles to the training loop\\n            for param_group in optimizer.param_groups:  \\n                param_group[\"lr\"] = lr\\n            track_lrs.append(lr)\\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\\n            loss.backward()\\n            if global_step > warmup_steps:        \\n                torch.nn.utils.clip_grad_norm_(\\n                    model.parameters(), max_norm=1.0\\n                )\\n                     \\n            optimizer.step() \\n            tokens_seen += input_batch.numel()\\n            if global_step % eval_freq == 0:\\n                train_loss, val_loss = evaluate_model(\\n                    model, train_loader, val_loader,\\n                    device, eval_iter\\n                )\\n                train_losses.append(train_loss)\\n                val_losses.append(val_loss)\\n                track_tokens_seen.append(tokens_seen)\\n                print(f\"Ep {epoch+1} (Iter {global_step:06d}): \"\\n                      f\"Train loss {train_loss:.3f}, \"\\n                      f\"Val loss {val_loss:.3f}\"\\n                )\\n                \\n        generate_and_print_sample(\\n            model, tokenizer, device, start_context\\n        )\\n    return train_losses, val_losses, track_tokens_seen, track_lrs\\nAfter defining the train_model function, we can use it in a similar fashion to train the\\nmodel compared to the train_model_simple method we used for pretraining:\\nimport tiktoken\\ntorch.manual_seed(123)\\nmodel = GPTModel(GPT_CONFIG_124M)\\nmodel.to(device)\\npeak_lr = 5e-4\\noptimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.1)\\ntokenizer = tiktoken.get_encoding(\"gpt2\")\\nn_epochs = 15\\ntrain_losses, val_losses, tokens_seen, lrs = train_model(\\n    model, train_loader, val_loader, optimizer, device, n_epochs=n_epochs,\\n    eval_freq=5, eval_iter=1, start_context=\"Every effort moves you\",\\n    tokenizer=tokenizer, warmup_steps=warmup_steps, \\n    initial_lr=1e-5, min_lr=1e-5\\n)\\nApplies the calculated \\nlearning rate to the optimizer\\nApplies gradient clipping \\nafter the warmup phase \\nto avoid exploding \\ngradients\\nEverything below here \\nremains unchanged \\ncompared to the \\ntrain_model_simple \\nfunction used in \\nchapter 5.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 342}, page_content='321\\nD.4\\nThe modified training function\\nThe training will take about 5 minutes to complete on a MacBook Air or similar lap-\\ntop and prints the following outputs:\\nEp 1 (Iter 000000): Train loss 10.934, Val loss 10.939\\nEp 1 (Iter 000005): Train loss 9.151, Val loss 9.461 \\nEvery effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\\nEp 2 (Iter 000010): Train loss 7.949, Val loss 8.184 \\nEp 2 (Iter 000015): Train loss 6.362, Val loss 6.876 \\nEvery effort moves you,,,,,,,,,,,,,,,,,,, the,,,,,,,,, the,,,,,,,,,,, \\nthe,,,,,,,, \\n... \\nEp 15 (Iter 000130): Train loss 0.041, Val loss 6.915 \\nEvery effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him \\nvindicated--and by me!\"  He laughed again, and threw back his head to look up \\nat the sketch of the donkey. \"There were days when I\\nLike pretraining, the model begins to overfit after a few epochs since it is a very small\\ndataset, and we iterate over it multiple times. Nonetheless, we can see that the func-\\ntion is working since it minimizes the training set loss.\\n Readers are encouraged to train the model on a larger text dataset and compare\\nthe results obtained with this more sophisticated training function to the results that\\ncan be obtained with the train_model_simple function.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 343}, page_content='322\\nappendix E\\nParameter-efficient\\nfine-tuning with LoRA\\nLow-rank adaptation (LoRA) is one of the most widely used techniques for parameter-\\nefficient fine-tuning. The following discussion is based on the spam classification fine-\\ntuning example given in chapter 6. However, LoRA fine-tuning is also applicable to\\nthe supervised instruction fine-tuning discussed in chapter 7.\\nE.1\\nIntroduction to LoRA\\nLoRA is a technique that adapts a pretrained model to better suit a specific, often\\nsmaller dataset by adjusting only a small subset of the model’s weight parameters.\\nThe “low-rank” aspect refers to the mathematical concept of limiting model adjust-\\nments to a smaller dimensional subspace of the total weight parameter space. This\\neffectively captures the most influential directions of the weight parameter changes\\nduring training. The LoRA method is useful and popular because it enables effi-\\ncient fine-tuning of large models on task-specific data, significantly cutting down\\non the computational costs and resources usually required for fine-tuning.\\n Suppose a large weight matrix W is associated with a specific layer. LoRA can be\\napplied to all linear layers in an LLM. However, we focus on a single layer for illus-\\ntration purposes.\\n When training deep neural networks, during backpropagation, we learn a ΔW\\nmatrix, which contains information on how much we want to update the original\\nweight parameters to minimize the loss function during training. Hereafter, I use\\nthe term “weight” as shorthand for the model’s weight parameters.\\n In regular training and fine-tuning, the weight update is defined as follows:'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 344}, page_content='323\\nE.1\\nIntroduction to LoRA\\nThe LoRA method, proposed by Hu et al. (https://arxiv.org/abs/2106.09685), offers\\na more efficient alternative to computing the weight updates ΔW by learning an\\napproximation of it:\\nwhere A and B are two matrices much smaller than W, and AB represents the matrix\\nmultiplication product between A and B. \\n Using LoRA, we can then reformulate the weight update we defined earlier:\\nFigure E.1 illustrates the weight update formulas for full fine-tuning and LoRA side\\nby side.\\nIf you paid close attention, you might have noticed that the visual representations of\\nfull fine-tuning and LoRA in figure E.1 differ slightly from the earlier presented for-\\nmulas. This variation is attributed to the distributive law of matrix multiplication,\\nwhich allows us to separate the original and updated weights rather than combine\\nthem. For example, in the case of regular fine-tuning with x as the input data, we can\\nexpress the computation as \\nPretrained\\nweights\\nW\\nWeight\\nupdate\\nΔW\\nOutputs\\nPretrained\\nweights\\nW\\nInputs\\nd\\nInputs\\nWeight update in regular ﬁne-tuning\\nWeight update in LoRA\\nOutputs\\nThe inner dimension r\\nis a hyperparameter.\\nLoRA matrices\\nand\\nA\\nB\\napproximate the weight\\nupdate matrix\\n.\\nΔW\\nThe weight\\nparameters in any\\nof the neural\\nnetwork layers\\nThe values by which the\\nweights are updated\\nduring training\\nFigure E.1\\nA comparison between weight update methods: regular fine-tuning and LoRA. Regular fine-tuning \\ninvolves updating the pretrained weight matrix W directly with ΔW (left). LoRA uses two smaller matrices, A and \\nB, to approximate ΔW, where the product AB is added to W, and r denotes the inner dimension, a tunable \\nhyperparameter (right).\\nPretrained\\nweights\\nW\\nWeight\\nupdate\\nΔW\\nOutputs\\nPretrained\\nweights\\nW\\nInputs\\nd\\nInputs\\nWeight update in regular ﬁne-tuning\\nWeight update in LoRA\\nOutputs\\nThe inner dimension r\\nis a hyperparameter.\\nLoRA matrices\\nand\\nA\\nB\\napproximate the weight\\nupdate matrix\\n.\\nΔW\\nThe weight\\nparameters in any\\nof the neural\\nnetwork layers\\nThe values by which the\\nweights are updated\\nduring training'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 345}, page_content='324\\nAPPENDIX E\\nParameter-efficient fine-tuning with LoRA\\nSimilarly, we can write the following for LoRA:\\nBesides reducing the number of weights to update during training, the ability to keep\\nthe LoRA weight matrices separate from the original model weights makes LoRA even\\nmore useful in practice. Practically, this allows for the pretrained model weights to\\nremain unchanged, with the LoRA matrices being applied dynamically after training\\nwhen using the model.\\n Keeping the LoRA weights separate is very useful in practice because it enables\\nmodel customization without needing to store multiple complete versions of an LLM.\\nThis reduces storage requirements and improves scalability, as only the smaller LoRA\\nmatrices need to be adjusted and saved when we customize LLMs for each specific cus-\\ntomer or application.\\n Next, let’s see how LoRA can be used to fine-tune an LLM for spam classification,\\nsimilar to the fine-tuning example in chapter 6.\\nE.2\\nPreparing the dataset\\nBefore applying LoRA to the spam classification example, we must load the dataset\\nand pretrained model we will work with. The code here repeats the data preparation\\nfrom chapter 6. (Instead of repeating the code, we could open and run the chapter 6\\nnotebook and insert the LoRA code from section E.4 there.)\\n First, we download the dataset and save it as CSV files.\\nfrom pathlib import Path\\nimport pandas as pd\\nfrom ch06 import (\\n    download_and_unzip_spam_data,\\n    create_balanced_dataset,\\n    random_split\\n)\\nurl = \\\\ \\n\"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\\nzip_path = \"sms_spam_collection.zip\"\\nextracted_path = \"sms_spam_collection\"\\ndata_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\\ndownload_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\\ndf = pd.read_csv(\\n    data_file_path, sep=\"\\\\t\", header=None, names=[\"Label\", \"Text\"]\\n)\\nbalanced_df = create_balanced_dataset(df)\\nbalanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\\ntrain_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\\ntrain_df.to_csv(\"train.csv\", index=None)\\nListing E.1\\nDownloading and preparing the dataset'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 346}, page_content='325\\nE.2\\nPreparing the dataset\\nvalidation_df.to_csv(\"validation.csv\", index=None)\\ntest_df.to_csv(\"test.csv\", index=None)\\nNext, we create the SpamDataset instances.\\nimport torch\\nfrom torch.utils.data import Dataset\\nimport tiktoken\\nfrom chapter06 import SpamDataset\\ntokenizer = tiktoken.get_encoding(\"gpt2\")\\ntrain_dataset = SpamDataset(\"train.csv\", max_length=None, \\n    tokenizer=tokenizer\\n)\\nval_dataset = SpamDataset(\"validation.csv\", \\n    max_length=train_dataset.max_length, tokenizer=tokenizer\\n)\\ntest_dataset = SpamDataset(\\n    \"test.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer\\n)\\nAfter creating the PyTorch dataset objects, we instantiate the data loaders.\\nfrom torch.utils.data import DataLoader\\nnum_workers = 0\\nbatch_size = 8\\ntorch.manual_seed(123)\\ntrain_loader = DataLoader(\\n    dataset=train_dataset,\\n    batch_size=batch_size,\\n    shuffle=True,\\n    num_workers=num_workers,\\n    drop_last=True,\\n)\\nval_loader = DataLoader(\\n    dataset=val_dataset,\\n    batch_size=batch_size,\\n    num_workers=num_workers,\\n    drop_last=False,\\n)\\ntest_loader = DataLoader(\\n    dataset=test_dataset,\\n    batch_size=batch_size,\\n    num_workers=num_workers,\\n    drop_last=False,\\n)\\nListing E.2\\nInstantiating PyTorch datasets\\nListing E.3\\nCreating PyTorch data loaders'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 347}, page_content='326\\nAPPENDIX E\\nParameter-efficient fine-tuning with LoRA\\nAs a verification step, we iterate through the data loaders and check that the batches\\ncontain eight training examples each, where each training example consists of 120\\ntokens:\\nprint(\"Train loader:\")\\nfor input_batch, target_batch in train_loader:\\n    pass\\nprint(\"Input batch dimensions:\", input_batch.shape)\\nprint(\"Label batch dimensions\", target_batch.shape)\\nThe output is \\nTrain loader:\\nInput batch dimensions: torch.Size([8, 120])\\nLabel batch dimensions torch.Size([8])\\nLastly, we print the total number of batches in each dataset:\\nprint(f\"{len(train_loader)} training batches\")\\nprint(f\"{len(val_loader)} validation batches\")\\nprint(f\"{len(test_loader)} test batches\")\\nIn this case, we have the following number of batches per dataset:\\n130 training batches\\n19 validation batches\\n38 test batches\\nE.3\\nInitializing the model\\nWe repeat the code from chapter 6 to load and prepare the pretrained GPT model.\\nWe begin by downloading the model weights and loading them into the GPTModel\\nclass.\\nfrom gpt_download import download_and_load_gpt2\\nfrom chapter04 import GPTModel\\nfrom chapter05 import load_weights_into_gpt\\nCHOOSE_MODEL = \"gpt2-small (124M)\"\\nINPUT_PROMPT = \"Every effort moves\"\\nBASE_CONFIG = {\\n    \"vocab_size\": 50257,        \\n    \"context_length\": 1024,     \\n    \"drop_rate\": 0.0,           \\n    \"qkv_bias\": True            \\n}\\nListing E.4\\nLoading a pretrained GPT model\\nVocabulary size\\nContext length\\nDropout rate\\nQuery-key-value bias'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 348}, page_content='327\\nE.3\\nInitializing the model\\nmodel_configs = {\\n    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\\n    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\\n    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\\n    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\\n}\\nBASE_CONFIG.update(model_configs[CHOOSE_MODEL])\\nmodel_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\\nsettings, params = download_and_load_gpt2(\\n    model_size=model_size, models_dir=\"gpt2\"\\n)\\nmodel = GPTModel(BASE_CONFIG)\\nload_weights_into_gpt(model, params)\\nmodel.eval()\\nTo ensure that the model was loaded corrected, let’s double-check that it generates\\ncoherent text:\\nfrom chapter04 import generate_text_simple\\nfrom chapter05 import text_to_token_ids, token_ids_to_text\\ntext_1 = \"Every effort moves you\"\\ntoken_ids = generate_text_simple(\\n    model=model,\\n    idx=text_to_token_ids(text_1, tokenizer),\\n    max_new_tokens=15,\\n    context_size=BASE_CONFIG[\"context_length\"]\\n)\\nprint(token_ids_to_text(token_ids, tokenizer))\\nThe following output shows that the model generates coherent text, which is an indi-\\ncator that the model weights are loaded correctly:\\nEvery effort moves you forward.\\nThe first step is to understand the importance of your work\\nNext, we prepare the model for classification fine-tuning, similar to chapter 6, where\\nwe replace the output layer:\\ntorch.manual_seed(123)\\nnum_classes = 2\\nmodel.out_head = torch.nn.Linear(in_features=768, out_features=num_classes)\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\nmodel.to(device)\\nLastly, we calculate the initial classification accuracy of the not-fine-tuned model (we\\nexpect this to be around 50%, which means that the model is not able to distinguish\\nbetween spam and nonspam messages yet reliably):'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 349}, page_content='328\\nAPPENDIX E\\nParameter-efficient fine-tuning with LoRA\\nfrom chapter06 import calc_accuracy_loader\\ntorch.manual_seed(123)\\ntrain_accuracy = calc_accuracy_loader(\\n    train_loader, model, device, num_batches=10\\n)\\nval_accuracy = calc_accuracy_loader(\\n    val_loader, model, device, num_batches=10\\n)\\ntest_accuracy = calc_accuracy_loader(\\n    test_loader, model, device, num_batches=10\\n)\\nprint(f\"Training accuracy: {train_accuracy*100:.2f}%\")\\nprint(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\\nprint(f\"Test accuracy: {test_accuracy*100:.2f}%\")\\nThe initial prediction accuracies are \\nTraining accuracy: 46.25%\\nValidation accuracy: 45.00%\\nTest accuracy: 48.75%\\nE.4\\nParameter-efficient fine-tuning with LoRA\\nNext, we modify and fine-tune the LLM using LoRA. We begin by initializing a LoRA-\\nLayer that creates the matrices A and B, along with the alpha scaling factor and the\\nrank (r) setting. This layer can accept an input and compute the corresponding out-\\nput, as illustrated in figure E.2.\\nIn code, this LoRA layer can be implemented as follows.\\n \\n \\n \\n \\nInputs\\nOutputs\\nThe inner dimension r\\nis a hyperparameter.\\nInitialize LoRA matrices\\nA\\nB\\nand\\n, which approximate\\nthe weight update matrix ΔW.\\nFigure E.2\\nThe LoRA matrices A and B are \\napplied to the layer inputs and are involved in \\ncomputing the model outputs. The inner \\ndimension r of these matrices serves as a \\nsetting that adjusts the number of trainable \\nparameters by varying the sizes of A and B.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 350}, page_content='329\\nE.4\\nParameter-efficient fine-tuning with LoRA\\nimport math\\nclass LoRALayer(torch.nn.Module):\\n    def __init__(self, in_dim, out_dim, rank, alpha):\\n        super().__init__()\\n        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\\n        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))   \\n        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\\n        self.alpha = alpha\\n    def forward(self, x):\\n        x = self.alpha * (x @ self.A @ self.B)\\n        return x\\nThe rank governs the inner dimension of matrices A and B. Essentially, this setting\\ndetermines the number of extra parameters introduced by LoRA, which creates bal-\\nance between the adaptability of the model and its efficiency via the number of\\nparameters used.\\n The other important setting, alpha, functions as a scaling factor for the output\\nfrom the low-rank adaptation. It primarily dictates the degree to which the output\\nfrom the adapted layer can affect the original layer’s output. This can be seen as a way\\nto regulate the effect of the low-rank adaptation on the layer’s output. The LoRALayer\\nclass we have implemented so far enables us to transform the inputs of a layer.\\n In LoRA, the typical goal is to substitute existing Linear layers, allowing weight\\nupdates to be applied directly to the pre-existing pretrained weights, as illustrated in\\nfigure E.3.\\nListing E.5\\nImplementing a LoRA layer\\nThe same initialization\\nused for Linear layers\\nin PyTorch\\nThe original weights\\nin a given layer of\\na model\\nComputing the outputs involves\\nboth the original weights and\\nthe LoRA weights\\nPretrained\\nweights\\nW\\nInputs\\nOutputs\\nLoRA matrices\\nand\\n,\\nA\\nB which\\napproximate the weight\\nupdate matrix ΔW\\nFigure E.3\\nThe integration of LoRA into a model layer. The original pretrained weights (W) \\nof a layer are combined with the outputs from LoRA matrices (A and B), which approximate \\nthe weight update matrix (ΔW). The final output is calculated by adding the output of the \\nadapted layer (using LoRA weights) to the original output.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 351}, page_content='330\\nAPPENDIX E\\nParameter-efficient fine-tuning with LoRA\\nTo integrate the original Linear layer weights, we now create a LinearWithLoRA layer.\\nThis layer utilizes the previously implemented LoRALayer and is designed to replace\\nexisting Linear layers within a neural network, such as the self-attention modules or\\nfeed-forward modules in the GPTModel.\\nclass LinearWithLoRA(torch.nn.Module):\\n    def __init__(self, linear, rank, alpha):\\n        super().__init__()\\n        self.linear = linear\\n        self.lora = LoRALayer(\\n            linear.in_features, linear.out_features, rank, alpha\\n        )\\n    def forward(self, x):\\n        return self.linear(x) + self.lora(x)\\nThis code combines a standard Linear layer with the LoRALayer. The forward method\\ncomputes the output by adding the results from the original linear layer and the\\nLoRA layer.\\n Since the weight matrix B (self.B in LoRALayer) is initialized with zero values, the\\nproduct of matrices A and B results in a zero matrix. This ensures that the multiplica-\\ntion does not alter the original weights, as adding zero does not change them.\\n To apply LoRA to the earlier defined GPTModel, we introduce a replace_linear_\\nwith_lora function. This function will swap all existing Linear layers in the model\\nwith the newly created LinearWithLoRA layers:\\ndef replace_linear_with_lora(model, rank, alpha):\\n    for name, module in model.named_children():\\n        if isinstance(module, torch.nn.Linear):    \\n            setattr(model, name, LinearWithLoRA(module, rank, alpha))\\n        else:   \\n            replace_linear_with_lora(module, rank, alpha)\\nWe have now implemented all the necessary code to replace the Linear layers in the\\nGPTModel with the newly developed LinearWithLoRA layers for parameter-efficient\\nfine-tuning. Next, we will apply the LinearWithLoRA upgrade to all Linear layers\\nfound in the multihead attention, feed-forward modules, and the output layer of the\\nGPTModel, as shown in figure E.4.\\n \\n \\n \\n \\nListing E.6\\nReplacing a LinearWithLora layer with Linear layers\\nReplaces the Linear layer \\nwith LinearWithLoRA\\nRecursively applies the same\\nfunction to child modules'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 352}, page_content='331\\nE.4\\nParameter-efficient fine-tuning with LoRA\\nBefore we apply the LinearWithLoRA layer upgrades, we first freeze the original model\\nparameters:\\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\\nprint(f\"Total trainable parameters before: {total_params:,}\")\\nfor param in model.parameters():\\n    param.requires_grad = False\\nWe update the\\nlayers\\nLinear\\nwith\\nlayers.\\nLinearWithLoRA\\nThe GPT model we\\nimplemented and\\nused in previous\\nchapters.\\nGPT\\nmodel\\nMasked multihead\\nattention\\nLayerNorm 2\\nFeed forward\\nLayerNorm 1\\nDropout\\nDropout\\nToken embedding layer\\nTokenized text\\nFinal LayerNorm\\nLinear output layer\\n12\\nDropout\\nPositional embedding layer\\nEvery effort moves you\\nTransformer\\nblock\\nFigure E.4\\nThe architecture of the GPT model. It highlights the parts of the model where Linear layers are \\nupgraded to LinearWithLoRA layers for parameter-efficient fine-tuning.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 353}, page_content='332\\nAPPENDIX E\\nParameter-efficient fine-tuning with LoRA\\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\\nprint(f\"Total trainable parameters after: {total_params:,}\")\\nNow, we can see that none of the 124 million model parameters are trainable:\\nTotal trainable parameters before: 124,441,346\\nTotal trainable parameters after: 0\\nNext, we use the replace_linear_with_lora to replace the Linear layers:\\nreplace_linear_with_lora(model, rank=16, alpha=16)\\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\\nprint(f\"Total trainable LoRA parameters: {total_params:,}\")\\nAfter adding the LoRA layers, the number of trainable parameters is as follows:\\nTotal trainable LoRA parameters: 2,666,528\\nAs we can see, we reduced the number of trainable parameters by almost 50× when\\nusing LoRA. A rank and alpha of 16 are good default choices, but it is also common to\\nincrease the rank parameter, which in turn increases the number of trainable parame-\\nters. Alpha is usually chosen to be half, double, or equal to the rank.\\n Let’s verify that the layers have been modified as intended by printing the model\\narchitecture:\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\nmodel.to(device)\\nprint(model)\\nThe output is\\nGPTModel(\\n  (tok_emb): Embedding(50257, 768)\\n  (pos_emb): Embedding(1024, 768)\\n  (drop_emb): Dropout(p=0.0, inplace=False)\\n  (trf_blocks): Sequential(\\n    ...\\n    (11): TransformerBlock(\\n      (att): MultiHeadAttention(\\n        (W_query): LinearWithLoRA(\\n          (linear): Linear(in_features=768, out_features=768, bias=True)\\n          (lora): LoRALayer()\\n        )\\n        (W_key): LinearWithLoRA(\\n          (linear): Linear(in_features=768, out_features=768, bias=True)\\n          (lora): LoRALayer()\\n        )\\n        (W_value): LinearWithLoRA(\\n          (linear): Linear(in_features=768, out_features=768, bias=True)\\n          (lora): LoRALayer()\\n        )'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 354}, page_content='333\\nE.4\\nParameter-efficient fine-tuning with LoRA\\n        (out_proj): LinearWithLoRA(\\n          (linear): Linear(in_features=768, out_features=768, bias=True)\\n          (lora): LoRALayer()\\n        )\\n        (dropout): Dropout(p=0.0, inplace=False)\\n      )\\n      (ff): FeedForward(\\n        (layers): Sequential(\\n          (0): LinearWithLoRA(\\n            (linear): Linear(in_features=768, out_features=3072, bias=True)\\n            (lora): LoRALayer()\\n          )\\n          (1): GELU()\\n          (2): LinearWithLoRA(\\n            (linear): Linear(in_features=3072, out_features=768, bias=True)\\n            (lora): LoRALayer()\\n          )\\n        )\\n      )\\n      (norm1): LayerNorm()\\n      (norm2): LayerNorm()\\n      (drop_resid): Dropout(p=0.0, inplace=False)\\n    )\\n  )\\n  (final_norm): LayerNorm()\\n  (out_head): LinearWithLoRA(\\n    (linear): Linear(in_features=768, out_features=2, bias=True)\\n    (lora): LoRALayer()\\n  )\\n)\\nThe model now includes the new LinearWithLoRA layers, which themselves consist of\\nthe original Linear layers, set to nontrainable, and the new LoRA layers, which we will\\nfine-tune.\\n Before we begin fine-tuning the model, let’s calculate the initial classification\\naccuracy:\\ntorch.manual_seed(123)\\ntrain_accuracy = calc_accuracy_loader(\\n    train_loader, model, device, num_batches=10\\n)\\nval_accuracy = calc_accuracy_loader(\\n    val_loader, model, device, num_batches=10\\n)\\ntest_accuracy = calc_accuracy_loader(\\n    test_loader, model, device, num_batches=10\\n)\\nprint(f\"Training accuracy: {train_accuracy*100:.2f}%\")\\nprint(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\\nprint(f\"Test accuracy: {test_accuracy*100:.2f}%\")'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 355}, page_content='334\\nAPPENDIX E\\nParameter-efficient fine-tuning with LoRA\\nThe resulting accuracy values are \\nTraining accuracy: 46.25%\\nValidation accuracy: 45.00%\\nTest accuracy: 48.75%\\nThese accuracy values are identical to the values from chapter 6. This result occurs\\nbecause we initialized the LoRA matrix B with zeros. Consequently, the product of\\nmatrices AB results in a zero matrix. This ensures that the multiplication does not\\nalter the original weights since adding zero does not change them.\\n Now let’s move on to the exciting part—fine-tuning the model using the training\\nfunction from chapter 6. The training takes about 15 minutes on an M3 MacBook Air\\nlaptop and less than half a minute on a V100 or A100 GPU.\\nimport time\\nfrom chapter06 import train_classifier_simple\\nstart_time = time.time()\\ntorch.manual_seed(123)\\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\\nnum_epochs = 5\\ntrain_losses, val_losses, train_accs, val_accs, examples_seen = \\\\\\n    train_classifier_simple(\\n        model, train_loader, val_loader, optimizer, device,\\n        num_epochs=num_epochs, eval_freq=50, eval_iter=5,\\n        tokenizer=tokenizer\\n    )\\nend_time = time.time()\\nexecution_time_minutes = (end_time - start_time) / 60\\nprint(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\\nThe output we see during the training is\\nEp 1 (Step 000000): Train loss 3.820, Val loss 3.462 \\nEp 1 (Step 000050): Train loss 0.396, Val loss 0.364 \\nEp 1 (Step 000100): Train loss 0.111, Val loss 0.229 \\nTraining accuracy: 97.50% | Validation accuracy: 95.00% \\nEp 2 (Step 000150): Train loss 0.135, Val loss 0.073 \\nEp 2 (Step 000200): Train loss 0.008, Val loss 0.052 \\nEp 2 (Step 000250): Train loss 0.021, Val loss 0.179 \\nTraining accuracy: 97.50% | Validation accuracy: 97.50%\\nEp 3 (Step 000300): Train loss 0.096, Val loss 0.080 \\nEp 3 (Step 000350): Train loss 0.010, Val loss 0.116 \\nTraining accuracy: 97.50% | Validation accuracy: 95.00% \\nEp 4 (Step 000400): Train loss 0.003, Val loss 0.151 \\nEp 4 (Step 000450): Train loss 0.008, Val loss 0.077 \\nEp 4 (Step 000500): Train loss 0.001, Val loss 0.147 \\nTraining accuracy: 100.00% | Validation accuracy: 97.50%\\nListing E.7\\nFine-tuning a model with LoRA layers'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 356}, page_content='335\\nE.4\\nParameter-efficient fine-tuning with LoRA\\nEp 5 (Step 000550): Train loss 0.007, Val loss 0.094 \\nEp 5 (Step 000600): Train loss 0.000, Val loss 0.056 \\nTraining accuracy: 100.00% | Validation accuracy: 97.50%\\nTraining completed in 12.10 minutes. \\nTraining the model with LoRA took longer than training it without LoRA (see chap-\\nter 6) because the LoRA layers introduce an additional computation during the for-\\nward pass. However, for larger models, where backpropagation becomes more costly,\\nmodels typically train faster with LoRA than without it.\\n As we can see, the model received perfect training and very high validation accuracy.\\nLet’s also visualize the loss curves to better see whether the training has converged:\\nfrom chapter06 import plot_values\\nepochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\\nexamples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\\nplot_values(\\n    epochs_tensor, examples_seen_tensor, \\n    train_losses, val_losses, label=\"loss\"\\n)\\nFigure E.5 plots the results.\\nIn addition to evaluating the model based on the loss curves, let’s also calculate the\\naccuracies on the full training, validation, and test set (during the training, we\\napproximated the training and validation set accuracies from five batches via the\\neval_iter=5 setting):\\ntrain_accuracy = calc_accuracy_loader(train_loader, model, device)\\nval_accuracy = calc_accuracy_loader(val_loader, model, device)\\ntest_accuracy = calc_accuracy_loader(test_loader, model, device)\\nprint(f\"Training accuracy: {train_accuracy*100:.2f}%\")\\nprint(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\\nprint(f\"Test accuracy: {test_accuracy*100:.2f}%\")\\nFigure E.5\\nThe training and \\nvalidation loss curves over six epochs \\nfor a machine learning model. \\nInitially, both training and validation \\nloss decrease sharply and then they \\nlevel off, indicating the model is \\nconverging, which means that it is \\nnot expected to improve noticeably \\nwith further training.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 357}, page_content='336\\nAPPENDIX E\\nParameter-efficient fine-tuning with LoRA\\nThe resulting accuracy values are \\nTraining accuracy: 100.00%\\nValidation accuracy: 96.64%\\nTest accuracy: 98.00%\\nThese results show that the model performs well across training, validation, and test\\ndatasets. With a training accuracy of 100%, the model has perfectly learned the train-\\ning data. However, the slightly lower validation and test accuracies (96.64% and\\n97.33%, respectively) suggest a small degree of overfitting, as the model does not gen-\\neralize quite as well on unseen data compared to the training set. Overall, the results\\nare very impressive, considering we fine-tuned only a relatively small number of model\\nweights (2.7 million LoRA weights instead of the original 124 million model weights).'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 358}, page_content='337\\nindex\\nSymbols\\n[BOS] (beginning of sequence) token 32\\n[EOS] (end of sequence) token 32\\n[PAD] (padding) token 32\\n@ operator 261\\n%timeit command 282\\n<|endoftext|> token 34\\n<|unk|> tokens 29–31, 34\\n== comparison operator 277\\nNumerics\\n04_preference-tuning-with-dpo folder 247\\n124M parameter 161\\n355M parameter 227\\nA\\nAdamW optimizer 148, 294\\nAI (artificial intelligence) 252\\nallowed_max_length 224, 233, 309\\nAlpaca dataset 233, 296\\nalpha scaling factor 328\\narchitectures, transformer 7–10\\nargmax function 134, 152–155, 190, 277\\narXiv 248\\nassign utility function 165\\nattention mechanisms\\ncausal 74–82\\ncoding 50, 54\\nimplementing self-attention with trainable \\nweights 64–74\\nmulti-head attention 82–91\\nproblem with modeling long sequences 52\\nself-attention mechanism 55–64\\nattention scores 57\\nattention weights, computing step by step\\n65–70\\nattn_scores 71\\nautograd engine 264\\nautomatic differentiation 263–265\\nengine 252\\npartial derivatives and gradients 263\\nautoregressive model 13\\nAxolotl 249\\nB\\nbackpropagation 137\\n.backward() method 112, 318\\nBahdanau attention mechanism 54\\nbase model 7\\nbatch normalization layers 276\\nbatch_size 233\\nBERT (bidirectional encoder representations from \\ntransformers) 8\\nBPE (byte pair encoding) 32–35\\nC\\ncalc_accuracy_loader function 192\\ncalc_loss_batch function 145, 193–194\\ncalc_loss_loader function 144, 194\\ncalculating, training and validation 140, 142\\nCausalAttention class 80–81, 86, 90\\nmodule 83–84\\nobject 86'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 359}, page_content='INDEX\\n338\\ncausal attention mask 190\\ncausal attention mechanism 74–82\\ncfg dictionary 115, 119\\nclassification\\nfine-tuning\\ncategories of 170\\npreparing dataset 172–175\\nfine-tuning for\\nadding classification head 183–190\\ncalculating classification loss and \\naccuracy 190–194\\nsupervised data 195–200\\nusing LLM as spam classifier 200\\ntasks 7\\nclassify_review function 200\\nclip_grad_norm_ function 317\\nclipping, gradient 317\\ncode for data loaders 301\\ncoding\\nattention mechanisms 54\\nGPT model 117–122\\ncollate function 211\\ncomputation graphs 261\\ncompute_accuracy function 277–278\\ncomputing gradients 258\\nconnections, shortcut 109–113\\ncontext, adding special tokens 29–32\\ncontext_length 47, 95\\ncontext vectors 57, 64, 85\\nconversational performance 236\\nconverting tokens into token IDs 24–29\\ncosine decay 313, 316\\ncreate_dataloader_v1 function 39\\ncross_entropy function 138–139\\nCUDA_VISIBLE_DEVICES environment \\nvariable 286\\ncustom_collate_draft_1 215\\ncustom_collate_draft_2 218\\ncustom_collate_fn function 224, 308\\nD\\ndata, sampling with sliding window 35–41\\nDataFrame 173\\ndata list 207, 209\\nDataLoader class 38, 211, 224, 270–272\\ndata loaders 175–181\\ncode for 301\\ncreating for instruction dataset\\n224–226\\nefficient 270–274\\nDataset class 38, 177, 270–272, 274\\ndatasets\\ndownloading 207\\npreparing 324\\nutilizing large 10\\nDDP (DistributedDataParallel) strategy 282\\nddp_setup function 286\\ndecode method 27, 33–34\\ndecoder 52\\ndecoding strategies to control randomness\\n151–159\\nmodifying text generation function 157\\ntemperature scaling 152–155\\ntop-k sampling 155\\ndeep learning 253\\nlibrary 252\\ndestroy_process_group function 284\\ndevice variable 224\\ndim parameter 101–102\\nDistributedDataParallel class 284\\nDistributedSampler 283–284\\nDolma: An Open Corpus of Three Trillion Tokens \\nfor LLM Pretraining Research (Soldaini \\net al.) 11\\ndot products 58\\nd_out argument 90, 301\\ndownload_and_load_gpt2 function 161, 163, 182\\ndrop_last parameter 273\\ndropout\\ndefined 78\\nlayers 276\\ndrop_rate 95\\n.dtype attribute 259\\nDummyGPTClass 98\\nDummyGPTModel 95, 97–98, 117\\nDummyLayerNorm 97, 99, 117\\nplaceholder 100\\nDummyTransformerBlock 97, 117\\nE\\nemb_dim 95\\nEmbedding layer 161\\nembedding size 46\\nemergent behavior 14\\nencode method 27, 33, 37\\nencoder 52\\nencoding word positions 43–47\\nentry dictionary 209\\neps variable 103\\n.eval() mode 126\\neval_iter value 200\\nevaluate_model function 147–148, 196'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 360}, page_content=\"INDEX\\n339\\nF\\nfeedforward layer 267\\nFeedForward module 107–108, 113\\nfeed forward network, implementing with GELU \\nactivations 105–109\\nfind_highest_gradient function 318\\nfine-tuning\\ncategories of 170\\ncreating data loaders for instruction \\ndataset 224–226\\nevaluating fine-tuned LLMs 238–247\\nextracting and saving responses 233–238\\nfor classification 169\\nadding classification head 183–190\\ncalculating classification loss and \\naccuracy 190–194\\ndata loaders 175–181\\nfine-tuning model on supervised data\\n195–200\\ninitializing model with pretrained \\nweights 181\\npreparing dataset 172–175\\nusing LLM as spam classifier 200\\ninstruction data 230–233\\ninstruction fine-tuning, overview 205\\nLLMs, to follow instructions 204\\norganizing data into training batches 211–223\\nsupervised instruction fine-tuning, preparing \\ndataset for 207–211\\nFineWeb Dataset 295\\nfirst_batch variable 39\\nformat_input function 209–210, 242, 307\\nforward method 97, 109, 267, 330\\nfoundation model 7\\nfully connected layer 267\\nfunctools standard library 224\\nG\\nGELU (Gaussian error linear unit) 105, 107, 293\\nactivation function 104, 111\\nGenAI (generative AI) 3\\ngenerate_and_print_sample function 147–148, \\n151, 154\\ngenerate function 157, 159, 167, 228, 234–235, \\n237, 305\\ngenerate_model_scores function 246\\ngenerate_simple function 157, 159\\ngenerate_text_simple function 125–126, 131–132, \\n134, 148, 151–153\\ngenerative text models, evaluating 129\\n__getitem__ method 271\\nGoogle Colab 257\\nGPT-2 94\\nmodel 230\\ntokenizer 176\\ngpt2-medium355M-sft.pth file 238\\nGPT-3 11, 94\\nGPT-4 239\\nGPT_CONFIG_124M dictionary 95, 97, 107, \\n116–117, 120, 127, 130\\nGPTDatasetV1 class 38–39\\ngpt_download.py Python module 161\\nGPT (Generative Pre-trained Transformer) 8, \\n18, 93\\narchitecture 12–14\\ncoding 117–122\\ncoding architecture 93–99\\nimplementing feed forward network with GELU \\nactivations 105–109\\nimplementing from scratch, shortcut \\nconnections 109–113\\nimplementing from scratch to generate text\\n92, 122\\nimplementing model from scratch 99–105, \\n113–116\\nGPTModel 119, 121–122, 133, 146, 182, 330\\nclass 122, 130, 182, 326\\ncode 141\\nimplementation 166\\ninstance 131, 159, 164–167\\nGPUs (graphics processing units), optimizing \\ntraining performance with 279–288\\n.grad attribute 318\\ngrad_fn value 268\\ngrad function 264\\ngradient clipping 313, 317\\ngradients 263\\ngreedy decoding 125, 152\\nI\\ninformation leakage 76\\n__init__ constructor 71, 81, 119, 266–267, 271\\ninitializing model 326\\ninitial_lr 314\\ninit_process_group function 284\\ninput_chunk tensor 38\\ninput_embeddings 47\\n'input' object 208\\ninstruction data, fine-tuning LLMs on 230–233\\ninstruction dataset 205\\nInstructionDataset class 212, 224, 308\"),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 361}, page_content=\"INDEX\\n340\\ninstruction fine-tuning 7, 170, 322\\ninstruction following, creating data loaders for \\ninstruction dataset 224–226\\n'instruction' object 208\\ninstruction–response pairs 207\\nloading pretrained LLMs 226–229\\noverview 205\\nK\\nkeepdim parameter 101\\nL\\nLayerNorm 103, 115, 117, 119\\nlayer normalization 99–105\\nlearning rate warmup 313–314\\n__len__ method 271\\nLIMA dataset 296\\nLinear layers 95, 107, 329–330, 332–333\\nLinear layer weights 330\\nLinearWithLoRA layer 330–331, 333\\nLitGPT 249\\nLLama 2 model 141\\nLlama 3 model 238\\nllama.cpp library 238\\nLLMs (large language models) 17–18\\napplications of 4\\nbuilding and using 5–7, 14\\ncoding architecture 93–99\\ncoding attention mechanisms, causal attention \\nmechanism 74–82\\nfine-tuning 230–233, 238–247, 295\\nfine-tuning for classification 183–194, 200\\nimplementing GPT model, implementing feed \\nforward network with GELU \\nactivations 105–109\\ninstruction fine-tuning, loading pretrained \\nLLMs 226–229\\noverview of 1–4\\npretraining 132, 140, 142, 146–151, 159\\ntraining function 313, 319–321\\ntraining loop, gradient clipping 317\\ntransformer architecture 7–10\\nutilizing large datasets 10\\nworking with text data, word embeddings 18–20\\nloading, pretrained weights from OpenAI\\n160–167\\nload_state_dict method 160\\nload_weights_into_gpt function 165–166, 182\\nlogistic regression loss function 293\\nlogits tensor 139\\nLoRALayer class 329–330\\nLoRA (low-rank adaptation) 247, 322\\nparameter-efficient fine-tuning 324, 326\\nloss.backward() function 112\\nlosses 140, 142\\nloss metric 132\\nlr (learning rate) 275\\nM\\nmachine learning 253\\nMachine Learning Q and AI (Raschka) 290\\nmacOS 282\\nmain function 286\\nmasked attention 74\\n.matmul method 261\\nmatrices 258–261\\nmax_length 38, 141, 178, 306\\nminbpe repository 291\\nmodel_configs table 164\\nmodel.eval() function 160\\nmodel.named_parameters() function 112\\nmodel.parameters() method 129\\nmodel_response 238\\nmodel.train() setting 276\\nmodel weights, loading and saving in PyTorch\\n159\\nModule base class 265\\nmps device 224\\nmp.spawn() call 286\\nmulti-head attention 80, 82–91\\nimplementing with weight splits 86–91\\nstacking multiple single-head attention \\nlayers 82–85\\nMultiHeadAttention class 86–87, 90–91, 292\\nMultiHeadAttentionWrapper class 83–87, 90\\nmultilayer neural networks, implementing\\n265–269\\nmultinomial function 153–155\\nmultiprocessing.spawn function 284\\nmultiprocessing submodule 284\\nN\\nNeuralNetwork model 284\\nneural networks\\nimplementing feed forward network with \\nGELU activations 105–109\\nimplementing multilayer neural networks\\n265–269\\nNEW_CONFIG dictionary 164\\nn_heads 95\"),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 362}, page_content=\"INDEX\\n341\\nnn.Linear layers 72\\nnn.Module 71, 97\\nnumel() method 120\\nnum_heads dimension 88\\nnum_tokens dimension 88\\nO\\nOllama application 238, 241\\nOllama Llama 3 method 309\\nollama run command 242\\nollama run llama3 command 240–241\\nollama serve command 239–242\\nOLMo 294\\none-dimensional tensor (vector) 259\\nOpenAI, loading pretrained weights from\\n160–167\\nOpenAI’s GPT-3 Language Model: A Technical \\nOverview 293\\noptimizer.step() method 276\\noptimizer.zero_grad() method 276\\nout_head 97\\noutput layer nodes 183\\n'output' object 208\\nP\\nparameter-efficient fine-tuning 322\\nLoRA (low-rank adaptation) 322\\npreparing dataset 324\\nparameters 129\\ncalculating 302\\nparams dictionary 162, 164–165\\npartial derivatives 263\\npartial function 224\\npeak_lr 314\\nperplexity 139\\nPhi-3 model 297\\nPHUDGE model 297\\npip installer 33\\nplot_losses function 232\\nplot_values function 199\\npos_embeddings 47\\nPost-LayerNorm 115\\npreference fine-tuning 298\\nPre-LayerNorm 115\\npretokenizes 212\\npretrained weights, initializing model with 181\\npretraining 7\\ncalculating text generation loss 132\\ncalculating training and validation set losses\\n140, 142\\ndecoding strategies to control randomness\\n151–159\\nloading and saving model weights in \\nPyTorch 159\\nloading pretrained weights from OpenAI\\n160–167\\non unlabeled data 128\\ntraining LLMs 146–151\\nusing GPT to generate text 130\\nprint_gradients function 112\\nprint_sampled_tokens function 155, 304\\nprint statement 24\\nPrometheus model 297\\nprompt styles 209\\n.pth extension 159\\nPython version 254\\nPyTorch\\nand Torch 256\\nautomatic differentiation 263–265\\ncomputation graphs 261\\ndata loaders 210\\ndataset objects 325\\nefficient data loaders 270–274\\nimplementing multilayer neural networks\\n265–269\\ninstalling 254–257\\nloading and saving model weights in 159\\noptimizing training performance with \\nGPUs 279–288\\noverview 251–257\\nsaving and loading models 278\\ntraining loops 274–278\\nunderstanding tensors 258–261\\nwith a NumPy-like API 258\\nQ\\nqkv_bias 95\\nQ query matrix 88\\nquery_llama function 243\\nquery_model function 242–243\\nR\\nrandom_split function 175\\nrank argument 286\\nraw text 6\\nregister_buffer 81\\nre library 22\\nReLU (rectified linear unit) 100, 105\\n.replace() method 235\\nreplace_linear_with_lora function 330, 332\"),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 363}, page_content='INDEX\\n342\\n.reshape method 260–261\\nre.split command 22\\nresponses, extracting and saving 233–238\\nretrieval-augmented generation 19\\nr/LocalLLaMA subreddit 248\\nRMSNorm 292\\nRNNs (recurrent neural networks) 52\\nS\\nsaving and loading models 278\\nscalars 258–261\\nscaled dot-product attention 64\\nscaled_dot_product function 292\\nscale parameter 103\\nsci_mode parameter 102\\nSelfAttention class 90\\nself-attention mechanism 55–64\\ncomputing attention weights for all input \\ntokens 61–64\\nimplementing with trainable weights 64–74\\nwithout trainable weights 56–61\\nSelfAttention_v1 class 71, 73\\nSelfAttention_v2 class 73\\nself.out_proj layer 90\\nself.register_buffer() call 81\\nself.use_shortcut attribute 111\\nSequential class 267\\nset_printoptions method 277\\nsettings dictionary 162, 164\\nSGD (stochastic gradient descent) 275\\n.shape attribute 260, 271\\nshift parameter 103\\nshortcut connections 109–113\\nSimpleTokenizerV1 class 27\\nSimpleTokenizerV2 class 29, 31, 33\\nsingle-head attention, stacking multiple layers\\n82–85\\nsliding window 35–41\\nsoftmax function 269, 276\\nsoftmax_naive function 60\\nSpamDataset class 176, 178\\nspawn function 286\\nspecial context tokens 29–32\\nstate_dict 160, 279\\nstride setting 39\\nstrip() function 229\\nsupervised data, fine-tuning model on 195–200\\nsupervised instruction fine-tuning 205\\npreparing dataset for 207–211\\nsupervised learning 253\\nSwiGLU (Swish-gated linear unit) 105\\nT\\ntarget_chunk tensor 38\\ntargets tensor 139\\ntemperature scaling 151–152, 154–155\\ntensor2d 259\\ntensor3d 259\\nTensor class 258\\ntensor library 252\\ntensors 258–261\\ncommon tensor operations 260\\nscalars, vectors, matrices, and tensors 258–261\\ntensor data types 259\\nthree-dimensional tensor 259\\ntwo-dimensional tensor 259\\ntest_data set 246\\ntest_loader 272\\ntest_set dictionary 237–238\\ntext completion 205\\ntext data 17\\nadding special context tokens 29–32\\nconverting tokens into token IDs 24–29\\ncreating token embeddings 42–43\\nencoding word positions 43–47\\nsliding window 35–41\\ntokenization, byte pair encoding 33–35\\nword embeddings 18–20\\ntext_data 314\\ntext generation 122\\nusing GPT to generate text 130\\ntext generation function, modifying 157\\ntext generation loss 132\\ntext_to_token_ids function 131\\ntiktoken package 176, 178\\n.T method 261\\n.to() method 259, 280\\ntoken_embedding_layer 46–47\\ntoken embeddings 42–43\\ntoken IDs 24–29\\ntoken_ids_to_text function 131\\ntokenization, byte pair encoding 33–35\\ntokenizing text 21–24\\ntop-k sampling 151, 155–156\\ntorch.argmax function 125\\ntorchaudio library 255\\ntorch.manual_seed(123) 272\\ntorch.nn.Linear layers 267\\ntorch.no_grad() context manager 269\\ntorch.save function 159\\ntorch.sum method 277\\ntorch.tensor function 258\\ntorchvision library 255'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 364}, page_content='INDEX\\n343\\ntotal_loss variable 145\\nToyDataset class 271\\ntqdm progress bar utility 242\\ntrain_classifier_simple function 197, 200\\ntrain_data subset 143\\ntraining, optimizing performance with GPUs\\n279–288\\nPyTorch computations on GPU devices 279\\nselecting available GPUs on multi-GPU \\nmachine 286–288\\nsingle-GPU training 280\\ntraining with multiple GPUs 282–288\\ntraining batches, organizing data into 211–223\\ntraining function 319–321\\nenhancing 313\\nmodified 319–321\\ntraining loops 274–278\\ncosine decay 316\\ngradient clipping 317\\nlearning rate warmup 314\\ntrain_loader 272\\ntrain_model_simple function 147, 149, 160, 195\\ntrain_ratio 142\\ntrain_simple_function 305\\ntransformer architecture 3, 7–10, 55\\nTransformerBlock class 115\\ntransformer blocks 93, 185\\nconnecting attention and linear layers in\\n113–116\\n.transpose method 87\\ntril function 75\\nU\\nUltraChat dataset 297\\nunbiased parameter 103\\nunlabeled data, decoding strategies to control \\nrandomness 151–159\\nV\\nval_data subset 143\\nvariable-length inputs 142\\nvectors 258–261\\n.view method 87\\nvocab_size 95\\nv vector 317\\nW\\n.weight attribute 129, 161\\nweight_decay parameter 200\\nweight parameters 66, 129\\nweights\\ninitializing model with pretrained weights 181\\nloading pretrained weights from OpenAI\\n160–167\\nweight splits 86–91\\nWk matrix 65, 71\\nWord2Vec 19\\nword embeddings 18–20\\nword positions, encoding 43–47\\nWq matrix 65, 71, 88\\nWv matrix 65, 71\\nX\\nX training example 268\\nZ\\nzero-dimensional tensor (scalar) 259'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 365}, page_content='For ordering information, go to www.manning.com\\nRELATED MANNING TITLES\\nDeep Learning with Python, Second Edition\\nby Francois Chollet\\nISBN 9781617296864\\n504 pages, $59.99 \\nOctober 2021\\nGenerative AI in Action\\nby Amit Bahree\\nISBN 9781633436947\\n469 pages (estimated), $59.99\\nOctober 2024 (estimated)\\nMachine Learning Algorithms in Depth\\nby Vadim Smolyakov\\nISBN 9781633439214\\n328 pages, $79.99\\nJuly 2024'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 366}, page_content='For ordering information, go to www.manning.com\\nRELATED MANNING TITLES\\nInside Deep Learning\\nby Edward Raff\\nForeword by Kirk Borne\\nISBN 9781617298639\\n600 pages, $59.99 \\nApril 2022\\nMath and Architectures of Deep Learning\\nby Krishnendu Chaudhury\\nwith Ananya H. Ashok, Sujay Narumanchi, \\nDevashish Shankar\\nForeword by Prith Banerjee\\nISBN 9781617296482\\n552 pages, $69.99 \\nApril 2024\\nTransformers in Action\\nby Nicole Koenigstein\\nISBN 9781633437883\\n393 pages (estimated), $59.99\\nFebruary 2025 (estimated)'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 367}, page_content='Hands-on projects for learning your way\\nliveProjects are an exciting way to develop your skills that’s just like learning on the job.\\nIn a Manning liveProject, you tackle a real-world IT challenge and work out your own \\nsolutions. To make sure you succeed, you’ll get 90 days of full and unlimited access to a \\nhand-picked list of Manning book and video resources.\\nHere’s how liveProject works:\\n•\\t \\x07Achievable milestones. Each project is broken down into steps and sections so \\nyou can keep track of your progress.\\n•\\t \\x07Collaboration and advice. Work with other liveProject participants through \\nchat, working groups, and peer project reviews.\\n•\\t \\x07Compare your results. See how your work shapes up against an expert \\nimplementation by the liveProject’s creator.\\n•\\t \\x07Everything you need to succeed. Datasets and carefully selected learning \\nresources come bundled with every liveProject.\\n•\\t \\x07Build your portfolio. All liveProjects teach skills that are in demand from \\nindustry. When you’re finished, you’ll have the satisfaction that comes with \\nsuccess and a real project to add to your portfolio.\\nExplore dozens of data, development, and cloud engineering \\nliveProjects at www.manning.com!'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 368}, page_content='GPT-like\\ndecoder-only\\ntransformer\\nInput text:\\nToken embeddings:\\nThis is an example.\\nTokenized text:\\nThis\\nis\\nan\\nexample\\nOutput text\\nPostprocessing steps\\nToken IDs:\\n.\\n12\\nThis section covers the\\nconcept of splitting\\ntext into tokens\\n389\\n133\\n2052\\n40134\\nA view of the text processing steps in the context of an LLM. The process starts with input \\ntext, which is broken down into tokens and then converted into numerical token IDs. These IDs \\nare linked to token embeddings that serve as the input for the GPT model. The model processes \\nthese embeddings and generates output text. Finally, the output undergoes postprocessing \\nsteps to produce the final text. This flow illustrates the basic operations of tokenization, \\nembedding, transformation, and postprocessing in a GPT model that is implemented from \\nthe ground up in this book.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext, AppendMode 1.1', 'creator': 'FrameMaker 16.0.1(Foxit Advanced PDF Editor)', 'creationdate': \"D:20240822090713Z00'00'\", 'source': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'file_path': '../data/pdf/_OceanofPDF.com_Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': \"D:20250903101523Z00'00'\", 'trapped': '', 'modDate': \"D:20250903101523Z00'00'\", 'creationDate': \"D:20240822090713Z00'00'\", 'page': 369}, page_content='Sebastian Raschka\\nISBN-13: 978-1-63343-716-6\\nP\\nhysicist Richard P. Feynman reportedly said, “I don’t \\nunderstand anything I can’t build.” Based on this same \\npowerful principle, bestselling author Sebastian \\nRaschka guides you step by step as you build a GPT-style \\nLLM that you can run on your laptop. Th is is an engaging \\nbook that covers each stage of the process, from planning \\nand coding to training and fi ne-tuning.\\nBuild a Large Language Model (From Scratch) is a practical and \\neminently-satisfying hands-on journey into the foundations\\nof generative AI. Without relying on any existing LLM \\nlibraries, you’ll code a base model, evolve it into a text clas-\\nsifi er, and ultimately create a chatbot that can follow your \\nconversational instructions. And you’ll really understand it \\nbecause you built it yourself!\\nWhat’s Inside\\n● Plan and code an LLM comparable to GPT-2\\n● Load pretrained weights\\n● Construct a complete training pipeline\\n● Fine-tune your LLM for text classifi cation\\n● Develop LLMs that follow human instructions\\nReaders need intermediate Python skills and some know-\\nledge of machine learning. Th e LLM you create will run on \\nany modern laptop and can optionally utilize GPUs.\\nSebastian Raschka is a Staff  Research Engineer at Lightning \\nAI, where he works on LLM research and develops open-\\nsource software.\\nTh e technical editor on this book was David Caswell.\\nFor print book owners, all ebook formats are free:\\nhttps://www.manning.com/freebook\\nBUILD A Large Language Model (FROM SCRATCH)\\nAI\\nM A N N I N G\\n“\\nTruly inspirational! It \\nmotivates you to put your \\n  new skills into action.”\\n—Benjamin Muskalla\\nSenior Engineer, GitHub \\n“\\nTh e most understandable \\nand comprehensive explana-\\ntion of language models yet! \\nIts unique and practical \\nteaching style achieves a level \\nof understanding you can’t \\nget any other way.”\\n—Cameron Wolfe\\nSenior Scientist, Netfl ix \\n“\\nSebastian combines deep \\nknowledge with practical \\nengineering skills and a knack \\nfor making complex ideas \\nsimple. Th is is the guide \\n  you need!”\\n—Chip Huyen, author of \\nDesigning Machine Learning Systems \\nand AI Engineering\\n“\\nDefi nitive, up-to-date cover-\\nage. Highly recommended!”\\n—Dr. Vahid Mirjalili, Senior Data \\nScientist, FM Global \\nSee first page'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 0}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 1}, page_content='MACHINE LEARNING\\n“This book provides a \\ngreat way to start off \\nwith deep learning, \\nwith plenty of examples \\nand well-explained \\nconcepts. A perfect \\nbook for readers of all \\nlevels who are interested \\nin the domain.”\\n—Vishwesh Ravi Shrimali\\nADAS Engineer\\nFundamentals of Deep Learning\\nUS $69.99\\t\\n CAN $87.99\\nISBN: 978-1-492-08128-7\\nTwitter: @oreillymedia\\nlinkedin.com/company/oreilly-media\\nyoutube.com/oreillymedia \\nWe’re in the midst of an AI research explosion. Deep learning \\nhas unlocked superhuman perception to power our push \\ntoward creating self-driving vehicles, defeating human experts \\nat a variety of difficult games including Go, and even generating \\nessays with shockingly coherent prose. But deciphering these \\nbreakthroughs often takes a PhD in machine learning and \\nmathematics. \\nThe updated second edition of this book describes the intuition \\nbehind these innovations without jargon or complexity. \\nPython-proficient programmers, software engineering \\nprofessionals, and computer science majors will be able to \\nreimplement these breakthroughs on their own and reason \\nabout them with a level of sophistication that rivals some of the \\nbest developers in the field.\\n•\\t Learn the mathematics behind machine learning jargon\\n•\\t Examine the foundations of machine learning and neural \\nnetworks\\n•\\t Manage problems that arise as you begin to make networks \\ndeeper\\n•\\t Build neural networks that analyze complex images\\n•\\t Perform effective dimensionality reduction using \\nautoencoders\\n•\\t Dive deep into sequence analysis to examine language\\n•\\t Explore methods in interpreting complex machine learning \\nmodels\\n•\\t Gain theoretical and practical knowledge on generative \\nmodeling\\n•\\t Understand the fundamentals of reinforcement learning\\nNithin Buduma is a machine learning \\nscientist at Cresta, a leader in the \\ncontact center intelligence space.\\nNikhil Buduma is cofounder and chief \\nscientist of Ambience Healthcare, a \\nSan Francisco-based company that \\nmakes autonomous technologies for \\nhealthcare delivery.\\nJoe Papa, founder of TeachMe.AI, has \\nover 25 years of experience in research \\nand development. He’s led AI research \\nteams with PyTorch at Booz Allen and \\nPerspecta Labs.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 2}, page_content='Nithin Buduma, Nikhil Buduma, and Joe Papa\\nwith contributions by Nicholas Locascio\\nFundamentals of Deep Learning\\nDesigning Next-Generation Machine\\nIntelligence Algorithms\\nSECOND EDITION\\nBoston\\nFarnham\\nSebastopol\\nTokyo\\nBeijing\\nBoston\\nFarnham\\nSebastopol\\nTokyo\\nBeijing'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 3}, page_content='978-1-492-08218-7\\nLSI\\nFundamentals of Deep Learning\\nby Nithin Buduma, Nikhil Buduma, and Joe Papa\\nCopyright © 2022 Nithin Buduma and Mobile Insights Technology Group, LLC. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles (http://oreilly.com). For more information, contact our corporate/institutional\\nsales department: 800-998-9938 or corporate@oreilly.com.\\nAcquisitions Editor: Rebecca Novack\\nDevelopment Editor: Melissa Potter\\nProduction Editor: Katherine Tozer\\nCopyeditor: Sonia Saruba\\nProofreader: Stephanie English\\nIndexer: Judith McConville\\nInterior Designer: David Futato\\nCover Designer: Karen Montgomery\\nIllustrator: Kate Dullea\\nJune 2017:\\n First Edition\\nMay 2022:\\n Second Edition\\nRevision History for the Second Edition\\n2022-05-16: First Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492082187 for release details.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Fundamentals of Deep Learning, the\\ncover image, and related trade dress are trademarks of O’Reilly Media, Inc.\\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use\\nof or reliance on this work. Use of the information and instructions contained in this work is at your\\nown risk. If any code samples or other technology this work contains or describes is subject to open\\nsource licenses or the intellectual property rights of others, it is your responsibility to ensure that your use\\nthereof complies with such licenses and/or rights.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 4}, page_content='Table of Contents\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ix\\n1. Fundamentals of Linear Algebra for Deep Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\\nData Structures and Operations                                                                                       1\\nMatrix Operations                                                                                                          3\\nVector Operations                                                                                                          6\\nMatrix-Vector Multiplication                                                                                       7\\nThe Fundamental Spaces                                                                                                  7\\nThe Column Space                                                                                                         7\\nThe Null Space                                                                                                              10\\nEigenvectors and Eigenvalues                                                                                        13\\nSummary                                                                                                                           15\\n2. Fundamentals of Probability. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  17\\nEvents and Probability                                                                                                    17\\nConditional Probability                                                                                                  20\\nRandom Variables                                                                                                            22\\nExpectation                                                                                                                       24\\nVariance                                                                                                                             25\\nBayes’ Theorem                                                                                                                27\\nEntropy, Cross Entropy, and KL Divergence                                                               29\\nContinuous Probability Distributions                                                                          32\\nSummary                                                                                                                           36\\n3. The Neural Network. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  39\\nBuilding Intelligent Machines                                                                                        39\\nThe Limits of Traditional Computer Programs                                                          40\\nThe Mechanics of Machine Learning                                                                            41\\niii'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 5}, page_content='The Neuron                                                                                                                       45\\nExpressing Linear Perceptrons as Neurons                                                                  47\\nFeed-Forward Neural Networks                                                                                    48\\nLinear Neurons and Their Limitations                                                                         51\\nSigmoid, Tanh, and ReLU Neurons                                                                               51\\nSoftmax Output Layers                                                                                                   54\\nSummary                                                                                                                           54\\n4. Training Feed-Forward Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  55\\nThe Fast-Food Problem                                                                                                  55\\nGradient Descent                                                                                                             57\\nThe Delta Rule and Learning Rates                                                                               58\\nGradient Descent with Sigmoidal Neurons                                                                 60\\nThe Backpropagation Algorithm                                                                                   61\\nStochastic and Minibatch Gradient Descent                                                                63\\nTest Sets, Validation Sets, and Overfitting                                                                    65\\nPreventing Overfitting in Deep Neural Networks                                                      71\\nSummary                                                                                                                           76\\n5. Implementing Neural Networks in PyTorch. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  77\\nIntroduction to PyTorch                                                                                                 77\\nInstalling PyTorch                                                                                                            77\\nPyTorch Tensors                                                                                                               78\\nTensor Init                                                                                                                     78\\nTensor Attributes                                                                                                          79\\nTensor Operations                                                                                                        80\\nGradients in PyTorch                                                                                                      83\\nThe PyTorch nn Module                                                                                                 84\\nPyTorch Datasets and Dataloaders                                                                                87\\nBuilding the MNIST Classifier in PyTorch                                                                  89\\nSummary                                                                                                                           93\\n6. Beyond Gradient Descent. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  95\\nThe Challenges with Gradient Descent                                                                        95\\nLocal Minima in the Error Surfaces of Deep Networks                                             96\\nModel Identifiability                                                                                                        97\\nHow Pesky Are Spurious Local Minima in Deep Networks?                                    98\\nFlat Regions in the Error Surface                                                                                 101\\nWhen the Gradient Points in the Wrong Direction                                                 104\\nMomentum-Based Optimization                                                                                106\\nA Brief View of Second-Order Methods                                                                    109\\nLearning Rate Adaptation                                                                                             111\\niv \\n| \\nTable of Contents'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 6}, page_content='AdaGrad—Accumulating Historical Gradients                                                     111\\nRMSProp—Exponentially Weighted Moving Average of Gradients                  112\\nAdam—Combining Momentum and RMSProp                                                   113\\nThe Philosophy Behind Optimizer Selection                                                            115\\nSummary                                                                                                                         116\\n7. Convolutional Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  117\\nNeurons in Human Vision                                                                                           117\\nThe Shortcomings of Feature Selection                                                                      118\\nVanilla Deep Neural Networks Don’t Scale                                                                121\\nFilters and Feature Maps                                                                                               122\\nFull Description of the Convolutional Layer                                                             127\\nMax Pooling                                                                                                                    131\\nFull Architectural Description of Convolution Networks                                       132\\nClosing the Loop on MNIST with Convolutional Networks                                  134\\nImage Preprocessing Pipelines Enable More Robust Models                                 136\\nAccelerating Training with Batch Normalization                                                     137\\nGroup Normalization for Memory Constrained Learning Tasks                           139\\nBuilding a Convolutional Network for CIFAR-10                                                    141\\nVisualizing Learning in Convolutional Networks                                                    143\\nResidual Learning and Skip Connections for Very Deep Networks                      147\\nBuilding a Residual Network with Superhuman Vision                                          149\\nLeveraging Convolutional Filters to Replicate Artistic Styles                                 152\\nLearning Convolutional Filters for Other Problem Domains                                 154\\nSummary                                                                                                                         155\\n8. Embedding and Representation Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  157\\nLearning Lower-Dimensional Representations                                                         157\\nPrincipal Component Analysis                                                                                    158\\nMotivating the Autoencoder Architecture                                                                 160\\nImplementing an Autoencoder in PyTorch                                                               161\\nDenoising to Force Robust Representations                                                              171\\nSparsity in Autoencoders                                                                                              174\\nWhen Context Is More Informative than the Input Vector                                    177\\nThe Word2Vec Framework                                                                                          179\\nImplementing the Skip-Gram Architecture                                                              182\\nSummary                                                                                                                         188\\n9. Models for Sequence Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  189\\nAnalyzing Variable-Length Inputs                                                                              189\\nTackling seq2seq with Neural N-Grams                                                                     190\\nImplementing a Part-of-Speech Tagger                                                                      192\\nTable of Contents \\n| \\nv'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 7}, page_content='Dependency Parsing and SyntaxNet                                                                           197\\nBeam Search and Global Normalization                                                                    203\\nA Case for Stateful Deep Learning Models                                                                206\\nRecurrent Neural Networks                                                                                         207\\nThe Challenges with Vanishing Gradients                                                                 210\\nLong Short-Term Memory Units                                                                                 213\\nPyTorch Primitives for RNN Models                                                                          218\\nImplementing a Sentiment Analysis Model                                                               219\\nSolving seq2seq Tasks with Recurrent Neural Networks                                         224\\nAugmenting Recurrent Networks with Attention                                                     227\\nDissecting a Neural Translation Network                                                                  230\\nSelf-Attention and Transformers                                                                                 239\\nSummary                                                                                                                         242\\n10. Generative Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  243\\nGenerative Adversarial Networks                                                                                244\\nVariational Autoencoders                                                                                             249\\nImplementing a VAE                                                                                                     259\\nScore-Based Generative Models                                                                                  264\\nDenoising Autoencoders and Score Matching                                                          269\\nSummary                                                                                                                         274\\n11. Methods in Interpretability. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  275\\nOverview                                                                                                                         275\\nDecision Trees and Tree-Based Algorithms                                                               276\\nLinear Regression                                                                                                           280\\nMethods for Evaluating Feature Importance                                                             281\\nPermutation Feature Importance                                                                             281\\nPartial Dependence Plots                                                                                          282\\nExtractive Rationalization                                                                                            283\\nLIME                                                                                                                                288\\nSHAP                                                                                                                               292\\nSummary                                                                                                                         297\\n12. Memory Augmented Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  299\\nNeural Turing Machines                                                                                               299\\nAttention-Based Memory Access                                                                                301\\nNTM Memory Addressing Mechanisms                                                                    303\\nDifferentiable Neural Computers                                                                                307\\nInterference-Free Writing in DNCs                                                                            309\\nDNC Memory Reuse                                                                                                     310\\nTemporal Linking of DNC Writes                                                                               311\\nvi \\n| \\nTable of Contents'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 8}, page_content='Understanding the DNC Read Head                                                                          312\\nThe DNC Controller Network                                                                                     313\\nVisualizing the DNC in Action                                                                                    314\\nImplementing the DNC in PyTorch                                                                            317\\nTeaching a DNC to Read and Comprehend                                                              321\\nSummary                                                                                                                         323\\n13. Deep Reinforcement Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  325\\nDeep Reinforcement Learning Masters Atari Games                                               325\\nWhat Is Reinforcement Learning?                                                                              326\\nMarkov Decision Processes                                                                                          328\\nPolicy                                                                                                                            329\\nFuture Return                                                                                                             330\\nDiscounted Future Return                                                                                        331\\nExplore Versus Exploit                                                                                                  331\\nϵ-Greedy                                                                                                                      333\\nAnnealed ϵ-Greedy                                                                                                    333\\nPolicy Versus Value Learning                                                                                       334\\nPole-Cart with Policy Gradients                                                                                  335\\nOpenAI Gym                                                                                                              335\\nCreating an Agent                                                                                                      335\\nBuilding the Model and Optimizer                                                                         337\\nSampling Actions                                                                                                       337\\nKeeping Track of History                                                                                          337\\nPolicy Gradient Main Function                                                                                338\\nPGAgent Performance on Pole-Cart                                                                       340\\nTrust-Region Policy Optimization                                                                              341\\nProximal Policy Optimization                                                                                     345\\nQ-Learning and Deep Q-Networks                                                                             347\\nThe Bellman Equation                                                                                               347\\nIssues with Value Iteration                                                                                        348\\nApproximating the Q-Function                                                                               348\\nDeep Q-Network                                                                                                        348\\nTraining DQN                                                                                                            349\\nLearning Stability                                                                                                       349\\nTarget Q-Network                                                                                                      350\\nExperience Replay                                                                                                      350\\nFrom Q-Function to Policy                                                                                      350\\nDQN and the Markov Assumption                                                                         351\\nDQN’s Solution to the Markov Assumption                                                          351\\nPlaying Breakout with DQN                                                                                    351\\nBuilding Our Architecture                                                                                        354\\nTable of Contents \\n| \\nvii'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 9}, page_content='Stacking Frames                                                                                                         354\\nSetting Up Training Operations                                                                               354\\nUpdating Our Target Q-Network                                                                            354\\nImplementing Experience Replay                                                                            355\\nDQN Main Loop                                                                                                        356\\nDQNAgent Results on Breakout                                                                              358\\nImproving and Moving Beyond DQN                                                                       358\\nDeep Recurrent Q-Networks                                                                                    359\\nAsynchronous Advantage Actor-Critic Agent                                                       359\\nUNsupervised REinforcement and Auxiliary Learning                                       360\\nSummary                                                                                                                         361\\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  363\\nviii \\n| \\nTable of Contents'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 10}, page_content='Preface\\nWith the reinvigoration of neural networks in the 2000s, deep learning has become\\nan extremely active area of research that is paving the way for modern machine\\nlearning. This book uses exposition and examples to help you understand major\\nconcepts in this complicated field. Large companies such as Google, Microsoft, and\\nFacebook have taken notice and are actively growing in-house deep learning teams.\\nFor the rest of us, deep learning is still a pretty complex and difficult subject to grasp.\\nResearch papers are filled to the brim with jargon, and scattered online tutorials do\\nlittle to help build a strong intuition for why and how deep learning practitioners\\napproach problems. Our goal is to bridge this gap.\\nIn this second edition, we provide more rigorous background sections in mathemat‐\\nics with the aim of better equipping you for the material in the rest of the book.\\nIn addition, we have updated chapters in sequence analysis, computer vision, and\\nreinforcement learning with deep dives into the latest advancements in the fields.\\nAnd finally, we have added new chapters in the fields of generative modeling and\\ninterpretability to provide you with a broader view of the field of deep learning. We\\nhope that these updates inspire you to practice deep learning on their own and apply\\ntheir learnings to solve meaningful problems in the real world.\\nPrerequisites and Objectives\\nThis book is aimed at an audience with a basic operating understanding of calculus\\nand Python programming. In this latest edition, we provide extensive mathematical\\nbackground chapters, specifically in linear algebra and probability, to prepare you for\\nthe material that lies ahead.\\nBy the end of the book, we hope you will be left with an intuition for how to\\napproach problems using deep learning, the historical context for modern deep\\nlearning approaches, and a familiarity with implementing deep learning algorithms\\nusing the PyTorch open source library.\\nix'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 11}, page_content='How Is This Book Organized?\\nThe first chapters of this book are dedicated to developing mathematical maturity via\\ndeep dives into linear algebra and probability, which are deeply embedded in the field\\nof deep learning. The next several chapters discuss the structure of feed-forward neu‐\\nral networks, how to implement them in code, and how to train and evaluate them on\\nreal-world datasets. The rest of the book is dedicated to specific applications of deep\\nlearning and understanding the intuition behind the specialized learning techniques\\nand neural network architectures developed for those applications. Although we\\ncover advanced research in these latter sections, we hope to provide a breakdown of\\nthese techniques that is derived from first principles and digestible.\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program\\nelements such as variable or function names, databases, data types, environment\\nvariables, statements, and keywords.\\nThis element signifies a general note.\\nThis element indicates a warning or caution.\\nUsing Code Examples\\nSupplemental material (code examples, exercises, etc.) is available for download at\\nhttps://github.com/darksigma/Fundamentals-of-Deep-Learning-Book.\\nIf you have a technical question or a problem using the code examples, please email\\nbookquestions@oreilly.com.\\nx \\n| \\nPreface'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 12}, page_content='This book is here to help you get your job done. In general, if example code is\\noffered with this book, you may use it in your programs and documentation. You\\ndo not need to contact us for permission unless you’re reproducing a significant\\nportion of the code. For example, writing a program that uses several chunks of code\\nfrom this book does not require permission. Selling or distributing examples from\\nO’Reilly books does require permission. Answering a question by citing this book\\nand quoting example code does not require permission. Incorporating a significant\\namount of example code from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “Fundamentals of Deep Learning by\\nNithin Buduma, Nikhil Buduma, and Joe Papa (O’Reilly). Copyright 2022 Nithin\\nBuduma and Mobile Insights Technology Group, LLC, 978-1-492-08218-7.”\\nIf you feel your use of code examples falls outside fair use or the permission given\\nabove, feel free to contact us at permissions@oreilly.com.\\nO’Reilly Online Learning\\nFor more than 40 years, O’Reilly Media has provided technol‐\\nogy and business training, knowledge, and insight to help\\ncompanies succeed.\\nOur unique network of experts and innovators share their knowledge and expertise\\nthrough books, articles, and our online learning platform. O’Reilly’s online learning\\nplatform gives you on-demand access to live training courses, in-depth learning\\npaths, interactive coding environments, and a vast collection of text and video from\\nO’Reilly and 200+ other publishers. For more information, visit https://oreilly.com.\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)\\nPreface \\n| \\nxi'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 13}, page_content='We have a web page for this book, where we list errata, examples, and any addi‐\\ntional information. You can access this page at https://oreil.ly/fundamentals-of-deep-\\nlearning-2e.\\nEmail bookquestions@oreilly.com to comment or ask technical questions about this\\nbook.\\nFor news and information about our books and courses, visit https://oreilly.com.\\nFind us on LinkedIn: https://www.linkedin.com/company/oreilly-media.\\nFollow us on Twitter: https://twitter.com/oreillymedia.\\nWatch us on YouTube: https://www.youtube.com/oreillymedia.\\nAcknowledgements\\nWe’d like to thank several people who have been instrumental in the completion of\\nthis text. We’d like to start by acknowledging Mostafa Samir and Surya Bhupatiraju,\\nwho contributed heavily to the content of Chapters 7 and 8. We also appreciate the\\ncontributions of Mohamed (Hassan) Kane and Anish Athalye, who worked on early\\nversions of the code examples in this book’s GitHub repository.\\nNithin and Nikhil\\nThis book would not have been possible without the never-ending support and\\nexpertise of our editor, Shannon Cutt. We’d also like to appreciate the commentary\\nprovided by our reviewers, Isaac Hodes, David Andrzejewski, Aaron Schumacher,\\nVishwesh Ravi Shrimali, Manjeet Dahiya, Ankur Patel, and Suneeta Mall, who pro‐\\nvided thoughtful, in-depth, and technical commentary on the original drafts of the\\ntext. Finally, we are thankful for all of the insight provided by our friends and family\\nmembers, including Jeff Dean, Venkat Buduma, William, and Jack, as we finalized the\\nmanuscript of the text.\\nJoe\\nUpdating the code for this book with PyTorch has been an enjoyable and exciting\\nexperience. No endeavor like this can be achieved by one person alone. First, I would\\nlike to thank the PyTorch community and its 2,100+ contributors for continuing to\\ngrow and improve PyTorch and its deep learning capabilities. It is because of you that\\nwe can demonstrate the concepts described in this book.\\nI am forever grateful to Rebecca Novack for bringing me into this project and for\\nher confidence in me as an author. Many thanks to Melissa Potter and the O’Reilly\\nproduction staff in making this updated version come to life.\\nxii \\n| \\nPreface'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 14}, page_content='For his encouragement and support, I’d like to thank Matt Kirk. He’s been my rock\\nthrough it all. Thank you for our countless chats full of ideas and resources.\\nSpecial thanks to my kids, Savannah, Caroline, George, and Forrest, for being patient\\nand understanding when Daddy had to work. And, most of all, thank you to my wife,\\nEmily, who has always supported my dreams throughout life. While I diligently wrote\\ncode, she cared for our newborn through sleepless nights while ensuring the “big”\\nkids had their needs met too. Without her, my contributions to this project would not\\nbe possible.\\nPreface \\n| \\nxiii'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 15}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 16}, page_content='CHAPTER 1\\nFundamentals of Linear Algebra\\nfor Deep Learning\\nIn this chapter, we cover important prerequisite knowledge that will motivate our\\ndiscussion of deep learning techniques in the main text and the optional sidebars at\\nthe end of select chapters. Deep learning has recently experienced a renaissance, both\\nin academic research and in the industry. It has pushed the limits of machine learning\\nby leaps and bounds, revolutionizing fields such as computer vision and natural\\nlanguage processing. However, it is important to remember that deep learning is, at\\nits core, a culmination of achievements in fields such as calculus, linear algebra, and\\nprobability. Although there are deeper connections to other fields of mathematics, we\\nfocus on the three listed here to help us broaden our perspective before diving into\\ndeep learning. These fields are key to unlocking both the big picture of deep learning\\nand the intricate subtleties that make it as exciting as it is. In this first chapter on\\nbackground, we cover the fundamentals of linear algebra.\\nData Structures and Operations\\nThe most important data structure in linear algebra (whenever we reference linear\\nalgebra in this text, we refer to its applied variety) is arguably the matrix, a 2D array\\nof numbers where each entry can be indexed via its row and column. Think of an\\nExcel spreadsheet, where you have offers from Company X and Company Y as two\\nrows, and the columns represent some characteristic of each offer, such as starting\\nsalary, bonus, or position, as shown in Table 1-1.\\n1'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 17}, page_content='Table 1-1. Excel spreadsheet\\n \\nCompany X\\nCompany Y\\nSalary\\n$50,000\\n$40,000\\nBonus\\n$5,000\\n$7,500\\nPosition\\nEngineer\\nData Scientist\\nThe table format is especially suited to keep track of such data, where you can index\\nby row and column to find, for example, Company X’s starting position. Matrices,\\nsimilarly, are a multipurpose tool to hold all kinds of data, where the data we work in\\nthis book is of numerical form. In deep learning, matrices are often used to represent\\nboth datasets and weights in a neural network. A dataset, for example, has many\\nindividual data points with any number of associated features. A lizard dataset might\\ncontain information on length, weight, speed, age, and other important attributes.\\nWe can represent this intuitively as a matrix or table, where each row represents an\\nindividual lizard, and each column represents a lizard feature, such as age. However,\\nas opposed to Table 1-1, the matrix stores only the numbers and assumes that the\\nuser has kept track of which rows correspond to which data points, which columns\\ncorrespond to which feature, and what the units are for each feature, as you can see in\\nFigure 1-1.\\nFigure 1-1. A comparison of tables and matrices\\nOn the right side, we have a matrix, where it’s assumed, for example, that the age\\nof each lizard is in years, and Komodo Ken weighs a whopping 50 kilograms! But\\nwhy even work with matrices when tables clearly give the user more information?\\nWell, in linear algebra and even deep learning, operations such as multiplication\\nand addition are done on the tabular data itself, but such operations can only be\\ncomputed efficiently when the data is in solely numerical format.\\nMuch of the work in linear algebra centers on the emergent properties of matrices,\\nwhich are especially interesting when the matrix has certain base attributes, and\\noperations on these data structures. Vectors, which can be seen as a subset type of\\nmatrices, are a 1D array of numbers. This data structure can be used to represent\\nan individual data point or the weights in a linear regression, for example. We cover\\nproperties of matrices and vectors as well as operations on both.\\n2 \\n| \\nChapter 1: Fundamentals of Linear Algebra for Deep Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 18}, page_content='Matrix Operations\\nMatrices can be added, subtracted, and multiplied—there is no division of matrices,\\nbut there exists a similar concept called inversion.\\nWhen indexing a matrix, we use a tuple, where the first index represents the row\\nnumber and the second index represents the column number. To add two matrices A\\nand B, one loops through each index (i,j) of the two matrices, sums the two entries at\\nthe current index, and places that result in the same index (i,j) of a new matrix C, as\\ncan be seen in Figure 1-2.\\nFigure 1-2. Matrix addition\\nThis algorithm implies that we can’t add two matrices of different shapes, since\\nindices that exist in one matrix wouldn’t exist in the other. It also implies that the\\nfinal matrix C is of the same shape as A and B. In addition to adding matrices,\\nwe can multiply a matrix by a scalar. This involves simply taking the scalar and\\nmultiplying each of the entries of the matrix by it (the shape of the resultant matrix\\nstays constant), as depicted in Figure 1-3.\\nFigure 1-3. Scalar-matrix multiplication\\nThese two operations, addition of matrices and scalar-matrix multiplication, lead us\\ndirectly to matrix subtraction, since computing A – B is the same as computing the\\nmatrix addition A + (–B), and computing –B is the product of a scalar –1 and the\\nmatrix B.\\nMultiplying two matrices starts to get interesting. For reasons beyond the scope of\\nthis text (motivations in a more theoretical flavor of linear algebra where matrices\\nrepresent linear transformations), we define the matrix product A · B as:\\nEquation 1-1. Matrix multiplication formula\\nA · B i, j = ∑k′ = 1\\nk\\nAi, k′Bk′, j\\nData Structures and Operations \\n| \\n3'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 19}, page_content='In simpler terms, this means that the value at the index (i,j) of A · B is the sum of the\\nproduct of the entries in the ith row of A with those of the jth column of B. Figure 1-4\\nis an example of matrix multiplication.\\nFigure 1-4. Matrix multiplication\\nIt follows that the rows of A and the columns of B must have the same length, so\\ntwo matrices can be multiplied only if the shapes align. We use the term dimension\\nto formally represent what we have referred to so far as shape: i.e., A is of dimen‐\\nsion m by k, meaning it has m rows and k columns, and B is of dimension k by n. If\\nthis weren’t the case, the formula for matrix multiplication would give us an indexing\\nerror. The dimension of the product is m by n, signifying an entry for every pair of\\nrows in A and columns in B. This is the computational way of thinking about matrix\\nmultiplication, and it doesn’t lend itself well to theoretical interpretation. We’ll call\\nEquation 1-1 the dot product interpretation of matrix multiplication, which will make\\nmore sense after reading “Vector Operations” on page 6.\\nNote that matrix multiplication is not commutative, i.e., A · B ≠B · A. Of course, if\\nwe were to take a matrix A that is 2 by 3 and a matrix B that is 3 by 5, for example,\\nby the rules of matrix multiplication, B · A doesn’t exist. However, even if the product\\nwere defined due to both matrices being square, where square means that the matrix\\nhas an equal number of rows and columns, the two products will not be the same\\n(this is an exercise for you to explore on your own). However, matrix multiplication is\\nassociative, i.e., A · B + C = A · B + A · C.\\nLet’s delve into matrix multiplication a bit further. After some algebraic manipulation,\\nwe can see that another way to formulate matrix multiplication is:\\nA · B · , j = A · B · , j\\nThis states that the jth column of the product A · B is the matrix product of A and\\nthe jth column of B, a vector. We’ll call this column vector interpretation of matrix\\nmultiplication, as can be seen in Figure 1-5.\\n4 \\n| \\nChapter 1: Fundamentals of Linear Algebra for Deep Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 20}, page_content='Figure 1-5. Matrix multiplication: another view\\nIn a later section, we cover matrix-vector multiplication and different ways to think\\nabout this computation, which leads to more exciting properties regarding matrices.\\nOne of the most important matrices in linear algebra is the identity matrix, which is\\na square matrix with 1s along the main diagonal and 0s in every other entry. This\\nmatrix is usually denoted as I. When computing the product of I with any other\\nmatrix A, the result is always A—thus its name, the identity matrix. Try multiplying\\na few matrices of your choosing with the appropriate-sized identity matrix to see why\\nthis is the case.\\nAs noted at the beginning of the section, there is no such division operation for\\nmatrices, but there is the concept of inversion. The inverse of matrix A is matrix B,\\nsuch that AB = BA = I, the identity matrix (similar in idea to a number’s reciprocal—\\nwhen dividing by a number on both sides of an equation, we can also think of this\\noperation as multiplying both sides by its reciprocal). If such a B exists, we denote\\nit as A−1. From this definition, we know that A must be, at the very least, a square\\nmatrix since we are able to multiply A on either side by the same matrix A−1, as\\nyou can see in Figure 1-6. Matrix inversion is deeply tied to other properties of\\nmatrices that we will discuss soon, which are the backbone of fundamental data\\nscience techniques. These techniques influenced their more complex neural variants,\\nwhich researchers still use to this day.\\nFigure 1-6. Matrix inversion\\nData Structures and Operations \\n| \\n5'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 21}, page_content='When trying to solve an equation such as Ax = b for x, we would multiply both sides\\non the left by A−1 to get x = A−1b if A is invertible. There exists another necessary\\ncondition for A to be invertible, which we’ll discuss later. \\nVector Operations\\nVectors can be seen as a subset of matrices, so a lot of the operations follow from\\nthe properties of addition, subtraction, multiplication, and inversion. However, there\\nis some vector-specific terminology we should cover. When a vector is of dimension\\n1 × n, we call this vector a row vector, and when the vector is of dimension n × 1,\\nwe call it a column vector. When taking matrix product of a row vector and a column\\nvector, we can see that the result is a single number—we call this operation the dot\\nproduct. Figure 1-7 is an example of the dot product of two vectors.\\nFigure 1-7. Dot product\\nNow the reason for the name dot product interpretation of matrix multiplication\\nmight make more sense. Looking back at Equation 1-1, we see that every entry in the\\nmatrix product A · B i, j is just the dot product of the corresponding row Ai, ·  and\\nthe corresponding column B · , j.\\nWhen the dot product of two vectors is 0, we term the two vectors to be orthogonal.\\nOrthogonality is a generalization of perpendicularity to any dimension, even those far\\nbeyond the ones we can imagine. You can check in the 2D case, for example, that any\\ntwo vectors are perpendicular if and only if (also termed iff) they have a dot product\\nof 0.\\nWhen we instead take the matrix product of a column vector and a row vector, we\\nsee that the result is quite surprisingly a matrix! This is termed the outer product.\\nFigure 1-8 is the outer product of the same two vectors from the dot product\\nexample, except their roles as row and column vectors have been reversed.\\nFigure 1-8. Outer product\\n6 \\n| \\nChapter 1: Fundamentals of Linear Algebra for Deep Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 22}, page_content='Matrix-Vector Multiplication\\nWhen multiplying a matrix A and a vector v, we can again do this via the dot\\nproduct interpretation of matrix multiplication, as described previously. However, if\\nwe instead manipulate the expression slightly, we’ll see that another way to formulate\\nthis product is:\\nAv = ∑jvjA · , j\\nWhere each vj is a constant to be multiplied with its corresponding column of A. Fig‐\\nure 1-9 is an example of this method in action.\\nFigure 1-9. Matrix-vector multiplication\\nThis section introduced matrix and vector operations, which are fundamental to\\nunderstanding the inner workings of a neural network. In the next section, we will\\nuse our knowledge of matrix and vector operations to concretely define some matrix\\nproperties, which serve as the basis for important data science and deep learning\\ntechniques.\\nThe Fundamental Spaces\\nIn this section, we will formally discuss some important matrix properties and\\nprovide some background knowledge on key algorithms in deep learning, such as\\nrepresentation learning.\\nThe Column Space\\nConsider the set of all possible vectors v and their products Av. We term this the col‐\\numn space of A, or C(A). The term column space is used because C(A) represents\\nall possible linear combinations of the columns of A, where a linear combination of\\nvectors is a sum of constant scalings of each vector. The constant scaling for each\\nThe Fundamental Spaces \\n| \\n7'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 23}, page_content='column vector of A is determined by the choice of v, as we just saw in the previous\\nsection.\\nThe column space is an example of a vector space, which is the space defined by a\\nlist of vectors and all possible linear combinations of this collection. Properties for\\nformally defining a vector space pop up directly from this intuition. For example, if\\na set of vectors is a vector space, then the vector that arises from multiplying any\\nvector in the space by a scalar must also be in the space. In addition, if we were to add\\nany two vectors in the space, the result should still be in the space. In both of these\\noperations, the vectors we start with are known to be in the vector space, and thus\\ncan be formulated as linear combinations of the original list. By performing scalar\\nmultiplication or addition on the vectors in question, we are just computing linear\\ncombinations of linear combinations, which are still linear combinations, as can be\\nseen in Figure 1-10.\\nFigure 1-10. The sum, or linear combination, of the two linear combinations 3a and 2b\\n+ 2c is still a linear combination of the original vectors a, b, and c\\n8 \\n| \\nChapter 1: Fundamentals of Linear Algebra for Deep Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 24}, page_content='We term these key properties of vector spaces closed under scalar multiplication\\nand closed under addition. If a set of vectors doesn’t always satisfy either of these\\nproperties, then the set clearly doesn’t contain all possible linear combinations of the\\noriginal list and is not a vector space.\\nAn example of a vector space you’re probably familiar with is ℝ3, or the entire space\\ndefined by the x-y-z coordinate axis. The reason for the notation ℝ3 is that each\\ncoordinate can take on any value in the reals, or ℝ, and there are three coordinates\\nthat uniquely define any such vector in this space. A collection of vectors that defines\\nthis space are the vectors (0,0,1),(0,1,0),(1,0,0), the unit vectors of each axis. Any\\nvector (a,b,c) in the space can be written as a*(1,0,0) + b*(0,1,0) + c*(0,0,1), a linear\\ncombination of the collection. In the other direction, any possible linear combination\\nof the three vectors represents some vector (a,b,c) that lies in ℝ3.\\nOften, there exist matrices A for which some columns are linear combinations of\\nother columns. For example, imagine if in our lizard dataset from Figure 1-2, we had\\nan additional feature for each lizard’s weight, but instead in pounds. This is a clear\\nredundancy in the data since this feature is completely determined by the feature for\\nweight in kilograms. In other words, the new feature is a linear combination of the\\nother features in the data—simply take the column for weight in kilograms, multiply\\nit by 2.2, and sum it with all the other columns multiplied by zero to get the column\\nfor weight in pounds. Logically, if we were to remove these sorts of redundancies\\nfrom A, then C(A) shouldn’t change. One method to do this is to first create a list\\nof all the original column vectors of A, where order is assigned arbitrarily. When\\niterating through the list, check to see if the current vector is a linear combination of\\nall the vectors that precede it. If so, remove this vector from the list and continue. It’s\\nclear that the removed vector provided no additional information beyond the ones\\nwe’ve already seen.\\nThe resulting list is called the basis of C(A), and the length of the basis is the dimen‐\\nsion of C(A). We say that the basis of any vector space spans the space, which means\\nthat all of the elements in the vector space can be formulated as a linear combination\\nof basis vectors. In addition, the basis vectors are linearly independent, which means\\nthat none of the vectors can be written as a linear combination of the others, i.e.,\\nno redundancies. Going back to the example where we defined vector space, (0,0,1),\\n(0,1,0),(1,0,0) would be a basis for the space ℝ3, since no vector in the list is a linear\\ncombination of the others, and this list spans the entire space. And instead, the list\\n(0,0,1),(0,1,0),(1,0,0),(2,5,1) spans the entire space, but is not linearly independent\\nbecause (2,5,1) can be written as a linear combination of the first three vectors (we\\ncall such a list of vectors a spanning list, and of course the set of bases for a vector\\nspace is a subset of the set of spanning lists for the same space).\\nAs we alluded to in the discussion of our lizard dataset, the basis of the column space,\\ngiven each lizard feature is a column, is a concise representation of the information\\nThe Fundamental Spaces \\n| \\n9'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 25}, page_content='represented in the feature matrix. In the real world, where we often have thousands\\nof features (e.g., each pixel in an image), achieving a concise representation of our\\ndata is quite desirable. Though this is a good start, identifying the clear redundancies\\nin our data often isn’t enough, as the randomness and complexity that exist in the\\nreal world tend to obscure these redundancies. Quantifying relationships between\\nfeatures can inform concise data representations, as we discuss at the end of this\\nchapter and in Chapter 9 on representation learning.\\nThe Null Space\\nAnother key vector space is the null space of a matrix A, or N(A). This space consists\\nof the vectors v such that Av = 0. We know that v = 0, the trivial solution, will\\nalways satisfy this property. If only the trivial solution is in the null space of a matrix,\\nwe call the space trivial. However, it is possible that there exist other solutions to\\nthis equation depending on the properties of A, or a nontrivial null space. For a\\nvector v to satisfy Av = 0, v must be orthogonal to each of the rows of A, as shown in\\nFigure 1-11.\\nFigure 1-11. The implication that the dot product between each row and the vector v\\nmust be equal to 0\\nLet’s assume A is of dimension 2 by 3, for example. In our case, A’s rows cannot\\nspan ℝ3 due to A having only two rows (remember from our recent discussion that\\nall bases have the same length, and all spanning lists are at least as long as all bases, so\\nA’s rows can be neither of these). At best, A’s rows define a plane in the 3D coordinate\\nsystem. The other two options are that the rows define a line or a point. The former\\noccurs when A either has two nonzero rows, where one is a multiple of the other, or\\nhas one zero row and one nonzero row. The latter occurs when A has two zero rows,\\nor in other words, is the zero matrix.\\nIn the case where A’s row space defines a plane (or even a line for that matter), all we’d\\nneed to do to find a vector in N(A) is:\\n10 \\n| \\nChapter 1: Fundamentals of Linear Algebra for Deep Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 26}, page_content='1. Pick any vector v that doesn’t lie in A’s row space.\\n1.\\n2. Find its projection v’ onto the row space, where the projection of v is defined\\n2.\\nas the vector in the space closest to v. Geometrically, the projection looks as if\\nwe had dropped a line down from the tip of v perpendicular to the space, and\\nconnected a vector from the origin to that point on the space.\\n3. Compute v – v’, which is orthogonal to the row space and thus, each row vector.\\n3.\\nFigure 1-12 depicts this.\\nFigure 1-12. Finding a vector in N(A)\\nNote that v – v’ is perpendicular to R(A), the row space of A, since\\nv’ was formed by dropping a perpendicular to the plane down from\\nits tip.\\nThe Fundamental Spaces \\n| \\n11'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 27}, page_content='An important takeaway is that nontrivial solutions to Av = 0 exist when the rows\\nof A do not span ℝ3; more generally, if A is of dimension m by n, nontrivial solutions\\nto Av = 0 exist when the row vectors do not span ℝn. The process is similar to that\\nshown: pick a vector in ℝn not in the row space, find its projection onto the row\\nspace, and subtract to get a vector in null space.\\nBut we still must show that N(A) itself is a vector space. We can easily see that any\\nlinear combination of nontrivial solutions to Av = 0 is still a solution. For example,\\ngiven two nontrivial solutions v1 and v2 and their linear combination c1v1 + c2v2,\\nwhere c1 and c2 are constants, we see that:\\nA c1v1 + c2v2\\n= A c1v1 + A c2v2\\n= c1Av1 + c2Av2\\n= c1 * 0 + c2 * 0\\n= 0\\n  \\nWhere the first equality arises from the associativity of matrix multiplication and the\\nsecond from the fact that c1 and c2 are constants. Note that this logic can be used\\nfor any number of nontrivial solutions, not just two. Thus, the null space is defined\\nby some collection of vectors that can be boiled down to a basis, and contains all\\npossible linear combinations of these vectors. These characteristics make the null\\nspace a vector space.\\nThis is all deeply connected to one of the key matrix operations presented, the matrix\\ninverse. We can think of a matrix’s inverse as undoing the action of a matrix upon any\\nother entity. For example, if we were to compute Av and multiply on the left by A−1,\\nwe should be left with our initial v. However, depending on the properties of A, there\\ncan exist ambiguities as to how to “undo” its action. For example, let’s say v was\\nsome nonzero vector, but for some reason, Av = 0. If we were to multiply on the left\\nby A−1, we’d be left with v = 0 instead of our initial v. This unfortunately goes against\\nthe properties of an inverse, and we declare that such a matrix is noninvertible,\\nor singular. But why does this happen in the first place? This goes back to our\\nobservation about ambiguities. Because an inverse is supposed to undo the action of a\\nmatrix, if there are multiple initial vectors that map to the same vector via the matrix’s\\naction, trying to undo this action is impossible. Going back to our example, we know\\nthat nonzero vectors are mapped to 0 by A when A has a nontrivial null space. Thus,\\nany matrix with a nontrivial null space is also singular.\\n12 \\n| \\nChapter 1: Fundamentals of Linear Algebra for Deep Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 28}, page_content='Next, we will cover eigenvectors and eigenvalues, which puts all of the information\\nwe’ve learned so far into practice.\\nEigenvectors and Eigenvalues\\nMatrices can act on vectors in many different ways. For most combinations of\\nmatrices and vectors, plotting the vector and its transformation doesn’t provide us\\nwith any interesting patterns. However, for certain matrices and specific vectors for\\nthose matrices, the action of the matrix upon the vector gives us an informative\\nand surprising result: the transformation is a scalar multiple of the original. We call\\nthese vectors eigenvectors, and the scalar multiple its corresponding eigenvalue. In this\\nsection, we discuss these very special vectors, relate back to the material presented in\\nthe previously, and begin the discussion connecting the theory of linear algebra with\\nthe practice of data science.\\nMore formally, an eigenvector for a matrix A is a nonzero vector v such that Av =\\ncv, where c is some constant (including zero, potentially), as shown in Figure 1-13.\\nFigure 1-13. The vector (1,1) is an eigenvector of our matrix, with a corresponding\\neigenvalue of 3\\nNote that if we were to pick any random vector, such as (2,5),\\nthe transformation wouldn’t look as meaningful as it does in\\nFigure 1-13.\\nOf course, if A is a rectangular matrix, it’s impossible for A to have any eigenvectors.\\nThe original vector and its transformation have different sizes, and thus transfor‐\\nmation couldn’t be a scalar multiple of the original. For this reason, we limit our\\ndiscussion in this section to square matrices.\\nThe simplest example is the identity matrix. Every nonzero vector is an eigenvector\\nof the identity matrix since Iv = v for all v, with each having an eigenvalue of\\nEigenvectors and Eigenvalues \\n| \\n13'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 29}, page_content='1. Oftentimes, however, the eigenvectors of a matrix won’t be so obvious. How do we\\nfind these vectors and their corresponding eigenvalues? We know the conditions of\\nany potential eigenvector; that is, if v is an eigenvector, it must satisfy Av = cv for\\nsome scalar c:\\nAv = cv\\nAv −cv = 0\\nA −cI v = 0\\nThe implication here is that if Av = cv, then A – cI must have a nontrivial null space.\\nIn the other direction, if we find a c such that A – cI has a nontrivial null space, the\\nnonzero vectors in the null space are eigenvectors of A. Of course, if A itself has a\\nnontrivial null space, then all nonzero v in the null space satisfy the above implication\\nwhen c is 0. More generally, however, we must find the c such that A – cI has a\\nnontrivial null space. As established previously, checking for a nontrivial null space\\nis equivalent to testing for a matrix being singular. For reasons beyond the scope of\\nthis text, one way to test if A −cI for some c is a singular matrix is to check whether\\nits determinant is 0. We won’t go into too much depth here, but we can think about\\nthe determinant as a function, or polynomial, that encodes properties of the matrix\\nand results in a value of 0 iff the matrix is singular.\\nHowever, it would be inefficient, and frankly impossible, for us to test every possible c\\nfor a zero determinant. We can instead think of c as a variable in an equation and\\nsolve for it via the characteristic polynomial, which is the determinant of the matrix\\nA – cI set equal to 0. The roots of this polynomial give us the eigenvalues of A. To find\\ntheir corresponding eigenvectors, we can plug each solution for c into A – cI and then\\nsolve for the v that make(s) (A – cI)v = 0.\\nCalculating the determinant for any matrix of reasonable size is\\nquite prohibitive in terms of computational cost. Although we\\nwon’t delve further into this, algorithms today use a version of\\nthe QR algorithm (named after the QR matrix decomposition) to\\ncalculate the eigenvalues of a matrix. If you’d like to learn more\\nabout these and similar such algorithms, we highly recommend\\nlecture notes or books on numerical linear algebra.\\nHow does our study of eigenvalues and eigenvectors connect to that of data science?\\nPrincipal component analysis, or PCA, is one of the most famous algorithms in data\\nscience, and it uses the eigenvectors and eigenvalues of a special matrix called the\\ncorrelation matrix, which represents the quantifiable relationships between features\\nalluded to earlier, to perform dimensionality reduction on the original data matrix.\\nWe will discuss correlation and related concepts in the next chapter on probability,\\nand learn more about PCA in Chapter 8.\\n14 \\n| \\nChapter 1: Fundamentals of Linear Algebra for Deep Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 30}, page_content='Summary\\nIn this chapter, we investigated some of the basics of applied linear algebra. We\\nlearned about the key data structures and operations that rule both applied linear\\nalgebra and deep learning, and different ways to view these fundamental operations.\\nFor example, we learned that the dot product view of matrix multiplication was\\nimportant from a computational lens, while the column vector approach led us into\\nour discussion on the fundamental spaces quite naturally. We also got a peek at some\\nof the surprising hidden properties of matrices, such as eigenvalues and eigenvectors,\\nand how these properties are widely utilized in data science even to this day. In\\nthe next chapter, we will learn about the field of probability, which is often used in\\ntandem with linear algebra to build complex, neural models used in the world.\\nSummary \\n| \\n15'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 31}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 32}, page_content='CHAPTER 2\\nFundamentals of Probability\\nProbability is a field of mathematics that quantifies our uncertainty regarding events.\\nFor example, when rolling dice or flipping a coin, barring any irregularities in the\\ndice or coin themselves, we are uncertain about the result to come. However, we can\\nquantify our belief in each of the potential outcomes via probabilities. We say, for\\nexample, that on every coin toss the probability of the coin showing up heads is 1\\n2.\\nAnd on every dice roll, we say the probability of a die facing up with a five is 1\\n6. These\\nare the sorts of probabilities we talk about with ease in our daily lives, but how can we\\ndefine and utilize them effectively? In this chapter we’ll discuss the fundamentals of\\nprobability and how they connect to key concepts in deep learning.\\nEvents and Probability\\nWhen running a trial such as rolling a dice or tossing a coin, we intuitively assign\\nsome belief to the trial’s possible outcomes. In this section, we aim to formalize some\\nof these concepts. In particular, we will begin by working in this discrete space, where\\ndiscrete signifies a finite or countably infinite number of possibilities. Both rolling a\\ndice and tossing a coin are in the discrete space—when rolling a fair dice there are\\nsix possible outcomes and when tossing a fair coin there are two. We term the entire\\nset of possibilities for an experiment the sample space. For example, the numbers one\\nthrough six would make up the sample space for rolling a fair dice. We can define\\nevents as subsets of the sample space. The event of rolling at least a three corresponds\\nwith the dice facing up any number in the subset of three, four, five, and six in\\nthe sample space defined previously. A set of probabilities that sum to one over all\\noutcomes in the sample space is termed a probability distribution over that sample\\nspace, and these distributions will be the main focus of our discussion.\\n17'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 33}, page_content='In general, we won’t worry too much about where exactly these probabilities come\\nfrom, as that requires a much more rigorous and thorough examination beyond\\nthe scope of this text. However, we will give some intuition about the different\\ninterpretations. At a high level, the frequentist view sees the probability of an outcome\\nas arising from its frequency over a long-run experiment. In the case of fair dice, this\\nview claims we can say the probability of any side of the dice showing up on a given\\nroll is 1\\n6, since performing a large number of rolls and counting up the occurrences of\\neach side will give us an estimate that is roughly this fraction. As the number of rolls\\nin the experiment grows, we see that this estimate gets closer and closer to the limit 1\\n6,\\nthe outcome’s probability.\\nOn the other hand, the Bayesian view of probability is based more on quantifying our\\nprior belief in hypotheses and how we update our beliefs in light of new data. For\\na fair dice, the Bayesian view would claim there is no prior information, both from\\nthe dice’s structure and the rolling process, that would suggest any side of the dice as\\nbeing more likely to turn up than any other side. Thus, we would say each outcome\\nhas probability 1\\n6, our prior belief. The set of probabilities, in this case all being 1\\n6,\\nassociated with each outcome is termed our prior. As we see new data, the Bayesian\\nview gives us a methodology to update our prior accordingly, where we term this\\nnew belief our posterior. This Bayesian view is sometimes directly applied to neural\\nnetwork training, where we first assume that each weight in the network has some\\nprior associated with it. As we train the network, we update the prior associated with\\neach weight accordingly to better fit the data we see. At the end of the training, we are\\nleft with a posterior distribution associated with each weight.\\nWe will assume throughout this chapter that the probabilities associated with any\\noutcome have been determined via reasonable methods, and focus on how we can\\nmanipulate these probabilities for use in our analyses. We start with the four tenets of\\nprobability, specifically in the discrete space:\\n1. The sum of probabilities for all possible outcomes in a sample space must be\\n1.\\nequal to one. In other words, the probability distribution over the sample space\\nmust sum to one. This should make sense intuitively, since the set of all outcomes\\nin the sample space must represent the entire set of possibilities. The probability\\ndistribution not summing to one would imply the existence of possibilities not\\naccounted for, which is contradictory. Mathematically, we say that for any valid\\nprobability distribution, ∑oP o = 1, where o represents an outcome.\\n2. Let E1 be an event, and recall that we define an event as a subset of possi‐\\n2.\\nble outcomes. We call E1\\nc the complement of E1, or all possible outcomes in\\nthe sample space that are not in E1. The second tenet of probability is that\\nP E1 = 1 −P E1\\nc . This is just an application of the first tenet—if this were not\\ntrue, it would clearly contradict the first tenet. In Figure 2-1, we see an example\\n18 \\n| \\nChapter 2: Fundamentals of Probability'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 34}, page_content='of this, where S represents the entire space of outcomes, and the event and its\\ncomplement together form the entirety of S.\\nFigure 2-1. Event A and its complement interact to form the entire set of possibili‐\\nties, S. The complement simply defines all the possibilities not originally in A.\\n3. Let E1 and E2 be two events, where E1 is a subset (not necessarily strict) of E2.\\n3.\\nThe third tenet is that P E1 ≤P E2 . This, again, shouldn’t be too surprising—\\nthe second event has at least as many outcomes as the first event, and all the\\noutcomes the first event has since the second is a superset of the first. If this\\ntenet were not true, that would imply the existence of outcomes with negative\\nprobability, which is impossible from our definitions.\\n4. The fourth and last tenet of probability is the principle of inclusion and\\n4.\\nexclusion, which states that P A ∪B = P A + P B −P A ∩B . For those not\\nfamiliar with this terminology, the ∪ denotes the union of the two events, a\\nset operation that takes the two events and returns an event that contains all\\nelements from the two original sets. The ∩, or intersection, is a set operation that\\nreturns an event that contains all elements belonging to both of the two original\\nsets. The idea behind the equality presented is that by just naively summing the\\nprobabilities of A and B, we double-count the elements that belong to both sets.\\nThus, to accurately obtain the probability of the union, we must subtract the\\nprobability of the intersection. In Figure 2-2, we show two events and what their\\nintersection would look like physically, while the union is all the outcomes in the\\ncombined area of the events.\\nFigure 2-2. The middle sliver is the overlap between the two sets, containing all the\\noutcomes that are in both sets. The union is all the events in the combined area of\\nthe two circles; if we were to add their probabilities naively, we would double-count\\nall the outcomes in the middle sliver.\\nEvents and Probability \\n| \\n19'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 35}, page_content='These tenets of probability find their way into everything that has to do with the field.\\nFor example, in deep learning, most of our problems fall into one of two categories:\\nregression and classification. In the latter, we train a neural model that can predict\\nthe likelihood that the input belongs to one of a discrete number of classes. The\\nfamous MNIST digits dataset, for example, provides us with pictures of digits and\\nassociated numerical labels in the range of 0 through 9. Our objective is to build a\\nclassifier that can take in this picture and return the most likely label as its guess.\\nThis is naturally formulated as a problem in probability—the classifier produces a\\nprobability distribution over the sample space, 0 through 9, for any given input and\\nits best guess is the digit that is assigned the highest probability. How does this relate\\nto our tenets? Since the classifier is producing a probability distribution, it must\\nfollow the tenets. For example, the probabilities associated with each digit must sum\\nto one—a quick back-of-the-envelope check to ensure the model isn’t buggy. In the\\nnext section, we cover probabilities where we are initially given relevant information\\nthat affects our beliefs and how to use that information.\\nConditional Probability\\nKnowing information often changes our beliefs, and by consequence, our probabili‐\\nties. Going back to our classic dice example, we may roll the dice thinking that it’s\\nfair, while in reality there’s a hidden weight at the dice’s core, making it more likely to\\nland up a number greater than three. As we roll the dice, we of course start to notice\\nthis pattern, and our belief regarding the dice’s fairness starts to shift. This is at the\\ncore of conditional probability itself. Instead of thinking simply about P biased  or\\nP fair , we have to think about probabilities like P biased information  instead.\\nThis quantity, which we term a conditional probability, is spoken as “the probability\\nthe dice is biased given the information we’ve seen.”\\nHow do we think about such probabilities intuitively? For starters, we must imagine\\nthat we are now in a different universe than the one we started in. The new universe\\nis one that incorporates the information we’ve seen since the start of the experiment,\\ne.g., our past dice rolls. Going back to our MNIST example, the probability distri‐\\nbution that the trained neural net produces is actually a conditional probability\\ndistribution. The probability that the input image is zero, for example, can be seen\\nas P(0|input). In plain English, we want to find the probability of a zero given all\\nof the pixels that make up the specific input image we fed into our neural net. Our\\nnew universe is the universe in which the input pixels have taken on this specific\\nconfiguration of values. This is distinct from simply looking at P(0), the probability\\nof returning a zero, which we can think about in terms of prior belief. Without any\\nknowledge of the input pixel configuration, we’d have no reason to believe that the\\npossibility of returning a zero is any more or less likely than that of any other digit.\\n20 \\n| \\nChapter 2: Fundamentals of Probability'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 36}, page_content='Sometimes, seeing certain information does not change our probabilities—we call\\nthis property independence. For example, Tom Brady may have thrown a touchdown\\npass after the third roll of our experiment, but incorporating that information into\\nour new universe should (hopefully!) have no impact on the likelihood of the dice\\nbeing biased. We state this independence property as P(biased|Tom Brady throws a\\ntouchdown pass) = P(biased). Note that any two events E1 and E2 that satisfy this\\nproperty are independent. Perhaps slightly more counterintuitively, if it happens to\\nbe the case that all of our dice rolls so far don’t numerically change our prior belief\\nregarding the dice’s fairness (maybe the dice rolls so far have shown up evenly across\\none through six and our initial prior belief was that the dice was fair), we’d still say\\nthat these events are independent. Finally, note that independence is symmetric: if\\nP E1 E2 = P E1 , then it is also the case that P E2 E1 = P E2 .\\nIn the previous section, we introduced intersection and union notation. It turns out\\nthat we can break down the intersection operation into a product of probabilities.\\nWe have the following equality: P E1 ∩E2 = P E1 E2 * P E2 . Let’s break down\\nthe intuition here. On the left side, we have the probability that both events E1\\nand E2 have occurred. On the right side, we have the same idea, but expressed\\nslightly differently. In the universe where both events have occurred, one way to\\narrive in this universe is to first have E2 occur, followed by E1. Porting this intuition\\ninto mathematical terms, we must first find the probability that E2 has occurred,\\nfollowed by the probability that E1 has occurred in the universe where E2 has already\\noccurred. How do we combine these two probabilities? Intuitively, it makes sense that\\nwe multiply them—we must have both events occur, the first unconditionally and the\\nsecond in the universe where the first has already occurred. Note that the order of\\nthese events doesn’t really matter, as both paths get us to the same universe. So, more\\ncompletely, P E1 ∩E2 = P E1 E2 * P E2 = P E2 E1 * P E1 .\\nHowever, some of these paths make much more physical sense than others. For\\nexample, if we think of E1 as the event where someone contracts a disease, and E2\\nas the event where the patient shows symptoms of the disease, the path in which the\\npatient contracts the disease and then shows symptoms makes much more physical\\nsense than the reverse.\\nIn \\nthe \\ncase \\nwhere \\nthe \\ntwo \\nevents \\nare \\nindependent, \\nwe \\nhave \\nthat\\nP E1 ∩E2 = P E1 E2 * P E2 = P E1 * P E2 . Hopefully this makes some intu‐\\nitive sense. In the independence scenario, the fact that E2 has occurred doesn’t\\naffect the chances of E1 occurring; i.e., incorporating this information into the new\\nuniverse doesn’t affect the probability of the next event. In the next section, we cover\\nrandom variables, which are relevant summaries of events and also have their own\\nprobability distributions.\\nConditional Probability \\n| \\n21'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 37}, page_content='Random Variables\\nOnce again, let’s consider the coin flipping experiment. If we flip a coin some finite\\nnumber of times, natural questions start to arise. How many heads did we encounter\\nduring our experiment? How many tails? How many tails until the first head? Every\\noutcome in such an experiment has an answer to each of the listed questions. If we\\nflip a coin say, five times, and we receive the sequence TTHHT, we have seen two\\nheads, three tails, and two tails until the first head.\\nWe can think of a random variable as a map, or a function, from the sample space to\\nanother space, such as the integers in Figure 2-3. Such a function would take as input\\nthe sequence TTHHT and output one of the three answers listed depending on the\\nquestion we ask. The value that the random variable takes on would be the output\\nassociated with result of the experiment. Although random variables are determinis‐\\ntic in that they map a given input to a single output, they are not deterministic in\\nthat they also have a distribution associated with their output space. This is due to the\\ninherent randomness in the experiment—depending on the probability of the input\\noutcome, its corresponding output may be more or less likely than other outputs.\\nFigure 2-3. Random variables X, Y, and Z all act on the same sample space, but have\\nvarying outputs. It’s important to keep in mind what you’re measuring!\\nNote that multiple inputs could map to the same output. For exam‐\\nple, X(HHH) = 3 in addition to X(HHTH) in Figure 2-3.\\nOne easy way to begin is to just think of this map as an identity function—whatever\\nwe flip or roll, its map in the output space is exactly the same as the input. Encoding\\na heads as a one and a tails as a zero, we can define a random variable representing\\n22 \\n| \\nChapter 2: Fundamentals of Probability'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 38}, page_content='the coin flip as whether the coin came up heads, i.e., C(1) = 1, where C is our random\\nvariable. In the dice scenario, the mapped output is the same as whatever we rolled,\\ni.e., D(5) = 5, where D is our random variable.\\nWhy should we care about random variables and their distributions? It turns out they\\nplay a vital role in deep learning and machine learning as a whole. For example, in\\nChapter 4, we will cover the concept of dropout, a technique for mitigating overfitting\\nin neural networks. The idea of a dropout layer is that, during training, it independ‐\\nently and at random masks every neuron in the previous layer with some probability.\\nThis prevents the network from becoming overly dependent on specific connections\\nor subnetworks. We can think of every neuron in the previous layer as representing\\na coin flip-type experiment. The only difference is that we set the probability of this\\nexperiment, rather than a fair coin having the default probability 1\\n2 of showing up\\neither side. Each neuron has a random variable X associated with it, with input one\\nif the dropout layer decides to mask it and zero otherwise. X is an identity function\\nfrom the input space to the output space, i.e., X(1) = 1 and X(0) = 0.\\nRandom variables, in general, need not be the identity map. Most functions you can\\nthink of are valid methods of mapping the input space to an output space where the\\nrandom variable is defined. For example, if the input space were every possible length\\nn sequence of coin flips, the function could be to count the number of heads in the\\nsequence and square it. Some random variables can even be expressed as functions\\nof other random variables, or a function of a function, as we will cover later. If we\\nagain consider the input space of every possible length n sequence of coin flips, the\\nrandom variable counting the number of heads in the input sequence is the same as\\ncounting whether each individual coin flip turned up heads and taking a sum of all\\nof those values. In mathematical terms, we say X = ∑i = 1\\nn\\nCi, where X is the random\\nvariable representing the total number of heads, and Ci is the binary random variable\\nassociated with the ith coin flip. Back to the dropout example, we can think of the\\nrandom variable representing the total number of masked-out neurons as the sum of\\nbinary random variables representing each neuron.\\nIn the future, when we want to refer to the event where the random variable takes\\non a specific value c (the domain being the output space we’ve been referring to, e.g.,\\nthe number of heads in a sequence of coin flips), we will write this concisely as X\\n= c. We denote the probability that the random variable takes on a specific value as\\nP(X = c), for example. The probability that the random variable takes on any given\\nvalue in the output space is just the sum of the probabilities of the inputs that map\\nto it. This should make some intuitive sense, as this is basically the fourth tenet of\\nprobability where the intersection between any two events is the empty set since all\\nthe events we start from are individual, distinct inputs. Note that P(X) itself is also a\\nprobability distribution that follows all the basic tenets of probability described in the\\nfirst section. In the next section, we consider statistics regarding random variables.\\nRandom Variables \\n| \\n23'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 39}, page_content='Expectation\\nAs we discussed, a random variable is a map from input space to output space, where\\ninputs are generated according to some probability distribution. The random variable\\ncan be thought of as a relevant summary of the input, and can take on many forms\\ndepending on the question we ask. Sometimes, it’s useful to understand statistics\\nregarding the random variable. For example, if we flip a coin eight times, how many\\nheads do we expect to see on average? And, of course, we don’t see the average\\nnumber of heads all the time—how much does the number of heads we see tend to\\nvary? The first quantity is what we call the random variable’s expectation, and the\\nsecond is the random variable’s variance.\\nFor a random variable X, we denote its expectation as E X . We can think of this\\nas the average value that X takes on, weighted by the probability of each of those\\noutcomes. Mathematically, this is written as E X = ∑oo * P X = o . Note that if all\\noutcomes o are equally likely, we get a simple average of all the outcomes. It makes\\nsense to use the probability of the outcome as a weighting, since some outcomes are\\nmore likely than others, and the average value we observe will be skewed toward\\nsuch outcomes. For a single fair coin flip, the expected number of heads would be\\n∑o ∈0, 1 o * P o = 0 * 0 . 5 + 1 * 0 . 5 = 0 . 5. In other words, we’d expect to see half of\\na head for any given fair coin flip. Of course, this makes no physical sense in that we\\ncould never possibly flip half of a head, but this gives you an idea of the proportions\\nwe’d expect to see over a long run experiment.\\nReturning to our example of length n sequences of coin flips, let’s try to find the\\nexpected number of heads in such a sequence. We have n + 1 possible number of\\nheads, and according to our formula, we’d need to find the probability of attaining\\neach possible number to use as our weights. Mathematically, we’d need to compute\\n∑x ∈0, ..., n x * P X = x , where X is the random variable representing the total num‐\\nber of heads. However, as n gets larger and larger, performing this calculation starts to\\nbecome more and more complicated.\\nInstead, let’s denote Xi as the binary random variable for the ith coin flip and use the\\nobservation we made in the last section of being able to break up the total number\\nof heads into a sum over heads/tails for all the individual coin flips. Since we know\\nX = X1 + X2 + ... + Xn, we can also say that E X = E X1 + X2 + ... + Xn . How does\\nmaking this substitution make our problem easier? We now introduce the concept\\nof linearity of expectation, which states we can break up the right side into the sum\\nE X1 + E X2 + ... + E Xn . We know that the expected number of heads for each\\nflip is 0.5, so the expected number of heads in a sequence of n flips is just 0.5*n. This\\nis much simpler than going down the previous route, as this approach’s difficulty does\\nnot scale with the number of flips.\\n24 \\n| \\nChapter 2: Fundamentals of Probability'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 40}, page_content='Let’s go over the simplification we made in a bit more detail. Mathematically, if we\\nhave any two independent random variables A and B:\\nE A + B = ∑a, b a + b * P A = a, B = b\\n= ∑a, b a + b * P A = a * P B = b\\n= ∑a, ba * P A = a * P B = b + b * P A = a * P B = b\\n= ∑a, ba * P A = a * P B = b + ∑a, bb * P A = a * P B = b\\n= ∑aa * P A = a ∑bP B = b + ∑bb * P B = b ∑aP A = a\\n= ∑aa * P A = a + ∑bb * P B = b\\n= E A + E B\\nNote that we made the independence assumption we talked about\\nearlier in the chapter here when we broke up the probability of\\nthe event A = a and the event B = b into a product of the two\\nindividual probabilities. The rest of the derivation doesn’t require\\nadditional assumptions, so we recommend working through the\\nalgebra on your own. Although we won’t show this for the depen‐\\ndent case, linearity of expectation also holds for dependent random\\nvariables.\\nGoing back to the dropout example, the expectation of the total number of masked\\nneurons can be broken up into a sum of expectations over each neuron. The expected\\nnumber of masked neurons, similarly to the expected number of heads in a sequence\\nof coin flips, is p*n, where p is the probability of being masked (and the expectation\\nof each individual binary random variable representing a neuron) and n is the\\nnumber of neurons.\\nAs mentioned, we don’t always see the expected number of occurrences of an event\\nin every repetition of an experiment. In some cases, such as the expected number of\\nheads in a single, fair coin flip from earlier, we never see it! Next, we will quantify\\nthe average deviation, or variance, from the expected value we see in repetitions of an\\nexperiment.\\nVariance\\nWe define the variance, or Var(X), as E X −μ 2 , where we let μ = E X . In plain\\nEnglish, this measure represents the average squared difference between the value X\\ntakes on and its expectation. Note that X −μ 2 itself is also a random variable since\\nit is a function of a function (X), which is still a function. Although we won’t get into\\nVariance \\n| \\n25'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 41}, page_content='too much detail about why we use this formula in particular, we encourage you to\\nthink about why we don’t use a formula such as E X −μ  instead. To obtain a slightly\\nsimpler form for the variance, we can perform the following simplification:\\nE X −μ 2 = E X 2 −2μX + μ2\\n= E X 2 −E 2μX + E μ2\\n= E X 2 −2μE X + μ2\\n= E X 2 −2E X 2 + E X 2\\n= E X 2 −E X 2\\nLet’s take a moment to go through each of these steps. In the first step, we fully\\nexpress the random variable as all of its component terms via classic binomial\\nexpansion. In the second step, we perform linearity of expectation to break out the\\ncomponent terms into their own, individual expectations. In the third step, we note\\nthat μ, or E X , and its square are both constants and thus can be pulled out of\\nthe surrounding expectation. They are constants since they are not a function of the\\nvalue X takes on and are instead evaluated using the entire domain (the set of values\\nX can take on). Constants can be seen as random variables that can take on only one\\nvalue, which is the constant itself. Thus, their expectations, or the average value the\\nrandom variable takes on, is the constant itself since we always see the constant. The\\nfinal steps are algebraic manipulations that bring us to the simplified result. Let’s use\\nthis formula to find the variance of the binary random variable representing a single\\nneuron under dropout, and p is the probability of the neuron being masked out:\\nE X 2 −E X 2 = ∑x ∈0, 1x2 * P X = x −∑x ∈0, 1x * P X = x\\n2\\n= ∑x ∈0, 1x2 * P X = x −p2\\n= p −p2\\n= p 1 −p\\nThese simplifications should make sense. We know from “Expectation” on page 24\\nthat the expectation of the binary random variable representing a neuron is just p,\\nand the rest is algebraic simplifications. We highly encourage you to work through\\nthese derivations on your own. As we start to think about the random variable\\nrepresenting the number of masked neurons in the entire layer, we naturally ask the\\nquestion of whether there exists a similar linearity property for variance as there does\\nfor expectation. Unfortunately, the property does not hold in general:\\n26 \\n| \\nChapter 2: Fundamentals of Probability'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 42}, page_content='Var A + B = E A + B 2 −E A + B 2\\n= E A2 + 2 * A * B + B2 −E A + E B\\n2\\n= E A2 + 2E A * B + E B2 −E A 2 −2E A E B −E B 2\\n= E A2 −E A 2 + E B2 −E B 2 + 2E A * B −2E A E B\\n= Var A + Var B + 2 E A * B −E A E B\\n= Var A + Var B + 2Cov A, B\\nAs we can see from the last line, the final term in the expression, which we call the\\ncovariance between the two random variables, ruins our hope for linearity. However,\\ncovariance is another key concept in probability—the intuition for covariance is that\\nit measures the dependence between two random variables. As one random variable\\nmore completely determines the value of another random variable (think of A as\\nthe number of heads in a sequence of coin flips and B as the number of tails in\\nthe same sequence of coin flips), the magnitude of the covariance increases. Thus, it\\nstands to reason that if A and B are independent random variables, the covariance\\nbetween them should be zero, and linearity should hold in this special case. We highly\\nencourage you to work through the math and show this on your own.\\nBack to the dropout example, the variance of the total number of masked neurons\\ncan be broken up into a sum of variances over each neuron, since each neuron is\\nmasked independently. The variance of the number of masked neurons is p(1 – p)*n,\\nwhere p(1 – p) is the variance for any given neuron and n is the number of neurons.\\nExpectation and variance in dropout allow us to understand more deeply what we\\nexpect to see when applying such a layer in a deep neural network.\\nBayes’ Theorem\\nReturning to our discussion on conditional probability, we noted that the probability\\nof intersection between two events could be written as a product of a conditional dis‐\\ntribution and a distribution over a single event. Let’s translate this into the language\\nof random variables, now that we have introduced this new terminology. We denote\\nA to be one random variable, and B to denote a second. Let a be a value that A can\\ntake on, and b be a value that B can take on. The analogy to the intersection operation\\nfor random variables is the joint probability distribution P(A=a,B=b), which denotes\\nthe event where A = a and B = b. We can think of A = a and B = b as individual\\nevents, and when we write P(A = a,B = b), we are considering the probability that\\nboth events have occurred, i.e., their intersection P A = a ∩B = b . Note that we\\ngenerally write the joint probability distribution as P(A,B), since this encompasses all\\npossible joint settings of the random variables A and B.\\nBayes’ Theorem \\n| \\n27'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 43}, page_content='We mentioned earlier that intersection operations could be written as the product of\\na conditional distribution and a distribution over a single event. Rewriting this in the\\nformat for random variables, we have P(A = a,B = b) = P(A = a|B = b)P(B = b). And\\nmore generally, considering all possible joint settings of the two random variables, we\\nhave P(A,B) = P(A|B)P(B). We also discussed how there always exists a second way of\\nwriting this joint distribution as a product: P(A = a,B = b) = P(B = b|A = a)P(A=a),\\nand more generally, P(A,B) = P(B|A)P(A). We noted that sometimes one of these\\npaths makes more sense than the other. For example, in the case where symptoms\\nare represented by A and disease is represented by B, the path in which B takes on\\na value b, and then A takes on a value a in that universe makes much more sense\\nthan the reverse since, biologically, people contract a disease first and only then show\\nsymptoms for that disease.\\nHowever, this doesn’t mean that the reverse isn’t useful. It is almost universally the\\ncase that people show up at a hospital with mild symptoms, and medical professio‐\\nnals must try to infer the most likely disease from these symptoms to effectively treat\\nthe underlying disease. Bayes’ Theorem gives us a way of calculating the probability of\\na disease given the observed symptoms. Since the same joint probability distribution\\ncan be written in the two ways mentioned in the previous paragraph, we have the\\nfollowing equality:\\nP B A = P A B P B\\nP A\\nIf B represents disease, while A represents symptoms, this gives us a method for com‐\\nputing the likelihood of any disease given the observed symptoms. Let’s analyze the\\nright side to see if the equality also makes intuitive sense. The likelihood of symptoms\\ngiven the disease times the likelihood of the disease is just the joint distribution,\\nwhich makes sense as the numerator here. The denominator is the likelihood of see‐\\ning those symptoms, which can also be expressed as a sum of the numerator over all\\npossible diseases. This is an instance of a more general process called marginalization,\\nor removing a subset of random variables from a joint distribution by summing over\\nall possible configurations of the subset:\\nP A = ∑bP A, B = b\\nIn more concise terms, we have:\\nP B = bquery A =\\nP B = bquery, A\\n∑bP B = b, A\\n28 \\n| \\nChapter 2: Fundamentals of Probability'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 44}, page_content='Bayes’ Theorem is a very valuable application of probability in the real world, espe‐\\ncially in the case of disease prediction. Additionally, if we replace the random variable\\nfor symptoms with a random variable representing the result of a test for a specific\\ndisease, and the random variable over all diseases with a random variable over pres‐\\nence of the specific disease, we can infer the likelihood of actually having a specific\\ndisease given a positive test for it using Bayes’ Theorem. This is a common problem\\nin most hospitals, and is especially relevant to epidemiology given the outbreak of\\nCOVID-19.\\nEntropy, Cross Entropy, and KL Divergence\\nProbability distributions, by definition, give us a way of comparing the likelihoods of\\nvarious possible events. However, even if we know the most likely event (or events)\\nthat is to occur, when running the experiment we are bound to see all sorts of\\nevents. In this section, we first consider the problem of defining a single metric that\\nencapsulates all of the uncertainty within a probability distribution, which we will\\ndefine as the distribution’s entropy.\\nLet’s set up the following scenario. I am a researcher who is running an experiment.\\nThe experiment could be something as simple as flipping a coin or rolling a dice.\\nYou are recording the results of the experiment. We are both in different rooms,\\nbut connected through a phone line. I run the experiment and receive a result, and\\ncommunicate that result to you via the phone. You record that result in a notebook,\\nwhere you pick some binary string representation of that result as what you write\\ndown. As a scribe, you are necessary in this situation—I may run hundreds of trials\\nand my memory is limited, so I cannot remember the results of all of my trials.\\nFor example, if I roll a dice and neither of us knows anything about the fairness of the\\ndice, you could denote the outcome one as “0,” two as “1,” three as “10,” four as “11,”\\nfive as “100,” and six as “101.” Whenever I communicate a result of the experiment\\nto you, you add that result’s corresponding string representation to the end of the\\nstring consisting of all results so far. If I were to roll a one, followed by two twos, and\\nfinally a one, using the encoding scheme defined so far you would have written down\\n“0110.”\\nAfter all runs of the experiment have ended, I have a meeting with you and try\\nto decipher this string “0110” into a sequence of outcomes for use in my research.\\nHowever, as the researcher, I am puzzled by this string—does it represent a one,\\nfollowed by two twos, and finally a one? Or does it represent a one, followed by a two,\\nfollowed by a three? Or even a one, followed by a four, followed by a one? It seems\\nthat there are at least a few possible translations of this string into outcomes using the\\nencoding scheme.\\nEntropy, Cross Entropy, and KL Divergence \\n| \\n29'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 45}, page_content='To prevent this situation from ever occurring again, we decide to enforce some limi‐\\ntations on the binary strings you can use to represent outcomes. We use what is called\\na prefix code, which disallows binary string representations of different outcomes\\nfrom being prefixes of each other. It’s not too difficult to see why this would result\\nin a unique translation of string to outcomes. Let’s say we have a binary string, some\\nprefix of which we have been able to successfully decode into a series of outcomes. To\\ndecode the rest of the string, or the suffix, we must first find the next outcome in the\\nseries. When we find a prefix of this suffix that translates to an outcome, we already\\nknow that, by definition, there is no smaller prefix that translates to a valid outcome.\\nWe now have a larger prefix of the binary string that has been successfully translated\\nto a series of outcomes. We then recursively use this logic until we have reached the\\nend of the string.\\nNow that we have some guidelines on string representations for outcomes, we redo\\nthe original experiment with one as “0,” two as “10,” three as “110,” four as “1110,” five\\nas “11110,” and six as “111110.” However, as noted earlier, I may carry out hundreds\\nof trials, and as the scribe you probably want to limit the amount of writing you have\\nto do. With no information about the dice, we can’t do too much better than this.\\nAssuming each outcome shows up with probability 1\\n6, the expected number of letters\\nyou’d need to write down per trial is 3.5. We could get down to 3 if we set one as\\n“000,” two as “001,” three as “010,” four as “011,” five as “100,” and six as “101,” for\\nexample.\\nBut what if we knew information about the dice? For example, what if it were a\\nweighted dice that showed up six almost all of the time? In that case, you probably\\nwant to assign a shorter binary string to six, for example “0” (instead of assigning “0”\\nto one) so you can limit the expected amount of writing you have to do. It makes\\nintuitive sense that, as the result of any single trial becomes more and more certain,\\nthe expected number of characters you’d need to write becomes lower by assigning\\nthe shortest binary strings to the most likely outcomes.\\nThis raises the question: given a probability distribution over outcomes, what is the\\noptimal encoding scheme, where optimal is defined as the fewest expected number\\nof characters you’d need to write per trial? Although this whole situation may feel\\na bit contrived, it provides us with a slightly different lens through which we can\\nunderstand the uncertainty within a probability distribution. As we noted, as the\\nresult of an experiment becomes more and more certain, the optimal encoding\\nscheme would allow the scribe to write fewer and fewer characters in expectation per\\ntrial. For example, in the extreme case where we already knew beforehand that a six\\nwould always show up, the scribe wouldn’t need to write anything down.\\n30 \\n| \\nChapter 2: Fundamentals of Probability'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 46}, page_content='It turns out that, although we won’t show it here, the best you can do is assign\\na binary string of length log2\\n1\\np xi  to each possible outcome xi, where p xi  is its\\nprobability. The expected string length of any given trial would then be:\\nEp x\\nlog2\\n1\\np x\\n= ∑xip xi log2\\n1\\np xi\\n= −∑xip xi log2 p xi\\nThis expression is defined as the entropy of a probability distribution. In the case\\nwhere we are completely certain of the final outcome (e.g., the dice always lands up\\nsix), we can evaluate the expression for entropy and see that we get a result of 0.\\nIn the case where we are completely certain of the final outcome (e.g., the dice always\\nlands up six), we can evaluate the expression for entropy and see that we get a result\\nof 0. Additionally, the probability distribution that has the highest entropy is the one\\nthat places equal probability over all possible outcomes. This is because, for any given\\ntrial, we are no more certain that a particular outcome will appear as opposed to any\\nother outcome. As a result, we cannot use the strategy of assigning a shorter string to\\nany single outcome.\\nNow that we have defined entropy, we can discuss cross entropy, which provides us a\\nway of measuring the distinctness of two distributions.\\nEquation 2-1. Cross entropy\\nCE p\\nq = Ep x\\nlog2\\n1\\nq x\\n= ∑xp x log2\\n1\\nq x = −∑xp x log2 q x\\nNote that cross entropy has a log\\n1\\nq x  term, which can be interpreted as the optimal\\nbinary string length assigned to each outcome, assuming outcomes appear according\\nto probability distribution q(x). However, note that this is an expectation with respect\\nto p(x), so how do we interpret this entire expression? Well, we can understand\\nthe cross entropy to mean the expected string length for any trial given we have\\noptimized for the encoding scheme for distribution q(x) while, in reality, all of\\nthe outcomes are appearing according to the distribution p(x). This can definitely\\nhappen in an experiment where we have only limited a priori information about the\\nexperiment, so we assume some distribution q(x) to optimize our encoding scheme,\\nbut as we carry out trials, we learn more information that gets us closer to the true\\ndistribution p(x).\\nThe KL divergence takes this logic a bit further. If we take the cross entropy, which\\ntells us the expected number of bits per trial given we have optimized our encoding\\nfor the incorrect distribution q(x), and subtract from that the entropy, which tells\\nEntropy, Cross Entropy, and KL Divergence \\n| \\n31'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 47}, page_content='us the expected number of bits per trial given we have optimized for the correct\\ndistribution p(x), we get the expected number of extra bits required to represent a\\ntrial when using q(x) compared to p(x). Here is the expression for the KL divergence:\\nKL p\\nq = Ep x\\nlog2\\n1\\nq x −log2\\n1\\np x\\n= Ep x\\nlog2\\np x\\nq x\\nAt the unique global minimum q(x) = p(x), the KL divergence is exactly zero. Why\\nthis is the unique minimum is a bit beyond the scope of this text, so we leave that as\\nan exercise for you.\\nIn practice, when trying to match the true distribution p(x) with a learned distribu‐\\ntion q(x), KL divergence is often minimized as an objective function. Most models\\nwill actually minimize the cross entropy in place of the KL divergence, which is\\neffectively the same optimization problem due to the KL being a difference between\\nthe cross entropy and the entropy of p(x), where the entropy of p(x) is a constant and\\nhas no dependence on the weights that parameterize q(x). Thus, the gradient with\\nrespect to the weights that parameterize q(x) when using either objective is the same.\\nOne common example where cross-entropy/KL divergence is optimized is in the\\nstandard training of a neural network classifier. The neural network’s objective\\nis to learn a distribution over target classes such that, for any given example xi,\\npθ y x = xi  matches the true distribution p(y|x = xi), which has all of its probabil‐\\nity mass placed over the true label yi and zero probability over all other classes.\\nMinimizing the sum of cross entropies between the learned distribution and the true\\ndistribution over all examples is actually the exact same as minimizing the negative\\nlog likelihood of the data. Both are valid interpretations of how neural networks are\\ntrained, and lead to the same objective function. We encourage you to try writing out\\nboth expressions independently to see this.\\nContinuous Probability Distributions\\nSo far, we have looked at probability distributions through the lens of discrete out‐\\ncomes and events. However, as it turns out, probability distributions aren’t just for\\nsets of discrete outcomes like the CIFAR-10 target classes or the MNIST digits. We\\ncan define probability distributions over sample spaces of infinite size, such as all\\nthe real numbers. In this section, we will extend principles covered in the previous\\nsections to the continuous realm.\\nIn the continuous realm, probability distributions are often referred to as probability\\ndensity functions, or PDFs. PDFs are nonnegative functions over a sample space, such\\nas all the reals, that integrate to one. Recall from calculus that the integration of a\\nfunction is the area of the region underneath the function, bounded by the x-axis.\\nPDFs follow the basic tenets introduced in the first section, but instead of adding\\n32 \\n| \\nChapter 2: Fundamentals of Probability'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 48}, page_content='the probability of outcomes to get the probability of an event, we use integration.\\nFor example, say X is a continuous random variable that is defined over all the real\\nnumbers. If we’d like to know the probability of the event P X ≤2 , all we’d need to\\ndo is integrate the PDF of X from negative infinity to 2.\\nBut how about the probability of any individual outcome, say P(X = 2)? Since we\\nuse integration to find probabilities in the continuous space, the probability of any\\nindividual outcome is actually zero due to the width of the region being infinitesimal.\\nWe instead use the term likelihood to distinguish between the probability of events\\nand the value that the PDF evaluates to when we input a setting of X. Likelihoods\\nare still valuable, as they tell us what individual outcomes we are most likely to see\\nwhen performing an experiment over a continuous space. Going forward, when con‐\\nsidering continuous probability distributions, we will only refer to events as having\\nprobability, rather than individual outcomes.\\nOne famous example of a continuous probability distribution is the uniform distri‐\\nbution over some interval on the real line. Under the uniform distribution, the\\nlikelihood of each outcome is the same, meaning that no outcome is any more likely\\nto appear than another. Thus, the uniform distribution looks like a rectangle, where\\nthe base of the rectangle is the interval constituting its domain, and the height, or the\\nlikelihood for each outcome, is the value that makes the area of the rectangle equal to\\none. Figure 2-4 shows the uniform distribution over the interval [0,0.5].\\nFigure 2-4. The uniform distribution has uniform height over its entire area, which\\nshows that each value in the domain of the distribution has equal likelihood.\\nThis example was chosen specifically to show a concrete difference between likeli‐\\nhoods and probabilities in the continuous realm. The height of the rectangle being\\nContinuous Probability Distributions \\n| \\n33'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 49}, page_content='2 was no error—there is no constraint on the magnitude of the likelihood in continu‐\\nous distributions, unlike probabilities, which must be less than or equal to 1.\\nAnother famous example of a continuous probability distribution is the Gaussian\\ndistribution, which is one of the more common ways in which data presents itself in\\nthe real world. The Gaussian distribution is defined by two parameters: its mean μ\\nand its standard deviation σ. The PDF of a Gaussian distribution is:\\nf x; μ, σ =\\n1\\nσ 2πe−1\\n2\\nx −μ\\nσ\\n2\\nWhy this function integrates to 1 over the real domain is beyond the scope of this\\nchapter, but one important characteristic of a Gaussian distribution is that its mean is\\nalso its unique mode. In other words, the outcome with the highest likelihood is also,\\nuniquely, the mean outcome. This is not the case for all distributions. For example,\\nFigure 2-4 does not have this property. The graph of a standard Gaussian, which has\\nmean zero and unit variance, is shown in Figure 2-5 (the PDF asymptotically reaches\\nzero in the limit in both directions).\\nFigure 2-5. The Gaussian distribution has a bell shape, with highest likelihood in the\\ncenter and dropping exponentially as the value in question gets farther and farther from\\nthe center.\\nWhy is the Gaussian distribution so prevalent in real-world data? One reason for this\\nis a theorem called the Central Limit Theorem (CLT). This theorem states that sums\\nof independent random variables converge to a Gaussian distribution as the number\\nof variables in the sum goes to infinity, even if each variable is not distributed as\\n34 \\n| \\nChapter 2: Fundamentals of Probability'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 50}, page_content='a Gaussian. One example is the number of masked neurons after a dropout layer\\nis applied. As the number of neurons from the previous layer goes to infinity, the\\nnumber of masked neurons (which is a sum of independent Bernoulli random vari‐\\nables, as discussed in “Random Variables” on page 22), when standardized correctly,\\nis approximately distributed as a standard Gaussian distribution. We won’t cover CLT\\nin much depth here, but it has more recently been extended to weakly dependent\\nvariables under certain special conditions.\\nMany real-world datasets can be seen as approximately sums of many random vari‐\\nables. For example, the distribution of a disease prevalence within a given population,\\nsimilarly to the number of masked neurons after applying dropout, is the sum of\\nmany Bernoulli random variables (where each person is a Bernoulli random variable\\nthat has a value of 1 if they have the disease and a value of 0 if they do not)—although\\nlikely dependent.\\nContinuous random variables are still functions, just as we defined discrete random\\nvariables. The only difference is that the range of this function is a continuous space.\\nTo compute the expectation and variance of a continuous random variable, all we\\nneed to do is replace our summations with integrations, as follows:\\nE X = ∫xx * f X = x dx\\nVar X = ∫x x −E X\\n2 * f X = x dx\\nAs an example, let’s evaluate the expectation for our uniform random variable defined\\nearlier. But first, confirm that it makes intuitive sense that the expectation should be\\n0.25, since the endpoints of the interval are 0 and 0.5 and all values in between are of\\nequal likelihood. Now, let’s evaluate the integral and see if the computation matches\\nour intuition:\\n∫0\\n0 . 5x * f x dx = ∫0\\n0 . 52xdx\\n= x2\\n0\\n0 . 5\\n= 0 . 25\\nWhere the superscript and the subscript of the | symbol represent the values at which\\nwe will evaluate the preceding function, which we will then difference to get the\\nvalue of the integral. We see that the expectation comes out to the same value as our\\nintuition, which is a great sanity check.\\nBayes’ Theorem also holds for continuous variables. The only major difference is\\nwhen marginalizing out a subset of variables, you will need to integrate over the\\nentire domain of the marginalized subset rather than taking a discrete sum over\\nContinuous Probability Distributions \\n| \\n35'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 51}, page_content='all possible configurations of the marginalized subset. Again, this is an example of\\nextending the tenets of probability to the continuous space by replacing summations\\nwith integrations. Here is Bayes’ Theorem for continuous probability distributions,\\nfollowing the notation from “Bayes’ Theorem” on page 27:\\nP B = bquery A =\\nP A B = bquery P B = bquery\\nP A\\n=\\nP A B = bquery P B = bquery\\n∫bP A, B = b db\\nAnd finally, we have our discussion on entropy, cross entropy, and KL divergence.\\nAll three of these extend nicely to the continuous space as well. We replace our\\nsummations with integrations and note that the properties introduced in the previous\\nsection still hold. For example, over a given domain, the distribution with the highest\\nentropy is the uniform distribution, and the KL divergence between two distributions\\nis zero if and only if the two distributions are the exact same. Here are the definitions\\nin their continuous form, following Equation 2-1:\\nH f x\\n= −∫x f x log2 f x dx\\nKL f x\\ng x\\n= ∫x f x log2\\nf x\\ng x dx\\nCE f x\\ng x\\n= −∫x f x log2 g x dx\\nOur extension of these concepts to the continuous space will come in handy in\\nChapter 10, where we model many distributions as Gaussians. Additionally, we use\\nthe KL divergence/cross-entropy terms as a regularization procedure on the complex‐\\nity of one of our learned distributions. Since KL divergence is only zero when the\\nquery distribution matches the target distribution, setting the target distribution to a\\nGaussian forces the learned distribution to approximate a Gaussian.\\nSummary\\nIn this chapter we covered the fundamentals of probability, first building the intuition\\nbehind the basics of probability distributions and then moving to relevant applica‐\\ntions of probability, such as conditional probability, random variables, expectation,\\nand variance. We saw the applications of probability in deep learning, such as how a\\nneural net parametrizes a probability distribution during classification tasks, and how\\nwe can quantify the mathematical properties of dropout, a regularization technique\\nin neural nets. Finally, we discussed measurements of uncertainty in probability dis‐\\ntributions such as entropy, and generalized these concepts to the continuous realm.\\n36 \\n| \\nChapter 2: Fundamentals of Probability'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 52}, page_content='Probability is a field that affects the choices in our everyday lives, and it’s key to\\nunderstand the meaning behind the numbers. Additionally, we hope that this intro‐\\nduction puts the rest of the book in perspective and allows you to more rigorously\\nunderstand future concepts. In the next chapter, we will discuss the structure of\\nneural networks, and the motivations behind their design.\\nSummary \\n| \\n37'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 53}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 54}, page_content='1 Kuhn, Deanna, et al. Handbook of Child Psychology. Vol. 2, Cognition, Perception, and Language. Wiley, 1998.\\nCHAPTER 3\\nThe Neural Network\\nBuilding Intelligent Machines\\nThe brain is the most incredible organ in the human body. It dictates the way we\\nperceive every sight, sound, smell, taste, and touch. It enables us to store memories,\\nexperience emotions, and even dream. Without it, we would be primitive organisms,\\nincapable of anything other than the simplest of reflexes. The brain is, inherently,\\nwhat makes us intelligent.\\nThe infant brain weighs only a single pound, but somehow it solves problems that\\neven our biggest, most powerful supercomputers find impossible. Within a matter of\\nmonths after birth, infants can recognize the faces of their parents, discern discrete\\nobjects from their backgrounds, and even tell voices apart. Within a year, they’ve\\nalready developed an intuition for natural physics, can track objects even when they\\nbecome partially or completely blocked, and can associate sounds with specific mean‐\\nings. And by early childhood, they have a sophisticated understanding of grammar\\nand thousands of words in their vocabularies.1\\nFor decades, we’ve dreamed of building intelligent machines with brains like ours—\\nrobotic assistants to clean our homes, cars that drive themselves, microscopes that\\nautomatically detect diseases. But building these artificially intelligent machines\\nrequires us to solve some of the most complex computational problems we have\\never grappled with; problems that our brains can already solve in a manner of micro‐\\nseconds. To tackle these problems, we’ll have to develop a radically different way of\\nprogramming a computer using techniques largely developed over the past decade.\\nThis is an extremely active field of artificial computer intelligence often referred to as\\ndeep learning.\\n39'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 55}, page_content='2 Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. “Gradient-Based Learning Applied to Document Recognition.”\\nProceedings of the IEEE, 86(11):2278-2324, November 1998.\\nThe Limits of Traditional Computer Programs\\nWhy exactly are certain problems so difficult for computers to solve? Well, it turns\\nout that traditional computer programs are designed to be very good at two things:\\n(1) performing arithmetic really fast and (2) explicitly following a list of instructions.\\nSo if you want to do some heavy financial number crunching, you’re in luck. Tradi‐\\ntional computer programs can do the trick. But let’s say we want to do something\\nslightly more interesting, like write a program to automatically read someone’s hand‐\\nwriting. Figure 3-1 will serve as a starting point.\\nFigure 3-1. Image from MNIST handwritten digit dataset2\\nAlthough every digit in Figure 3-1 is written in a slightly different way, we can easily\\nrecognize every digit in the first row as a zero, every digit in the second row as a one,\\netc. Let’s try to write a computer program to crack this task. What rules could we use\\nto tell one digit from another?\\nWell, we can start simple! For example, we might state that we have a zero if our\\nimage has only a single, closed loop. All the examples in Figure 3-1 seem to fit this\\nbill, but this isn’t really a sufficient condition. What if someone doesn’t perfectly close\\nthe loop on their zero? And, as in Figure 3-2, how do you distinguish a messy zero\\nfrom a six?\\n40 \\n| \\nChapter 3: The Neural Network'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 56}, page_content='Figure 3-2. A zero that’s algorithmically difficult to distinguish from a six\\nYou could potentially establish some sort of cutoff for the distance between the\\nstarting point of the loop and the ending point, but it’s not exactly clear where we\\nshould be drawing the line. But this dilemma is only the beginning of our worries.\\nHow do we distinguish between threes and fives? Or between fours and nines? We\\ncan add more and more rules, or features, through careful observation and months of\\ntrial and error, but it’s quite clear that this isn’t going to be an easy process.\\nMany other classes of problems fall into this same category: object recognition,\\nspeech comprehension, automated translation, etc. We don’t know what program to\\nwrite because we don’t know how it’s done by our brains.  And even if we did know\\nhow to do it, the program might be horrendously complicated.\\nThe Mechanics of Machine Learning\\nTo tackle these classes of problems, we’ll have to use a different kind of approach.\\nA lot of the things we learn in school growing up have much in common with\\ntraditional computer programs. We learn how to multiply numbers, solve equations,\\nand take derivatives by internalizing a set of instructions. But the things we learn at\\nan extremely early age, the things we find most natural, are learned by example, not\\nby formula.\\nFor instance, when we were two years old, our parents didn’t teach us how to recog‐\\nnize a dog by measuring the shape of its nose or the contours of its body. We learned\\nto recognize a dog by being shown multiple examples and being corrected when we\\nmade the wrong guess. When we were born, our brains provided us with a model that\\ndescribed how we would be able to see the world. As we grew up, that model would\\ntake in our sensory inputs and make a guess about what we were experiencing. If that\\nguess was confirmed by our parents, our model would be reinforced. If our parents\\nsaid we were wrong, we’d modify our model to incorporate this new information.\\nOver our lifetime, our model becomes more and more accurate as we assimilate more\\nand more examples. Obviously all of this happens subconsciously, but we can use this\\nto our advantage.\\nDeep learning is a subset of a more general field of AI called machine learning, which\\nis predicated on this idea of learning from example. In machine learning, instead of\\nteaching a computer a massive list of rules to solve the problem, we give it a model\\nThe Mechanics of Machine Learning \\n| \\n41'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 57}, page_content='with which it can evaluate examples, and a small set of instructions to modify the\\nmodel when it makes a mistake. We expect that, over time, a well-suited model would\\nbe able to solve the problem extremely accurately.\\nLet’s be a little bit more rigorous about what this means so we can formulate this\\nidea mathematically. Let’s define our model to be a function ℎx, θ . The input x\\nis an example expressed in vector form. For example, if x were a grayscale image,\\nthe vector’s components would be pixel intensities at each position, as shown in\\nFigure 3-3.\\nFigure 3-3. The process of vectorizing an image for a machine learning algorithm\\n42 \\n| \\nChapter 3: The Neural Network'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 58}, page_content='3 Rosenblatt, Frank. “The perceptron: A Probabilistic Model for Information Storage and Organization in the\\nBrain.” Psychological Review 65.6 (1958): 386.\\nThe input θ is a vector of the parameters that our model uses. Our machine learning\\nprogram tries to perfect the values of these parameters as it is exposed to more and\\nmore examples. We’ll see this in action and in more detail in Chapter 4.\\nTo develop a more intuitive understanding for machine learning models, let’s walk\\nthrough a quick example. Let’s say we wanted to determine how to predict exam\\nperformance based on the number of hours of sleep we get and the number of\\nhours we study the previous day. We collect a lot of data, and for each data point\\nx = x1 x2\\nT, we record the number of hours of sleep we got (x1), the number of\\nhours we spent studying (x2), and whether we performed above or below the class\\naverage. Our goal, then, might be to learn a model ℎx, θ  with parameter vector\\nθ = θ0 θ1 θ2\\nT such that:\\nℎx, θ =\\n−1\\nif xT ·\\nθ1\\nθ2\\n+ θ0 < 0\\n1\\nif xT ·\\nθ1\\nθ2\\n+ θ0 ≥0\\nSo we guess that the blueprint for our model ℎx, θ  is as described (geometrically,\\nthis particular blueprint describes a linear classifier that divides the coordinate plane\\ninto two halves). Then, we want to learn a parameter vector θ such that our model\\nmakes the right predictions (−1 if we perform below average, and 1 otherwise) given\\nan input example x. This model is called a linear perceptron, and it’s a model that’s\\nbeen used since the 1950s.3 Let’s assume our data is as shown in Figure 3-4.\\nThe Mechanics of Machine Learning \\n| \\n43'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 59}, page_content='Figure 3-4. Sample data for our exam predictor algorithm and a potential classifier\\nThen it turns out that by selecting θ = −24 3 4 T, our machine learning model\\nmakes the correct prediction on every data point:\\nℎx, θ =\\n−1\\nif 3x1 + 4x2 −24 < 0\\n1\\nif 3x1 + 4x2 −24 ≥0\\nAn optimal parameter vector θ positions the classifier so that we make as many\\ncorrect predictions as possible. In most cases, there are many (or even infinitely\\nmany) possible choices for θ that are optimal. Fortunately for us, most of the time\\nthese alternatives are so close to one another that the difference is negligible. If this is\\nnot the case, we may want to collect more data to narrow our choice of θ.\\nWhile the setup seems reasonable, there are still some pretty significant questions\\nthat remain. First off, how do we even come up with an optimal value for the\\nparameter vector θ in the first place? Solving this problem requires a technique\\ncommonly known as optimization. An optimizer aims to maximize the performance\\nof a machine learning model by iteratively tweaking its parameters until the error\\nis minimized. We’ll begin to tackle this question of learning parameter vectors in\\n44 \\n| \\nChapter 3: The Neural Network'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 60}, page_content='4 Bubeck, Sébastien. “Convex Optimization: Algorithms and Complexity.” Foundations and Trends® in Machine\\nLearning. 8.3-4 (2015): 231-357.\\n5 Restak, Richard M. and David Grubin. The Secret Life of the Brain. Joseph Henry Press, 2001.\\nmore detail in Chapter 4, when we describe the process of gradient descent.4 In later\\nchapters, we’ll try to find ways to make this process even more efficient.\\nSecond, it’s quite clear that this particular model (the linear perceptron model) is\\nquite limited in the relationships it can learn. For example, the distributions of data\\nshown in Figure 3-5 cannot be described well by a linear perceptron.\\nFigure 3-5. As our data takes on more complex forms, we need more complex models to\\ndescribe them\\nBut these situations are only the tip of the iceberg. As we move on to much more\\ncomplex problems, such as object recognition and text analysis, our data becomes\\nextremely high dimensional, and the relationships we want to capture become highly\\nnonlinear. To accommodate this complexity, recent research in machine learning has\\nattempted to build models that resemble the structures utilized by our brains. It’s\\nessentially this body of research, commonly referred to as deep learning, that has had\\nspectacular success in tackling problems in computer vision and natural language\\nprocessing. These algorithms not only far surpass other kinds of machine learning\\nalgorithms, but also rival (or even exceed) the accuracies achieved by humans.\\nThe Neuron\\nThe foundational unit of the human brain is the neuron. A tiny piece of the brain,\\nabout the size of grain of rice, contains over 10,000 neurons, each of which forms an\\naverage of 6,000 connections with other neurons.5 It’s this massive biological network\\nthat enables us to experience the  world around us. Our goal in this section is to use\\nthis natural structure to build machine learning models that solve problems in an\\nanalogous way.\\nThe Neuron \\n| \\n45'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 61}, page_content='6 McCulloch, Warren S., and Walter Pitts. “A Logical Calculus of the Ideas Immanent in Nervous Activity.” The\\nBulletin of Mathematical Biophysics. 5.4 (1943): 115-133.\\nAt its core, the neuron is optimized to receive information from other neurons,\\nprocess this information in a unique way, and send its result to other cells. This\\nprocess is summarized in Figure 3-6. The neuron receives its inputs along antennae-\\nlike structures called dendrites. Each of these incoming connections is dynamically\\nstrengthened or weakened based on how often it is used (this is how we learn new\\nconcepts), and it’s the strength of each connection that determines the contribution\\nof the input to the neuron’s output. After being weighted by the strength of their\\nrespective connections, the inputs are summed together in the cell body. This sum is\\nthen transformed into a new signal that’s propagated along the cell’s axon and sent off\\nto other neurons.\\nFigure 3-6. A functional description of a biological neuron’s structure\\nWe can translate this functional understanding of the neurons in our brain into an\\nartificial model that we can represent on our computer. Such a model is described\\nin Figure 3-7, leveraging the approach first pioneered in 1943 by Warren S. McCul‐\\nloch and Walter H. Pitts.6 Just as in biological neurons, our artificial neuron takes in\\nsome number of inputs, x1, x2, ..., xn, each of which is multiplied by a specific weight,\\nw1, w2, ..., wn. These weighted inputs are, as before, summed to produce the logit of\\nthe neuron, z = ∑i = 0\\nn\\nwixi. In many cases, the logit also includes a bias, which is a\\nconstant (not shown in the figure). The logit is then passed through a function f to\\nproduce the output y = f z . This output can be transmitted to other neurons.\\n46 \\n| \\nChapter 3: The Neural Network'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 62}, page_content='Figure 3-7. Schematic for a neuron in an artificial neural net\\nWe’ll conclude our mathematical discussion of the artificial neuron by re-expressing\\nits functionality in vector form. Let’s reformulate the inputs as a vector x = [x1 x2 …\\nxn] and the weights of the neuron as w = [w1 w2 … wn]. Then we can re-express the\\noutput of the neuron as y = f x · w + b , where b is the bias term. We can compute\\nthe output by performing the dot product of the input and weight vectors, adding in\\nthe bias term to produce the logit, and then applying the transformation function.\\nWhile this seems like a trivial reformulation, thinking about neurons as a series of\\nvector manipulations will be crucial to how we implement them in software later in\\nthis book.\\nExpressing Linear Perceptrons as Neurons\\nIn “The Mechanics of Machine Learning” on page 41, we talked about using machine\\nlearning models to capture the relationship between success on exams and time spent\\nstudying and sleeping. To tackle this problem, we constructed a linear perceptron\\nclassifier that divided the Cartesian coordinate plane into two halves:\\nℎx, θ =\\n−1 if 3x1 + 4x2 −24 < 0\\n1\\nif 3x1 + 4x2 −24 ≥0\\nAs shown in Figure 3-4, this is an optimal choice for θ because it correctly classifies\\nevery sample in our dataset. Here, we show that our model h is easily using a neuron.\\nConsider the neuron depicted in Figure 3-8. The neuron has two inputs, a bias, and\\nuses the function:\\nf z = −1\\nif z < 0\\n1\\nif z ≥0\\nIt’s easy to show that our linear perceptron and the neuronal model are perfectly\\nequivalent. And in general, it’s quite simple to show that singular neurons are strictly\\nmore expressive than linear perceptrons. Every linear perceptron can be expressed as\\na single neuron, but single neurons can also express models that cannot be expressed\\nby any linear perceptron.\\nExpressing Linear Perceptrons as Neurons \\n| \\n47'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 63}, page_content='7 Mountcastle, Vernon B. “Modality and Topographic Properties of Single Neurons of Cat’s Somatic Sensory\\nCortex.” Journal of Neurophysiology 20.4 (1957): 408-434.\\nFigure 3-8. Expressing our exam performance perceptron as a neuron\\nFeed-Forward Neural Networks\\nAlthough single neurons are more powerful than linear perceptrons, they’re not\\nnearly expressive enough to solve complicated learning problems. There’s a reason\\nour brain is made of more than one neuron. For example, it is impossible for a single\\nneuron to differentiate handwritten digits. So to tackle much more complicated tasks,\\nwe’ll have to take our machine learning model even further.\\nThe neurons in the human brain are organized in layers. In fact, the human cerebral\\ncortex (the structure responsible for most of human intelligence) is made up of six\\nlayers.7 Information flows from one layer to another until sensory input is converted\\ninto conceptual understanding. For example, the bottommost layer of the visual\\ncortex receives raw visual data from the eyes. This information is processed by each\\nlayer and passed on to the next until, in the sixth layer, we conclude whether we are\\nlooking at a cat, or a soda can, or an airplane.\\nBorrowing from these concepts, we can construct an artificial neural network. A\\nneural network comes about when we start hooking up neurons to each other, the\\ninput data, and to the output nodes, which correspond to the network’s answer to a\\nlearning problem. Figure 3-9 demonstrates a simple example of an artificial neural\\nnetwork, similar to the architecture described in McCulloch and Pitt’s work in 1943.\\n48 \\n| \\nChapter 3: The Neural Network'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 64}, page_content='Figure 3-9. A feed-forward neural network with three layers (input, one hidden, and\\noutput) and three neurons per layer\\nThe bottom layer of the network pulls in the input data. The top layer of neurons\\n(output nodes) computes our final answer. The middle layer(s) of neurons are\\ncalled the hidden layers, and we let wi, j\\nk  be the weight of the connection between\\nthe itℎ neuron in the ktℎ layer with the jtℎ neuron in the k + 1st layer. These weights\\nconstitute our parameter vector, θ, and just as before, our ability to solve problems\\nwith neural networks depends on finding the optimal values to plug into θ.\\nWe note that in this example, connections traverse only from a lower layer to a\\nhigher layer. There are no connections between neurons in the same layer, and\\nthere are no connections that transmit data from a higher layer to a lower layer.\\nThese neural networks are called feed-forward networks, and we start by discussing\\nthese networks because they are the simplest to analyze. We present this analysis\\nFeed-Forward Neural Networks \\n| \\n49'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 65}, page_content='(specifically, the process of selecting the optimal values for the weights) in Chapter 4.\\nMore complicated connectivities will be addressed in later chapters.\\nWe’ll discuss the major types of layers that are utilized in feed-forward neural net‐\\nworks, but before we proceed, here’s a couple of important notes to keep in mind:\\n1. As we mentioned, the layers of neurons that lie sandwiched between the first\\nlayer of neurons (input layer) and the last layer of neurons (output layer) are\\ncalled the hidden layers. This is where most of the magic is happening when the\\nneural net tries to solve problems. Whereas (as in the handwritten digit example)\\nwe would previously have to spend a lot of time identifying useful features,\\nthe hidden layers automate this process for us. Oftentimes, taking a look at the\\nactivities of hidden layers can tell you a lot about the features the network has\\nautomatically learned to extract from the data.\\n2. Although in Figure 3-9 every layer has the same number of neurons, this is\\nneither necessary nor recommended. More often than not, hidden layers have\\nfewer neurons than the input layer to force the network to learn compressed\\nrepresentations of the original input. For example, while our eyes obtain raw\\npixel values from our surroundings, our brain thinks in terms of edges and\\ncontours. This is because the hidden layers of biological neurons in our brain,\\nforce us to come up with better representations for everything we perceive.\\n3. It is not required that every neuron has its output connected to the inputs of\\nall neurons in the next layer. In fact, selecting which neurons to connect to\\nwhich other neurons in the next layer is an art that comes from experience. We’ll\\ndiscuss this issue in more depth as we work through various examples of neural\\nnetworks.\\n4. The inputs and outputs are vectorized representations. For example, you might\\nimagine a neural network where the inputs are the individual pixel RGB values\\nin an image represented as a vector (refer to Figure 3-3). The last layer might\\nhave two neurons that correspond to the answer to our problem: 1, 0  if the\\nimage contains a dog, 0, 1  if the image contains a cat, 1, 1  if it contains both,\\nand 0, 0  if it contains neither.\\nWe’ll also observe that, similarly to our reformulation for the neuron, we can also\\nmathematically express a neural network as a series of vector and matrix operations.\\nLet’s consider the input to the itℎ layer of the network to be a vector x = [x1 x2 …\\nxn]. We’d like to find the vector y = [y1 y2 … ym] produced by propagating the input\\nthrough the neurons. We can express this as a simple matrix multiply if we construct\\na weight matrix W of size n × m and a bias vector of size m. In this matrix, each\\ncolumn corresponds to a neuron, where the jtℎ element of the column corresponds\\nto the weight of the connection pulling in the jtℎ element of the input. In other\\nwords, y = ƒ(WTx + b), where the transformation function is applied to the vector\\n50 \\n| \\nChapter 3: The Neural Network'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 66}, page_content='element-wise. This reformulation will become all the more critical as we begin to\\nimplement these networks in software.\\nLinear Neurons and Their Limitations\\nMost neuron types are defined by the function f they apply to their logit z. Let’s first\\nconsider layers of neurons that use a linear function in the form of f z = az + b. For\\nexample, a neuron that attempts to estimate a cost of a meal in a fast-food restaurant\\nwould use a linear neuron where a = 1 and b = 0. Using f z = z and weights equal\\nto the price of each item, the linear neuron in Figure 3-10 would take in some\\nordered triple of servings of burgers, fries, and sodas, and output the price of the\\ncombination.\\nFigure 3-10. An example of a linear neuron\\nLinear neurons are easy to compute with, but they run into serious limitations. In\\nfact, it can be shown that any feed-forward neural network consisting of only linear\\nneurons can be expressed as a network with no hidden layers. This is problematic\\nbecause, as we discussed, hidden layers are what enable us to learn important features\\nfrom the input data. In other words, to learn complex relationships, we need to use\\nneurons that employ some sort of nonlinearity.\\nSigmoid, Tanh, and ReLU Neurons\\nThree major types of neurons are used in practice that introduce nonlinearities in\\ntheir computations. The first of these is the sigmoid neuron, which uses the function:\\nf z =\\n1\\n1 + e−z\\nIntuitively, this means that when the logit is very small, the output of a logistic\\nneuron is close to 0. When the logit is very large, the output of the logistic neuron is\\nLinear Neurons and Their Limitations \\n| \\n51'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 67}, page_content='8 Nair, Vinod, and Geoffrey E. Hinton. “Rectified Linear Units Improve Restricted Boltzmann Machines.”\\nProceedings of the 27th International Conference on Machine Learning (ICML-10), 2010.\\nclose to 1. In-between these two extremes, the neuron assumes an S-shape, as shown\\nin Figure 3-11.\\nFigure 3-11. The output of a sigmoid neuron as z varies\\nTanh neurons use a similar kind of S-shaped nonlinearity, but instead of ranging\\nfrom 0 to 1, the output of tanh neurons ranges from −1 to 1. As you would expect,\\nthey use f z = tanh z . The resulting relationship between the output y and the\\nlogit z is depicted in Figure 3-12. When S-shaped nonlinearities are used, the tanh\\nneuron is often preferred over the sigmoid neuron because it is zero-centered.\\nA different kind of nonlinearity is used by the Rectified Linear Unit (ReLU) neuron. It\\nuses the function f z = max 0, z , resulting in a characteristic hockey-stick-shaped\\nresponse, as shown in Figure 3-13.\\nThe ReLU has recently become the neuron of choice for many tasks (especially in\\ncomputer vision) for a number of reasons, despite some drawbacks.8 We’ll discuss\\nthese reasons in Chapter 7, as well as strategies to combat the potential pitfalls.\\n52 \\n| \\nChapter 3: The Neural Network'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 68}, page_content='Figure 3-12. The output of a tanh neuron as z varies\\nFigure 3-13. The output of a ReLU neuron as z varies\\nSigmoid, Tanh, and ReLU Neurons \\n| \\n53'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 69}, page_content='Softmax Output Layers\\nOftentimes, we want our output vector to be a probability distribution over a set of\\nmutually exclusive labels. For example, let’s say we want to build a neural network\\nto recognize handwritten digits from the MNIST dataset. Each label (0 through 9) is\\nmutually exclusive, but it’s unlikely that we will be able to recognize digits with 100%\\nconfidence. Using a probability distribution gives us a better idea of how confident\\nwe are in our predictions. As a result, the desired output vector is of the following\\nform, where ∑i = 0\\n9\\npi = 1:\\np0 p1 p2 p3 ... p9\\nThis is achieved by using a special output layer called a softmax layer. Unlike in other\\nkinds of layers, the output of a neuron in a softmax layer depends on the outputs of\\nall the other neurons in its layer. This is because we require the sum of all the outputs\\nto be equal to 1. Letting zi be the logit of the itℎ softmax neuron, we can achieve this\\nnormalization by setting its output to:\\nyi =\\nezi\\n∑jezj\\nA strong prediction would have a single entry in the vector close to 1, while the\\nremaining entries would be close to 0. A weak prediction would have multiple\\npossible labels that are more or less equally likely.\\nSummary\\nIn this chapter, we’ve built a basic intuition for machine learning and neural net‐\\nworks. We’ve talked about the basic structure of a neuron, how feed-forward neural\\nnetworks work, and the importance of nonlinearity in tackling complex learning\\nproblems. In the next chapter, we will begin to build the mathematical background\\nnecessary to train a neural network to solve problems. Specifically, we will talk about\\nfinding optimal parameter vectors, best practices while training neural networks, and\\nmajor challenges. In later chapters, we will take these foundational ideas to build\\nmore specialized neural architectures.\\n54 \\n| \\nChapter 3: The Neural Network'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 70}, page_content='CHAPTER 4\\nTraining Feed-Forward Neural Networks\\nThe Fast-Food Problem\\nWe’re beginning to understand how we can tackle some interesting problems using\\ndeep learning, but one big question still remains: how exactly do we figure out what\\nthe parameter vectors (the weights for all of the connections in our neural network)\\nshould be? This is accomplished by a process commonly referred to as training (see\\nFigure 4-1). During training, we show the neural net a large number of training\\nexamples and iteratively modify the weights to minimize the errors we make on the\\ntraining examples. After enough examples, we expect that our neural network will be\\nquite effective at solving the task it’s been trained to do.\\nFigure 4-1. This is the neuron we want to train for the fast-food problem\\nLet’s continue with an example from Chapter 3 involving a linear neuron: every single\\nday, we purchase a restaurant meal consisting of burgers, fries, and sodas. We buy\\nsome number of servings for each item. We want to be able to predict how much a\\nmeal is going to cost us, but the items don’t have price tags. The only thing the cashier\\n55'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 71}, page_content='will tell us is the total price of the meal. We want to train a single linear neuron to\\nsolve this problem. How do we do it?\\nOne idea is to be intelligent about picking our training cases. For one meal we could\\nbuy only a single serving of burgers, for another we could buy only a single serving\\nof fries, and then for our last meal we could buy a single serving of soda. In general,\\nintelligently selecting training examples is a good idea. Lots of research shows that\\nby engineering a clever training set, you can make your neural network a lot more\\neffective. The issue with using this approach alone is that in real situations, it rarely\\never gets you close to the solution. For example, there’s no clear analog of this strategy\\nin image recognition. It’s just not a practical solution.\\nInstead, we try to motivate a solution that works well in general. Let’s say we have\\na large set of training examples. Then we can calculate what the neural network will\\noutput on the itℎ training example using the simple formula in the diagram. We\\nwant to train the neuron so that we pick the most optimal weights possible—the\\nweights that minimize the errors we make on the training examples. In this case, let’s\\nsay we want to minimize the square error over all of the training examples that we\\nencounter. More formally, if we know that t i  is the true answer for the itℎ training\\nexample, and y i  is the value computed by the neural network, we want to minimize\\nthe value of the error function E:\\nE = 1\\n2 ∑i t i −y i 2\\nThe squared error is zero when our model makes a perfectly correct prediction on\\nevery training example. Moreover, the closer E is to 0, the better our model is. As a\\nresult, our goal is to select our parameter vector θ (the values for all the weights in\\nour model) such that E is as close to 0 as possible.\\nNow at this point you might be wondering why we need to bother ourselves with\\nerror functions when we can treat this problem as a system of equations. After all, we\\nhave a bunch of unknowns (weights) and we have a set of equations (one for each\\ntraining example). That would automatically give us an error of 0, assuming that we\\nhave a consistent set of training examples.\\nThat’s a smart observation, but the insight unfortunately doesn’t generalize well.\\nRemember that although we’re using a linear neuron here, linear neurons aren’t used\\nvery much in practice because they’re constrained in what they can learn. And the\\nmoment we start using nonlinear neurons like the sigmoidal, tanh, or ReLU neurons\\nwe talked about at the end of Chapter 3, we can no longer set up a system of linear\\nequations. Clearly, we need a better strategy to tackle the training process.\\n56 \\n| \\nChapter 4: Training Feed-Forward Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 72}, page_content='Gradient Descent\\nLet’s visualize how we might minimize the squared error over all of the training\\nexamples by simplifying the problem. Say our linear neuron has only two inputs\\n(and thus only two weights, w1 and w2). Then we can imagine a 3D space where\\nthe horizontal dimensions correspond to the weights w1 and w2, and the vertical\\ndimension corresponds to the value of the error function E. In this space, points in\\nthe horizontal plane correspond to different settings of the weights, and the height\\nat those points corresponds to the incurred error. If we consider the errors we make\\nover all possible weights, we get a surface in this 3D space, in particular, a quadratic\\nbowl as shown in Figure 4-2.\\nFigure 4-2. The quadratic error surface for a linear neuron\\nWe can also conveniently visualize this surface as a set of elliptical contours, where\\nthe minimum error is at the center of the ellipses. In this setup, we are working in a\\n2D plane where the dimensions correspond to the two weights. Contours correspond\\nto settings of w1 and w2 that evaluate to the same value of E. The closer the contours\\nare to each other, the steeper the slope. In fact, it turns out that the direction of the\\nsteepest descent is always perpendicular to the contours. This direction is expressed\\nas a vector known as the gradient.\\nNow we can develop a high-level strategy for how to find the values of the weights\\nthat minimizes the error function. Suppose we randomly initialize the weights of\\nour network so we find ourselves somewhere on the horizontal plane. By evaluating\\nthe gradient at our current position, we can find the direction of steepest descent,\\nand we can take a step in that direction. Then we’ll find ourselves at a new position\\nthat’s closer to the minimum than we were before. We can reevaluate the direction of\\nsteepest descent by taking the gradient at this new position and taking a step in this\\nGradient Descent \\n| \\n57'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 73}, page_content='1 Rosenbloom, P. “The Method of Steepest Descent.” Proceedings of Symposia in Applied Mathematics. Vol. 6.\\n1956.\\nnew direction. It’s easy to see that, as shown in Figure 4-3, following this strategy will\\neventually get us to the point of minimum error. This algorithm is known as gradient\\ndescent, and we’ll use it to tackle the problem of training individual neurons and the\\nmore general challenge of training entire networks.1\\nFigure 4-3. Visualizing the error surface as a set of contours\\nThe Delta Rule and Learning Rates\\nBefore we derive the exact algorithm for training our fast-food neuron, we have a\\nquick note on hyperparameters. In addition to the weight parameters defined in our\\nneural network, learning algorithms also require a couple of additional parameters to\\ncarry out the training process. One of these so-called hyperparameters is the learning\\nrate.\\nIn practice, at each step of moving perpendicular to the contour, we need to deter‐\\nmine how far we want to walk before recalculating our new direction. This distance\\nneeds to depend on the steepness of the surface. Why? The closer we are to the mini‐\\nmum, the shorter we want to step forward. We know we are close to the minimum\\nbecause the surface is a lot flatter, so we can use the steepness as an indicator of\\nhow close we are to the minimum. However, if our error surface is rather mellow,\\ntraining can potentially take a large amount of time. As a result, we often multiply the\\n58 \\n| \\nChapter 4: Training Feed-Forward Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 74}, page_content='gradient by a factor ϵ, the learning rate. Picking the learning rate is a hard problem\\n(Figure 4-4). As we just discussed, if we pick a learning rate that’s too small, we risk\\ntaking too long during the training process. But if we pick a learning rate that’s too\\nbig, we’ll mostly likely start diverging away from the minimum. In Chapter 5, we’ll\\nlearn about various optimization techniques that utilize adaptive learning rates to\\nautomate the process of selecting learning rates.\\nFigure 4-4. Convergence is difficult when our learning rate is too large\\nNow, we are finally ready to derive the delta rule for training our linear neuron.\\nIn order to calculate how to change each weight, we evaluate the gradient, which\\nis essentially the partial derivative of the error function with respect to each of the\\nweights. In other words, we want:\\nΔwk = −ϵ ∂E\\n∂wk\\n= −ϵ ∂\\n∂wk\\n1\\n2 ∑i t i −y i 2\\n= ∑iϵ t i −y i\\n∂yi\\n∂wk\\n= ∑iϵxk\\ni t i −y i\\nThe Delta Rule and Learning Rates \\n| \\n59'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 75}, page_content='Applying this method of changing the weights at every iteration, we are finally able to\\nutilize gradient descent.\\nGradient Descent with Sigmoidal Neurons\\nIn this section and the next, we will deal with training neurons and neural networks\\nthat utilize nonlinearities. We use the sigmoidal neuron as a model, and leave the\\nderivations for other nonlinear neurons as an exercise for you. For simplicity, we\\nassume that the neurons do not use a bias term, although our analysis easily extends\\nto this case. We merely need to assume that the bias is a weight on an incoming\\nconnection whose input value is always one.\\nLet’s recall the mechanism by which logistic neurons compute their output value\\nfrom their inputs:\\nz = ∑kwkxk\\ny =\\n1\\n1 + e−z\\nThe neuron computes the weighted sum of its inputs, the logit z. It then feeds its\\nlogit into the input function to compute y, its final output. Fortunately for us, these\\nfunctions have nice derivatives, which makes learning easy! For learning, we want to\\ncompute the gradient of the error function with respect to the weights. To do so, we\\nstart by taking the derivative of the logit with respect to the inputs and the weights:\\n∂z\\n∂wk = xk\\n∂z\\n∂xk = wk\\nAlso, quite surprisingly, the derivative of the output with respect to the logit is quite\\nsimple if you express it in terms of the output:\\ndy\\ndz =\\ne−z\\n1 + e−z 2\\n=\\n1\\n1 + e−z\\ne−z\\n1 + e−z\\n=\\n1\\n1 + e−z 1 −\\n1\\n1 + e−z\\n= y 1 −y\\n60 \\n| \\nChapter 4: Training Feed-Forward Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 76}, page_content='2 Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. “Learning Representations by Back-\\nPropagating Errors.” Cognitive Modeling 5.3 (1988): 1.\\nWe then use the chain rule to get the derivative of the output with respect to each\\nweight:\\n∂y\\n∂wk = dy\\ndz\\n∂z\\n∂wk = xky 1 −y\\nPutting all of this together, we can now compute the derivative of the error function\\nwith respect to each weight:\\n∂E\\n∂wk = ∑i\\n∂E\\n∂y i\\n∂y i\\n∂wk = −∑ixk\\ni y i 1 −y i\\nt i −y i\\nThus, the final rule for modifying the weights becomes:\\nΔwk = ∑iϵxk\\ni y i 1 −y i\\nt i −y i\\nAs you may notice, the new modification rule is just like the delta rule, except with\\nextra multiplicative terms included to account for the logistic component of the\\nsigmoidal neuron.\\nThe Backpropagation Algorithm\\nNow we’re finally ready to tackle the problem of training multilayer neural networks\\n(instead of just single neurons). To accomplish this task, we’ll use an approach\\nknown as backpropagation, pioneered by David E. Rumelhart, Geoffrey E. Hinton,\\nand Ronald J. Williams in 1986.2 So what’s the idea behind backpropagation? We don’t\\nknow what the hidden units ought to be doing, but what we can do is compute how\\nfast the error changes as we change a hidden activity. From there, we can figure out\\nhow fast the error changes when we change the weight of an individual connection.\\nEssentially, we’ll be trying to find the path of steepest descent. The only catch is\\nthat we’re going to be working in an extremely high-dimensional space. We start by\\ncalculating the error derivatives with respect to a single training example.\\nEach hidden unit can affect many output units. Thus, we’ll have to combine many\\nseparate effects on the error in an informative way. Our strategy will be one of\\ndynamic programming. Once we have the error derivatives for one layer of hidden\\nunits, we’ll use them to compute the error derivatives for the activities of the layer\\nbelow. And once we find the error derivatives for the activities of the hidden units, it’s\\nThe Backpropagation Algorithm \\n| \\n61'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 77}, page_content='quite easy to get the error derivatives for the weights leading into a hidden unit. We’ll\\nredefine some notation for ease of discussion and refer to Figure 4-5.\\nFigure 4-5. Reference diagram for the derivation of the backpropagation algorithm\\nThe subscript we use will refer to the layer of the neuron. The symbol y will refer\\nto the activity of a neuron, as usual. Similarly, the symbol z will refer to the logit of\\nthe neuron. We start by taking a look at the base case of the dynamic programming\\nproblem. Specifically, we calculate the error function derivatives at the output layer:\\nE = 1\\n2 ∑j ∈output tj −yj\\n2\\n∂E\\n∂yj = −tj −yj\\nNow we tackle the inductive step. Let’s presume we have the error derivatives for layer\\nj. We next aim to calculate the error derivatives for the layer below it, layer i. To\\ndo so, we must accumulate information about how the output of a neuron in layer i\\naffects the logits of every neuron in layer j. This can be done as follows, using the fact\\nthat the partial derivative of the logit with respect to the incoming output data from\\nthe layer beneath is merely the weight of the connection wij:\\n∂E\\n∂yi = ∑j\\n∂E\\n∂zj\\ndzj\\ndyi = ∑jwij\\n∂E\\n∂zj\\n62 \\n| \\nChapter 4: Training Feed-Forward Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 78}, page_content='Furthermore, we observe the following:\\n∂E\\n∂zj = ∂E\\n∂yj\\ndyj\\ndzj = yj 1 −yj\\n∂E\\n∂yj\\nCombining these two, we can finally express the error derivatives of layer i in terms\\nof the error derivatives of layer j:\\n∂E\\n∂yi = ∑jwijyj 1 −yj\\n∂E\\n∂yj\\nOnce we’ve gone through the whole dynamic programming routine, having filled up\\nthe table appropriately with all of our partial derivatives (of the error function with\\nrespect to the hidden unit activities), we can then determine how the error changes\\nwith respect to the weights. This gives us a way to modify the weights after each\\ntraining example:\\n∂E\\n∂wij =\\n∂zj\\n∂wij\\n∂E\\n∂zj = yiyj 1 −yj\\n∂E\\n∂yj\\nFinally, to complete the algorithm, just as before, we merely sum up the partial\\nderivatives over all the training examples in our dataset. This gives us the following\\nmodification formula:\\nΔwij = −∑k ∈datasetϵyi\\nk yj\\nk 1 −yj\\nk\\n∂E k\\n∂yj\\nk\\nThis completes our description of the backpropagation algorithm.\\nStochastic and Minibatch Gradient Descent\\nIn the algorithms we described in “The Backpropagation Algorithm” on page 61,\\nwe used a version of gradient descent known as batch gradient descent. The idea\\nbehind batch gradient descent is that we use our entire dataset to compute the error\\nsurface and then follow the gradient to take the path of steepest descent. For a simple\\nquadratic error surface, this works quite well. But in most cases, our error surface\\nmay be a lot more complicated. Let’s consider the scenario in Figure 4-6.\\nStochastic and Minibatch Gradient Descent \\n| \\n63'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 79}, page_content='Figure 4-6. Batch gradient descent is sensitive to saddle points, which can lead to\\npremature convergence\\nWe have only a single weight, and we use random initialization and batch gradient\\ndescent to find its optimal setting. The error surface, however, has a flat region (also\\nknown as saddle point in high-dimensional spaces), and if we get unlucky, we might\\nfind ourselves getting stuck while performing gradient descent.\\nAnother potential approach is stochastic gradient descent (SGD), where at each iter‐\\nation, our error surface is estimated with respect to only a single example. This\\napproach is illustrated by Figure 4-7, where instead of a single static error surface,\\nour error surface is dynamic. As a result, descending on this stochastic surface\\nsignificantly improves our ability to navigate flat regions.\\nFigure 4-7. The stochastic error surface fluctuates with respect to the batch error surface,\\nenabling saddle point avoidance\\n64 \\n| \\nChapter 4: Training Feed-Forward Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 80}, page_content='The major pitfall of SGD, however, is that looking at the error incurred one example\\nat a time may not be a good enough approximation of the error surface. This, in\\nturn, could potentially make gradient descent take a significant amount of time.\\nOne way to combat this problem is using minibatch gradient descent. In minibatch\\ngradient descent, at every iteration we compute the error surface with respect to\\nsome subset of the total dataset (instead of just a single example). This subset is\\ncalled a minibatch, and in addition to the learning rate, minibatch size is another\\nhyperparameter. Minibatches strike a balance between the efficiency of batch gradient\\ndescent and the local-minima avoidance afforded by stochastic gradient descent. In\\nthe context of backpropagation, our weight update step becomes:\\nΔwij = −∑k ∈minibatcℎϵyi\\nk yj\\nk 1 −yj\\nk\\n∂E k\\n∂yj\\nk\\nThis is identical to what we derived in the previous section, but instead of summing\\nover all the examples in the dataset, we sum over the examples in the current\\nminibatch. For a more theoretical discussion of why SGD and minibatch gradient\\ndescent result in an unbiased estimate of the gradient over the total dataset, please\\nrefer to “Neural Net Learning Theory” on page 74.\\nTest Sets, Validation Sets, and Overfitting\\nOne of the major issues with artificial neural networks is that the models are quite\\ncomplicated. For example, let’s consider a neural network that pulls data from an\\nimage from the MNIST database (28 × 28 pixels), feeds into two hidden layers with\\n30 neurons, and finally reaches a softmax layer of 10 neurons. The total number of\\nparameters in the network is nearly 25,000. This can be quite problematic, and to\\nunderstand why, let’s consider a new toy example, illustrated in Figure 4-8.\\nTest Sets, Validation Sets, and Overfitting \\n| \\n65'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 81}, page_content='Figure 4-8. Two potential models that might describe our dataset: a linear model versus\\na degree 12 polynomial\\nWe are given a bunch of data points on a flat plane, and our goal is to find a curve\\nthat best describes this dataset (i.e., will allow us to predict the y coordinate of a new\\npoint given its x coordinate). Using the data, we train two different models: a linear\\nmodel and a degree 12 polynomial. Which curve should we trust? The line that gets\\nalmost no training example correct? Or the complicated curve that hits every single\\npoint in the dataset? At this point we might trust the linear fit because it seems much\\nless contrived. But just to be sure, let’s add more data to our dataset. The result is\\nshown in Figure 4-9.\\nNow the verdict is clear: the linear model is not only better subjectively but also\\nquantitatively (measured using the squared error metric). This leads to an interesting\\npoint about training and evaluating machine learning models. By building a very\\ncomplex model, it’s quite easy to perfectly fit our training dataset because we give\\nour model enough degrees of freedom to contort itself to fit the observations in the\\ntraining set. But when we evaluate such a complex model on new data, it performs\\npoorly. In other words, the model does not generalize well. This is a phenomenon\\ncalled overfitting, and it is one of the biggest challenges that a machine learning\\nengineer must combat. This becomes an even more significant issue in deep learning,\\nwhere our neural networks have large numbers of layers containing many neurons.\\n66 \\n| \\nChapter 4: Training Feed-Forward Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 82}, page_content='The number of connections in these models is astronomical, reaching the millions.\\nAs a result, overfitting is commonplace.\\nFigure 4-9. Evaluating our model on new data indicates that the linear fit is a much\\nbetter model than the degree 12 polynomial\\nLet’s see how this looks in the context of a neural network. Say we have a neural\\nnetwork with two inputs, a softmax output of size 2, and a hidden layer with 3, 6,\\nor 20 neurons. We train these networks using minibatch gradient descent (batch size\\n10), and the results, visualized using ConvNetJS, are shown in Figure 4-10.\\nFigure 4-10. A visualization of neural networks with 3, 6, and 20 neurons (in that order)\\nin their hidden layer\\nTest Sets, Validation Sets, and Overfitting \\n| \\n67'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 83}, page_content='It’s already quite apparent from these images that as the number of connections in\\nour network increases, so does our propensity to overfit to the data. We can similarly\\nsee the phenomenon of overfitting as we make our neural networks deep. These\\nresults are shown in Figure 4-11.\\nFigure 4-11. Neural networks with one, two, and four hidden layers (in that order) of\\nthree neurons each\\nThis leads to three major observations. First, the machine learning engineer is always\\nworking with a direct trade-off between overfitting and model complexity. If the\\nmodel isn’t complex enough, it may not be powerful enough to capture all of the\\nuseful information necessary to solve a problem. However, if our model is very\\ncomplex (especially if we have a limited amount of data at our disposal), we run the\\nrisk of overfitting. Deep learning takes the approach of solving complex problems\\nwith complex models and taking additional countermeasures to prevent overfitting.\\nWe’ll see a lot of these measures in this and later chapters.\\nSecond, it is misleading to evaluate a model using the data we used to train it. Using\\nthe example in Figure 4-8, this would falsely suggest that the degree 12 polynomial\\nmodel is preferable to a linear fit. As a result, we almost never train our model\\non the entire dataset. Instead, we split up our data into a training set and a test set\\n(Figure 4-12). This enables us to make a fair evaluation of our model by directly\\nmeasuring how well it generalizes on new data it has not yet seen.\\nIn the real world, large datasets are hard to come by, so it might\\nseem like a waste to not use all of the data at our disposal during\\nthe training process. Consequently, it may be tempting to reuse\\ntraining data for testing or cut corners while compiling test data. Be\\nforewarned: if the test set isn’t well constructed, we won’t be able\\ndraw any meaningful conclusions about our model.\\n68 \\n| \\nChapter 4: Training Feed-Forward Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 84}, page_content='Figure 4-12. Nonoverlapping training and test sets\\nThird, it’s quite likely that while we’re training our data, there’s a point in time when\\ninstead of learning useful features, we start overfitting to the training set. To avoid\\nthat, we want to be able to stop the training process as soon as we start overfitting to\\nprevent poor generalization. To do this, we divide our training process into epochs.\\nAn epoch is a single iteration over the entire training set. If we have a training set\\nof size d and we are doing minibatch gradient descent with batch size b, then an\\nepoch would be equivalent to d\\nb model updates. At the end of each epoch, we want\\nto measure how well our model is generalizing. To do this, we use an additional\\nvalidation set, which is shown in Figure 4-13.\\nFigure 4-13. A validation set to prevent overfitting during the training process\\nAt the end of an epoch, the validation set will tell us how the model does on data\\nit has yet to see. If the accuracy on the training set continues to increase while the\\naccuracy on the validation set stays the same (or decreases), it’s a good sign that it’s\\ntime to stop training because we’re overfitting.\\nThe validation set is also helpful as a proxy measure of accuracy during the\\nprocess of hyperparameter optimization. We’ve covered several hyperparameters so\\nfar (learning rate, minibatch size, etc.), but we have yet to develop a framework\\nfor how to find the optimal values for these hyperparameters. One potential\\nway to find the optimal setting of hyperparameters is by applying a grid search,\\nwhere we pick a value for each hyperparameter from a finite set of options\\n(e.g., ϵ ∈0 . 001, 0 . 01, 0 . 1 , batch size ∈16, 64, 128 , ...), and train the model with\\nevery possible permutation of hyperparameter choices. We elect the combination\\nTest Sets, Validation Sets, and Overfitting \\n| \\n69'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 85}, page_content='3 Nelder, John A., and Roger Mead. “A Simplex Method for Function Minimization.” The Computer Journal 7.4\\n(1965): 308-313.\\nof hyperparameters with the best performance on the validation set and report the\\naccuracy of the model trained with the best combination on the test set.3\\nWith this in mind, before we jump into describing the various ways to directly\\ncombat overfitting, let’s outline the workflow we use when building and training\\ndeep learning models. The workflow is described in detail in Figure 4-14. It is a\\ntad intricate, but it’s critical to understand the pipeline to ensure that we’re properly\\ntraining our neural networks.\\nFigure 4-14. Detailed workflow for training and evaluating a deep learning model\\nFirst, we define our problem rigorously. This involves determining our inputs, the\\npotential outputs, and the vectorized representations of both. For instance, let’s say\\nour goal was to train a deep learning model to identify cancer. Our input would be an\\nRBG image, which can be represented as a vector of pixel values. Our output would\\nbe a probability distribution over three mutually exclusive possibilities: (1) normal,\\n70 \\n| \\nChapter 4: Training Feed-Forward Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 86}, page_content='(2) benign tumor (a cancer that has yet to metastasize), or (3) malignant tumor (a\\ncancer that has already metastasized to other organs).\\nAfter we define our problem, we need to build a neural network architecture to solve\\nit. Our input layer would have to be of appropriate size to accept the raw data from\\nthe image, and our output layer would have to be a softmax of size 3. We will also\\nhave to define the internal architecture of the network (number of hidden layers,\\nthe connectivities, etc.). We’ll further discuss the architecture of image recognition\\nmodels when we talk about convolutional neural networks in Chapter 6. At this\\npoint, we also want to collect a significant amount of data for training or modeling.\\nThis data would probably be in the form of uniformly sized pathological images\\nthat have been labeled by a medical expert. We shuffle and divide this data up into\\nseparate training, validation, and test sets.\\nFinally, we’re ready to begin gradient descent. We train the model on our training set\\nfor an epoch at a time. At the end of each epoch, we ensure that our error on the\\ntraining set and validation set is decreasing. When one of these stops improving, we\\nterminate and make sure we’re happy with the model’s performance on the test data.\\nIf we’re unsatisfied, we need to rethink our architecture or reconsider whether the\\ndata we collect has the information required to make the prediction we’re interested\\nin making. If our training set error stopped improving, we probably need to do a\\nbetter job of capturing the important features in our data. If our validation set error\\nstopped improving, we probably need to take measures to prevent overfitting.\\nIf, however, we are happy with the performance of our model on the training data,\\nthen we can measure its performance on the test data, which the model has never\\nseen before this point. If it is unsatisfactory, we need more data in our dataset because\\nthe test set seems to consist of example types that weren’t well represented in the\\ntraining set. Otherwise, we are finished!\\nPreventing Overfitting in Deep Neural Networks\\nSeveral techniques have been proposed to prevent overfitting during the training\\nprocess. In this section, we’ll discuss these techniques in detail.\\nOne method of combatting overfitting is called regularization. Regularization modi‐\\nfies the objective function that we minimize by adding additional terms that penalize\\nlarge weights. We change the objective function so that it becomes Error + λf θ ,\\nwhere f θ  grows larger as the components of θ grow larger, and λ is the regulariza‐\\ntion strength (another hyperparameter). The value we choose for λ determines how\\nmuch we want to protect against overfitting. A λ = 0 implies that we do not take any\\nmeasures against the possibility of overfitting. If λ is too large, then our model will\\nprioritize keeping θ as small as possible over trying to find the parameter values that\\nPreventing Overfitting in Deep Neural Networks \\n| \\n71'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 87}, page_content='4 Tikhonov, Andrei Nikolaevich, and Vladlen Borisovich Glasko. “Use of the Regularization Method in Non-\\nLinear Problems.” USSR Computational Mathematics and Mathematical Physics 5.3 (1965): 93-107.\\nperform well on our training set. As a result, choosing λ is a very important task and\\ncan require some trial and error.\\nThe most common type of regularization in machine learning is L2 regularization.4 It\\ncan be implemented by augmenting the error function with the squared magnitude\\nof all weights in the neural network. In other words, for every weight w in the\\nneural network, we add 1\\n2λw2 to the error function. The L2 regularization has the\\nintuitive interpretation of heavily penalizing peaky weight vectors and preferring\\ndiffuse weight vectors. This has the appealing property of encouraging the network to\\nuse all of its inputs a little rather than using only some of its inputs a lot. Of particular\\nnote is that during the gradient descent update, using the L2 regularization ultimately\\nmeans that every weight is decayed linearly to zero. Because of this phenomenon, L2\\nregularization is also commonly referred to as weight decay.\\nWe can visualize the effects of L2 regularization using ConvNetJS. Similar to Figures\\n2-10 and 2-11, we use a neural network with 2 inputs, a softmax output of size 2,\\nand a hidden layer with 20 neurons. We train the networks using minibatch gradient\\ndescent (batch size 10) and regularization strengths of 0.01, 0.1, and 1. The results can\\nbe seen in Figure 4-15.\\nFigure 4-15. A visualization of neural networks trained with regularization strengths of\\n0.01, 0.1, and 1 (in that order)\\nAnother common type of regularization is L1 regularization. Here, we add the term\\nλ w  for every weight w in the neural network. The L1 regularization has the intrigu‐\\ning property that it leads the weight vectors to become sparse during optimization\\n(i.e., close to exactly zero). Neurons with L1 regularization end up using only a\\nsmall subset of their most important inputs and become quite resistant to noise in\\nthe inputs. In comparison, weight vectors from L2 regularization are usually diffuse,\\nsmall numbers. L1 regularization is useful when you want to understand exactly\\n72 \\n| \\nChapter 4: Training Feed-Forward Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 88}, page_content='5 Srebro, Nathan, Jason DM Rennie, and Tommi S. Jaakkola. “Maximum-Margin Matrix Factorization.” NIPS,\\nVol. 17, 2004.\\n6 Srivastava, Nitish, et al. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting.” Journal of\\nMachine Learning Research 15.1 (2014): 1929-1958.\\nwhich features are contributing to a decision. If this level of feature analysis isn’t\\nnecessary, we prefer to use L2 regularization because it empirically performs better.\\nMax norm constraints have a similar goal of attempting to restrict θ from becoming\\ntoo large, but they do this more directly.5 Max norm constraints enforce an absolute\\nupper bound on the magnitude of the incoming weight vector for every neuron\\nand use projected gradient descent to enforce the constraint. So any time a gradient\\ndescent step moves the incoming weight vector such that w 2 > c, we project the\\nvector back onto the ball (centered at the origin) with radius c. Typical values of c are\\n3 and 4. One of the nice properties is that the parameter vector cannot grow out of\\ncontrol (even if the learning rates are too high) because the updates to the weights are\\nalways bounded.\\nDropout is a different kind of method for preventing overfitting that has become\\none of the most favored methods of preventing overfitting in deep neural networks.6\\nWhile training, dropout is implemented by only keeping a neuron active with some\\nprobability p (a hyperparameter), or setting it to zero otherwise. Intuitively, this\\nforces the network to be accurate even in the absence of certain information. It\\nprevents the network from becoming too dependent on any one neuron (or any small\\ncombination of neurons). Expressed more mathematically, it prevents overfitting by\\nproviding a way of approximately combining exponentially many different neural\\nnetwork architectures efficiently. The process of dropout is expressed in Figure 4-16.\\nDropout is pretty intuitive, but there are some important intricacies to consider.\\nFirst, we’d like the outputs of neurons during test time to be equivalent to their\\nexpected outputs at training time. We could fix this naively by scaling the output at\\ntest time. For example, if p = 0 . 5, neurons must halve their outputs at test time in\\norder to have the same (expected) output they would have during training. This is\\neasy to see because a neuron’s output is set to 0 with probability 1 −p. This means\\nthat if a neuron’s output prior to dropout was x, then after dropout, the expected\\noutput would be E output = px + 1 −p · 0 = px. This naive implementation of\\ndropout is undesirable, however, because it requires scaling of neuron outputs at test\\ntime. Test-time performance is extremely critical to model evaluation, so it’s always\\npreferable to use inverted dropout, where the scaling occurs at training time instead\\nof at test time. In inverted dropout, any neuron whose activation hasn’t been silenced\\nhas its output divided by p before the value is propagated to the next layer. With this\\nfix, E output = p · x\\np + 1 −p · 0 = x, and we can avoid arbitrarily scaling neuronal\\noutput at test time.\\nPreventing Overfitting in Deep Neural Networks \\n| \\n73'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 89}, page_content='Figure 4-16. Dropout sets each neuron in the network as inactive with some random\\nprobability during each minibatch of training\\nNeural Net Learning Theory\\nLet’s cover some of the theory underpinning SGD and minibatch gradient descent.\\nWe noted some of the empirical gains from SGD and minibatch gradient descent in\\nthis chapter. Here, we try to understand why it is even “OK,” in theory, to use these\\nlearning algorithms as a substitute for the gradient update over the entire dataset,\\nwhich can often be intractable with many training examples. The intractability arises\\nfrom having to take the partial derivative with respect to every weight in the neural\\nnetwork for every training example in any given iteration, which is prohibitive for\\nlarge datasets and neural networks of even moderate size.\\nHere, for simplicity, we will refer to X as the data matrix, which is a matrix of\\ndimension n by d, where n refers to the number of inputs in the entire dataset and\\nd refers to the number of features associated with any given input. In other words,\\neach row of X corresponds to a single training example. In addition to X, we have y,\\na vector of labels, or desired outputs, associated with each input of the data matrix. As\\ncan be inferred, y is of dimension n by 1. We refer to f · , θ  as the function defined\\nby the neural network, and θ as the weights (which we intend to learn through the\\ntraining process) that parametrize it. Finally, we refer to L y, y  as the error, or loss,\\nfunction when comparing the predicted output y with the true output y.\\n74 \\n| \\nChapter 4: Training Feed-Forward Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 90}, page_content='Let’s start with some basic assumptions. Where do X and y even come from? We\\nassume that there is some underlying data-generating process from which X and y\\nare produced. That is, there exists a joint distribution over inputs and labels p x, y\\nfrom which the dataset we observe has been randomly sampled. In classical learning\\ntheory, we would like our learned function, or neural network f · , θ , to minimize\\nthe objective:\\nEp x, y L f x, θ , y\\nor the expected loss with respect to the true distribution p x, y . We term this\\nobjective the population risk. Why minimize the population risk? Intuitively, the more\\npopular any given pairing of input datapoint x i  and output label y i  under the true\\ndistribution, the more we would like weight the output of f x i , θ  resembling y i .\\nL f x i , θ , y i  quantifies how well (or in this case, equivalently, how poorly given\\nL is an error function) the output of f x i , θ  resembles y i . The optimal weight is\\nsimply p x i , y i , which quantifies the popularity of the pairing. Putting this logic\\ntogether over all possible pairings, we arrive at the population risk objective. Note\\nthat this logic holds regardless of whether x and y are discrete or continuous.\\nUnfortunately, we have no access to the true distribution. If we did, our problem\\nwould already be solved (more accurately, is instantly solved when y is discrete). The\\nbest we can do is approximate p x, y  with the data we see. We call this empirical\\ndistribution pD x, y , which, in the limit of infinite data, converges to p x, y , making\\nany gradient update based on the empirical distribution unbiased. The domain of the\\nempirical distribution is our training set. Instead of minimizing the population risk,\\nwe minimize the empirical risk as a proxy:\\nEpD x, y L f x; θ , y\\n= 1\\nn ∑i = 1\\nn\\nL f x i ; θ , y i\\nNow, we would like to find a set of weights θ, that minimize the empirical risk. We do\\nthat by taking the gradient of the objective with respect to θ:\\n∇θEpD x, y L f x; θ , y\\n= ∇θ\\n1\\nn ∑i = 1\\nn\\nL f x i ; θ , y i\\n= 1\\nn ∑i = 1\\nn\\n∇θL f x i ; θ , y i\\nPreventing Overfitting in Deep Neural Networks \\n| \\n75'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 91}, page_content='We now see that the newly introduced empirical risk minimization is the theoretical\\nmotivation for the gradient update over the entire dataset presented in “The Backpro‐\\npagation Algorithm” on page 61. The only, subtle difference is the factor 1\\nn, which is a\\nconstant that can be rolled into the learning rate. Extending the last equality, we have:\\n1\\nn ∑i = 1\\nn\\n∇θL f x i ; θ , y i\\n= EpD x, y ∇θL f x; θ , y\\nWe can do something that uses almost the exact same logic, which allows us to use\\nthe empirical risk as a proxy for population risk: approximate the expectation with\\nrespect to the empirical distribution via sampling. Again, this is unbiased since we\\nachieve the empirical distribution in the limit of infinite samples. The number of\\nsamples is left as a hyperparameter, where using a single sample has been popularly\\ntermed SGD; using a relatively small number of samples has been popularly termed\\nminibatch gradient descent.\\nSummary\\nIn this chapter, we’ve learned all of the basics involved in training feed-forward neural\\nnetworks. We’ve talked about gradient descent, the backpropagation algorithm, as\\nwell as various methods we can use to prevent overfitting. In the next chapter,\\nwe’ll put these lessons into practice when we use the PyTorch library to efficiently\\nimplement our first neural networks. Then in Chapter 6, we’ll return to the problem\\nof optimizing objective functions for training neural networks and design algorithms\\nto significantly improve performance. These improvements will enable us to process\\nmuch more data, which means we’ll be able to build more comprehensive models.\\n76 \\n| \\nChapter 4: Training Feed-Forward Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 92}, page_content='CHAPTER 5\\nImplementing Neural Networks in PyTorch\\nIntroduction to PyTorch\\nIn this chapter, you will learn the basics of PyTorch, one of the most popular deep\\nlearning frameworks in use today. PyTorch was introduced by Facebook’s AI Research\\nLab in 2016 and gained users rapidly, both in industry and in research, through\\nthe following years. One reason for PyTorch’s widespread adoption was its intuitive,\\nPythonic feel, which fit naturally into the preexisting workstreams and coding para‐\\ndigms followed by deep learning practitioners.\\nIn particular, this chapter will discuss the data structures utilized by PyTorch, how to\\ndefine neural models in PyTorch, and how to connect data with models for training\\nand testing. Finally, we implement a practical example in PyTorch—a classifier for the\\nMNIST digits dataset, complete with code for training and testing the classifier.\\nInstalling PyTorch\\nInstalling a CPU-compatible version of PyTorch is relatively simple. The PyTorch\\ndocs recommend using conda, a package management system. Within conda, you\\ncan create multiple environments, where an environment is a context that encap‐\\nsulates all of your package installs. Access to a package does not transfer across\\nenvironments—this allows the user to have a clean separation between different\\ncontexts by downloading packages within individual environments. We recommend\\nthat you create a conda environment for deep learning purposes that you can switch\\ninto whenever necessary. We refer you to the conda docs for guidance on how to\\ndownload conda and further notes on environments.\\n77'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 93}, page_content='Once you have installed conda, created your deep learning environment, and\\nswitched into it, the PyTorch docs recommend running the following code from\\nyour command line to download a CPU-compatible version of PyTorch on macOS:\\nconda install pytorch torchvision torchaudio -c pytorch\\nNote that with this install come torchvision and torchaudio, which are specialized\\npackages for working with image data and audio data, respectively. If you are on a\\nLinux system, the docs recommend running the following code from your command\\nline:\\nconda install pytorch torchvision torchaudio cpuonly -c pytorch\\nNow, you can navigate to a Python shell (still in your deep learning environment),\\nand the following command should run with no issues:\\nimport torch\\nIt is important to get this command running with no errors in your Python shell\\nbefore moving on to running the code in the following sections, as they all require the\\nability to import the PyTorch package.\\nPyTorch Tensors\\nTensors are the primary data structure by which PyTorch stores and manipulates\\nnumerical information. Tensors can be seen as a generalization of arrays and matri‐\\nces, which we covered in detail in our introduction to linear algebra in Chapter 1.\\nSpecifically, tensors, as a generalization of 2D matrices and 1D arrays, can store mul‐\\ntidimensional data such as batches of three-channel images. Note that this requires\\n4D data storage, since each image is 3D (including the channel dimension), and a\\nfourth dimension that is required to index each individual image. Tensors can even\\nrepresent dimensionalities beyond the 4D space, although the usage of such tensors\\nin practice is uncommon.\\nIn PyTorch, tensors are utilized universally. They are used to represent the inputs to\\nmodels, the weight layers within the models themselves, and the outputs of models.\\nThe standard linear algebra operations of transposition, addition, multiplication,\\ninversion, etc., can all be run on tensors.\\nTensor Init\\nHow do we initialize tensors? We can initialize a tensor from a variety of data types.\\nSome examples are Python lists and Python numerical primitives:\\narr = [1,2]\\ntensor = torch.tensor(arr)\\nval = 2.0\\ntensor = torch.tensor(val)\\n78 \\n| \\nChapter 5: Implementing Neural Networks in PyTorch'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 94}, page_content='Tensors can also be initialized from NumPy arrays, allowing PyTorch to be integrated\\neasily into existing data science and machine learning workflows:\\nimport numpy as np\\nnp_arr = np.array([1,2])\\nx_t = torch.from_numpy(np_arr)\\nAdditionally, tensors can be formed via some common PyTorch API endpoints:\\nzeros_t = torch.zeros((2,3)) # Returns 2x3 tensor of zeros\\nones_t = torch.ones((2,3)) # Returns 2x3 tensor of ones\\nrand_t = torch.randn((2,3)) # Returns 2x3 tensor of random numbers\\nTensor Attributes\\nIn the examples we just saw, we passed a tuple as the argument to each function\\ncall. The number of indices in the tuple is the dimensionality of the tensor to be\\ncreated, while the number at each index represents the desired size of that particular\\ndimension. To access the dimensionality of a tensor, we can call its shape attribute:\\nzeros_t.shape # Returns torch.Size([2, 3])\\nCalling the shape attribute on any of the previous examples should return the same\\ntuple as the input argument, assuming the tensor has not been significantly modified\\nin-between.\\nWhat are some other attributes of tensors? In addition to dimension, tensors also\\nstore information on the type of data being stored: floating point, complex, integer,\\nand boolean. There exist subtypes within each of these categories, but we won’t go\\ninto the differences between each subtype here. It’s also important to note that a\\ntensor cannot contain a mix and match of various data types—all data within a single\\ntensor must be of the same data type. To access the data type of a tensor, we can call\\nits dtype attribute:\\nx_t = torch.tensor(2.0)\\nx_t.dtype # Returns torch.float32\\nAdditionally, although we haven’t shown this yet, we can set the data type of a tensor\\nduring initialization. Extending one of our previous examples:\\narr = [1,2]\\nx_t = torch.tensor(arr, dtype=torch.float32)\\nIn addition to the data type and shape of a tensor, we can also learn the device\\non which the tensor is allocated. These devices include the famous CPU, which is\\nstandard with any computer and is the default storage for any tensor, and the GPU,\\nor graphics processing unit, which is a specialized data processing unit often used in\\nthe image space. GPUs massively speed up many common tensor operations such as\\nmultiplication via parallel processing over hundreds of small, specialized cores, thus\\nPyTorch Tensors \\n| \\n79'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 95}, page_content=\"making them immensely useful for most deep learning applications. To access the\\ntensor device, we can call its device attribute:\\nx_t.device # Returns device(type='cpu') by default\\nSimilarly to data type, we can set the device of a tensor upon initialization:\\n# PyTorch will use GPU if it's available\\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\\narr = [1,2]\\nx_t = torch.tensor(arr, dtype=torch.float32, device=device)\\nThis is a common approach to checking whether a GPU is available via code and\\nusing a GPU if it is available.  If the GPU is not available, it will use a CPU without\\nerror.\\nIf you have defined a tensor with a certain set of attributes and would like to modify\\nthese attributes, you can use the to function:\\nx_t = x_t.to(device, dtype=torch.int)\\nAnd finally, as we’ll cover in “Gradients in PyTorch” on page 83, PyTorch tensors\\ncan be initialized with the argument requires_grad, which when set to True, stores\\nthe tensor’s gradient in an attribute called grad.\\nTensor Operations\\nThe PyTorch API provides us with many possible tensor operations, ranging from\\ntensor arithmetic to tensor indexing. In this section we will cover some of the more\\nuseful tensor operations—ones that you will likely use often in your deep learning\\napplications.\\nOne of the most basic operations is multiplying a tensor by some scalar c. This can be\\nachieved via the code:\\nc = 10\\nx_t = x_t*c\\nThis results in an element-wise product of the scalar with the entries of the tensor.\\nAnother one of the most basic tensor operations is tensor addition and subtraction.\\nTo do this, we can simply add tensors via +. Subtraction follows directly from being\\nable to do addition and multiplying the second tensor by the scalar –1:\\nx1_t = torch.zeros((1,2))\\nx2_t = torch.ones((1,2))\\nx1_t + x2_t\\n# returns tensor([[1., 1.]])\\nThe result is an element-wise sum of the two tensors. This can be seen as a direct\\ngeneralization of matrix addition for any dimensionality. Note that this direct gener‐\\nalization implicitly assumes the same constraint we discussed for matrix addition a\\n80 \\n| \\nChapter 5: Implementing Neural Networks in PyTorch\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 96}, page_content='while ago: that the two tensors being summed are of the same dimension. PyTorch,\\nsimilarly, will accept any two broadcastable inputs with no issues, where broadcasting\\nis a procedure by which the two inputs are resolved to a common shape, and broad‐\\ncastable refers to whether it is even possible for the two inputs to be resolved to a\\ncommon shape. If the two tensors are already of the same shape, no broadcasting\\nis necessary. We refer you to the PyTorch documentation for more information on\\nhow the API determines if the two inputs are broadcastable, and how broadcasting is\\nperformed in such cases.\\nTensor multiplication is another useful operation to become familiar with. Tensor\\nmultiplication works the same as matrix and vector multiplication when the dimen‐\\nsionality of each tensor is less than or equal to 2. However, tensor multiplication\\nalso works on tensors of arbitrarily high dimensionality, given the two tensors are\\ncompatible. We can think of tensor multiplication in high dimensions as batched\\nmatrix multiplications: imagine we have two tensors, the first is of shape (2,1,2)\\nand the second is of shape (2,2,2). We can further represent the first tensor as a\\nlength-two list of 1 × 2 matrices, while the second is a length-two list of 2 × 2\\nmatrices. Their product is a length-two list, where index i of the product is the matrix\\nproduct of index i of the first tensor and index i of the second tensor, as shown in\\nFigure 5-1.\\nFigure 5-1. To help visualize the general tensor multiplication method, this figure shows\\nthe matrix multiplication that occurs before restacking.\\nRestacking the resultant list into a 3D tensor, we see that the product is of shape\\n(2,1,2). Now, we can generalize this to four dimensions, where instead of imagining\\nwe have a list of matrices, we represent each 4D tensor as a grid of matrices and the\\n(i,j)-th index of the product is the matrix product of the (i,j)-th indices of the two 4D\\ninput tensors. We represent this mathematically:\\nPi, j, x, z = ∑yAi, j, x, y * Bi, j, y, z\\nThis procedure is generalizable to any dimensionality, assuming that the two input\\ntensors follow the constraints of matrix multiplication. As with tensor addition, there\\nare exceptions that involve broadcasting, though we won’t cover those in detail here.\\nPyTorch Tensors \\n| \\n81'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 97}, page_content='We refer you to the PyTorch documentation for detailed information on broadcast‐\\ning. To multiply two tensors in PyTorch, you can use the torch matmul function:\\nx1_t = torch.tensor([[1,2],[3,4]])\\nx2_t = torch.tensor([[1,2,3],[4,5,6]])\\ntorch.matmul(x1_t, x2_t) # Returns tensor([[9,12,15],[19,26,33]])\\nIn addition to arithmetic operations on tensors, we can also index and slice tensors.\\nIf you have prior experience with NumPy, you’ll notice that PyTorch indexing is very\\nsimilar and is based on linear algebra fundamentals. If you have a 3D tensor, you can\\naccess the value at position (i,j,k) via the following code:\\ni,j,k = 0,1,1\\nx3_t = torch.tensor([[[3,7,9],[2,4,5]],[[8,6,2],[3,9,1]]])\\nprint(x3_t)\\n# out:\\n# tensor([[[3, 7, 9],\\n#          [2, 4, 5]],\\n#         [[8, 6, 2],\\n#          [3, 9, 1]]])\\nx3_t[i,j,k]\\n# out:\\n# tensor(4)\\nTo access larger slices of the tensor, say the matrix at position 0 in a 3D tensor, you\\ncan use the following code:\\nx3_t[0] # Returns the matrix at position 0 in tensor\\nx3_t[0,:,:] # Also returns the matrix at position 0 in tensor!\\n# out:\\n# tensor([[3, 7, 9],\\n#         [2, 4, 1]])\\nwhere the two lines of code are interpreted to be equivalent by the PyTorch API.\\nThis is because using a single indexer, such as x3_t[0], implicitly assumes that the\\nuser would like to access all indices (i,j,k) that satisfy the condition i = 0 (i.e., the top\\nmatrix in the stack of matrices that is the original 3D tensor). Usage of the : symbol\\nmakes this implicit assumption clear by telling PyTorch directly that the user would\\nnot like to subset the data at that dimension. We can also use the : symbol to subset\\nthe data, for example:\\nx3_t[0,1:3,:]\\n# returns tensor([[2, 4, 5]])\\nwhere the last line of code is interpreted as: find all indices (i,j,k) such that i = 0, j ≥1,\\nand j < 3 (: follows the standard Python list indexing convention of being inclusive\\nat the start of the defined range and exclusive at the end). In plain English, we want to\\naccess the second and third rows of the top matrix in the stack of matrices that is the\\n82 \\n| \\nChapter 5: Implementing Neural Networks in PyTorch'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 98}, page_content='original 3D tensor. Note that this usage of : is consistent with standard Python list\\nindexing.\\nIn addition to accessing indices or slices of a tensor, we can also set those indices and\\nslices to new values. In the single index case, this is as simple as:\\nx3_t[0,1,2] = 1\\n# out:\\n# tensor([[[3, 7, 9],\\n#          [2, 4, 1]],\\n#         [[8, 6, 2],\\n#          [3, 9, 1]]])\\nTo set larger slices of the tensor, the most straightforward way is to define a tensor\\nthat is of the same dimensionality as the slice, and use the following code:\\nx_t = torch.randn(2,3,4)\\nsub_tensor = torch.randn(2,4)\\nx_t[0,1:3,:] = sub_tensor\\nAdditionally, via broadcasting, we can do things like:\\nx_t[0,1:3,:] = 1\\nsub_tensor = torch.randn(1,4)\\nx_t[0,1:3,:] = sub_tensor\\nThe first line sets the entirety of those two rows to 1, and the second sets both rows of\\nthe slice to the single row passed in as sub_tensor. In the next section, we will show\\nhow to compute the gradients of a function in PyTorch, and how to access the values\\nof those gradients.\\nGradients in PyTorch\\nJust as a recap, let’s recall derivatives and partial derivatives from calculus. The partial\\nderivative of a function, which could be as simple as a polynomial function of a\\nfew variables to something as complex as a neural network, with respect to one of\\nthe function’s inputs represents the rate of change of the output of the function as\\nthat input’s value changes slightly. So, large magnitude derivatives indicate that the\\noutput is very volatile with small changes in the input (think f x = x10 when x is of\\nmoderate size), while small magnitude derivatives indicate that the output is relatively\\nstable with small changes in the input (think f x = x\\n10). If the function takes in more\\nthan one input, the gradient is the vector that is composed of all of these partial\\nderivatives:\\nf x, y, z = x2 + y2 + z2\\nGradients in PyTorch \\n| \\n83'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 99}, page_content='∂f\\n∂x = ∇xf x, y, z = 2x\\n∇f = 2x 2y 2z\\nContinuing from this example, how would we represent this in PyTorch? We can use\\nthe following code:\\nx = torch.tensor(2.0, requires_grad=True)\\ny = torch.tensor(3.0, requires_grad=True)\\nz = torch.tensor(1.5, requires_grad=True)\\nf = x**2+y**2+z**2\\nf.backward()\\nx.grad, y.grad, z.grad\\n# out:\\n# (tensor(4.), tensor(6.), tensor(3.))\\nThe call to backward() computes the partial derivative of the output f with respect\\nto each of the input variables. We should expect the values for x.grad, y.grad, and\\nz.grad to be 4.0, 6.0, and 3.0, respectively. In the case of neural networks, we can\\nrepresent the neural network as f x, θ , where f is the neural network, x is some\\nvector representing the input, and θ is the parameters of f. Instead of computing the\\ngradient of the output of f with respect to x as done in the previous example, we\\ncompute the gradient of the loss of the output of f with respect to θ. Adjusting θ via\\nthe gradient will eventually lead to a setting of θ that results in a small loss for the\\ntraining data and one that hopefully generalizes to data that f hasn’t seen before. In\\nthe next section, we will introduce the building blocks of neural networks.\\nThe PyTorch nn Module\\nThe PyTorch nn module provides all of the baseline functionality necessary for\\ndefining, training, and testing a model. To import the nn module, all you need to do is\\nrun the following line of code:\\n import torch.nn as nn\\nIn this section, we will cover some of the most common uses of the nn module. For\\nexample, to initialize a weight matrix needed for a feed-forward neural network, you\\ncan use the following code:\\nin_dim, out_dim = 256, 10\\nvec = torch.randn(256)\\nlayer = nn.Linear(in_dim, out_dim, bias=True)\\nout = layer(vec)\\nThis defines a single layer with bias in a feed-forward neural network, which is a\\nmatrix of weights that takes as input a vector of dimension 256 and outputs a vector\\nof dimension 10. The last line of code demonstrates how we can easily apply this layer\\n84 \\n| \\nChapter 5: Implementing Neural Networks in PyTorch'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 100}, page_content='to an input vector and store the output in a new tensor. If we wanted to do the same\\nthing using only our knowledge from prior sections, we would need to manually\\ndefine a weight matrix W and bias vector b via torch.tensor and explicitly compute:\\nW = torch.rand(10,256)\\nb = torch.zeros(10,1)\\nout = torch.matmul(W, vec) + b\\nThe nn module’s Linear layer allows us to abstract away these manual operations so\\nwe can write clean, concise code.\\nA feed-forward neural network can be thought of as simply a composition of such\\nlayers, for example:\\nin_dim, feature_dim, out_dim = 784, 256, 10\\nvec = torch.randn(784)\\nlayer1 = nn.Linear(in_dim, feature_dim, bias=True)\\nlayer2 = nn.Linear(feature_dim, out_dim, bias=True)\\nout = layer2(layer1(vec))\\nThis code represents a neural network that is the function composition\\nlayer2(layer1(vec)), or mathematically: W2 W1 * x + b1 + b2. To represent more\\ncomplex, nonlinear functions, the nn module additionally provides nonlinearities\\nsuch as ReLU, which can be accessed via nn.ReLU, and tanh, which can be accessed\\nvia nn.Tanh. These nonlinearities are applied in between layers, as follows:\\nrelu = nn.ReLU()\\nout  = layer2(relu(layer1(vec)))\\nWe’ve gone over almost everything necessary to define a model in PyTorch. The last\\nthing to cover is the nn.Module class—the base class from which all neural networks\\nare subclassed in PyTorch.\\nThe nn.Module class has one important method that your specific model’s subclass\\nwill override. This method is the forward method, and it defines how the layers\\ninitialized in your model’s constructor interact with the input to generate the model’s\\noutput. Here is an example of some code that can be used to encapsulate the simple\\ntwo-layer neural network we just defined:\\nclass BaseClassifier(nn.Module):\\n  def __init__(self, in_dim, feature_dim, out_dim):\\n    super(BaseClassifier, self).__init__()\\n    self.layer1 = nn.Linear(in_dim, feature_dim, bias=True)\\n    self.layer2 = nn.Linear(feature_dim, out_dim, bias=True)\\n    self.relu = nn.ReLU()\\n  def forward(self, x):\\n    x = self.layer1(x)\\n    x = self.relu(x)\\n    out = self.layer2(x)\\n    return out\\nThe PyTorch nn Module \\n| \\n85'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 101}, page_content='We’ve written our first neural network in PyTorch! BaseClassifier is a bug-free\\nmodel class that can be instantiated after defining in_dim, feature_dim, and out_dim.\\nThe constructor takes in these three variables as arguments in the constructor, which\\nmakes the model flexible in terms of layer size. This is the sort of model that can\\nbe used effectively as a first-pass classifier for datasets such as MNIST, as we will\\ndemonstrate in “Building the MNIST Classifier in PyTorch” on page 89. To generate\\nthe output of a model on some input, we can use the model as follows:\\nno_examples = 10\\nin_dim, feature_dim, out_dim = 784, 256, 10\\nx = torch.randn((no_examples, in_dim))\\nclassifier = BaseClassifier(in_dim, feature_dim, out_dim)\\nout = classifier(x)\\nNote that we implicitly call the forward function when using the classifier model\\nas a function in the final line. Comparing this to the initial approach of manually\\ndefining each layer’s parameters as a torch tensor and computing the output via\\nmatmul operations, this is a much more clean, modular, and reusable approach to\\ndefining neural networks.\\nIn addition to being able to define the model, instantiate it, and run data through\\nit, we must be able to train and test the model. To train (and test) the model, we\\nneed a loss metric to evaluate the model. During training, once we calculate this loss\\nmetric, we can use our knowledge from the previous section and call backward() on\\nthe computed loss. This will store the gradient in each parameter p’s grad attribute.\\nSince we have defined a classifier model, we can use the cross-entropy loss metric\\nfrom PyTorch nn:\\nloss = nn.CrossEntropyLoss()\\ntarget = torch.tensor([0,3,2,8,2,9,3,7,1,6])\\ncomputed_loss = loss(out, target)\\ncomputed_loss.backward()\\nIn the preceding code, target is a tensor of shape (no_examples), and each index\\nrepresents the ground truth class of the input corresponding with that index. Now\\nthat we’ve computed the gradient of the loss of the minibatch of examples with\\nrespect to all of the parameters in the classifier, we can perform the gradient descent\\nstep. When defining a neural network as a subclass of nn.Module, we can access all\\nof its parameters via the parameters() function—another convenience provided by\\nthe PyTorch API. To view the shape of each parameter in the neural network, you can\\nrun the code:\\nfor p in classifier.parameters():\\n  print(p.shape)\\n# out:\\n# torch.Size([256, 784])\\n# torch.Size([256])\\n86 \\n| \\nChapter 5: Implementing Neural Networks in PyTorch'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 102}, page_content='# torch.Size([10, 256])\\n# torch.Size([10])\\nAs we can see, the first layer has 256 × 784 weights and a bias vector of length 256.\\nThe last layer has 10 × 256 weights and a bias vector of length 10.\\nDuring gradient descent, we need to adjust the parameters based on their gradients.\\nWe could do this manually, but PyTorch has abstracted away this functionality into\\nthe torch.optim module. This module provides functionality for determining the\\noptimizer, which may be more complex than classic gradient descent, and updating\\nthe parameters of the model. You can define the optimizer as follows:\\nfrom torch import optim\\nlr = 1e-3\\noptimizer = optim.SGD(classifier.parameters(), lr=lr) \\nThis code creates an optimizer that will update the parameters of the classifier via\\nSGD at the end of each minibatch. To actually perform this update, you can use the\\nfollowing code:\\noptimizer.step() # Updates parameters via SGD\\noptimizer.zero_grad() # Zeroes out gradients between minibatches\\nIn the simple case of a feed-forward network as defined in BaseClassifier, the\\ntesting mode of such a network is the same as the training mode—we can just\\ncall classifier(test_x) on any minibatch in the test set to evaluate the model.\\nHowever, as we’ll discuss later, this is not true for all neural architectures.\\nThis code works for a single minibatch—performing training over the entire dataset\\nwould require manually shuffling the dataset at each epoch and splitting the dataset\\ninto minibatches that can be iterated through. Thankfully, PyTorch has also abstrac‐\\nted this process out into what are called PyTorch datasets and dataloaders. In the next\\nsection, we will cover these modules in detail.\\nPyTorch Datasets and Dataloaders\\nThe PyTorch Dataset is a base class that can be used to access your specific data. In\\npractice, you would subclass the Dataset class by overriding two important methods:\\n__len__() and __getitem__(). The first method, as you can probably tell from its\\nname, refers to the length of the dataset—i.e., the number of examples that the model\\nwill be trained or tested on. If we think of the dataset as a list of examples, the second\\nmethod takes as input an index and returns the example at that index. Each example\\nconsists of both the data point (e.g., image) and label (e.g., value from 0 to 9 in the\\ncase of MNIST). Here is some example code for a dataset:\\nimport os\\nfrom PIL import Image\\nPyTorch Datasets and Dataloaders \\n| \\n87'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 103}, page_content='from torchvision import transforms\\nclass ImageDataset(Dataset):\\n  def __init__(self, img_dir, label_file):\\n    super(ImageDataset, self).__init__()\\n    self.img_dir = img_dir\\n    self.labels = torch.tensor(np.load(label_file, allow_pickle=True))\\n    self.transforms = transforms.ToTensor()\\n  \\n  def __getitem__(self, idx):\\n    img_pth = os.path.join(self.img_dir, \"img_{}.jpg\".format(idx))\\n    img = Image.open(img_pth)\\n    img = self.transforms(img).flatten()\\n    label = self.labels[idx]\\n    return {\"data\":img, \"label\":label}\\n  \\n  def __len__(self):\\n    return len(self.labels)\\nIn this example, we assume that the directory containing our dataset consists of\\nimages that follow the naming convention img-idx.png, where idx refers to the index\\nof the image. Additionally, we assume that our ground-truth labels are stored in a\\nsaved NumPy array, which can be loaded and indexed using idx to find each image’s\\ncorresponding label.\\nThe DataLoader class in PyTorch takes as input a dataset instantiation, and abstracts\\naway all of the heavy lifting required to load in the dataset by the minibatch and\\nshuffle the dataset between epochs. Although we won’t go behind the scenes in too\\nmuch depth, the DataLoader class does make use of Python’s multiprocessing built-in\\nmodule to efficiently load minibatches in parallel. Here is some example code that\\nputs everything together:\\ntrain_dataset = ImageDataset(img_dir=\\'./data/train/\\',\\n                             label_file=\\'./data/train/labels.npy\\')\\ntrain_loader = DataLoader(train_dataset,\\n                          batch_size=4,\\n                          shuffle=True)\\nTo iterate through these dataloaders, use the following code as a template:\\nfor minibatch in train_loader:\\n  data, labels = minibatch[\\'data\\'], minibatch[\\'label\\']\\n  print(data)\\n  print(labels)\\nThe data returned is a tensor of shape (64,784) and the labels returned are of shape\\n(64,). As you can tell, the dataloader also does the work of stacking all of the examples\\ninto a single tensor that can simply be run through the network:\\n88 \\n| \\nChapter 5: Implementing Neural Networks in PyTorch'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 104}, page_content='for minibatch in train_loader:\\n  data, labels = minibatch[\\'data\\'], minibatch[\\'label\\']\\n  out = classifier(data) # to be completed in the next section!\\nwhere out is of shape (64,10) in the case of MNIST. In the next section, we will put\\ntogether all of our learnings to build a neural architecture that can be trained and\\ntested on the MNIST dataset, provide code samples for training and testing the model\\nby building off of work in this section, and show example training and testing loss\\ncurves.\\nBuilding the MNIST Classifier in PyTorch\\nIt’s time to build an MNIST classifier in PyTorch. For the most part, we can reuse a lot\\nof the code presented and explained earlier:\\nimport matplotlib.pyplot as plt\\nimport torch\\nfrom torch import optim\\nimport torch.nn as nn\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom torchvision.datasets import MNIST\\nfrom torchvision.transforms import ToTensor\\nclass BaseClassifier(nn.Module):\\n  def __init__(self, in_dim, feature_dim, out_dim):\\n    super(BaseClassifier, self).__init__()\\n    self.classifier = nn.Sequential(\\n        nn.Linear(in_dim, feature_dim, bias=True),\\n        nn.ReLU(),\\n        nn.Linear(feature_dim, out_dim, bias=True)\\n    )\\n    \\n  def forward(self, x):\\n    return self.classifier(x)\\n# Load in MNIST dataset from PyTorch\\ntrain_dataset = MNIST(\".\", train=True,\\n                      download=True, transform=ToTensor())\\ntest_dataset = MNIST(\".\", train=False,\\n                     download=True, transform=ToTensor())\\ntrain_loader = DataLoader(train_dataset,\\n                          batch_size=64, shuffle=True)\\ntest_loader = DataLoader(test_dataset,\\n                         batch_size=64, shuffle=False)\\nNote that, by default, the minibatch tensors and model parameters are on CPU, so\\nthere was no need to call the to function on each of these to change the device.\\nAlso, the MNIST dataset provided by PyTorch unfortunately does not come with a\\nvalidation set, so we’ll do our best to use insights solely from the training loss curve to\\ninform our final hyperparameter decision for the test set:\\nBuilding the MNIST Classifier in PyTorch \\n| \\n89'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 105}, page_content='# Instantiate model, optimizer, and hyperparameter(s)\\nin_dim, feature_dim, out_dim = 784, 256, 10\\nlr=1e-3\\nloss_fn = nn.CrossEntropyLoss()\\nepochs=40\\nclassifier = BaseClassifier(in_dim, feature_dim, out_dim)\\noptimizer = optim.SGD(classifier.parameters(), lr=lr)\\ndef train(classifier=classifier,\\n          optimizer=optimizer,\\n          epochs=epochs,\\n          loss_fn=loss_fn):\\n  classifier.train()\\n  loss_lt = []\\n  for epoch in range(epochs):\\n    running_loss = 0.0\\n    for minibatch in train_loader:\\n      data, target = minibatch\\n      data = data.flatten(start_dim=1)\\n      out = classifier(data)\\n      computed_loss = loss_fn(out, target)\\n      computed_loss.backward()\\n      optimizer.step()\\n      optimizer.zero_grad()\\n      # Keep track of sum of loss of each minibatch\\n      running_loss += computed_loss.item()\\n    loss_lt.append(running_loss/len(train_loader))\\n    print(\"Epoch: {} train loss: {}\".format(epoch+1, \\n          running_loss/len(train_loader)))\\n  plt.plot([i for i in range(1,epochs+1)], loss_lt)\\n  plt.xlabel(\"Epoch\")\\n  plt.ylabel(\"Training Loss\")\\n  plt.title(\\n      \"MNIST Training Loss: optimizer {}, lr {}\".format(\"SGD\", lr))\\n  plt.show()\\n  # Save state to file as checkpoint\\n  torch.save(classifier.state_dict(), \\'mnist.pt\\')\\ndef test(classifier=classifier,\\n          loss_fn = loss_fn):\\n  classifier.eval()\\n  accuracy = 0.0\\n  computed_loss = 0.0\\n  with torch.no_grad():\\n      for data, target in test_loader:\\n          data = data.flatten(start_dim=1)\\n          out = classifier(data)\\n          _, preds = out.max(dim=1)\\n90 \\n| \\nChapter 5: Implementing Neural Networks in PyTorch'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 106}, page_content='# Get loss and accuracy\\n          computed_loss += loss_fn(out, target)\\n          accuracy += torch.sum(preds==target)\\n          \\n      print(\"Test loss: {}, test accuracy: {}\".format(\\n          computed_loss.item()/(len(test_loader)*64), \\n          accuracy*100.0/(len(test_loader)*64)))\\nAdditionally, note that we call classifier.train() and classifier.eval() at the\\nbeginning of the training and test functions, respectively. The calls to these functions\\ncommunicate to the PyTorch backend whether the model is in training mode or\\ninference mode. You might be wondering why we need to call classifier.train()\\nand classifier.eval() if there is no difference between the behavior of the neural\\nnetwork at train and test time. Although this is true in our first-pass example, the\\ntraining and testing modes for other neural architectures are not necessarily the\\nsame. For example, if dropout layers are added to the model architecture, the dropout\\nlayers need to be ignored during the testing phase. We add in the calls to train() and\\neval() here since it is generally considered good practice to do so.\\nAs a first step, we need to set some starting hyperparameters for model training. We\\nstart with a slightly conservative learning rate in 1e-4 and inspect the training loss\\ncurve and testing accuracy after 40 epochs, or iterations through the entire dataset.\\nFigure 5-2 shows a graph of the training loss curve through the epochs.\\nFigure 5-2. We see signs of underfitting as the model performance on the training set is\\nfailing to level out, meaning we have not yet settled into a local optimum\\nBuilding the MNIST Classifier in PyTorch \\n| \\n91'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 107}, page_content='We can see that this loss curve is not particularly close to leveling out near the end of\\ntraining, which we’d hope to start seeing for a model training at a sufficient learning\\nrate. And although we don’t have a validation set to confirm our suspicions, we have\\nstrong reason to suspect that a higher learning rate would help. After setting the\\nlearning rate to a slightly more aggressive 1e-3, we observe a training loss curve that\\nis much more in line with what we’d hope to see (Figure 5-3).\\nFigure 5-3. This leveling out of the loss curve is more like what we’d expect to see with an\\nappropriate learning rate for the problem\\nThe loss curve starts to level out only near the end of training. This trend indicates\\nthat the model is likely in the sweet spot between underfitting to the training data,\\nlike our previous attempt, and overfitting to the training data. Evaluating the trained\\nmodel at 40 epochs on the test set achieves an accuracy of 91%! Although this is\\nnowhere close to the top performers on MNIST today, which primarily use convolu‐\\ntional neural classifiers, it is a great start. We recommend you try some extensions to\\nthe code, such as increasing the number of hidden layers and substituting in a more\\nsophisticated optimizer.\\n92 \\n| \\nChapter 5: Implementing Neural Networks in PyTorch'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 108}, page_content='Summary\\nIn this chapter, we covered the basics of PyTorch and its functionality. Specifically,\\nwe learned the concept of tensors in PyTorch, and how these tensors store numerical\\ninformation. Additionally, we learned how to manipulate tensors via tensor opera‐\\ntions, access the data within a tensor, and set a few important attributes. We also\\ndiscussed gradients in PyTorch and how they can be stored within a tensor. We built\\nour first neural network via standard nn functionality in the the PyTorch nn module\\nsection. Comparing the nn-based approach with an approach that used PyTorch ten‐\\nsors solely out of the box showed much of the effective abstraction that the nn module\\nprovides, lending to its ease of use. And finally, we put all of our learnings together in\\nthe final section, where we trained and tested an MNIST digits feed-forward neural\\nclassifier to 91% accuracy on the PyTorch-provided test set. Although we covered\\nmuch of the fundamentals and have equipped you with the knowledge you need to\\nget your hands dirty, we have only scratched the surface of all that the PyTorch API\\nhas to offer—we encourage you to explore further and improve upon the models we\\nbuilt in this section. We recommend that you visit the PyTorch documentation to\\nlearn more and build your own neural nets, including trying other architectures, on\\na variety of online datasets, such as the CIFAR-10 image recognition datasets. In the\\nnext section, we will cover neural network implementation, one of the other most\\npopular deep learning frameworks in use today.\\nSummary \\n| \\n93'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 109}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 110}, page_content='1 Bengio, Yoshua, et al. “Greedy Layer-Wise Training of Deep Networks.” Advances in Neural Information\\nProcessing Systems 19 (2007): 153.\\nCHAPTER 6\\nBeyond Gradient Descent\\nThe Challenges with Gradient Descent\\nThe fundamental ideas behind neural networks have existed for decades, but it wasn’t\\nuntil recently that neural network-based learning models have become mainstream.\\nOur fascination with neural networks has everything to do with their expressiveness,\\na quality we’ve unlocked by creating networks with many layers. As we have discussed\\nin previous chapters, deep neural networks are able to crack problems that were\\npreviously deemed intractable. Training deep neural networks end to end, however, is\\nfraught with difficult challenges that took many technological innovations to unravel,\\nincluding massive labeled datasets (ImageNet, CIFAR-10, etc.), better hardware in the\\nform of GPU acceleration, and several algorithmic discoveries.\\nFor several years, researchers resorted to layer-wise greedy pretraining to grapple\\nwith the complex error surfaces presented by deep learning models.1 These time-\\nintensive strategies would try to find more accurate initializations for the model’s\\nparameters one layer at a time before using minibatch gradient descent to converge\\nto the optimal parameter settings. More recently, however, breakthroughs in optimi‐\\nzation methods have enabled us to train models directly in an end-to-end fashion.\\nIn this chapter, we will discuss several of these breakthroughs. The next couple of\\nsections will focus primarily on local minima and whether they pose hurdles for\\nsuccessfully training deep models. Then we will further explore the nonconvex error\\nsurfaces induced by deep models, why vanilla minibatch gradient descent falls short,\\nand how modern nonconvex optimizers overcome these pitfalls.\\n95'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 111}, page_content='Local Minima in the Error Surfaces of Deep Networks\\nThe primary challenge in optimizing deep learning models is that we are forced to\\nuse minimal local information to infer the global structure of the error surface. This\\nis difficult because there is usually very little correspondence between local and global\\nstructure. Take the following analogy as an example.\\nLet’s assume you’re an insect on the continental United States. You’re dropped ran‐\\ndomly on the map, and your goal is to find the lowest point on this surface. How do\\nyou do it? If all you can observe is your immediate surroundings, this seems like an\\nintractable problem. If the surface of the US were bowl-shaped (or mathematically\\nspeaking, convex) and we were smart about our learning rate, we could use the\\ngradient descent algorithm to eventually find the bottom of the bowl. But the surface\\nof the US is extremely complex, that is to say, is a nonconvex surface, which means\\nthat even if we find a valley (a local minimum), we have no idea if it’s the lowest valley\\non the map (the global minimum). In Chapter 4, we talked about how a minibatch\\nversion of gradient descent can help navigate a troublesome error surface when there\\nare spurious regions of magnitude zero gradients. But as we can see in Figure 6-1,\\neven a stochastic error surface won’t save us from a deep local minimum.\\nFigure 6-1. Minibatch gradient descent may aid in escaping shallow local minima, but\\noften fails when dealing with deep local minima, as shown\\nNow comes the critical question. Theoretically, local minima pose a significant issue.\\nBut in practice, how common are local minima in the error surfaces of deep net‐\\nworks? And in which scenarios are they actually problematic for training? In the\\nfollowing two sections, we’ll pick apart common misconceptions about local minima.\\n96 \\n| \\nChapter 6: Beyond Gradient Descent'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 112}, page_content='Model Identifiability\\nThe first source of local minima is tied to a concept commonly referred to as model\\nidentifiability. One observation about deep neural networks is that their error surfaces\\nare guaranteed to have a large—and in some cases, an infinite—number of local\\nminima. There are two major reasons this observation is true.\\nThe first is that within a layer of a fully connected feed-forward neural network,\\nany rearrangement of neurons will still give you the same final output at the end of\\nthe network. We illustrate this using a simple three-neuron layer in Figure 6-2. As a\\nresult, within a layer with n neurons, there are n! ways to rearrange parameters. And\\nfor a deep network with l layers, each with n neurons, we have a total of n!l equiva‐\\nlent configurations.\\nFigure 6-2. Rearranging neurons in a layer of a neural network results in equivalent\\nconfigurations due to symmetry\\nIn addition to the symmetries of neuron rearrangements, nonidentifiability is present\\nin other forms in certain kinds of neural networks. For example, there is an infinite\\nnumber of equivalent configurations that for an individual ReLU neuron result in\\nequivalent networks. Because an ReLU uses a piecewise linear function, we are free to\\nmultiply all of the incoming weights by any nonzero constant k while scaling all of\\nModel Identifiability \\n| \\n97'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 113}, page_content='2 Goodfellow, Ian J., Oriol Vinyals, and Andrew M. Saxe. “Qualitatively characterizing neural network optimi‐\\nzation problems.” arXiv preprint arXiv:1412.6544 (2014).\\nthe outgoing weights by 1\\nk without changing the behavior of the network. We leave\\nthe justification for this statement as an exercise for you.\\nUltimately, however, local minima that arise because of the nonidentifiability of deep\\nneural networks are not inherently problematic. This is because all nonidentifiable\\nconfigurations behave in an indistinguishable fashion no matter what input values\\nthey are fed. This means they will achieve the same error on the training, validation,\\nand testing datasets. In other words, all of these models will have learned equally\\nfrom the training data and will have identical behavior during generalization to\\nunseen examples.\\nInstead, local minima are only problematic when they are spurious. A spurious\\nlocal minimum corresponds to a configuration of weights in a neural network that\\nincurs a higher error than the configuration at the global minimum. If these kinds\\nof local minima are common, we quickly run into significant problems while using\\ngradient-based optimization methods because we can take only local structure into\\naccount.\\nHow Pesky Are Spurious Local Minima in Deep Networks?\\nFor many years, deep learning practitioners blamed all of their troubles in training\\ndeep networks on spurious local minima, albeit with little evidence. Today, it remains\\nan open question whether spurious local minima with a high error rate relative to\\nthe global minimum are common in practical deep networks. However, many recent\\nstudies seem to indicate that most local minima have error rates and generalization\\ncharacteristics that are very similar to global minima.\\nOne way we might try to naively tackle this problem is by plotting the value of the\\nerror function over time as we train a deep neural network. This strategy, however,\\ndoesn’t give us enough information about the error surface because it is difficult to\\ntell whether the error surface is “bumpy,” or whether we merely have a difficult time\\nfiguring out which direction we should be moving in.\\nTo more effectively analyze this problem, Goodfellow et al. (a team of researchers\\ncollaborating between Google and Stanford) published a paper in 2014 that attempted\\nto separate these two potential confounding factors.2 Instead of analyzing the error\\nfunction over time, they cleverly investigated what happens on the error surface\\nbetween a randomly initialized parameter vector and a successful final solution by\\nusing linear interpolation. So, given a randomly initialized parameter vector θi and\\n98 \\n| \\nChapter 6: Beyond Gradient Descent'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 114}, page_content='stochastic gradient descent (SGD) solution θf, we aim to compute the error function\\nat every point along the linear interpolation θα = α · θf + 1 −α · θi.\\nThey wanted to investigate whether local minima would hinder our gradient-based\\nsearch method even if we knew which direction to move in. They showed that for\\na wide variety of practical networks with different types of neurons, the direct path\\nbetween a randomly initialized point in the parameter space and a stochastic gradient\\ndescent solution isn’t plagued with troublesome local minima.\\nWe can even demonstrate this ourselves using the feed-forward ReLU network we\\nbuilt in Chapter 5. Using a checkpoint file that we saved while training our original\\nfeed-forward network, we can reinstantiate the model using load_state_dict and\\ntorch.load:\\n# Load checkpoint from SGD training\\nIN_DIM, FEATURE_DIM, OUT_DIM = 784, 256, 10\\nmodel = Net(IN_DIM, FEATURE_DIM, OUT_DIM)\\nmodel.load_state_dict(torch.load(\\'mnist.pt\\'))\\nIn PyTorch, we cannot access a model’s parameters directly since the model.parame\\nters() method returns a generator that provides only a copy of the parameters.\\nTo modify a model’s parameters, we use torch.load to read the state dictionary\\ncontaining the parameter values from the file, and then use load_state_dict to set\\nthe model’s parameters with these values.\\nInstead of using torch.load to load the state dictionary from a file, we can also\\naccess the state dictionary from a model itself using the state_dict method:\\nimport copy\\n# Access parameters with state_dict\\nopt_state_dict = copy.deepcopy(model.state_dict())\\nfor param_tensor in opt_state_dict:\\n    print(param_tensor, \"\\\\t\",\\n          opt_state_dict[param_tensor].size())\\n# outputs:\\n# classifier.1.weight   torch.Size([256, 784])\\n# classifier.1.bias     torch.Size([256])\\n# classifier.3.weight   torch.Size([256, 256])\\n# classifier.3.bias     torch.Size([256])\\n# classifier.5.weight   torch.Size([10, 256])\\n# classifier.5.bias     torch.Size([10])\\nNote that we need to use the copy.deepcopy method to copy a dictionary with\\nits values. Just setting opt_state_dict = model.state_dict() would result in a\\nHow Pesky Are Spurious Local Minima in Deep Networks? \\n| \\n99'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 115}, page_content='shallow copy, and opt_state_dict would be changed when we load our model with\\ninterpolated parameters later. \\nNext, we instantiate a new model with randomly initialized parameters and save\\nthose parameters as rand_state_dict:\\n# Create randomly initialized network\\nmodel_rand = Net(IN_DIM, FEATURE_DIM, OUT_DIM)\\nrand_state_dict = copy.deepcopy(model_rand.state_dict())\\nWith these two networks appropriately initialized, we can now construct the linear\\ninterpolation using the mixing parameters alpha and beta:\\n# Create a new state_dict for interpolated parameters\\ntest_model = Net(IN_DIM, FEATURE_DIM, OUT_DIM)\\ntest_state_dict = copy.deepcopy(test_model.state_dict())\\nalpha = 0.2\\nbeta = 1.0 - alpha\\nfor p in opt_state_dict:\\n    test_state_dict[p] = (opt_state_dict[p] * beta +\\n                          rand_state_dict[p] * alpha)\\nNext, we will compute the average loss over the entire test dataset using the model\\nwith the interpolated parameters.  For convenience, let’s create a function for\\ninference:\\ndef inference(testloader, model, loss_fn):\\n  running_loss = 0.0\\n  with torch.no_grad():\\n    for inputs, labels in testloader:\\n      outputs = model(inputs)\\n      loss = loss_fn(outputs, labels)\\n      running_loss += loss\\n  running_loss /= len(testloader)\\n  return running_loss\\nFinally,  we can vary the value of alpha to understand how the error surface changes\\nas we traverse the line between the randomly initialized point and the final SGD\\nsolution:\\nresults = []\\nfor alpha in torch.arange(-2, 2, 0.05):\\n  beta = 1.0 - alpha\\n  \\n  # Compute interpolated parameters\\n  for p in opt_state_dict:\\n    test_state_dict[p] = (opt_state_dict[p] * beta +\\n                          rand_state_dict[p] * alpha)\\n  \\n  # Load interpolated parameters into test model\\n  model.load_state_dict(test_state_dict)\\n  \\n100 \\n| \\nChapter 6: Beyond Gradient Descent'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 116}, page_content='# Compute loss given interpolated parameters\\n  loss = inference(trainloader, model, loss_fn)\\n  results.append(loss.item())\\nThis creates Figure 6-3, which we can inspect ourselves. In fact, if we run this\\nexperiment over and over again, we find that there are no truly troublesome local\\nminima that would get us stuck. It seems that the true struggle of gradient descent\\nisn’t the existence of troublesome local minima, but instead is that we have a tough\\ntime finding the appropriate direction to move in. We’ll return to this thought a little\\nlater.\\nFigure 6-3. The cost function of a three-layer feed-forward network as we linearly\\ninterpolate on the line connecting a randomly initialized parameter vector and an SGD\\nsolution\\nFlat Regions in the Error Surface\\nAlthough  it seems that our analysis is devoid of troublesome local minimum, we\\ndo notice a peculiar flat region where the gradient approaches zero when we get to\\napproximately alpha=1. This point is not a local minima, so it is unlikely to get us\\ncompletely stuck, but it seems like the zero gradient might slow down learning if we\\nare unlucky enough to encounter it.\\nFlat Regions in the Error Surface \\n| \\n101'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 117}, page_content='More generally, given an arbitrary function, a point at which the gradient is the\\nzero vector is called a critical point. Critical points come in various flavors. We’ve\\nalready talked about local minima. It’s also not hard to imagine their counterparts,\\nthe local maxima, which don’t really pose much of an issue for SGD. But then there\\nare these strange critical points that lie somewhere in between. These “flat” regions\\nthat are potentially pesky but not necessarily deadly are called saddle points. It turns\\nout that as our function has more and more dimensions (i.e., we have more and\\nmore parameters in our model), saddle points are exponentially more likely than\\nlocal minima. Let’s try to intuit why.\\nFor a 1D cost function, a critical point can take one of three forms, as shown\\nin Figure 6-4.  Loosely, let’s assume each of these three configurations is equally\\nlikely. This means that given a random critical point in a random 1D function, it has\\none-third probability of being a local minimum. This means that if we have a total\\nof k critical points, we can expect to have a total of k\\n3 local minima.\\nFigure 6-4. Analyzing a critical point along a single dimension\\nWe can also extend this to higher dimensional functions. Consider a cost function\\noperating in a d-dimensional space. Let’s take an arbitrary critical point. It turns out\\nthat figuring out if this point is a local minimum, local maximum, or a saddle point\\nis a little bit trickier than in the one-dimensional case. Consider the error surface\\nin Figure 6-5. Depending on how you slice the surface (from A to B or from C to D),\\nthe critical point looks like either a minimum or a maximum. In reality, it’s neither.\\nIt’s a more complex type of saddle point.\\n102 \\n| \\nChapter 6: Beyond Gradient Descent'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 118}, page_content='3 Dauphin, Yann N., et al. “Identifying and attacking the saddle point problem in high-dimensional non-convex\\noptimization.” Advances in Neural Information Processing Systems. 2014.\\nFigure 6-5. A saddle point over a 2D error surface\\nIn general, in a d-dimensional parameter space, we can slice through a critical point\\non d different axes. A critical point can be a local minimum only if it appears as a\\nlocal minimum in every single one of the d 1D subspaces. Using the fact that a critical\\npoint can come in one of three different flavors in a one-dimensional subspace, we\\nrealize that the probability that a random critical point is in a random function is 1\\n3d.\\nThis means that a random function with k critical points has an expected number\\nof k\\n3d local minima. In other words, as the dimensionality of our parameter space\\nincreases, local minima become exponentially more rare. A more rigorous treatment\\nof this topic is outside the scope of this book, but was explored more extensively by\\nDauphin et al. in 2014.3\\nSo what does this mean for optimizing deep learning models? For stochastic gradient\\ndescent, it’s still unclear. It seems like these flat segments of the error surface are pesky\\nbut ultimately don’t prevent stochastic gradient descent from converging to a good\\nanswer. However, it does pose serious problems for methods that attempt to directly\\nsolve for a point where the gradient is zero. This has been a major hindrance to the\\nusefulness of certain second-order optimization methods for deep learning models,\\nwhich we will discuss later.\\nFlat Regions in the Error Surface \\n| \\n103'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 119}, page_content='When the Gradient Points in the Wrong Direction\\nUpon analyzing the error surfaces of deep networks, it seems like the most critical\\nchallenge to optimizing deep networks is finding the correct trajectory to move in.\\nIt’s no surprise, however, that this is a major challenge when we look at what happens\\nto the error surface around a local minimum. As an example, we consider an error\\nsurface defined over a 2D parameter space, as shown in Figure 6-6.\\nFigure 6-6. Local information encoded by the gradient usually does not corroborate the\\nglobal structure of the error surface\\nRevisiting the contour diagrams we explored in Chapter 4, notice that the gradient\\nisn’t usually a very good indicator of the good trajectory. Specifically, only when the\\ncontours are perfectly circular does the gradient always point in the direction of the\\nlocal minimum. However, if the contours are extremely elliptical (as is usually the\\ncase for the error surfaces of deep networks), the gradient can be as inaccurate as 90\\ndegrees away from the correct direction.\\nWe extend this analysis to an arbitrary number of dimensions using some mathemat‐\\nical formalism. For every weight wi in the parameter space, the gradient computes\\nthe value of ∂E\\n∂wi, or how the value of the error changes as we change the value of wi.\\nTaken together over all weights in the parameter space, the gradient gives us the\\ndirection of steepest descent. The general problem with taking a significant step in\\nthis direction, however, is that the gradient could be changing under our feet as we\\nmove! We demonstrate this simple fact in Figure 6-7. Going back to the 2D example,\\n104 \\n| \\nChapter 6: Beyond Gradient Descent'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 120}, page_content='if our contours are perfectly circular and we take a big step in the direction of the\\nsteepest descent, the gradient doesn’t change direction as we move. However, this is\\nnot the case for highly elliptical contours.\\nFigure 6-7. The direction of the gradient changes as we move along the direction of steep‐\\nest descent, as determined from a starting point; the gradient vectors are normalized to\\nidentical length to emphasize the change in direction of the gradient vector\\nMore generally, we can quantify how the gradient changes under our feet as we\\nmove in a certain direction by computing second derivatives. Specifically, we want to\\nmeasure \\n∂∂E /∂wj\\n∂wi\\n, which tells us how the gradient component for wj changes as we\\nchange the value of wi. We can compile this information into a special matrix known\\nas the Hessian matrix (H). And when describing an error surface where the gradient\\nchanges underneath our feet as we move in the direction of steepest descent, this\\nmatrix is said to be ill-conditioned.\\nHessian Limits Optimization\\nCertain properties of the Hessian matrix (specifically that it is real and symmetric)\\nallow us to efficiently determine the second derivative (which approximates the\\ncurvature of a surface) as we move in a specific direction. Specifically, if we have a\\nunit vector d, the second derivative in that direction is given by dHd. We can now use\\na second-order approximation via Taylor series to understand what happens to the\\nerror function as we step from the current parameter vector x(i) to a new parameter\\nvector x along gradient vector g evaluated at x(i):\\nE x ≈E x i\\n+ x −x i ⊤g + 1\\n2 x −x i ⊤H x −x i\\nWhen the Gradient Points in the Wrong Direction \\n| \\n105'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 121}, page_content='If we go further to state that we will be moving ϵ units in the opposite direction of the\\ngradient, we can simplify our expression even more:\\nE x i −ϵg ≈E x i\\n−ϵg⊤g + 1\\n2ϵ2g⊤Hg\\nThis expression consists of three terms: (1) the value of the error function at the\\noriginal parameter vector, (2) the improvement in error afforded by the magnitude of\\nthe gradient, and (3) a correction term that incorporates the curvature of the surface\\nas represented by the Hessian matrix.\\nIn general, we should be able to use this information to design better optimization\\nalgorithms. For instance, we can even naively take the second-order approximation\\nof the error function to determine the learning rate at each step that maximizes\\nthe reduction in the error function. It turns out, however, that computing the Hes‐\\nsian matrix exactly is a difficult task. In the next several sections, we’ll describe\\noptimization breakthroughs that tackle ill-conditioning without directly computing\\nthe Hessian matrix.\\nMomentum-Based Optimization\\nFundamentally, the problem of an ill-conditioned Hessian matrix manifests itself in\\nthe form of gradients that fluctuate wildly. As a result, one popular mechanism for\\ndealing with ill-conditioning bypasses the computation of the Hessian, and instead,\\nfocuses on how to cancel out these fluctuations over the duration of training.\\nOne way to think about how we might tackle this problem is by investigating how\\na ball rolls down a hilly surface. Driven by gravity, the ball eventually settles into\\na minimum on the surface, but for some reason, it doesn’t suffer from the wild\\nfluctuations and divergences that happen during gradient descent. Why is this the\\ncase? Unlike in stochastic gradient descent (which uses only the gradient), there\\nare two major components that determine how a ball rolls down an error surface.\\nThe first, which we already model in SGD as the gradient, is what we commonly\\nrefer to as acceleration. But acceleration does not single-handedly determine the\\nball’s movements. Instead, its motion is more directly determined by its velocity.\\nAcceleration indirectly changes the ball’s position only by modifying its velocity.\\nVelocity-driven motion is desirable because it counteracts the effects of a wildly\\nfluctuating gradient by smoothing the ball’s trajectory over its history. Velocity serves\\nas a form of memory, and this allows us to more effectively accumulate movement\\nin the direction of the minimum while canceling out oscillating accelerations in\\northogonal directions. Our goal, then, is to somehow generate an analog for velocity\\nin our optimization algorithm. We can do this by keeping track of an exponentially\\nweighted decay of past gradients. The premise is simple: every update is computed by\\n106 \\n| \\nChapter 6: Beyond Gradient Descent'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 122}, page_content='4 Polyak, Boris T. “Some methods of speeding up the convergence of iteration methods.” USSR Computational\\nMathematics and Mathematical Physics 4.5 (1964): 1-17.\\ncombining the update in the last iteration with the current gradient. Concretely, we\\ncompute the change in the parameter vector as follows:\\n   vi = mvi −1 −ϵgi\\n   θi = θi −1 + vi\\nWe use the momentum hyperparameter m to determine what fraction of the previous\\nvelocity to retain in the new update, and add this “memory” of past gradients to our\\ncurrent gradient. This approach is commonly referred to as momentum.4 Because the\\nmomentum term increases the step size we take, using momentum may require a\\nreduced learning rate compared to vanilla stochastic gradient descent.\\nTo better visualize how momentum works, we’ll explore a toy example. Specifically,\\nwe’ll investigate how momentum affects updates during a random walk. A random\\nwalk is a succession of randomly chosen steps. In our example, we’ll imagine a\\nparticle on a line that, at every time interval, randomly picks a step size between –10\\nand 10 and takes a moves in that direction. This is simply expressed as:\\nrand_walk = [torch.randint(-10, 10, (1,1)) for x in range(100)]\\nWe’ll then simulate what happens when we use a slight modification of momentum\\n(i.e., the standard exponentially weighted moving average algorithm) to smooth our\\nchoice of step at every time interval. Again, we can concisely express this as:\\nmomentum = 0.1\\nmomentum_rand_walk = \\\\\\n      [torch.randint(-10, 10, (1,1)) for x in range(100)]\\nfor i in range(1, len(rand_walk) - 1):\\n  prev = momentum_rand_walk[i-1]\\n  rand_choice = torch.randint(-10, 10, (1,1)).item()\\n  new_step = momentum * prev + (1 - momentum) * rand_choice\\n  momentum_rand_walk[i] = new_step\\nThe results, as we vary the momentum from 0 to 1, are quite staggering. Momentum\\nsignificantly reduces the volatility of updates. The larger the momentum, the less\\nresponsive we are to new updates (e.g., a large inaccuracy on the first estimation of\\ntrajectory propagates for a significant period of time). We summarize the results of\\nour toy experiment in Figure 6-8.\\nMomentum-Based Optimization \\n| \\n107'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 123}, page_content='Figure 6-8. Momentum smooths volatility in the step sizes during a random walk using\\nan exponentially weighted moving average\\nTo investigate how momentum actually affects the training of feed-forward neural\\nnetworks, we can retrain our trusty MNIST feed-forward network with a PyTorch\\nmomentum optimizer. In this case, we can get away with using the same learning rate\\n(0.01) with a typical momentum of 0.9:\\noptimizer = optim.SGD(model.parameters(),\\n                      lr = 0.01,\\n                      momentum = 0.9)\\noptimizer.step()\\nNotice that when we create a PyTorch optimizer, we need to pass in model.param\\neters(). The resulting speedup is staggering. We display how the cost function\\nchanges over time by comparing the visualizations in Figure 6-9. The figure demon‐\\nstrates that to achieve a cost of 0.1 without momentum (right) requires nearly 18,000\\nsteps (minibatches), whereas with momentum (left), we require just over 2,000.\\n108 \\n| \\nChapter 6: Beyond Gradient Descent'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 124}, page_content='5 Sutskever, Ilya, et al. “On the importance of initialization and momentum in deep learning.” ICML (3) 28\\n(2013): 1139-1147.\\nFigure 6-9. Comparing training a feed-forward network with (right) and without (left)\\nmomentum demonstrates a massive decrease in training time\\nRecently, more work has explored how the classical momentum technique can be\\nimproved. Sutskever et al. in 2013 proposed an alternative called Nesterov momen‐\\ntum, which computes the gradient on the error surface at θ + vi −1 during the velocity\\nupdate instead of at θ.5 This subtle difference seems to allow Nesterov momentum to\\nchange its velocity in a more responsive way. It’s been shown that this method has\\nclear benefits in batch gradient descent (convergence guarantees and the ability to use\\na higher momentum for a given learning rate as compared to classical momentum),\\nbut it’s not entirely clear whether this is true for the more stochastic minibatch\\ngradient descent used in most deep learning optimization approaches.\\nNerestov momentum is supported in PyTorch out-of-the-box by setting the nesterov\\nargument:\\noptimizer = optim.SGD(model.parameters(),\\n                      lr = 0.01,\\n                      momentum = 0.9,\\n                      nesterov = True)\\nA Brief View of Second-Order Methods\\nAs we discussed, computing the Hessian is a computationally difficult task, and\\nmomentum afforded us significant speedup without having to worry about it alto‐\\ngether. Several second-order methods, however, have been researched over the past\\nseveral years that attempt to approximate the Hessian directly. For completeness, we\\ngive a broad overview of these methods, but a detailed treatment is beyond the scope\\nof this text.\\nThe first is conjugate gradient descent, which arises out of attempting to improve on\\na naive method of steepest descent. In steepest descent, we compute the direction\\nA Brief View of Second-Order Methods \\n| \\n109'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 125}, page_content='6 Møller, Martin Fodslette. “A Scaled Conjugate Gradient Algorithm for Fast Supervised Learning.” Neural\\nNetworks 6.4 (1993): 525-533.\\n7 Broyden, C. G. “A New Method of Solving Nonlinear Simultaneous Equations.” The Computer Journal 12.1\\n(1969): 94-99.\\n8 Bonnans, Joseph-Frédéric, et al. Numerical Optimization: Theoretical and Practical Aspects. Springer Science &\\nBusiness Media, 2006.\\nof the gradient and then line search to find the minimum along that direction. We\\njump to the minimum and then recompute the gradient to determine the direction\\nof the next line search. It turns out that this method ends up zigzagging a significant\\namount, as shown in Figure 6-10, because each time we move in the direction of\\nsteepest descent, we undo a little bit of progress in another direction. A remedy\\nto this problem is moving in a conjugate direction relative to the previous choice\\ninstead of the direction of steepest descent. The conjugate direction is chosen by\\nusing an indirect approximation of the Hessian to linearly combine the gradient and\\nour previous direction. With a slight modification, this method generalizes to the\\nnonconvex error surfaces we find in deep networks.6\\nFigure 6-10. The method of steepest descent often zigzags; conjugate descent attempts to\\nremedy this issue\\nAn alternative optimization algorithm known as the Broyden–Fletcher–Goldfarb–\\nShanno (BFGS) algorithm attempts to compute the inverse of the Hessian matrix\\niteratively and use the inverse Hessian to more effectively optimize the parameter\\nvector.7 In its original form, BFGS has a significant memory footprint, but recent\\nwork has produced a more memory-efficient version known as L-BFGS.8\\nIn general, while these methods hold some promise, second-order methods are still\\nan area of active research and are unpopular among practitioners. PyTorch does,\\nhowever, support L-BFGS as well as other second-order methods, such as Averaged\\nStochastic Gradient Descent, for your own experimentation.\\n110 \\n| \\nChapter 6: Beyond Gradient Descent'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 126}, page_content='9 Duchi, John, Elad Hazan, and Yoram Singer. “Adaptive Subgradient Methods for Online Learning and\\nStochastic Optimization.” Journal of Machine Learning Research 12.Jul (2011): 2121-2159.\\nLearning Rate Adaptation\\nAs we have discussed previously, another major challenge for training deep networks\\nis appropriately selecting the learning rate. Choosing the correct learning rate has\\nlong been one of the most troublesome aspects of training deep networks because\\nit has a major impact on a network’s performance. A learning rate that is too small\\ndoesn’t learn quickly enough, but a learning rate that is too large may have difficulty\\nconverging as we approach a local minimum or region that is ill-conditioned.\\nOne of the major breakthroughs in modern deep network optimization was the\\nadvent of learning rate adaption. The basic concept behind learning rate adaptation\\nis that the optimal learning rate is appropriately modified over the span of learning\\nto achieve good convergence properties. Over the next several sections, we’ll discuss\\nAdaGrad, RMSProp, and Adam, three of the most popular adaptive learning rate\\nalgorithms.\\nAdaGrad—Accumulating Historical Gradients\\nThe first algorithm we’ll discuss is AdaGrad, which attempts to adapt the global\\nlearning rate over time using an accumulation of the historical gradients, first pro‐\\nposed by Duchi et al. in 2011.9 Specifically, we keep track of a learning rate for each\\nparameter. This learning rate is inversely scaled with respect to the square root of the\\nsum of the squares (root mean square) of all the parameter’s historical gradients.\\nWe can express this mathematically. We initialize a gradient accumulation vec‐\\ntor r0 = 0. At every step, we accumulate the square of all the gradient parameters\\nas follows (where the ⊙ operation is element-wise tensor multiplication):\\n ri = ri −1 + gi ⊙gi\\nThen we compute the update as usual, except our global learning rate ϵ is divided by\\nthe square root of the gradient accumulation vector:\\nθi = θi −1 −\\nϵ\\nδ ⊕\\nri\\n⊙g\\nNote that we add a tiny number δ (~10−7) to the denominator to prevent division\\nby zero. Also, the division and addition operations are broadcast to the size of\\nthe gradient accumulation vector and applied element-wise. In PyTorch, a built-in\\noptimizer allows for easily utilizing AdaGrad as a learning algorithm:\\nLearning Rate Adaptation \\n| \\n111'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 127}, page_content='10 Tieleman, Tijmen, and Geoffrey Hinton. “Lecture 6.5-rmsprop: Divide the Gradient by a Running Average of\\nIts Recent Magnitude.” COURSERA: Neural Networks for Machine Learning 4.2 (2012).\\noptimizer = optim.Adagrad(model.parameters(),\\n                          lr = 0.01,\\n                          weight_decay = 0,\\n                          initial_accumulator_value = 0)\\nThe only hitch is that in PyTorch, the δ and initial gradient accumulation vector are\\nrolled together into the initial_accumulator_value argument.\\nOn a functional level, this update mechanism means that the parameters with the\\nlargest gradients experience a rapid decrease in their learning rates, while parameters\\nwith smaller gradients observe only a small decrease in their learning rates. The ulti‐\\nmate effect is that AdaGrad forces more progress in the more gently sloped directions\\non the error surface, which can help overcome ill-conditioned surfaces. This results\\nin some good theoretical properties, but in practice, training deep learning models\\nwith AdaGrad can be somewhat problematic. Empirically, AdaGrad has a tendency to\\ncause a premature drop in learning rate, and as a result doesn’t work particularly well\\nfor some deep models. In the next section, we’ll describe RMSProp, which attempts to\\nremedy this shortcoming.\\nRMSProp—Exponentially Weighted Moving Average of Gradients\\nWhile AdaGrad works well for simple convex functions, it isn’t designed to navigate\\nthe complex error surfaces of deep networks. Flat regions may force AdaGrad to\\ndecrease the learning rate before it reaches a minimum. The conclusion is that simply\\nusing a naive accumulation of gradients isn’t sufficient.\\nOur solution is to bring back a concept we introduced earlier while discussing\\nmomentum to dampen fluctuations in the gradient. Compared to naive accumula‐\\ntion, exponentially weighted moving averages also enables us to “toss out” measure‐\\nments that we made a long time ago. More specifically, our update to the gradient\\naccumulation vector is now as follows:\\n ri = ρri −1 + 1 −ρ gi ⊙gi\\nThe decay factor ρ determines how long we keep old gradients. The smaller the decay\\nfactor, the shorter the effective window. Plugging this modification into AdaGrad\\ngives rise to the RMSProp learning algorithm, first proposed by Geoffrey Hinton.10\\n112 \\n| \\nChapter 6: Beyond Gradient Descent'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 128}, page_content='11 Kingma, Diederik, and Jimmy Ba. “Adam: A Method for Stochastic Optimization.” arXiv preprint\\narXiv:1412.6980 (2014).\\nIn PyTorch, we can instantiate the RMSProp optimizer with the following code. Note\\nthat in this case, unlike in AdaGrad, we pass in δ separately as the epsilon argument\\nto the constructor:\\noptimizer = optim.RMSprop(model.parameters(),\\n                          lr = 0.01,\\n                          alpha = 0.99,\\n                          eps = 1e-8,\\n                          weight_decay = 0,\\n                          momentum = 0)\\nAs the template suggests, we can utilize RMSProp with momentum (specifically\\nNerestov momentum). Overall, RMSProp has been shown to be a highly effective\\noptimizer for deep neural networks, and is a default choice for many seasoned\\npractitioners.\\nAdam—Combining Momentum and RMSProp\\nBefore concluding our discussion of modern optimizers, we discuss one final algo‐\\nrithm—Adam.11 Spiritually, we can think about Adam as a variant combination of\\nRMSProp and momentum.\\nThe basic idea is as follows. We want to keep track of an exponentially weighted mov‐\\ning average of the gradient (essentially the concept of velocity in classical momen‐\\ntum), which we can express as follows:\\n    mi = β1mi – 1 + (1 – β1)gi\\nThis is our approximation of what we call the first moment of the gradient, or E gi .\\nAnd similarly to RMSProp, we can maintain an exponentially weighted moving\\naverage of the historical gradients. This is our estimation of what we call the second\\nmoment of the gradient, or E gi ⊙gi :\\nvi = β2vi −1 + 1 −β2 gi ⊙gi\\nHowever, it turns out these estimations are biased relative to the real moments\\nbecause we start off by initializing both vectors to the zero vector. In order to remedy\\nthis bias, we derive a correction factor for both estimations. Here, we describe the\\nderivation for the estimation of the second moment. The derivation for the first\\nmoment, which is analogous to the derivation here, is left as an exercise for you.\\nLearning Rate Adaptation \\n| \\n113'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 129}, page_content='We begin by expressing the estimation of the second moment in terms of all past\\ngradients. This is done by simply expanding the recurrence relationship:\\nvi = β2vi −1 + 1 −β2 gi ⊙gi\\nvi = β2\\ni −1 1 −β2 g1 ⊙g1 + β2\\ni −2 1 −β2 g2 ⊙g2 + ... + 1 −β2 gi ⊙gi\\nvi = 1 −β2 ∑k = 1\\ni\\nβi −kgk ⊙gk\\nWe can then take the expected value of both sides to determine how our estimation\\nE vi  compares to the real value of E gi ⊙gi :\\nE vi = E 1 −β2 ∑k = 1\\ni\\nβi −kgk ⊙gk\\nWe can also assume that E gk ⊙gk ≈E gi ≈gi  because even if the second moment\\nof the gradient has changed since a historical value, β2 should be chosen so that the\\nold second moments of the gradients are essentially decayed out of relevancy. As a\\nresult, we can make the following simplification:\\nE vi ≈E gi ⊙gi 1 −β2 ∑k = 1\\ni\\nβi −k\\nE vi ≈E gi ⊙gi 1 −β2i\\nNote that we make the final simplification using the elementary algebraic iden‐\\ntity 1 −xn = 1 −x 1 + x + ... + xn −1 . The results of this derivation and the analo‐\\ngous derivation for the first moment are the following correction schemes to account\\nfor the initialization bias:\\nm̃i = \\nmi\\n1 −β1i\\nvi =\\nvi\\n1 −β2i\\nWe can then use these corrected moments to update the parameter vector, resulting\\nin the final Adam update:\\nθi = θi −1 −\\nϵ\\nδ ⊕\\nvim̃i\\n114 \\n| \\nChapter 6: Beyond Gradient Descent'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 130}, page_content='Recently, Adam has gained popularity because of its corrective measures against the\\nzero initialization bias (a weakness of RMSProp) and its ability to combine the core\\nconcepts behind RMSProp with momentum more effectively. PyTorch exposes the\\nAdam optimizer through the following constructor:\\noptimizer = optim.Adam(model.parameters(),\\n                       lr = 0.001,\\n                       betas = (0.9, 0.999),\\n                       eps = 1e-08,\\n                       weight_decay = 0,\\n                       amsgrad = False)\\nThe default hyperparameter settings for Adam for PyTorch generally perform quite\\nwell, but Adam is also generally robust to choices in hyperparameters. The only\\nexception is that the learning rate may need to be modified in certain cases from the\\ndefault value of 0.001.\\nThe Philosophy Behind Optimizer Selection\\nIn this chapter, we’ve discussed several strategies that are used to make navigating\\nthe complex error surfaces of deep networks more tractable. These strategies have\\nculminated in several optimization algorithms, each with its own benefits and short‐\\ncomings.\\nWhile it would be awfully nice to know when to use which algorithm, there is very\\nlittle consensus among expert practitioners. Currently, the most popular algorithms\\nare minibatch gradient descent,  minibatch gradient with momentum, RMSProp,\\nRMSProp with momentum, Adam, and AdaDelta (which we haven’t discussed here,\\nbut is also supported by PyTorch). We encourage you to experiment with these\\noptimization algorithms on the feed-forward network model we built.\\nOne important point, however, is that for most deep learning practitioners, the best\\nway to push the cutting edge of deep learning is not by building more advanced\\noptimizers. Instead, the vast majority of breakthroughs in deep learning over the past\\nseveral decades have been obtained by discovering architectures that are easier to\\ntrain instead of trying to wrangle with nasty error surfaces. We’ll begin focusing on\\nhow to leverage architecture to more effectively train neural networks in the rest of\\nthis book.\\nThe Philosophy Behind Optimizer Selection \\n| \\n115'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 131}, page_content='Summary\\nIn this chapter, we discussed several challenges that arise when trying to train deep\\nnetworks with complex error surfaces. We discussed how while the challenges of spu‐\\nrious local minima are likely exaggerated, saddle points and ill-conditioning do pose\\na serious threat to the success of vanilla minibatch gradient descent. We described\\nhow momentum can be used to overcome ill-conditioning, and briefly discussed\\nrecent research in second-order methods to approximate the Hessian matrix. We also\\ndescribed the evolution of adaptive learning rate optimizers, which tune the learning\\nrate during the training process for better convergence.\\nNext, we’ll begin tackling the larger issue of network architecture and design. We’ll\\nexplore computer vision and how we might design deep networks that learn effec‐\\ntively from complex images.\\n116 \\n| \\nChapter 6: Beyond Gradient Descent'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 132}, page_content='1 Hubel, David H., and Torsten N. Wiesel. “Receptive Fields and Functional Architecture of Monkey Striate\\nCortex.” The Journal of Physiology 195.1 (1968): 215-243.\\n2 Cohen, Adolph I. “Rods and Cones.” Physiology of Photoreceptor Organs. Springer Berlin Heidelberg, 1972.\\n63-110.\\nCHAPTER 7\\nConvolutional Neural Networks\\nNeurons in Human Vision\\nThe human sense of vision is unbelievably advanced. Within fractions of seconds,\\nwe can identify objects within our field of view, without thought or hesitation. Not\\nonly can we name objects we are looking at, we can also perceive their depth,\\nperfectly distinguish their contours, and separate the objects from their backgrounds.\\nSomehow our eyes take in raw voxels of color data, but our brain transforms that\\ninformation into more meaningful primitives—lines, curves, and shapes—that might\\nindicate, for example, that we’re looking at a house cat.1\\nFoundational to the human sense of vision is the neuron. Specialized neurons are\\nresponsible for capturing light information in the human eye.2 This light information\\nis then preprocessed, transported to the visual cortex of the brain, and then finally\\nanalyzed to completion. Neurons are single-handedly responsible for all of these\\nfunctions. As a result, intuitively, it would make a lot of sense to extend our neural\\nnetwork models to build better computer vision systems. In this chapter, we will use\\nour understanding of human vision to build effective deep learning models for image\\nproblems. But before we jump in, let’s take a look at more traditional approaches to\\nimage analysis and why they fall short.\\n117'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 133}, page_content='3 Viola, Paul, and Michael Jones. “Rapid Object Detection using a Boosted Cascade of Simple Features.”\\nComputer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society\\nConference on. Vol. 1. IEEE, 2001.\\nThe Shortcomings of Feature Selection\\nLet’s begin by considering a simple computer vision problem. I give you a randomly\\nselected image, such as the one in Figure 7-1. Your task is to tell me if there is a\\nhuman face in this picture. This is exactly the problem that Paul Viola and Michael\\nJones tackled in their seminal paper published in 2001.3\\nFigure 7-1. A hypothetical face-recognition algorithm should detect a face in this photo‐\\ngraph of former US President Barack Obama\\nFor a human like you or me, this task is completely trivial. For a computer, however,\\nthis is a difficult problem. How do we teach a computer that an image contains a\\nface? We could try to train a traditional machine learning algorithm (like the one we\\ndescribed in Chapter 3) by giving it the raw pixel values of the image and hoping it\\ncan find an appropriate classifier. Turns out this doesn’t work well at all because the\\nsignal-to-noise ratio is much too low for any useful learning to occur. We need an\\nalternative.\\nThe compromise that was eventually reached was essentially a trade-off between the\\ntraditional computer program, where the human defined all of the logic, and a pure\\n118 \\n| \\nChapter 7: Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 134}, page_content='machine learning approach, where the computer did all of the heavy lifting. In this\\ncompromise, a human would choose the features (perhaps hundreds or thousands)\\nthat they believed were important in making a classification decision. In doing so, the\\nhuman would be producing a lower-dimensional representation of the same learning\\nproblem. The machine learning algorithm would then use these new feature vectors\\nto make classification decisions. Because the feature extraction process improves the\\nsignal-to-noise ratio (assuming the appropriate features are picked), this approach\\nhad quite a bit of success compared to the state-of-the-art at the time.\\nViola and Jones had the insight that faces had certain patterns of light and dark\\npatches that they could exploit. For example, there is a difference in light intensity\\nbetween the eye region and the upper cheeks. There is also a difference in light\\nintensity between the nose bridge and the two eyes on either side. These detectors are\\nshown in Figure 7-2.\\nFigure 7-2. Viola-Jones intensity detectors\\nBy themselves, each of these features is not very effective at identifying a face.\\nBut when used together (through a classic machine learning algorithm known as\\nboosting, described in the original manuscript), their combined effectiveness drasti‐\\ncally increases. On a dataset of 130 images and 507 faces, the algorithm achieves a\\n91.4% detection rate with 50 false positives. The performance was unparalleled at\\nthe time, but there are fundamental limitations of the algorithm. If a face is partially\\ncovered with shade, the light intensity comparisons no longer work. Moreover, if the\\nalgorithm is looking at a face on a crumpled flier or the face of a cartoon character, it\\nwould most likely fail.\\nThe Shortcomings of Feature Selection \\n| \\n119'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 135}, page_content='4 Deng, Jia, et al. “ImageNet: A Large-Scale Hierarchical Image Database.” Computer Vision and Pattern\\nRecognition, 2009. CVPR 2009. IEEE Conference. IEEE, 2009.\\n5 Perronnin, Florent, Jorge Sénchez, and Yan Liu Xerox. “Large-Scale Image Categorization with Explicit Data\\nEmbedding.” Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference. IEEE, 2010.\\n6 Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. “ImageNet Classification with Deep Convolutional\\nNeural Networks.” Advances in Neural Information Processing Systems. 2012.\\nThe problem is the algorithm hasn’t really learned that much about what it means\\nto “see” a face. Beyond differences in light intensity, our brain uses a vast number\\nof visual cues to realize that our field of view contains a human face, including\\ncontours, relative positioning of facial features, and color. And even if there are slight\\ndiscrepancies in one of our visual cues (for example, if parts of the face are blocked\\nfrom view or if shade modifies light intensities), our visual cortex can still reliably\\nidentify faces.\\nTo use traditional machine learning techniques to teach a computer to “see,” we need\\nto provide our program with a lot more features to make accurate decisions. Before\\nthe advent of deep learning, huge teams of computer vision researchers would take\\nyears to debate about the usefulness of different features. As the recognition problems\\nbecame more and more intricate, researchers had a difficult time coping with the\\nincrease in complexity.\\nTo illustrate the power of deep learning, consider the ImageNet challenge, one of\\nthe most prestigious benchmarks in computer vision (sometimes even referred to as\\nthe Olympics of computer vision).4 Every year, researchers attempt to classify images\\ninto one of 200 possible classes given a training dataset of approximately 450,000\\nimages. The algorithm is given five guesses to get the right answer before it moves\\nonto the next image in the test dataset. The goal of the competition is to push\\nthe state-of-the-art in computer vision to rival the accuracy of human vision itself\\n(approximately 95% to 96%).\\nIn 2011, the winner of the ImageNet benchmark had an error rate of 25.7%, making\\na mistake on one out of every four images.5 Definitely a huge improvement over\\nrandom guessing, but not good enough for any sort of commercial application. Then\\nin 2012, Alex Krizhevsky from Geoffrey Hinton’s lab at the University of Toronto did\\nthe unthinkable. Pioneering a deep learning architecture known as a convolutional\\nneural network for the first time on a challenge of this size and complexity, he\\nblew the competition out of the water. The runner-up in the competition scored a\\ncommendable 26.1% error rate. But AlexNet, over the course of just a few months\\nof work, completely crushed 50 years of traditional computer vision research with an\\nerror rate of approximately 16%.6 It would be no understatement to say that AlexNet\\nsingle-handedly put deep learning on the map for computer vision and completely\\nrevolutionized the field.\\n120 \\n| \\nChapter 7: Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 136}, page_content='Vanilla Deep Neural Networks Don’t Scale\\nThe fundamental goal in applying deep learning to computer vision is to remove the\\ncumbersome, and ultimately limiting, feature selection process. As we discussed in\\nChapter 3, deep neural networks are perfect for this process because each layer of a\\nneural network is responsible for learning and building up features to represent the\\ninput data that it receives. A naive approach might be for us to use a vanilla deep\\nneural network using the network layer primitive we designed in Chapter 5 for the\\nMNIST dataset to achieve the image classification task.\\nIf we attempt to tackle the image classification problem in this way, however, we’ll\\nquickly face a pretty daunting challenge, visually demonstrated in Figure 7-3. In\\nMNIST, our images were only 28 × 28 pixels and were black and white. As a result,\\na neuron in a fully connected hidden layer would have 784 incoming weights. This\\nseems pretty tractable for the MNIST task, and our vanilla neural net performed\\nquite well. This technique, however, does not scale well as our images grow larger.\\nFor example, for a full-color 200 × 200 pixel image, our input layer would have\\n200 × 200 × 3 = 120,000 weights. And we’re going to want to have lots of these\\nneurons over multiple layers, so these parameters add up quite quickly. Clearly, this\\nfull connectivity is not only wasteful, but also means that we’re much more likely to\\noverfit to the training dataset.\\nFigure 7-3. The density of connections between layers increases intractably as the size of\\nthe image increases\\nThe convolutional network takes advantage of the fact that we’re analyzing images,\\nand sensibly constrains the architecture of the deep network so that we drastically\\nreduce the number of parameters in our model. Inspired by how human vision\\nVanilla Deep Neural Networks Don’t Scale \\n| \\n121'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 137}, page_content='7 LeCun, Yann, et al. “Handwritten Digit Recognition with a Back-Propagation Network.” Advances in Neural\\nInformation Processing Systems. 1990.\\nworks, layers of a convolutional network have neurons arranged in three dimensions,\\nso layers have a width, height, and depth, as shown in Figure 7-4.7\\nAs we’ll see, the neurons in a convolutional layer are connected to only a small,\\nlocal region of the preceding layer, so we avoid the wastefulness of fully connected\\nneurons. A convolutional layer’s function can be expressed simply: it processes a\\nthree-dimensional volume of information to produce a new three-dimensional vol‐\\nume of information. We’ll take a closer look at how this works in the next section.\\nFigure 7-4. Convolutional layers arrange neurons in three dimensions, so layers have\\nwidth, height, and depth\\nFilters and Feature Maps\\nIn order to motivate the primitives of the convolutional layer, let’s build an intuition\\nfor how the human brain pieces together raw visual information into an understand‐\\ning of the world around us. One of the most influential studies in this space came\\nfrom David Hubel and Torsten Wiesel, who discovered that parts of the visual cortex\\nare responsible for detecting edges. In 1959, they inserted electrodes into the brain\\nof a cat and projected black-and-white patterns on the screen. They found that some\\n122 \\n| \\nChapter 7: Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 138}, page_content='8 Hubel, David H., and Torsten N. Wiesel. “Receptive Fields of Single Neurones in the Cat’s Striate Cortex.” The\\nJournal of Physiology 148.3 (1959): 574-591.\\nneurons fired only when there were vertical lines, others when there were horizontal\\nlines, and still others when the lines were at particular angles.8\\nFurther work determined that the visual cortex was organized in layers. Each layer is\\nresponsible for building on the features detected in the previous layers—from lines,\\nto contours, to shapes, to entire objects. Furthermore, within a layer of the visual\\ncortex, the same feature detectors were replicated over the whole area in order to\\ndetect features in all parts of an image. These ideas significantly impacted the design\\nof convolutional neural nets.\\nThe first concept that arose was that of a filter, and it turns out that here, Viola\\nand Jones were actually pretty close. A filter is essentially a feature detector, and to\\nunderstand how it works, let’s consider the toy image in Figure 7-5.\\nFigure 7-5. We’ll analyze this simple black-and-white image as a toy example\\nLet’s say that we want to detect vertical and horizontal lines in the image. One\\napproach would be to use an appropriate feature detector, as shown in Figure 7-6. For\\nexample, to detect vertical lines, we would use the feature detector on the top, slide it\\nacross the entirety of the image, and at every step check if we have a match. We keep\\ntrack of our answers in the matrix in the top right. If there’s a match, we shade the\\nappropriate box black. If there isn’t, we leave it white. This result is our feature map,\\nand it indicates where we’ve found the feature we’re looking for in the original image.\\nWe can do the same for the horizontal line detector (bottom), resulting in the feature\\nmap in the bottom-right corner.\\nFilters and Feature Maps \\n| \\n123'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 139}, page_content='Figure 7-6. Applying filters that detect vertical and horizontal lines on our toy example\\nThis operation is called a convolution. We take a filter and we multiply it over\\nthe entire area of an input image. Using the following scheme, let’s try to express\\nthis operation as neurons in a network. In this scheme, layers of neurons in a\\nfeed-forward neural net represent either the original image or a feature map. Filters\\nrepresent combinations of connections (one such combination is highlighted in Fig‐\\nure 7-7) that get replicated across the entirety of the input. In Figure 7-7, connections\\nof the same color are restricted to always have the same weight. We can achieve this\\nby initializing all the connections in a group with identical weights and by always\\naveraging the weight updates of a group before applying them at the end of each\\niteration of backpropagation. The output layer is the feature map generated by this\\nfilter. A neuron in the feature map is activated if the filter contributing to its activity\\ndetected an appropriate feature at the corresponding position in the previous layer.\\n124 \\n| \\nChapter 7: Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 140}, page_content='Figure 7-7. Representing filters and feature maps as neurons in a convolutional layer\\nLet’s denote the ktℎ feature map in layer m as mk. Moreover, let’s denote the corre‐\\nsponding filter by the values of its weights W. Then, assuming the neurons in the\\nfeature map have bias bk (note that the bias is kept identical for all of the neurons in a\\nfeature map), we can mathematically express the feature map as follows:\\nmij\\nk = f W * x ij + bk\\nThis mathematical description is simple and succinct, but it doesn’t completely\\ndescribe filters as they are used in convolutional neural networks. Specifically, filters\\ndon’t just operate on a single feature map. They operate on the entire volume of\\nfeature maps that have been generated at a particular layer. For example, consider\\na situation in which we would like to detect a face at a particular layer of a convo‐\\nlutional net. And we have accumulated three feature maps, one for eyes, one for\\nnoses, and one for mouths. We know that a particular location contains a face if\\nthe corresponding locations in the primitive feature maps contain the appropriate\\nfeatures (two eyes, a nose, and a mouth). In other words, to make decisions about\\nthe existence of a face, we must combine evidence over multiple feature maps. This\\nis equally necessary for an input image that is of full color. These images have pixels\\nrepresented as RGB values, so we require three slices in the input volume (one slice\\nfor each color). As a result, feature maps must be able to operate over volumes, not\\njust areas. This is shown in Figure 7-8. Each cell in the input volume is a neuron. A\\nlocal portion is multiplied with a filter (corresponding to weights in the convolutional\\nlayer) to produce a neuron in a filter map in the following volumetric layer of\\nneurons.\\nFilters and Feature Maps \\n| \\n125'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 141}, page_content='Figure 7-8. A full-color RGB image as a volume and applying a volumetric convolutional\\nfilter\\nAs we discussed in the previous section, a convolutional layer (which consists of a set\\nof filters) converts one volume of values into another volume of values. The depth of\\nthe filter corresponds to the depth of the input volume. This is so that the filter can\\ncombine information from all the features that have been learned. The depth of the\\noutput volume of a convolutional layer is equivalent to the number of filters in that\\nlayer, because each filter produces its own slice. We visualize these relationships in\\nFigure 7-9.\\nFigure 7-9. A three-dimensional visualization of a convolutional layer, where each filter\\ncorresponds to a slice in the resulting output volume\\n126 \\n| \\nChapter 7: Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 142}, page_content='In the next section, we will use these concepts and fill in some of the gaps to create a\\nfull description of a convolutional layer.\\nFull Description of the Convolutional Layer\\nLet’s use the concepts we’ve developed so far to complete the description of the\\nconvolutional layer. First, a convolutional layer takes in an input volume. This input\\nvolume has the following characteristics:\\n• Its width win\\n• Its height ℎin\\n• Its depth din\\n• Its zero padding p\\nThis volume is processed by a total of k filters, which represent the weights and\\nconnections in the convolutional network. These filters have a number of hyperpara‐\\nmeters, which are described as follows:\\n• Their spatial extent e, which is equal to the filter’s height and width.\\n• Their stride s, or the distance between consecutive applications of the filter on\\nthe input volume. If we use a stride of 1, we get the full convolution described in\\nthe previous section. We illustrate this in Figure 7-10.\\n• The bias b (a parameter learned like the values in the filter), which is added to\\neach component of the convolution.\\nFigure 7-10. A filter’s stride hyperparameter\\nFull Description of the Convolutional Layer \\n| \\n127'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 143}, page_content='This results in an output volume with the following characteristics:\\n• Its function f, which is applied to the incoming logit of each neuron in the\\noutput volume to determine its final value\\n• Its width wout =\\nwin −e + 2p\\ns\\n+ 1\\n• Its height ℎout =\\nℎin −e + 2p\\ns\\n+ 1\\n• Its depth dout = k\\nThe mtℎ “depth slice” of the output volume, where 1 ≤m ≤k, corresponds to the\\nfunction f applied to the sum of the mtℎ filter convoluted over the input volume\\nand the bias bm. Moreover, this means that per filter, we have dine2 parameters. In\\ntotal, that means the layer has kdine2 parameters and k biases. To demonstrate this\\nin action, we provide an example of a convolutional layer in Figures 7-11 and 7-12\\nwith a 5 × 5 × 3 input volume with zero padding p = 1. We’ll use two 3 × 3 × 3 filters\\n(spatial extent ) with a stride s = 2. We’ll use a linear function to produce the output\\nvolume, which will be of size 3 × 3 × 2. We apply the first convolutional filter to the\\nupper-leftmost 3 × 3 piece of the input volume to generate the upper-leftmost entry\\nof the first depth slice.\\nGenerally, it’s wise to keep filter sizes small (size 3 × 3 or 5 × 5). Less commonly,\\nlarger sizes are used (7 × 7) but only in the first convolutional layer. Having more\\nsmall filters is an easy way to achieve high representational power while also incur‐\\nring a smaller number of parameters. It’s also suggested to use a stride of 1 to capture\\nall useful information in the feature maps, and a zero padding that keeps the output\\nvolume’s height and width equivalent to the input volume’s height and width.\\n128 \\n| \\nChapter 7: Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 144}, page_content='Figure 7-11. A convolutional layer with an input volume of width 5, height 5, depth 3,\\nzero padding 1, and 2 filters (with spatial extent 3 and applied with a stride of 2) results\\nin an output volume of 3 × 3 × 2\\nFull Description of the Convolutional Layer \\n| \\n129'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 145}, page_content='Figure 7-12. Using the same setup as Figure 7-11, we generate the next value in the first\\ndepth slice of the output volume\\nPyTorch provides us with a convenient operation to easily perform a 2D convolution\\non a minibatch of input volumes:\\nimport torch.nn as nn\\nlayer = nn.Conv2d(in_channels = 3,\\n                  out_channels = 64,\\n130 \\n| \\nChapter 7: Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 146}, page_content='kernel_size = (5, 5),\\n                  stride = 2,\\n                  padding = 1\\n                  )\\nHere, in_channels represents the depth, din, or number of input planes. For color\\nimages, the number of input channels often equals three, representing the RGB\\nchannels. The nn.Conv2d layer will accept as an input a four-dimensional tensor of\\nsize, bin * din * ℎin * win, where bin is the number of examples in our minibatch.\\nThe out_channels argument represents the number of output planes or feature maps.\\nThe kernel_size argument determines the filter size or spatial extent, e, while the\\nstride and padding arguments determine the stride size, s, and zero padding size, p,\\nrespectively. Note that you can pass in equal dimension settings with a single value as\\nshown here with stride and padding.\\nMax Pooling\\nTo aggressively reduce dimensionality of feature maps and sharpen the located\\nfeatures, we sometimes insert a max pooling layer after a convolutional layer. The\\nessential idea behind max pooling is to break up each feature map into equally sized\\ntiles. Then we create a condensed feature map. Specifically, we create a cell for each\\ntile, compute the maximum value in the tile, and propagate this maximum value into\\nthe corresponding cell of the condensed feature map. This process is illustrated in\\nFigure 7-13.\\nFigure 7-13. Max pooling significantly reduces parameters as we move up the network\\nMax Pooling \\n| \\n131'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 147}, page_content='9 Graham, Benjamin. “Fractional Max-Pooling.” arXiv Preprint arXiv:1412.6071 (2014).\\nMore rigorously, we can describe a pooling layer with two parameters:\\n• Its spatial extent e\\n• Its stride s\\nIt’s important to note that only two major variations of the pooling layer are used.\\nThe first is the nonoverlapping pooling layer with e = 2, s = 2. The second is the\\noverlapping pooling layer with e = 3, s = 2. The resulting dimensions of each feature\\nmap are as follows:\\n• Its width wout =\\nwin −e\\ns\\n+ 1\\n• Its height ℎout =\\nℎin −e\\ns\\n+ 1\\nOne interesting property of max pooling is that it is locally invariant. This means that\\neven if the inputs shift around a little bit, the output of the max pooling layer stays\\nconstant. This has important implications for visual algorithms. Local invariance is a\\nuseful property if we care more about whether some feature is present than exactly\\nwhere it is. However, enforcing large amounts of local invariance can destroy our\\nnetwork’s ability to carry important information. As a result, we usually keep the\\nspatial extent of our pooling layers quite small.\\nSome recent work along this line has come out of the University of Warwick from\\nGraham,9 who proposes a concept called fractional max pooling. In fractional max\\npooling, a pseudorandom number generator is used to generate tilings with nonin‐\\nteger lengths for pooling. Here, fractional max pooling functions as a strong regular‐\\nizer, helping prevent overfitting in convolutional networks.\\nFull Architectural Description of Convolution Networks\\nNow that we’ve described the building blocks of convolutional networks, we start\\nputting them together. Figure 7-14 depicts several architectures that might be of\\npractical use.\\nOne theme we notice as we build deeper networks is that we reduce the number\\nof pooling layers and instead stack multiple convolutional layers in tandem. This\\nis generally helpful because pooling operations are inherently destructive. Stacking\\nseveral convolutional layers before each pooling layer allows us to achieve richer\\nrepresentations.\\n132 \\n| \\nChapter 7: Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 148}, page_content='10 Simonyan, Karen, and Andrew Zisserman. “Very Deep Convolutional Networks for Large-Scale Image Recog‐\\nnition.” arXiv Preprint arXiv:1409.1556 (2014).\\nFigure 7-14. Various convolutional network architectures of various complexities\\nAs a practical note, deep convolutional networks can take up a significant amount of\\nspace, and most casual practitioners are usually bottlenecked by the memory capacity\\non their GPU. The VGGNet architecture, for example, takes approximately 90 MB of\\nmemory on the forward pass per image, and more than 180 MB of memory on the\\nbackward pass to update the parameters.10 Many deep networks make a compromise\\nby using strides and spatial extents in the first convolutional layer that reduce the\\namount of information that needs to be propagated up the network.\\nFull Architectural Description of Convolution Networks \\n| \\n133'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 149}, page_content=\"Closing the Loop on MNIST with Convolutional Networks\\nNow that we have a better understanding of how to build networks that effectively\\nanalyze images, we’ll revisit the MNIST challenge we’ve tackled over the past several\\nchapters. Here, we’ll use a convolutional network to learn how to recognize handwrit‐\\nten digits. Our feed-forward network was able to achieve a 98.2% accuracy. Our goal\\nwill be to push the envelope on this result.\\nTo tackle this challenge, we’ll build a convolutional network with a pretty stan‐\\ndard architecture (modeled after the second network in Figure 7-14): two convolu‐\\ntional/ReLU/maxpooling stacks, followed by a fully connected layer with dropout and\\na terminal fully connected layer. Building the network is easy in PyTorch using the\\nbuilt-in nn classes, as shown in the following code:\\nclass MNISTConvNet(nn.Module):\\n  def __init__(self):\\n    super(MNISTConvNet, self).__init__()\\n    self.conv1 = nn.Sequential(\\n        nn.Conv2d(1, 32, 5, padding='same'),\\n        nn.ReLU(),\\n        nn.MaxPool2d(2)\\n    )\\n    self.conv2 = nn.Sequential(\\n        nn.Conv2d(32, 64, 5, padding='same'),\\n        nn.ReLU(),\\n        nn.MaxPool2d(2)\\n    )\\n    self.fc1 = nn.Sequential(\\n        nn.Flatten(),\\n        nn.Linear(7*7*64, 1024),\\n        nn.Dropout(0.5),\\n        nn.Linear(1024, 10)\\n    )\\n  def forward(self, x):\\n    x = self.conv1(x)\\n    x = self.conv2(x)\\n    return self.fc1(x)\\nThe __init__ method generates two Conv2d/ReLU/MaxPool blocks followed by a\\nblock containing two fully connected layers. The convolutional layers are created with\\na particular shape. By default, the stride is set to be 1, while the padding is set to same\\nto keep the width and height constant between input and output tensors. By default,\\neach nn.Conv2d constructor automatically initializes the weights.\\nThe max pooling layers consist of nonoverlapping windows of size k. The default,\\nas recommended, is k=2, and we’ll use this default in our MNIST convolutional\\nnetwork.\\n134 \\n| \\nChapter 7: Convolutional Neural Networks\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 150}, page_content='The forward method defines how our layers and blocks are connected together to\\nperform the forward pass or inference.\\nThe code here is quite easy to follow. The input is expected to be a tensor of size\\nN × 1 × 28 × 28 , where N is the number of examples in a minibatch, 28 is the width\\nand height of each image, and 1 is the depth (because the images are black and white;\\nif the images were in RGB color, the depth would instead be 3 to represent each color\\nmap).\\nThe first block, conv1, builds a convolutional layer with 32 filters that have spatial\\nextent 5. This results in taking an input volume of depth 1 and emitting an output\\ntensor of depth 32. This is then passed through a max pooling layer that compresses\\nthe information. The second block, conv2, then builds a second convolutional layer\\nwith 64 filters, again with spatial extent 5, taking an input tensor of depth 32 and\\nemitting an output tensor of depth 64. This, again, is passed through a max pooling\\nlayer to compress information.\\nWe then prepare to pass the output of the max pooling layer into a fully connected\\nlayer. To do this, we flatten the tensor. We can do this by computing the full size of\\neach “subtensor” in the minibatch. We have 64 filters, which corresponds to the depth\\nof 64. We now have to determine the height and width after passing through two\\nmax pooling layers. Using the formulas we found in the previous section, it’s easy to\\nconfirm that each feature map has a height and width of 7. Confirming this is left as\\nan exercise for you.\\nWe use a fully connected layer to compress the flattened representation into a hidden\\nstate of size 1,024. We use a dropout probability in this layer of 0.5 during training\\nand 1 during model evaluation (standard procedure for employing dropout). Finally,\\nwe send this hidden state into a output layer with 10 bins (the softmax is, as usual,\\nperformed in the loss constructor for better performance).\\nFinally, we train our network using the Adam optimizer. After several epochs over the\\ndataset, we achieve an accuracy of 99.4%, which isn’t state-of-the-art (approximately\\n99.7 to 99.8%), but is respectable:\\nlr = 1e-4\\nnum_epochs = 40\\nmodel = MNISTConvNet()\\nloss_fn = nn.CrossEntropyLoss()\\noptimizer = optim.SGD(model.parameters(), lr=lr)\\nfor epochs in range(num_epochs):\\n  running_loss = 0.0\\n  num_correct = 0\\n  for inputs, labels in trainloader:\\n    optimizer.zero_grad()\\n    outputs = model(inputs)\\nClosing the Loop on MNIST with Convolutional Networks \\n| \\n135'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 151}, page_content=\"loss = loss_fn(outputs, labels)\\n    loss.backward()\\n    running_loss += loss.item()\\n    optimizer.step()\\n    _, idx = outputs.max(dim=1)\\n    num_correct += (idx == labels).sum().item()\\n  print('Loss: {} Accuracy: {}'.format(running_loss/len(trainloader),\\n        num_correct/len(trainloader)))\\nImage Preprocessing Pipelines Enable\\nMore Robust Models\\nSo far we’ve been dealing with rather tame datasets. Why is MNIST a tame dataset?\\nWell, fundamentally, MNIST has already been preprocessed so that all the images in\\nthe dataset resemble each other. The handwritten digits are perfectly cropped in just\\nthe same way; there are no color aberrations because MNIST is black and white; and\\nso on. Natural images, however, are an entirely different beast.\\nNatural images are messy, and as a result, there are a number of preprocessing\\noperations that we can utilize in order to make training slightly easier. Fortunately,\\nPyTorch offers a package called Torchvision that includes many commonly used\\ntransforms for image processing. One technique that is supported out of the box in\\nPyTorch is image whitening. The basic idea behind whitening is to zero-center every\\npixel in an image by subtracting out the mean of the dataset and normalizing to unit\\n1 variance. This helps us correct for potential differences in dynamic range between\\nimages. In PyTorch, we can achieve this using the Normalize transform:\\nfrom torchvision import transforms\\ntransform = transforms.Normalize(mean = (0.1307,),\\n                                 std = (0.3081,)\\n                                 )\\nThe magic numbers for mean, 0.1307, and std, 0.3081, were computed over the\\nentire MNIST dataset, and this technique is called dataset normalization. We can also\\nexpand our dataset artificially by randomly cropping the image, flipping the image,\\nmodifying saturation, modifying brightness, etc:\\n136 \\n| \\nChapter 7: Convolutional Neural Networks\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 152}, page_content='11 S. Ioffe, C. Szegedy. “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covari‐\\nate Shift.” arXiv Preprint arXiv:1502.03167. 2015.\\ntransform = transforms.Compose([\\n      transforms.RandomCrop(224),\\n      transforms.RandomHorizontalFlip(),\\n      transforms.ColorJitter(brightness=0,\\n                             contrast=0,\\n                             saturation=0,\\n                             hue=0),\\n      transforms.ToTensor(),\\n      transforms.Normalize(mean = (0.1307,),\\n                           std = (0.3081,)\\n                           )\\n      ])\\nHere, we use the Compose transform to create a sequence of transforms from a list.\\nAfter applying random cropping, flipping, and color adjustments, we convert the\\nimage data to a PyTorch tensor and normalize the data. PyTorch models require the\\ndata to be in tensor format, and these last two steps are common practice in using\\nPyTorch for deep learning.\\nApplying these transformations helps us build networks that are robust to the differ‐\\nent kinds of variations that are present in natural images, and make predictions with\\nhigh fidelity in spite of potential distortions.\\nAccelerating Training with Batch Normalization\\nIn 2015, researchers from Google devised an exciting way to even further accelerate\\nthe training of feed-forward and convolutional neural networks using a technique\\ncalled batch normalization.11 We can think of the intuition behind batch normaliza‐\\ntion like a tower of blocks, as shown in Figure 7-15.\\nWhen a tower of blocks is stacked together neatly, the structure is stable. However, if\\nwe randomly shift the blocks, we could force the tower into configurations that are\\nincreasingly unstable. Eventually the tower falls apart.\\nA similar phenomenon can happen during the training of neural networks. Imagine\\na two-layer neural network. In the process of training the weights of the network,\\nthe output distribution of the neurons in the bottom layer begins to shift. The result\\nof the changing distribution of outputs from the bottom layer means that the top\\nlayer not only has to learn how to make the appropriate predictions, but it also needs\\nto somehow modify itself to accommodate the shifts in incoming distribution. This\\nsignificantly slows down training, and the magnitude of the problem compounds the\\nmore layers we have in our networks.\\nAccelerating Training with Batch Normalization \\n| \\n137'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 153}, page_content='Figure 7-15. Batch normalization reduces shifts in the distribution of inputs of layers\\nNormalization of image inputs helps out the training process by making it more\\nrobust to variations. Batch normalization takes this a step further by normalizing\\ninputs to every layer in our neural network. Specifically, we modify the architecture\\nof our network to include operations that:\\n1. Grab the vector of logits incoming to a layer before they pass through the\\nnonlinearity.\\n2. Normalize each component of the vector of logits across all examples of the\\nminibatch by subtracting the mean and dividing by the standard deviation (we\\nkeep track of the moments using an exponentially weighted moving average).\\n3. Given normalized inputs x̂, use an affine transform to restore representational\\npower with two vectors of (trainable) parameters: γx̂ + β.\\nPyTorch provides a BatchNorm2d class to perform batch normalization for a convolu‐\\ntional layer:\\nlayer = nn.BatchNorm2d(num_features=32,\\n                       eps=1e-05,\\n                       momentum=0.1,\\n                       affine = True,\\n                       track_running_stats = True)\\nHere, the num_features argument represents the depth, or number of channels, of\\nthe inputs to the batch normalization layer. Hence, batch normalization is performed\\nover the channel dimension, computing the mean and variance of each minibatch of\\n2D channels. The num_features is the only required argument. All other arguments\\nare set to their defaults.\\n138 \\n| \\nChapter 7: Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 154}, page_content='The BatchNorm2d layer performs the following affine transformation:\\ny =\\nx −E x\\nVar x + ϵ * γ + β\\nThe parameters γ and β are learnable parameters and will be trained during the\\ntraining process if affine = True. Otherwise, the mean is subtracted from the inputs\\nand divided by standard deviation to be normalized. The ϵ argument is only used for\\nmathematical stability.\\nWhen track_running_stats = True, this layer will keep track of the running mean\\nand variance for use in evaluation mode. The running mean and variance are upda‐\\nted using the momentum value.\\nWe can also express batch normalization for nonconvolutional feed-forward layers by\\nusing the BatchNorm1d constructor. Here, we set only num_features = 32 and use\\nthe defaults for other arguments:\\nlayer = nn.BatchNorm1d(num_features=32)\\nIn addition to speeding up training by preventing significant shifts in the distribution\\nof inputs to each layer, batch normalization also allows us to significantly increase\\nthe learning rate. Moreover, batch normalization acts as a regularizer and removes\\nthe need for dropout and (when used) L2 regularization. Although we don’t leverage\\nit here, the authors also claim that batch regularization largely removes the need\\nfor photometric distortions, and we can expose the network to more “real” images\\nduring the training process. In the next section, we will motivate and discuss a variant\\nof normalization across the feature axis, rather than the batch.\\nGroup Normalization for Memory Constrained\\nLearning Tasks\\nVarious forms of normalization in image processing have been studied and utilized in\\nthe last decade. The most famous of these is batch normalization. Just to recap from\\nthe previous section, this technique computes the channel-wise mean and variance of\\nthe output of each convolutional layer, normalizes each channel using the computed\\nstatistics, and then feeds the normalized output to the next convolutional layer. Thus,\\nany given channel in the normalized output will have the same mean and variance\\n(zero and one, respectively) across batches. In practice, the model will also learn a\\nmean parameter β and a standard deviation parameter γ, which are then applied to\\nthe normalized output such that it has mean β and standard deviation γ before being\\nfed into the subsequent layer. This process is used to reduce the shift in distribution\\nof any given channel from one batch to the next. Note that this is only a reduction of\\nthe shift and not a complete removal of it, since the channel distribution might still\\nGroup Normalization for Memory Constrained Learning Tasks \\n| \\n139'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 155}, page_content='12 Wu et. al. “Group Normalization.” 2018. https://arxiv.org/abs/1803.08494.\\nlook completely different from one batch to the next even though they have the same\\nmean and variance. In theory, and as has been observed empirically, reducing this\\ninternal covariate shift stabilizes training and results in strong performance gains.\\nHowever, in cases where the batch size is large, the channel-wise mean and variance\\ncomputations lead to large memory costs. Additionally, the size of the batch itself is\\nvery important for batch normalization, as smaller batch sizes degrade performance\\nsignificantly due to noisy mean and variance estimates. To avoid the issues that\\ncome with computations along the batch dimension, group normalization was intro‐\\nduced.12 Instead of performing a normalization along the batch dimension, group\\nnormalization is performed along the channel dimension and is thus unaffected by\\nthe aforementioned issues. Group normalization predefines a number of groups of\\nchannels and, for each instance, computes the mean μ and variance σ for each group\\nof channels in each instance of the batch. Each set of computed β and γ parameters\\nis used to normalize the set of entries from which they were computed. Additionally,\\nsimilarly to batch normalization, an offset/mean parameter β and a scale/standard\\ndeviation parameter γ are separately learned for each entry set.\\nThis is similar to another popular technique known as layer normalization, which is\\neffectively batch normalization but across the full length of the channel dimension\\nrather than the full length of the batch dimension. Note that layer normalization\\nis also just a special case of group normalization, where the number of groups of\\nchannels is set to one. Figure 7-16 compares batch normalization with group normal‐\\nization and layer normalization. The blocked-off section in each cube demonstrates\\nthe dimension along which normalization occurs and the group of entries that are\\nnormalized together. Note that we condense the standard 4D representation into 3D\\nfor visualization purposes.\\nFigure 7-16. Comparison of batch normalization, layer normalization, and group\\nnormalization\\n140 \\n| \\nChapter 7: Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 156}, page_content='13 Krizhevsky, Alex, and Geoffrey Hinton. “Learning Multiple Layers of Features from Tiny Images.” University\\nof Toronto (2009).\\nYou may be wondering why techniques like group normalization and layer normal‐\\nization are even effective. After all, it seems as though batch normalization is only\\nuseful due to forcing each feature (or channels in our case) to have the same mean\\nand variance. For some insight, the initial paper on layer normalization states that the\\nreason for normalizing the features for each instance separately is that “changes in the\\noutput of one layer will tend to cause highly correlated changes in the summed input\\nto the next layer.” In summary, the neurons that make up every subsequent layer in\\nthe feed-forward network will see the same statistics from one training example to\\nthe next with layer normalization.\\nFurthermore, why group normalization over layer normalization? In Wu et al., the\\nidea behind using group normalization is that it is less restrictive than layer normal‐\\nization—a different distribution can be learned for each group of features, signifying\\nthe ability to learn potentially different levels of contribution and importance for\\ndifferent groups.\\nNow that we have sufficiently covered group normalization as a concept, its connec‐\\ntion to prior work, and motivation for using group normalization in practice, we can\\nnow dive into some PyTorch code for implementing group normalization.\\nPyTorch provides a torch.nn.GroupNorm class to create group normalization layers:\\nlayer = nn.GroupNorm(num_groups=1,\\n                     num_channels=32)\\nWe need to specify only the number of groups and number of channels. Now that\\nwe’ve developed an enhanced toolkit for analyzing natural images with convolutional\\nnetworks, we’ll build a classifier for tackling the CIFAR-10 challenge.\\nBuilding a Convolutional Network for CIFAR-10\\nThe CIFAR-10 challenge consists of 32 × 32 color images that belong to one of 10\\npossible classes.13 This is a surprisingly hard challenge because it can be difficult for\\neven a human to figure out what is in a picture. An example is shown in Figure 7-17.\\nBuilding a Convolutional Network for CIFAR-10 \\n| \\n141'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 157}, page_content='Figure 7-17. A dog from the CIFAR-10 dataset\\nIn this section, we’ll build networks both with and without batch normalization\\nas a basis of comparison. We increase the learning rate by 10-fold for the batch\\nnormalization network to take full advantage of its benefits. We’ll display code for\\nonly the batch normalization network here because building the vanilla convolutional\\nnetwork is similar.\\nWe distort random 24 × 24 crops of the input images to feed into our network for\\ntraining. We use the example code provided by Google to do this. We’ll jump right\\ninto the network architecture. To start, let’s take a look at how we integrate batch\\nnormalization into the convolutional and fully connected layers. As expected, batch\\nnormalization happens to the logits before they’re fed into a nonlinearity:\\n142 \\n| \\nChapter 7: Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 158}, page_content='class Net(nn.Module):\\n    def __init__(self):\\n        super(Net, self).__init__()\\n        self.block1 = nn.Sequential(\\n            nn.Conv2d(1, 32, 3, 1),\\n            nn.BatchNorm2d(32),\\n            nn.ReLU(inplace=True),\\n            nn.Conv2d(32, 64, 3, 1),\\n            nn.BatchNorm2d(64),\\n            nn.ReLU(inplace=True),\\n            nn.MaxPool2d(2),\\n            nn.Dropout(0.25),\\n        )\\n        self.block2 = nn.Sequential(\\n            nn.Flatten(),\\n            nn.Linear(9216, 128),\\n            nn.BatchNorm1d(128),\\n            nn.ReLU(inplace=True),\\n            nn.Dropout(0.5),\\n            nn.Linear(128,10),\\n            nn.BatchNorm1d(10)\\n        )\\n    def forward(self, x):\\n        x = self.block1(x)\\n        return self.block2(x)\\nFinally, we use the Adam optimizer to train our convolutional networks. After some\\namount of time training, our networks are able to achieve an impressive 92.3%\\naccuracy on the CIFAR-10 task without batch normalization and 96.7% accuracy with\\nbatch normalization. This result actually matches (and potentially exceeds) current\\nstate-of-the-art research on this task. In the next section, we’ll take a closer look at\\nlearning and visualize how our networks perform.\\nVisualizing Learning in Convolutional Networks\\nOn a high level, the simplest thing that we can do to visualize training is plot\\nthe cost function and validation errors over time as training progresses. We can\\nclearly demonstrate the benefits of batch normalization by comparing the rates of\\nconvergence between our two networks. Plots taken in the middle of the training\\nprocess are shown in Figure 7-18.\\nVisualizing Learning in Convolutional Networks \\n| \\n143'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 159}, page_content='Figure 7-18. Training a convolutional network without batch normalization (left) versus\\nwith batch normalization (right)\\nWithout batch normalization, cracking the 90% accuracy threshold requires over\\n80,000 minibatches. On the other hand, with batch normalization, crossing the same\\nthreshold requires only slightly over 14,000 minibatches.\\nWe can also inspect the filters that our convolutional network learns in order to\\nunderstand what the network finds important to its classification decisions. Con‐\\nvolutional layers learn hierarchical representations, so we’d hope that the first con‐\\nvolutional layer learns basic features (edges, simple curves, etc.), and the second\\nconvolutional layer will learn more complex features. Unfortunately, the second\\n144 \\n| \\nChapter 7: Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 160}, page_content='14 Maaten, Laurens van der, and Geoffrey Hinton. “Visualizing Data Using t-SNE.” Journal of Machine Learning\\nResearch 9. Nov (2008): 2579-2605.\\nconvolutional layer is difficult to interpret even if we decided to visualize it, so we\\nonly include the first layer filters in Figure 7-19.\\nFigure 7-19. A subset of the learned filters in the first convolutional layer of our network\\nWe can make out a number of interesting features in our filters: vertical, horizontal,\\nand diagonal edges, in addition to small dots or splotches of one color surrounded by\\nanother. We can be confident that our network is learning relevant features because\\nthe filters are not just noise.\\nWe can also try to visualize how our network has learned to cluster various kinds\\nof images pictorially. To illustrate this, we take a large network that has been trained\\non the ImageNet challenge and then grab the hidden state of the fully connected\\nlayer just before the softmax for each image. We then take this high-dimensional\\nrepresentation for each image and use an algorithm known as t-Distributed Stochastic\\nNeighbor Embedding, or t-SNE, to compress it to a 2D representation that we can vis‐\\nualize.14 We don’t cover the details of t-SNE here, but there are a number of publicly\\nVisualizing Learning in Convolutional Networks \\n| \\n145'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 161}, page_content='15 Image credit: Andrej Karpathy. http://cs.stanford.edu/people/karpathy/cnnembed.\\navailable software tools that will do it for us, including the script. We visualize the\\nembeddings in Figure 7-20, and the results are quite spectacular.\\nFigure 7-20. The t-SNE embedding (center) surrounded by zoomed-in subsegments of\\nthe embedding (periphery)15\\nAt first, on a high level, it seems that images that are similarly colored are closer\\ntogether. This is interesting, but what’s even more striking is when we zoom into\\nparts of the visualization, we realize that it’s more than just color. We realize that all\\npictures of boats are in one place, all pictures of humans are in another place, and\\nall pictures of butterflies are in yet another location in the visualization. Quite clearly,\\nconvolutional networks have spectacular learning capabilities.\\n146 \\n| \\nChapter 7: Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 162}, page_content=\"16 He et. al. “Deep Residual Learning for Image Recognition.” arXiv Preprint arXiv:1512.03385. 2015.\\nResidual Learning and Skip Connections for\\nVery Deep Networks\\nWe have made great progress in the field of computer vision over the past decade,\\nand in this section we introduce one of the more recent advancements. Earlier,\\nwe discussed AlexNet, which was a breakthrough in neural methods applied to\\nimage classification. Since then, researchers have pushed toward deeper and deeper\\narchitectures in the hope of solving image classification. However, since AlexNet’s\\nbreakthrough, at least a few reputable studies tended to see decreases in training\\naccuracy when naively stacking layers as compared to their shallower counterparts.\\nIt’s particularly interesting that the problem isn’t even overfitting (as is suggested by\\na low training accuracy and a high validation accuracy), which would be understand‐\\nable for a network with such a large number of parameters. Additionally, we can\\neasily construct a deep network by ourselves that has the exact same performance\\nas its shallow counterpart: take the trained shallow network layers and simply stack\\nlayers that perform the identity operation. The fact that we do worse via a specialized\\noptimization algorithm compared to our naive construction is quite astounding.\\nThe problem is that training stalls for some inexplicable reason, settling in a local\\nminimum that we can’t get out of. Unfortunately, the theoretical justification for this\\nis still a bit hazy.\\nIn 2015, He et al.16 introduced the ResNet34 architecture, a deep architecture that\\nsurpassed all of its peers in major image classification competitions. With a version\\nthat consisted of over 30 trainable layers, He et al. redefined how we train deep\\ncomputer vision architectures. In particular, their contribution was the introduction\\nof what we now call skip connections, which add the feature vector obtained from a\\nlayer to the feature vector obtained one or two layers after the current layer. More\\nprecisely, let’s say we are midway through the network so far and our original input\\nx has been converted to some intermediate representation x’. The skip connection\\nwould take x’ and add it to the result of the next layer, F(x'), before passing the\\nrepresentation on to the following layer G. So instead of seeing F(x'), G sees F(x') + x’.\\nNote that the skip connection does not need to add the current representation to the\\nresult of F. As represented in Figure 7-21, we could also add x’ to the result of G, so\\nthe next layer H sees G(F(x')) + x’ instead of just G(F(x')).\\nResidual Learning and Skip Connections for Very Deep Networks \\n| \\n147\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 163}, page_content=\"Figure 7-21. The skip connection here skips F and G, summing the input to F with the\\noutput of G, which comprises the input to H\\nThese skip connections are just the identity operation, so they\\nadd no additional parameters to train. Additionally, since the skip\\nconnection is the identity operation, it must be the case that x’\\nand G(F(x')), in the example where the skip connection skips two\\nlayers, must be the same dimension. If this were not the case, we\\nwould not be able to add the two feature vectors. This does place\\na constraint on the network architecture, but we hope to construct\\na deep network anyway, and this approach lends itself well to such\\nnetworks since we wouldn’t want the dimensionality to decrease\\ntoo rapidly (recall the discussion on padding).\\nIt’s natural to ask why skip connections work so well. After all, it does seem like\\na pretty simple modification to the plain deep network architecture. Let’s think\\nback to the original motivation: through experimentation, researchers had noticed a\\ndegradation in performance as networks got deeper and deeper. However, it must be\\nthe case that deeper networks are able to perform at least as well as their shallower\\ncounterparts, since we can construct a naive solution where the additional layers\\nare the identity mapping. It’s also important to note that the representations learned\\nby shallower counterparts such as AlexNet are quite good, as they achieved state-of-\\nthe-art performance just a couple of years prior. If we make the assumption that\\nrepresentations at downstream layers in deep networks are only going to be slightly\\ndifferent from one layer to the next, which is reasonable due to the fact that shallower\\nnetworks still can learn very good representations, it would instead make sense to\\noptimize the difference between representations (which should be close to zero for\\nall weights) rather than attempt to achieve something close to the identity operation,\\nwhich is a very specific and imbalanced weight setting.\\nThat’s where residual connections come in. The downstream layers of the neural net‐\\nwork, such as F and G, are learning precisely this difference between representations\\nand then adding the difference back to the incoming representation x’ to achieve\\nan only slightly different representation G(F(x')) + x’. This is in contrast with the\\ntraditional feed-forward neural network paradigm, which would attempt to learn a\\n148 \\n| \\nChapter 7: Convolutional Neural Networks\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 164}, page_content='weight setting that is approximately close to identity for F and G, which seems like\\na much harder problem. In the next section, we will put our knowledge together to\\nbuild a residual network.\\nBuilding a Residual Network with Superhuman Vision\\nIn the previous section, we discussed residual connections and how they allow for\\nimproved gradient flow through deep neural networks. In this section, we will repli‐\\ncate the implementation of a neural network with residual connections, specifically\\nthe ResNet34 architecture from He et al.’s original.\\nPyTorch’s Torchvision library provides constructors for many commonly used\\nresnets. We can use it to create a ResNet34 model:\\nfrom torchvision.models import resnet34\\nmodel = resnet34()\\nLet’s see how resnet34 creates a residual network.\\nMost versions of residual networks consist of the following structure:\\n• Convolutional block (CONV->BN->ReLU->MAXPOOL)\\n• Four residual layers\\n• A classifier block with average pooling and a linear layer\\nEach residual layer consists of one or more residual blocks. For example, the layers\\nF and G from Figure 7-21 form a residual block. Here is the PyTorch code for a\\nsimplified implementation of a residual block for ResNet34:\\nclass ResidualBlock(nn.Module):\\n  def __init__(self, in_layers, out_layers, downsample=None):\\n    super(ResidualBlock, self).__init__()\\n    self.conv1 = nn.Conv2d(in_layers, out_layers,\\n                           kernel_size=3, stride=1, padding=1)\\n    self.bn1 = nn.BatchNorm2d(out_layers)\\n    self.conv2 = nn.Conv2d(out_layers, out_layers,\\n                           kernel_size=3, stride=1, padding=1)\\n    self.bn2 = nn.BatchNorm2d(out_layers)\\n    self.downsample = downsample\\n    self.relu = nn.ReLU(inplace=True)\\n  def forward(self, inp):\\n    # Residual block\\n    out = self.conv1(inp)\\n    out = self.bn1(out)\\n    out = self.relu(out)\\n    out = self.conv2(out)\\n    out = self.bn2(out)\\nBuilding a Residual Network with Superhuman Vision \\n| \\n149'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 165}, page_content='if self.downsample:\\n      inp = self.downsample(inp)\\n    \\n    # Shortcut connection\\n    out += inp\\n    return out\\nSimilarly to the previous section, each residual block in the ResNet34 architecture\\nconsists of two convolutional layers. The downsample argument allows for an optional\\ndownsampler function. The purpose of downsampling is to match the dimensions of\\nthe input with the output of the residual block, if the two are of different dimensions.\\nThe following is an example of a downsampler that matches the number of channels\\nof the input to that of the output of the residual block. Note that this downsampler\\ndoes not change the size of each feature map given the kernel_size is 1 and the\\nstride is also only 1, and affects the dimensions only by increasing the number of\\nfeature maps from 64 to 128:\\ndownsample = nn.Sequential(\\n      nn.Conv2d(64, 128, kernel_size=1, stride=1, bias=False),\\n      nn.BatchNorm2d(128)\\n    )\\nThe number of residual blocks for each of the four residual layers in ResNet34 is\\ndefined as [3, 4, 6, 3], respectively. The ResNet34 architecture is named this way\\nbecause it has 33 convolutional layers and 1 fully connected layer at the end, which\\nserves as the predictor portion of the network. The 33 convolutional layers are\\narranged in four sections that have 3, 4, 6, and 3 residual blocks, in that order. To get\\nto the total of 33, there is a single convolutional layer at the beginning that operates\\non the original image input, which is assumed to have 3 channels.\\nThe following PyTorch code initializes each of these components, closely modeled\\nafter the official PyTorch implementation of the various versions presented in the\\noriginal paper. The first component, up to the max pool, operates on the original\\ninput, and each of the following components requires downsampling only between\\ncomponents. This is because, within each component, the input and output of each\\nResidualBlock are of the same dimension. Although we won’t show it explicitly in\\nthis section, the combination of a kernel_size of 3, stride of 1, and padding of 1\\nensures that the size of each feature map stays constant from beginning to end. Addi‐\\ntionally, given the number of feature maps stays constant within each component, all\\ndimensions end up remaining the same:\\nclass ResNet34(nn.Module):\\n  def __init__(self):\\n    super(ResNet34, self).__init__()\\n    self.conv1 = nn.Sequential(\\n150 \\n| \\nChapter 7: Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 166}, page_content='nn.Conv2d(3, 64, kernel_size=7,\\n                stride=2, padding=3, bias=False),\\n      nn.BatchNorm2d(64),\\n      nn.ReLU(),\\n      nn.MaxPool2d(kernel_size=3,\\n                   stride=2, padding=1)\\n    )\\n    # Note that each ResidualBlock has 2 conv layers\\n    # 3 blocks in a row, 6 conv layers\\n    self.comp1 = nn.Sequential(\\n      ResidualBlock(64, 64),\\n      ResidualBlock(64, 64),\\n      ResidualBlock(64, 64)\\n    )\\n    # 4 blocks in a row, 8 conv layers\\n    downsample1 = nn.Sequential(\\n      nn.Conv2d(64, 128, kernel_size=1,\\n             stride=1, bias=False),\\n      nn.BatchNorm2d(128)\\n    )\\n    self.comp2 = nn.Sequential(\\n      ResidualBlock(64, 128, downsample=downsample1),\\n      ResidualBlock(128, 128),\\n      ResidualBlock(128, 128),\\n      ResidualBlock(128, 128)\\n    )\\n    \\n    # 6 blocks in a row, 12 conv layers\\n    downsample2 = nn.Sequential(\\n      nn.Conv2d(128, 256, kernel_size=1, stride=1, bias=False),\\n      nn.BatchNorm2d(256)\\n    )\\n    self.comp3 = nn.Sequential(\\n      ResidualBlock(128, 256, downsample=downsample2),\\n      ResidualBlock(256, 256),\\n      ResidualBlock(256, 256),\\n      ResidualBlock(256, 256),\\n      ResidualBlock(256, 256),\\n      ResidualBlock(256, 256),\\n    )\\n    \\n    # 3 blocks in a row, 6 conv layers\\n    downsample3 = nn.Sequential(\\n      nn.Conv2d(256, 512, kernel_size=1, stride=1, bias=False),\\n      nn.BatchNorm2d(512)\\n    )\\n    self.comp4 = nn.Sequential(\\n      ResidualBlock(256, 512, downsample=downsample3),\\n      ResidualBlock(512, 512),\\n      ResidualBlock(512, 512)   \\nBuilding a Residual Network with Superhuman Vision \\n| \\n151'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 167}, page_content='17 Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. “A Neural Algorithm of Artistic Style.” arXiv\\nPreprint arXiv:1508.06576 (2015).\\n    )\\n    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\\n    # ImageNet classifier: 1000 classes\\n    self.fc = nn.Linear(512, 1000)\\n  def forward(self, inp):\\n    out = self.conv1(inp)\\n    \\n    out = self.comp1(out)\\n    out = self.comp2(out)\\n    out = self.comp3(out)\\n    out = self.comp4(out)\\n    out = self.avgpool(out)\\n    out = torch.flatten(out, 1)\\n    out = self.fc(out)\\n    return out\\nIn the next section, we will present some of the latest advancements in computer\\nvision regarding neural style transfer.\\nLeveraging Convolutional Filters to Replicate\\nArtistic Styles\\nOver the past couple of years, we’ve also developed algorithms that leverage convo‐\\nlutional networks in much more creative ways. One of these algorithms is called\\nneural style.17 The goal of neural style is to be able to take an arbitrary photograph\\nand render it as if it were painted in the style of a famous artist. This seems like a\\ndaunting task, and it’s not exactly clear how we might approach this problem if we\\ndidn’t have a convolutional network. However, it turns out that clever manipulation\\nof convolutional filters can produce spectacular results on this problem.\\nLet’s take a pretrained convolutional network. We’re dealing with three images. The\\nfirst two are the source of content p and the source of style a. The third image is the\\ngenerated image x. Our goal is to derive an error function that we can backpropagate\\nthat, when minimized, will perfectly combine the content of the desired photograph\\nand the style of the desired artwork.\\nWe start with content first. If a layer in the network has kl filters, then it produces a\\ntotal of kl feature maps. Let’s call the size of each feature map ml, the height times the\\nwidth of the feature map. This means that the activations in all the feature maps of\\nthis layer can be stored in a matrix F(l) of size kl × ml. We can also represent all the\\n152 \\n| \\nChapter 7: Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 168}, page_content='activations of the photograph in a matrix P(l) and all the activations of the generated\\nimage in the matrix X(l). We use the relu4_2 of the original VGGNet:\\nEcontent(p, x) = ∑ij(Pij\\n(l) – Xij\\n(l))2\\nNow we can try tackling style. To do this we construct a matrix known as the\\nGram matrix, which represents correlations between feature maps in a given layer.\\nThe correlations represent the texture and feel that is common among all features,\\nirrespective of which features we’re looking at. Constructing the Gram matrix, which\\nis of size kl × kl, for a given image, is done as follows:\\nG(l)\\nij = ∑c = 0\\nml F(l)\\nic F(l)\\njc\\nWe can compute the Gram matrices for both the artwork in matrix A(l) and the\\ngenerated image in G(l). We can then represent the error function as:\\nEstyle a, x =\\n1\\n4kl\\n2ml\\n2 ∑l = 1\\nL\\n∑ij\\n1\\nL Aij\\nl −Gij\\nl\\n2\\nHere, we weight each squared difference equally (dividing by the number of layers\\nwe want to include in our style reconstruction). Specifically, we use the relu1_1,\\nrelu2_1, relu3_1, relu4_1, and relu5_1 layers of the original VGGNet. We omit\\na full discussion of the TensorFlow code for brevity, but the results, as shown in\\nFigure 7-22, are again quite spectacular. We mix a photograph of the iconic MIT\\ndome and Leonid Afremov’s Rain Princess.\\nLeveraging Convolutional Filters to Replicate Artistic Styles \\n| \\n153'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 169}, page_content='18 Image credit: Anish Athalye.\\n19 Karpathy, Andrej, et al. “Large-scale Video Classification with Convolutional Neural Networks.” Proceedings of\\nthe IEEE Conference on Computer Vision and Pattern Recognition. 2014.\\n20 Abdel-Hamid, Ossama, et al. “Applying Convolutional Neural Networks Concepts to Hybrid NN-HMM\\nModel for Speech Recognition.” IEEE International Conference on Acoustics, Speech, and Signal Processing\\n(ICASSP), Kyoto, 2012, pp. 4277-4280.\\nFigure 7-22. The result of mixing the Rain Princess with a photograph of the MIT dome18\\nLearning Convolutional Filters for Other Problem Domains\\nAlthough our examples in this chapter focus on image recognition, there are sev‐\\neral other problem domains in which convolutional networks are useful. A natural\\nextension of image analysis is video analysis. In fact, using five-dimensional tensors\\n(including time as a dimension) and applying three-dimensional convolutions is\\nan easy way to extend the convolutional paradigm to video.19 Convolutional filters\\nhave also been successfully used to analyze audiograms.20 In these applications, a\\nconvolutional network slides over an audiogram input to predict phonemes on the\\nother side.\\nLess intuitively, convolutional networks have also found some use in natural language\\nprocessing. We’ll see some examples of this in later chapters. More exotic uses of con‐\\nvolutional networks include teaching algorithms to play board games, and analyzing\\nbiological molecules for drug discovery. We’ll also discuss both of these examples in\\nlater chapters of this book.\\n154 \\n| \\nChapter 7: Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 170}, page_content='Summary\\nIn this chapter, we learned how to build neural networks that analyze images. We\\ndeveloped the concept of a convolution, and leveraged this idea to create tractable\\nnetworks that can analyze both simple and more complex natural images. We built\\nseveral of these convolutional networks in TensorFlow and leveraged various image\\nprocessing pipelines and batch normalization to make training our networks faster\\nand more robust. Finally, we visualized the learning of convolutional networks and\\nexplored other interesting applications of the technology.\\nImages were easy to analyze because we were able to come up with effective ways\\nto represent them as tensors. In other situations (e.g., natural language), it’s less\\nclear how one might represent our input data as tensors. To tackle this problem as\\na stepping stone to new deep learning models, we’ll develop some key concepts in\\nvector embeddings and representation learning in the next chapter.\\nSummary \\n| \\n155'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 171}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 172}, page_content='CHAPTER 8\\nEmbedding and Representation Learning\\nLearning Lower-Dimensional Representations\\nIn the previous chapter, we motivated the convolutional architecture using a simple\\nargument. The larger our input vector, the larger our model. Large models with lots\\nof parameters are expressive, but they’re also increasingly data hungry. This means\\nthat without sufficiently large volumes of training data, we will likely overfit. Convo‐\\nlutional architectures help us cope with the curse of dimensionality by reducing the\\nnumber of parameters in our models without necessarily diminishing expressiveness.\\nRegardless, convolutional networks still require large amounts of labeled training\\ndata. And for many problems, labeled data is scarce and expensive to generate.\\nOur goal in this chapter will be to develop effective learning models in situations\\nwhere labeled data is scarce, but wild, unlabeled data is plentiful. We’ll approach\\nthis problem by learning embeddings, or low-dimensional representations, in an\\nunsupervised fashion. Because these unsupervised models allow us to offload all of\\nthe heavy lifting of automated feature selection, we can use the generated embeddings\\nto solve learning problems using smaller models that require less data. This process is\\nsummarized in Figure 8-1.\\nIn the process of developing algorithms that learn good embeddings, we’ll also\\nexplore other applications of learning lower-dimensional representations, such as\\nvisualization and semantic hashing. We’ll start by considering situations where all\\nof the important information is already contained within the original input vector\\nitself. In this case, learning embeddings is equivalent to developing an effective\\ncompression algorithm.\\n157'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 173}, page_content='Figure 8-1. Using embeddings to automate feature selection in the face of scarce labeled\\ndata\\nIn the next section, we’ll introduce principal component analysis (PCA), a classic\\nmethod for dimensionality reduction. In subsequent sections, we’ll explore more\\npowerful neural methods for learning compressive embeddings.\\nPrincipal Component Analysis\\nThe basic concept behind PCA is to find a set of axes that communicates the most\\ninformation about our dataset. More specifically, if we have  d-dimensional data, we’d\\nlike to find a new set of m < d dimensions that conserves as much valuable informa‐\\ntion from the original dataset as possible. For simplicity, let’s choose d = 2, m = 1.\\nAssuming that variance corresponds to information, we can perform this transforma‐\\ntion through an iterative process. First, we find a unit vector along which the dataset\\nhas maximum variance. Because this direction contains the most information, we\\nselect this direction as our first axis. Then from the set of vectors orthogonal to\\nthis first choice, we pick a new unit vector along which the dataset has maximum\\nvariance. This is our second axis.\\nWe continue this process until we have found a total of d new vectors that represent\\nnew axes. We project our data onto this new set of axes. We then decide a good value\\nfor m and toss out all but the first m axes (the principal components, which store the\\nmost information). The result is shown in Figure 8-2.\\nFigure 8-2. An illustration of PCA for dimensionality reduction to capture the dimension\\nwith the most information (as proxied by variance)\\n158 \\n| \\nChapter 8: Embedding and Representation Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 174}, page_content='For the mathematically inclined, we can view this operation as a projection onto the\\nvector space spanned by the top m eigenvectors of the dataset’s correlation matrix,\\nwhich is equivalent to the dataset’s covariance matrix when the dataset has been\\nz-score normalized (zero-mean and unit-variance per input dimension). Let us repre‐\\nsent the dataset as a matrix X with dimensions n × d (i.e., n inputs of  d dimensions).\\nWe’d like to create an embedding matrix T with dimensions n × m. We can compute\\nthe matrix using the relationship T = X, where each column of W corresponds to an\\neigenvector of the matrix 1\\nnXΤX. Those with linear algebra background or core data\\nscience experience may be seeing a striking parallel between PCA and the singular\\nvalue decomposition (SVD), which we cover in more depth in “Theory: PCA and\\nSVD” on page 187.\\nWhile PCA has been used for decades for dimensionality reduction, it spectacularly\\nfails to capture important relationships that are piecewise linear or nonlinear. Take,\\nfor instance, the example illustrated in Figure 8-3.\\nThe example shows data points selected at random from two concentric circles. We\\nhope that PCA will transform this dataset so that we can pick a single new axis that\\nallows us to easily separate the dots. Unfortunately for us, there is no linear direction\\nthat contains more information here than another (we have equal variance in all\\ndirections). Instead, as human beings, we notice that information is being encoded in\\na nonlinear way, in terms of how far points are from the origin. With this information\\nin mind, we notice that the polar transformation (expressing points as their distance\\nfrom the origin, as the new horizontal axis, and their angle bearing from the original\\nx-axis, as the new vertical axis) does just the trick.\\nFigure 8-3 highlights the shortcomings of an approach like PCA in capturing impor‐\\ntant relationships in complex datasets. Because most of the datasets we are likely to\\nencounter in the wild (images, text, etc.) are characterized by nonlinear relationships,\\nwe must develop a theory that will perform nonlinear dimensionality reduction.\\nDeep learning practitioners have closed this gap using neural models, which we’ll\\ncover in the next section.\\nPrincipal Component Analysis \\n| \\n159'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 175}, page_content='Figure 8-3. A situation in which PCA fails to optimally transform the data for dimen‐\\nsionality reduction\\nMotivating the Autoencoder Architecture\\nWhen we talked about feed-forward networks, we discussed how each layer learned\\nprogressively more relevant representations of the input. In fact, in Chapter 7, we\\ntook the output of the final convolutional layer and used that as a lower-dimensional\\nrepresentation of the input image. Putting aside the fact that we want to generate\\nthese low-dimensional representations in an unsupervised fashion, there are funda‐\\nmental problems with these approaches in general. Specifically, while the selected\\nlayer does contain information from the input, the network has been trained to pay\\nattention to the aspects of the input that are critical to solving the task at hand. As\\na result, there’s a significant amount of information loss with respect to elements of\\nthe input that may be important for other classification tasks, but potentially less\\nimportant than the one immediately at hand.\\nHowever, the fundamental intuition here still applies. We define a new network\\narchitecture that we call the autoencoder. We first take the input and compress it into\\na low-dimensional vector. This part of the network is called the encoder because it is\\nresponsible for producing the low-dimensional embedding or code. The second part\\nof the network, instead of mapping the embedding to an arbitrary label as we would\\n160 \\n| \\nChapter 8: Embedding and Representation Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 176}, page_content='1 Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. “Reducing the Dimensionality of Data with Neural\\nNetworks.” Science 313.5786 (2006): 504-507.\\nin a feed-forward network, tries to invert the computation of the first half of the\\nnetwork and reconstruct the original input. This piece is known as the decoder. The\\noverall architecture is illustrated in Figure 8-4.\\nFigure 8-4. The autoencoder architecture attempts to construct a high-dimensional input\\ninto a low-dimensional embedding and then uses that low-dimensional embedding to\\nreconstruct the input\\nTo demonstrate the surprising effectiveness of autoencoders, we’ll build and visualize\\nthe autoencoder architecture in Figure 8-4. Specifically, we will highlight its superior\\nability to separate MNIST digits as compared to PCA.\\nImplementing an Autoencoder in PyTorch\\nThe seminal paper “Reducing the Dimensionality of Data with Neural Networks,”\\nwhich describes the autoencoder, was written by Hinton and Salakhutdinov in 2006.1\\nTheir hypothesis was that the nonlinear complexities afforded by a neural model\\nwould allow them to capture structure that linear methods, such as PCA, would\\nmiss. To demonstrate this point, they ran an experiment on MNIST using both an\\nautoencoder and PCA to reduce the dataset into 2D data points. In this section, we\\nwill recreate their experimental setup to validate this hypothesis and further explore\\nthe architecture and properties of feed-forward autoencoders.\\nThe setup shown in Figure 8-5 is built with the same principle, but the 2D embedding\\nis now treated as the input, and the network attempts to reconstruct the original\\nimage. Because we are essentially applying an inverse operation, we architect the\\ndecoder network so that the autoencoder has the shape of an hourglass. The output\\nof the decoder network is a 784-dimensional vector that can be reconstructed into a\\n28 × 28 image:\\nclass Decoder(nn.Module):\\n  def __init__(self, n_in, n_hidden_1, n_hidden_2, n_hidden_3, n_out):\\n    super(Decoder, self).__init__()\\n    self.layer1 = nn.Sequential(\\n        nn.Linear(n_in, n_hidden_1, bias=True),\\n        nn.BatchNorm1d(n_hidden_1),\\n        nn.Sigmoid())\\n    self.layer2 = nn.Sequential(\\nImplementing an Autoencoder in PyTorch \\n| \\n161'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 177}, page_content='nn.Linear(n_hidden_1, n_hidden_2, bias=True),\\n        nn.BatchNorm1d(n_hidden_2),\\n        nn.Sigmoid())\\n    self.layer3 = nn.Sequential(\\n        nn.Linear(n_hidden_2, n_hidden_3, bias=True),\\n        nn.BatchNorm1d(n_hidden_3),\\n        nn.Sigmoid())\\n    n_size = math.floor(math.sqrt(n_out))\\n    self.layer4 = nn.Sequential(\\n        nn.Linear(n_hidden_3, n_out, bias=True),\\n        nn.BatchNorm1d(n_out),\\n        nn.Sigmoid(),\\n        nn.Unflatten(1, torch.Size([1, n_size,n_size])))\\n    \\n  def forward(self, x):\\n    x = self.layer1(x)\\n    x = self.layer2(x)\\n    x = self.layer3(x)\\n    return self.layer4(x)\\nFigure 8-5. The experimental setup for dimensionality reduction of the MNIST dataset\\nemployed by Hinton and Salakhutdinov, 2006\\n162 \\n| \\nChapter 8: Embedding and Representation Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 178}, page_content='In order to accelerate training, we’ll reuse the batch normalization strategy we\\nemployed in Chapter 7. Also, because we’d like to visualize the results, we’ll avoid\\nintroducing sharp transitions in our neurons. In this example, we’ll use sigmoidal\\nneurons instead of our usual ReLU neurons:\\ndecoder = Decoder(2,250,500,1000,784)\\nFinally, we need to construct a measure (or objective function) that describes how\\nwell our model functions. Specifically, we want to measure how close the reconstruc‐\\ntion is to the original image. We can measure this simply by computing the distance\\nbetween the original 784-dimensional input and the reconstructed 784-dimensional\\noutput. More specifically, given an input vector I and a reconstruction O, we’d like\\nto minimize the value of ∥I −O ∥=\\n∑i Ii −Oi\\n2, also known as the L2 norm of\\nthe difference between the two vectors. We average this function over the whole\\nminibatch to generate our final objective function. Finally, we’ll train the network\\nusing the Adam optimizer, logging a scalar summary of the error incurred at every\\nminibatch using torch.utils.tensorboard.SummaryWriter. In PyTorch, we can\\nconcisely express the loss and training operations as follows:\\nloss_fn = nn.MSELoss()\\noptimizer = optim.Adam(decoder.parameters(),\\n                       lr = 0.001,\\n                       betas=(0.9,0.999),\\n                       eps=1e-08)\\ntrainset = datasets.MNIST(\\'.\\',\\n                          train=True,\\n                          transform=transforms.ToTensor(),\\n                          download=True)\\ntrainloader = DataLoader(trainset,\\n                         batch_size=32,\\n                         shuffle=True)\\n# Training Loop\\nNUM_EPOCHS = 5\\nfor epoch in range(NUM_EPOCHS):\\n  for input, labels in trainloader:\\n    optimizer.zero_grad()\\n    code = encoder(input)\\n    output = decoder(code)\\n    #print(input.shape, output.shape)\\n    loss = loss_fn(output, input)\\n    optimizer.step()\\n  print(f\"Epoch: {epoch} Loss: {loss}\")\\nFinally, we’ll need a method to evaluate the generalizability of our model. As usual,\\nwe’ll use a validation dataset and compute the same L2 norm measurement for model\\nevaluation. In addition, we’ll collect image summaries so that we can compare both\\nthe input images and the reconstructions:\\nImplementing an Autoencoder in PyTorch \\n| \\n163'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 179}, page_content='i = 0\\nwith torch.no_grad():\\n  for images, labels in trainloader:\\n    if i == 3:\\n      break\\n    grid = utils.make_grid(images)\\n    plt.figure()\\n    plt.imshow(grid.permute(1,2,0))\\n    \\n    code = encoder(images)\\n    output = decoder(code)\\n    \\n    grid = utils.make_grid(output)\\n    plt.figure()\\n    plt.imshow(grid.permute(1,2,0))\\n    i += 1\\nWe can visualize the model graph, the training and validation costs, and the image\\nsummaries using TensorBoard. Simply run the following command:\\n$ tensorboard --logdir ~/path/to/mnist_autoencoder_hidden=2_logs\\nThen navigate your browser to http://localhost:6006/. The results of the “Graph” tab\\nare shown in Figure 8-6.\\nThanks to how we’ve namespaced the components of our model graph, our model\\nis nicely organized. We can easily click through the components and delve deeper,\\ntracing how data flows up through the various layers of the encoder and through\\nthe decoder, how the optimizer reads the output of our training module, and how\\ngradients in turn affect all of the components of the model.\\nWe also visualize both the training (after each minibatch) and validation costs (after\\neach epoch), closely monitoring the curves for potential overfitting. The TensorBoard\\nvisualizations of the costs over the span of training are shown in Figure 8-7. As we\\nwould expect for a successful model, both the training and validation curves decrease\\nuntil they flatten off asymptotically. After approximately 200 epochs, we attain a\\nvalidation cost of 4.78. While the curves look promising, it’s difficult, upon first\\nglance, to understand whether we’ve reached a plateau at a “good” cost, or whether\\nour model is still doing a poor job of reconstructing the original inputs.\\n164 \\n| \\nChapter 8: Embedding and Representation Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 180}, page_content='Figure 8-6. TensorBoard allows us to neatly view the high-level components and data\\nflow of our computation graph (top) and also click through to more closely inspect the\\ndata flows of individual subcomponents (bottom)\\nImplementing an Autoencoder in PyTorch \\n| \\n165'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 181}, page_content='Figure 8-7. The cost incurred on the training set (logged after each minibatch) and on\\nthe validation set (logged after each epoch)\\nTo get a sense of what that means, let’s explore the MNIST dataset. We pick an\\narbitrary image of a 1 from the dataset and call it X. In Figure 8-8, we compare the\\nimage to all other images in the dataset. Specifically, for each digit class, we compute\\nthe average of the L2 costs, comparing X to each instance of the digit class. As a visual\\naid, we also include the average of all of the instances for each digit class.\\n166 \\n| \\nChapter 8: Embedding and Representation Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 182}, page_content='Figure 8-8. The image of the 1 on the left is compared to all of the other digits in the\\nMNIST dataset; each digit class is represented visually with the average of all of its\\nmembers and labeled with the average of the L2 costs, comparing the 1 on the left with\\nall of the class members\\nOn average, X is 5.75 units away from other 1s in MNIST. In terms of L2 distance, the\\nnon-1 digits closest to the X are the 7s (8.94 units) and the digits farthest are the 0s\\n(11.05 units). Given these measurements, it’s quite apparent that with an average cost\\nof 4.78, our autoencoder is producing high-quality reconstructions.\\nBecause we are collecting image summaries, we can confirm this hypothesis directly\\nby inspecting the input images and reconstructions directly. The reconstructions for\\nthree randomly chosen samples from the test set are shown in Figure 8-9.\\nFigure 8-9. A side-by-side comparison of the original inputs (from the validation set)\\nand reconstructions after 5, 100, and 200 epochs of training\\nImplementing an Autoencoder in PyTorch \\n| \\n167'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 183}, page_content='After five epochs, we can start to make out some of the critical strokes of the\\noriginal image that are being picked by the autoencoder, but for the most part, the\\nreconstructions are still hazy mixtures of closely related digits. By 100 epochs, the\\n0 and 4 are reconstructed with strong strokes, but it looks like the autoencoder is\\nstill having trouble differentiating between 5s, 3s, and possibly 8s. However, by 200\\nepochs, it’s clear that even this more difficult ambiguity is clarified, and all of the\\ndigits are crisply reconstructed.\\nFinally, we’ll complete the section by exploring the 2D codes produced by traditional\\nPCA and autoencoders. We’ll want to show that autoencoders produce better visuali‐\\nzations. In particular, we’ll want to show that autoencoders do a much better job of\\nvisually separating instances of different digit classes than PCA. We’ll start by quickly\\ncovering the code we use to produce 2D PCA codes:\\nfrom sklearn import decomposition\\nimport input_data\\nmnist = input_data.read_data_sets(\"data/\", one_hot=False)\\npca = decomposition.PCA(n_components=2)\\npca.fit(mnist.train.images)\\npca_codes = pca.transform(mnist.test.images)\\nWe first pull up the MNIST dataset. We’ve set the flag one_hot=False because we’d\\nlike the labels to be provided as integers instead of one-hot vectors (as a quick\\nreminder, a one-hot vector representing an MNIST label would be a vector of size 10\\nwith the itℎ component set to one to represent digit i and the rest of the components\\nset to zero). We use the commonly used machine learning library scikit-learn to\\nperform the PCA, setting the n_components=2 flat so that scikit-learn knows to\\ngenerate 2D codes. We can also reconstruct the original images from the 2D codes\\nand visualize the reconstructions:\\nfrom matplotlib import pyplot as plt\\npca_recon = pca.inverse_transform(pca_codes[:1])\\nplt.imshow(pca_recon[0].reshape((28,28)), cmap=plt.cm.gray)\\nplt.show()\\n168 \\n| \\nChapter 8: Embedding and Representation Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 184}, page_content='The code snippet shows how to visualize the first image in the test dataset, but we\\ncan easily modify the code to visualize any arbitrary subset of the dataset. Comparing\\nthe PCA reconstructions to the autoencoder reconstructions in Figure 8-10, it’s quite\\nclear that the autoencoder vastly outperforms PCA with 2D codes. In fact, the PCA’s\\nperformance is somewhat reminiscent of the autoencoder only five epochs into\\ntraining. It has trouble distinguishing 5s from 3s and 8s, 0s from 8s, and 4s from\\n9s. Repeating the same experiment with 30-dimensional codes provides significant\\nimprovement to the PCA reconstructions, but they are still significantly worse than\\nthe 30-dimensional autoencoder.\\nFigure 8-10. Comparing the reconstructions by both PCA and autoencoder side by side\\nNow, to complete the experiment, we must load up a saved PyTorch model, retrieve\\nthe 2D codes, and plot both the PCA and autoencoder codes. We’re careful to rebuild\\nthe PyTorch graph exactly how we set it up during training. We pass the path to\\nthe model checkpoint we saved during training as a command-line argument to the\\nscript. Finally, we use a custom plotting function to generate a legend and appropri‐\\nately color data points of different digit classes.\\nIn the resulting visualization in Figure 8-11, it is extremely difficult to make out sepa‐\\nrable clusters in the 2D PCA codes; the autoencoder has clearly done a spectacular\\njob at clustering codes of different digit classes. This means that a simple machine\\nlearning model is going to be able to much more effectively classify data points\\nconsisting of autoencoder embeddings as compared to PCA embeddings.\\nImplementing an Autoencoder in PyTorch \\n| \\n169'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 185}, page_content='Figure 8-11. 2D embeddings produced by PCA (top) and by an autoencoder (bottom)\\nIn this section, we successfully set up and trained a feed-forward autoencoder and\\ndemonstrated that the resulting embeddings were superior to PCA, a classical dimen‐\\nsionality reduction method. In the next section, we’ll explore a concept known as\\ndenoising, which acts as a form of regularization by making our embeddings more\\nrobust.\\n170 \\n| \\nChapter 8: Embedding and Representation Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 186}, page_content='2 Vincent, Pascal, et al. “Extracting and Composing Robust Features with Denoising Autoencoders.” Proceedings\\nof the 25th International Conference on Machine Learning. ACM, 2008.\\nDenoising to Force Robust Representations\\nDenoising improves the ability of the autoencoder to generate embeddings that are\\nresistant to noise. The human ability for perception is surprisingly resistant to noise.\\nTake Figure 8-12, for example. Despite the fact that I’ve corrupted half of the pixels\\nin each image, you still have no problem making out the digit. In fact, even easily\\nconfused digits (like the 2 and the 7) are still distinguishable.\\nFigure 8-12. Human perception allows us to identify even obscured digits\\nOne way to look at this phenomenon is probabilistically. Even if we’re exposed to\\na random sampling of pixels from an image, if we have enough information, our\\nbrain is still capable of concluding the ground truth of what the pixels represent with\\nmaximal probability. Our mind is able to, quite literally, fill in the blanks to draw a\\nconclusion. Even though only a corrupted version of a digit hits our retina, our brain\\nis still able to reproduce the set of activations (i.e., the code or embedding) that we\\nnormally would use to represent the image of that digit. This is a property we might\\nhope to enforce in our embedding algorithm, and it was first explored by Vincent et\\nal. in 2008, when they introduced the denoising autoencoder.2\\nThe basic principles behind denoising are quite simple. We corrupt some fixed\\npercentage of the pixels in the input image by setting them to zero. Given an original\\ninput X, let’s call the corrupted version C X . The denoising autoencoder is identical\\nto the vanilla autoencoder except for one detail: the input to the encoder network\\nis the corrupted C X  instead of X. In other words, the autoencoder is forced to\\nlearn a code for each input that is resistant to the corruption mechanism and is able\\nDenoising to Force Robust Representations \\n| \\n171'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 187}, page_content='3 Bengio, Yoshua, et al. “Generalized Denoising Auto-Encoders as Generative Models.” Advances in Neural\\nInformation Processing Systems. 2013.\\nto interpolate through the missing information to recreate the original, uncorrupted\\nimage.\\nWe can also think about this process more geometrically. Let’s say we had a 2D\\ndataset with various labels. Let’s take all of the data points in a particular category\\n(i.e., with some fixed label), and call this subset of data points S. While any arbitrary\\nsampling of points could end up taking any form while visualized, we presume that\\nfor real-life categories, there is some underlying structure that unifies all of the points\\nin S. This underlying, unifying geometric structure is known as a manifold. The\\nmanifold is the shape that we want to capture when we reduce the dimensionality\\nof our data; and as Bengio et al. described in 2013, our autoencoder is implicitly\\nlearning this manifold as it learns how to reconstruct data after pushing it through\\na bottleneck (the code layer).3 The autoencoder must figure out whether a point\\nbelongs to one manifold or another when trying to generate a reconstruction of an\\ninstance with potentially different labels.\\nAs an illustration, let’s consider the scenario in Figure 8-13, where the points in S are\\na simple low-dimensional manifold (a solid circle in the diagram). In part A, we see\\nour data points in S (black xs) and the manifold that best describes them. We also\\nobserve an approximation of our corruption operation. Specifically, the arrow and\\nnonconcentric circle demonstrate all the ways in which the corruption could possibly\\nmove or modify a data point. Given that we are applying this corruption operation to\\nevery data point (i.e., along the entire manifold), this corruption operation artificially\\nexpands the dataset to not only include the manifold but also all of the points\\nin space around the manifold, up to a maximum margin of error. This margin is\\ndemonstrated by the dashed circles in A, and the dataset expansion is illustrated by\\nthe x’s in part B. Finally the autoencoder is forced to learn to collapse all of the data\\npoints in this space back to the manifold. In other words, by learning which aspects\\nof a data point are generalizable, broad strokes, and which aspects are “noise,” the\\ndenoising autoencoder learns to approximate the underlying manifold of S.\\n172 \\n| \\nChapter 8: Embedding and Representation Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 188}, page_content='Figure 8-13. The denoising objective enables our model to learn the manifold (dark\\ncircle) by learning to map corrupted data (light x’s in B and C) to uncorrupted data\\n(dark x’s) by minimizing the error (arrows in C) between their representations\\nWith the philosophical motivations of denoising in mind, we can now make a small\\nmodification to our autoencoder script to build a denoising autoencoder:\\ndef corrupt_input(x):\\n    corrupting_matrix = 2.0*torch.rand_like(x)\\n    \\n    return x * corrupting_matrix\\n# x = mnist data image of shape 28*28=784\\nx = torch.rand((28,28))\\ncorrupt = 1.0 # set to 1.0 to corrupt input\\nc_x = (corrupt_input(x) * corrupt) + (x * (1 - corrupt))\\nThis code snippet corrupts the input if the corrupt variable is equal to 1, and it\\nrefrains from corrupting the input if the corrupt variable is equal to 0. After making\\nthis modification, we can rerun our autoencoder, resulting in the reconstructions\\nshown in Figure 8-14. It’s quite apparent that the denoising autoencoder has faithfully\\nreplicated our incredible human ability to fill in the missing pixels.\\nDenoising to Force Robust Representations \\n| \\n173'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 189}, page_content='Figure 8-14. We apply a corruption operation to the dataset and train a denoising\\nautoencoder to reconstruct the original, uncorrupted images\\nSparsity in Autoencoders\\nOne of the most difficult aspects of deep learning is a problem known as interpreta‐\\nbility. Interpretability is a property of a machine learning model that measures how\\neasy it is to inspect and explain its process and/or output. Deep models are generally\\ndifficult to interpret because of the nonlinearities and massive numbers of parameters\\nthat make up a model. While deep models are generally more accurate, a lack of\\ninterpretability often hinders their adoption in highly valuable, but highly risky,\\napplications. For example, if a machine learning model is predicting that a patient has\\nor does not have cancer, the doctor will likely want an explanation to confirm the\\nmodel’s conclusion.\\nWe can address one aspect of interpretability by exploring the characteristics of the\\noutput of an autoencoder. In general, an autoencoder’s representations are dense, and\\nthis has implications with respect to how the representation changes as we make\\ncoherent modifications to the input. Consider the situation in Figure 8-15.\\n174 \\n| \\nChapter 8: Embedding and Representation Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 190}, page_content='Figure 8-15. The activations of a dense representation combine and overlay information\\nfrom multiple features in ways that are difficult to interpret\\nThe autoencoder produces a dense representation, that is, the representation of the\\noriginal image is highly compressed. Because we have only so many dimensions\\nto work with in the representation, the activations of the representation combine\\ninformation from multiple features in ways that are extremely difficult to disentangle.\\nThe result is that as we add components or remove components, the output represen‐\\ntation changes in unexpected ways. It’s virtually impossible to interpret how and why\\nthe representation is generated in the way it is.\\nThe ideal outcome for us is if we can build a representation where there is a 1-to-1\\ncorrespondence, or close to a 1-to-1 correspondence, between high-level features and\\nindividual components in the code. When we are able to achieve this, we get very\\nclose to the system described in Figure 8-16, which shows how the representation\\nchanges as we add and remove components. The representation is the sum of the\\nindividual strokes in the image. With the right combination of space and sparsity, a\\nrepresentation is more interpretable.\\nFigure 8-16. How activations in the representation change with the addition and\\nremoval of strokes\\nSparsity in Autoencoders \\n| \\n175'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 191}, page_content='4 Ranzato, Marc’Aurelio, et al. “Efficient Learning of Sparse Representations with an Energy-Based Model.”\\nProceedings of the 19th International Conference on Neural Information Processing Systems. MIT Press, 2006.\\n5 Ranzato, Marc’Aurelio, and Martin Szummer. “Semi-supervised Learning of Compact Document Representa‐\\ntions with Deep Networks.” Proceedings of the 25th International Conference on Machine Learning. ACM, 2008.\\n6 Makhzani, Alireza, and Brendan Frey. “k-Sparse Autoencoders.” arXiv preprint arXiv:1312.5663 (2013).\\nWhile this is the ideal outcome, we’ll have to think through what mechanisms we can\\nleverage to enable this interpretability in the representation. The issue here is clearly\\nthe bottlenecked capacity of the code layer; but unfortunately, increasing the capacity\\nof the code layer alone is not sufficient. In the medium case, while we can increase\\nthe size of the code layer, there is no mechanism that prevents each individual feature\\npicked up by the autoencoder from affecting a large fraction of the components with\\nsmaller magnitudes. In the more extreme case, where the features that are picked up\\nare more complex and therefore more bountiful, the capacity of the code layer may\\nbe even larger than the dimensionality of the input. In this case, the code layer has so\\nmuch capacity that the model could quite literally perform a “copy” operation where\\nthe code layer learns no useful representation.\\nWhat we really want is to force the autoencoder to utilize as few components of the\\nrepresentation vector as possible, while still effectively reconstructing the input. This\\nis similar to the rationale behind using regularization to prevent overfitting in simple\\nneural networks, as we discussed in Chapter 4, except we want as many components\\nto be zero (or extremely close to zero) as possible. As in Chapter 4, we’ll achieve this\\nby modifying the objective function with a sparsity penalty, which increases the cost\\nof any representation that has a large number of nonzero components:\\nESparse = E + β · SparsityPenalty\\nThe value of β determines how strongly we favor sparsity at the expense of generating\\nbetter reconstructions. For the mathematically inclined, you would do this by treating\\nthe values of each of the components of every representation as the outcome of a\\nrandom variable with an unknown mean. We would then employ a measure of diver‐\\ngence comparing the distribution of observations of this random variable (the values\\nof each component) and the distribution of a random variable whose mean is known\\nto be 0. A measure that is often used to this end is the Kullback-Leibler (often referred\\nto as KL) divergence. Further discussion on sparsity in autoencoders is beyond the\\nscope of this text, but they are covered by Ranzato et al. (20074 and 20085). More\\nrecently, the theoretical properties and empirical effectiveness of introducing an\\nintermediate function before the code layer that zeroes out all but k of the maximum\\nactivations in the representation were investigated by Makhzani and Frey (2014).6\\nThese k-Sparse autoencoders were shown to be just as effective as other mechanisms\\n176 \\n| \\nChapter 8: Embedding and Representation Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 192}, page_content='of sparsity despite being shockingly simple to implement and understand (as well as\\ncomputationally more efficient).\\nThis concludes our discussion of autoencoders. We’ve explored how we can use\\nautoencoders to find strong representations of data points by summarizing their con‐\\ntent. This mechanism of dimensionality reduction works well when the independent\\ndata points are rich and contain all of the relevant information pertaining to their\\nstructure in their original representation. In the next section, we’ll explore strategies\\nthat we can use when the main source of information is in the context of the data\\npoint instead of the data point itself.\\nWhen Context Is More Informative than the Input Vector\\nSo far, we’ve mostly focused on the concept of dimensionality reduction. In dimen‐\\nsionality reduction, we generally have rich inputs that contain lots of noise on top of\\nthe core, structural information that we care about. In these situations, we want to\\nextract this underlying information while ignoring the variations and noise that are\\nextraneous to this fundamental understanding of the data.\\nIn other situations, we have input representations that say very little at all about\\nthe content that we are trying to capture. In these situations, our goal is not to\\nextract information but rather to gather information from context to build useful\\nrepresentations. All of this probably sounds too abstract to be useful at this point, so\\nlet’s concretize these ideas with a real example.\\nBuilding models for language is a tricky business. The first problem we have to over‐\\ncome when building language models is finding a good way to represent individual\\nwords. At first glance, it’s not entirely clear how one builds a good representation.\\nLet’s start with the naive approach, considering Figure 8-17.\\nFigure 8-17. Generating one-hot vector representations for words using a simple\\ndocument\\nWhen Context Is More Informative than the Input Vector \\n| \\n177'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 193}, page_content='If a document has a vocabulary V with V  words, we can represent the words with\\none-hot vectors. We have V -dimensional representation vectors, and we associate\\neach unique word with an index in this vector. To represent unique word wi, we set\\nthe itℎ component of the vector to be 1, and zero out all of the other components.\\nHowever, this representation scheme seems rather arbitrary. This vectorization does\\nnot make similar words into similar vectors. This is problematic, because we’d like\\nour models to know that the words “jump” and “leap” have similar meanings. Simi‐\\nlarly, we’d like our models to know when words are verbs or nouns or prepositions.\\nThe naive one-hot encoding of words to vectors does not capture any of these\\ncharacteristics. To address this challenge, we’ll need to find some way of discovering\\nthese relationships and encoding this information into a vector.\\nIt turns out that one way to discover relationships between words is by analyzing\\ntheir surrounding context. For example, synonyms such as “jump” and “leap” can be\\nused interchangeably in their respective contexts. In addition, both words generally\\nappear when a subject is performing the action over a direct object. We use this\\nprinciple all the time when we run across new vocabulary while reading. For example,\\nif we read the sentence “The warmonger argued with the crowd,” we can immediately\\ndraw conclusions about the word “warmonger” even if we don’t already know the\\ndictionary definition. In this context, “warmonger” precedes a word we know to be\\na verb, which makes it likely that “warmonger” is a noun and the subject of this\\nsentence. Also, the “warmonger” is “arguing,” which might imply that a “warmonger”\\nis generally a combative or argumentative individual. Overall, as illustrated in Fig‐\\nure 8-18, by analyzing the context (i.e., a fixed window of words surrounding a target\\nword), we can quickly surmise the meaning of the word.\\nFigure 8-18. Analyzing context to determine a word’s meaning\\nIt turns out we can use the same principles we used when building the autoencoder\\nto build a network that builds strong, distributed representations. Two strategies are\\nshown in Figure 8-19. One possible method (shown in A) passes the target through\\nan encoder network to create an embedding. Then we have a decoder network take\\nthis embedding; but instead of trying to reconstruct the original input as we did with\\n178 \\n| \\nChapter 8: Embedding and Representation Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 194}, page_content='7 Mikolov, Tomas, et al. “Distributed Representations of Words and Phrases and their Compositionality.”\\nAdvances in Neural Information Processing Systems. 2013.\\n8 Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. “Efficient Estimation of Word Representations in\\nVector Space.” ICLR Workshop, 2013.\\nthe autoencoder, the decoder attempts to construct a word from the context. The\\nsecond possible method (shown in B) does exactly the reverse: the encoder takes a\\nword from the context as input, producing the target.\\nFigure 8-19. General architectures for designing encoders and decoders that generate\\nembeddings by mapping words to their respective contexts (A) or vice versa (B)\\nIn the next section, we’ll describe how we use this strategy (along with some slight\\nmodifications for performance) to produce word embeddings in practice.\\nThe Word2Vec Framework\\nWord2Vec, a framework for generating word embeddings, was pioneered by Mikolov\\net al. The original paper detailed two strategies for generating embeddings, similar to\\nthe two strategies for encoding context we discussed in the previous section.\\nThe first flavor of Word2Vec that Mikolov et al. introduced was the Continuous Bag\\nof Words (CBOW) model.7 This model is much like strategy B from Figure 8-19. The\\nCBOW model used the encoder to create an embedding from the full context (treated\\nas one input) and predict the target word. It turns out this strategy works best for\\nsmaller datasets, an attribute that is further discussed in the original paper.\\nThe second flavor of Word2Vec is the Skip-Gram model, introduced by Mikolov et al.8\\nThe Skip-Gram model does the inverse of CBOW, taking the target word as an input,\\nand then attempting to predict one of the words in the context. Let’s walk through a\\ntoy example to explore what the dataset for a Skip-Gram model looks like.\\nConsider the sentence “the boy went to the bank.” If we broke this sentence down into\\na sequence of (context, target) pairs, we would obtain [([the, went], boy), ([boy, to],\\nwent), ([went, the], to), ([to, bank], the)]. Taking this a step further, we have to split\\neach (context, target) pair into (input, output) pairs where the input is the target and\\nThe Word2Vec Framework \\n| \\n179'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 195}, page_content='the output is one of the words from the context. From the first pair ([the, went], boy),\\nwe would generate the two pairs (boy, the) and (boy, went). We continue to apply this\\noperation to every (context, target) pair to build our dataset. Finally, we replace each\\nword with its unique index i ∈0, 1, ..., V\\n−1  corresponding to its index in the\\nvocabulary.\\nThe structure of the encoder is surprisingly simple. It is essentially a lookup table with\\nV  rows, where the itℎ row is the embedding corresponding to the itℎ vocabulary\\nword. All the encoder has to do is take the index of the input word and output the\\nappropriate row in the lookup table. This an efficient operation because on a GPU,\\nthis operation can be represented as a product of the transpose of the lookup table\\nand the one-hot vector representing the input word. We can implement this simply in\\nPyTorch with the following PyTorch function:\\nemb = nn.Embedding(10, 100)\\nx = torch.tensor([0])\\nout = emb(x)\\nWhere out is the embedding matrix, and x is a tensor of indices we want to look\\nup. For information on optional parameters, we refer you to the PyTorch API\\ndocumentation.\\nThe decoder is slightly trickier because we make some modifications for perfor‐\\nmance. The naive way to construct the decoder would be to attempt to reconstruct\\nthe one-hot encoding vector for the output, which we could implement with a\\nrun-of-the-mill feed-forward layer coupled with a softmax. The only concern is that\\nit’s inefficient because we have to produce a probability distribution over the whole\\nvocabulary space.\\nTo reduce the number of parameters, Mikolov et al. used a strategy for implementing\\nthe decoder known as noise-contrastive estimation (NCE). The strategy is illustrated\\nin Figure 8-20. A binary logistic regression compares the embedding of the target\\nwith the embedding of a context word and randomly sampled noncontext words. We\\nconstruct a loss function describing how effectively the embeddings enable identifica‐\\ntion of words in the context of the target versus words outside the context of the\\ntarget.\\n180 \\n| \\nChapter 8: Embedding and Representation Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 196}, page_content='Figure 8-20. The NCE strategy\\nThe NCE strategy uses the lookup table to find the embedding for the output, as\\nwell as embeddings for random selections from the vocabulary that are not in the\\ncontext of the input. We then employ a binary logistic regression model that, one\\nat a time, takes the input embedding and the embedding of the output or random\\nselection, and then outputs a value between 0 to 1 corresponding to the probability\\nthat the comparison embedding represents a vocabulary word present in the input’s\\ncontext. We then take the sum of the probabilities corresponding to the noncontext\\ncomparisons and subtract the probability corresponding to the context comparison.\\nThis value is the objective function that we want to minimize (in the optimal scenario\\nwhere the model has perfect performance, the value will be –1).\\nAn example of implementing NCE in PyTorch can be found on GitHub.\\nWhile Word2Vec is admittedly not a deep machine learning model, we discuss it here\\nfor many reasons. First, it thematically represents a strategy (finding embeddings\\nThe Word2Vec Framework \\n| \\n181'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 197}, page_content='using context) that generalizes to many deep learning models. When we learn about\\nmodels for sequence analysis in Chapter 9, we’ll see this strategy employed for gener‐\\nating skip-thought vectors to embed sentences. Moreover, when we start building\\nmore and more models for language starting in Chapter 9, we’ll find that using\\nWord2Vec embeddings instead of one-hot vectors to represent words will yield far\\nsuperior results.\\nNow that we understand how to architect the Skip-Gram model and its importance,\\nwe can start implementing it in PyTorch.\\nImplementing the Skip-Gram Architecture\\nTo build the dataset for our Skip-Gram model, we’ll utilize a modified version of the\\nPyTorch Word2Vec data reader in input_word_data.py. We’ll start off by setting a\\ncouple of important parameters for training and regularly inspecting our model. Of\\nparticular note, we employ a minibatch size of 32 examples and train for 5 epochs\\n(full passes through the dataset). We’ll use embeddings of size 128. We’ll use a context\\nwindow of five words to the left and to the right of each target word, and sample four\\ncontext words from this window. Finally, we’ll use 64 randomly chosen noncontext\\nwords for NCE.\\nImplementing the embedding layer is not particularly complicated. We merely have\\nto initialize the lookup table with a matrix of values:\\nvocab_size = 500\\nemb_vector_len = 128\\nembedding = nn.Embedding(num_embeddings = vocab_size,\\n                         embedding_dim = emb_vector_len)\\nPyTorch does not currently have a built-in NCE loss function. However, there are\\nsome implementations on the internet.  One example is the info-nce-pytorch library:\\npip install info-nce-pytorch\\nWe utilize InfoNCE to compute the NCE cost for each training example, and then\\ncompile all of the results in the minibatch into a single measurement:\\nloss = InfoNCE()\\nbatch_size, embedding_size = 32, 128\\nquery = embedding(outputs)\\npositive_key = embedding(targets)\\noutput = loss(query, positive_key)\\nNow that we have our objective function expressed as a mean of the NCE costs, we\\nset up the training as usual. Here, we follow in the footsteps of Mikolov et al. and\\nemploy stochastic gradient descent with a learning rate of 0.1:\\n182 \\n| \\nChapter 8: Embedding and Representation Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 198}, page_content=\"optimizer = optim.SGD(embedding.parameters(),\\n                      lr = 0.1)\\ndef train(inputs, targets, embedding):\\n  optimizer.zero_grad()\\n  input_emb = embedding(inputs)\\n  target_emb = embedding(targets)\\n  loss = loss_fn(input_emb, target_emb)\\n  loss.backward()\\n  optimizer.step()\\n  return loss\\nWe also inspect the model regularly using a validation function, which normalizes the\\nembeddings in the lookup table and uses cosine similarity to compute distances for a\\nset of validation words from all other words in the vocabulary:\\ncosine_similarity = nn.CosineSimilarity()\\ndef evaluate(inputs, targets, embedding):\\n  with torch.no_grad():\\n    input_emb = embedding(inputs)\\n    target_emb = embedding(targets)\\n    norm = torch.sum(input_emb, dim=1)\\n    normalized = input_emb/norm\\n    score = cosine_similarity(normalized, target_emb)\\n    return normalized, score\\nPutting all of these components together, we’re finally ready to run the Skip-Gram\\nmodel. We skim over this portion of the code because it is very similar to how we\\nconstructed models in the past. The only difference is the additional code during the\\ninspection step. We randomly select 20 validation words out of the 500 most common\\nwords in our vocabulary of 10,000 words. For each of these words, we use the cosine\\nsimilarity function we built to find the nearest neighbors:\\nn_epochs=1\\nfor epoch in range(n_epochs):\\n  # Train\\n  running_loss = 0.0\\n  for inputs, targets in trainloader:\\n    loss = train(inputs, targets)\\n    running_loss += loss.item()\\n  \\n  writer.add_scalar('Train Loss',\\n                    running_loss/len(trainloader), epoch)\\n  #Validate\\n  running_score = 0.0\\n  for inputs, targets in valloader:\\n    _, score = evaluate(inputs, targets)\\n    running_score += score\\n  \\n  writer.add_scalar('Val Score',\\n                    running_score/len(valloader), epoch)\\nImplementing the Skip-Gram Architecture \\n| \\n183\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 199}, page_content='The code starts to run, and we can begin to see how the model evolves over time.\\nAt the beginning, the model does a poor job of embedding (as is apparent from\\nthe inspection step). However, by the time training completes, the model has clearly\\nfound representations that effectively capture the meanings of individual words:\\nancient: egyptian, cultures, mythology, civilization, etruscan, \\ngreek, classical, preserved\\nhowever: but, argued, necessarily, suggest, certainly, nor, \\nbelieve, believed\\ntype: typical, kind, subset, form, combination, single, \\ndescription, meant\\nwhite: yellow, black, red, blue, colors, grey, bright, dark\\nsystem: operating, systems, unix, component, variant, versions, \\nversion, essentially\\nenergy: kinetic, amount, heat, gravitational, nucleus, \\nradiation, particles, transfer\\nworld: ii, tournament, match, greatest, war, ever, championship, \\ncold\\ny: z, x, n, p, f, variable, mathrm, sum,\\nline: lines, ball, straight, circle, facing, edge, goal, yards,\\namong: amongst, prominent, most, while, famous, particularly, \\nargue, many\\nimage: png, jpg, width, images, gallery, aloe, gif, angel\\nkingdom: states, turkey, britain, nations, islands, namely, \\nireland, rest\\nlong: short, narrow, thousand, just, extended, span, length, \\nshorter\\nthrough: into, passing, behind, capture, across, when, apart, \\ngoal\\ni: you, t, know, really, me, want, myself, we\\nsource: essential, implementation, important, software, content, \\ngenetic, alcohol, application\\nbecause: thus, while, possibility, consequently, furthermore, \\nbut, certainly, moral\\neight: six, seven, five, nine, one, four, three, b\\n184 \\n| \\nChapter 8: Embedding and Representation Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 200}, page_content=\"french: spanish, jacques, pierre, dutch, italian, du, english, \\nbelgian\\nwritten: translated, inspired, poetry, alphabet, hebrew, \\nletters, words, read\\nWhile not perfect, there are some strikingly meaningful clusters captured here. Num‐\\nbers, countries, and cultures are clustered close together. The pronoun “I” is clustered\\nwith other pronouns. The word “world” is interestingly close to both “championship”\\nand “war.” And the word “written” is found to be similar to “translated,” “poetry,”\\n“alphabet,” “letters,” and “words.”\\nFinally, we conclude this section by visualizing our word embeddings in Figure 8-21.\\nTo display our 128-dimensional embeddings in 2D space, we’ll use a visualization\\nmethod known as t-SNE. If you’ll recall, we also used t-SNE in Chapter 7 to visualize\\nthe relationships between images in ImageNet. Using t-SNE is quite simple, as it has a\\nbuilt-in function in the commonly used machine learning library scikit-learn.\\nWe can construct the visualization using the following code:\\ntsne = TSNE(perplexity=30, n_components=2, init='pca', \\n            n_iter=5000)\\nplot_embeddings = np.asfarray(final_embeddings[:plot_num,:], \\n                              dtype='float')\\nlow_dim_embs = tsne.fit_transform(plot_embeddings)\\nlabels = [reverse_dictionary[i] for i in xrange(plot_only)]\\ndata.plot_with_labels(low_dim_embs, labels)\\nIn Figure 8-21, we notice that similar concepts are closer together than disparate\\nconcepts, indicating that our embeddings encode meaningful information about the\\nfunctions and definitions of individual words.\\nImplementing the Skip-Gram Architecture \\n| \\n185\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 201}, page_content='Figure 8-21. Skip-Gram embeddings using t-SNE\\nFor a more detailed exploration of the properties of word embeddings and interesting\\npatterns (verb tenses, countries and capitals, analogy completion, etc.), we refer you\\nto the original Mikolov et al. paper.\\n186 \\n| \\nChapter 8: Embedding and Representation Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 202}, page_content='Theory: PCA and SVD\\nThose who have taken any form of applied linear algebra are probably familiar with\\nthe SVD, one of the most important matrix factorizations in all of linear algebra.\\nFor those uninitiated with the SVD, I will first explain the key concepts behind it\\n(assuming some prior linear algebra knowledge) before jumping into its relationship\\nwith PCA.\\nThe \\nSVD \\nstates \\nthat \\nany \\nmatrix \\nM \\nwith \\ndimension \\nm by n \\ncan\\nbe \\nfactorized \\ninto \\nthe \\nform \\nUΣV ⊺ \\n(where \\n⊺ \\nrepresents \\nthe \\ntrans‐\\npose operation) with U of dimension m by m, Σ of dimension m by n,\\nand V of dimension n by n . The matrices U and V are both orthogonal matri‐\\nces. Orthogonal matrices are square matrices made up of orthonormal column vec‐\\ntors. An important fact about orthogonal matrices is that their transposes are also\\northogonal matrices, so V ⊺ in the decomposition is still orthogonal. In addition, the\\ntranspose of an orthogonal matrix is its inverse, so we have U ⊺U = UU ⊺= Im and\\nV ⊺V = VV ⊺= In. Σ is a rectangular diagonal matrix with only nonnegative entries\\nalong its diagonal, which are termed the singular values of the M. Although the SVD\\nitself is not unique, the singular values of a matrix are. If we inspect the product Σx,\\nwhere x is any random vector, we note that Σ simply acts as a scaling factor for each\\ndimension of x due to Σ being a diagonal matrix (and when rectangular diagonal,\\neither adds dimensions with value 0 when tall or removes dimensions when wide).\\nAnother important and potentially more nonobvious property of orthogonal matrices\\nis that they preserve the length, or L2 norm, of any vector they are multiplied by\\n(this is left as an exercise for you). Orthogonal matrices can change only a vector’s\\norientation, and thus, we characterize the action of an orthogonal matrix upon a\\nvector as a rotation. We call norms such as the L2 norm rotationally invariant for this\\nreason. One famous example of an orthogonal matrix you’re already familiar with is\\nthe identity matrix I—this matrix maps any vector to itself, so we can think of it as a\\nrotation of 0.\\nTo understand the SVD more intuitively, let’s imagine the matrix-vector product Mx\\ndecomposed as UΣV ⊺x. Based on our discussion so far, we can see that the action\\nof the matrix M upon x can be decomposed into a rotation, followed by a scaling,\\nfollowed by another rotation.\\nNow that we have an intuitive understanding of SVD, let’s connect it back to the PCA\\nalgorithm presented in the main text. Let’s assume we have a data matrix X, which\\nis of dimension d by n, where d represents the number of features per datapoint\\nand n represents the number of datapoints. Again, we assume for simplicity that\\nthe rows of X have been z-score normalized. The PCA algorithm can be reduced\\nto taking the eigendecomposition of the correlation matrix 1\\nnXX ⊺, which we will\\nImplementing the Skip-Gram Architecture \\n| \\n187'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 203}, page_content='represent as PDP⊺. The matrix of eigenvalues D is a diagonal matrix, while the\\nmatrix P is a matrix of the corresponding eigenvectors as columns. Generally, the\\neigendecomposition of a matrix looks like PDP−1, but here the correlation matrix\\nis symmetric so the eigenvectors are orthogonal (we leave this as an exercise for\\nyou). Thus, we can represent P as an orthogonal matrix once the eigenvectors are\\nnormalized to unit length, at which point the inverse and transpose are equal.\\nInstead of working with the correlation matrix, let’s instead work with the data matrix\\nfirst and then move to the correlation matrix. We first represent X as UΣV ⊺. Now, we\\nexpress the correlation matrix in terms of components of the SVD:\\n1\\nnXX ⊺= 1\\nnUΣV ⊺VΣ⊺U ⊺\\n= 1\\nnUΣ2U ⊺\\nWe already see the obvious parallels to the correlation matrix’s eigendecomposition:\\nU is orthogonal, and the square of the singular value matrix is also a diagonal matrix\\n(this matrix is just the diagonal matrix of the squares of all the singular values). One\\ncan show that U’s columns, also termed the left singular vectors, are the eigenvectors\\nof the correlation matrix. Imagine multiplying 1\\nnUΣ2U ⊺ by Uei, where ei is a vector\\nof all zeroes except for a one at the index corresponding to any single column (Uei is\\nthe ith column of U). In practice, it is actually ideal to use the SVD of the data matrix\\nrather than take the eigendecomposition of the correlation matrix due to precision\\nissues when calculating the correlation matrix, which is a product of two potentially\\nvery large matrices.\\nSummary\\nIn this chapter, we explored various methods in representation learning. We learned\\nabout how we can perform effective dimensionality reduction using autoencoders.\\nWe also learned about denoising and sparsity, which augment autoencoders with\\nuseful properties. After discussing autoencoders, we shifted our attention to repre‐\\nsentation learning when context of an input is more informative than the input itself.\\nWe learned how to generate embeddings for English words using the Skip-Gram\\nmodel, which will prove useful as we explore deep learning models for understanding\\nlanguage. In the next chapter, we will build on this tangent to analyze language and\\nother sequences using deep learning.\\n188 \\n| \\nChapter 8: Embedding and Representation Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 204}, page_content='CHAPTER 9\\nModels for Sequence Analysis\\nSurya Bhupatiraju\\nAnalyzing Variable-Length Inputs\\nUp until now, we’ve worked only with data with fixed sizes: images from MNIST,\\nCIFAR-10, and ImageNet. These models are incredibly powerful, but there are many\\nsituations in which fixed-length models are insufficient. The vast majority of inter‐\\nactions in our daily lives require a deep understanding of sequences—whether it’s\\nreading the morning newspaper, making a bowl of cereal, listening to the radio,\\nwatching a presentation, or deciding to execute a trade on the stock market. To\\nadapt to variable-length inputs, we’ll have to be a little bit more clever about how we\\napproach designing deep learning models.\\nFigure 9-1 illustrates how our feed-forward neural networks break when analyzing\\nsequences. If the sequence is the same size as the input layer, the model can perform\\nas expected. It’s even possible to deal with smaller inputs by padding zeros to the end\\nof the input until it’s the appropriate length. However, the moment the input exceeds\\nthe size of the input layer, naively using the feed-forward network no longer works.\\nFeed-forward networks thrive on fixed input size problems. Zero padding can\\naddress the handling of smaller inputs, but when naively utilized, these models break\\nwhen inputs exceed the fixed input size.\\n189'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 205}, page_content='Figure 9-1. Broken feed-forward network\\nNot all hope is lost, however. In the next couple of sections, we’ll explore several\\nstrategies we can leverage to “hack” feed-forward networks to handle sequences.\\nLater in the chapter, we’ll analyze the limitations of these hacks and discuss new\\narchitectures to address them. We will conclude the chapter by discussing some of\\nthe most advanced architectures explored to date to tackle some of the most difficult\\nchallenges in replicating human-level logical reasoning and cognition over sequences.\\nTackling seq2seq with Neural N-Grams\\nIn this section, we’ll begin exploring a feed-forward neural network architecture that\\ncan process a body of text and produce a sequence of part-of-speech (POS) tags. In\\nother words, we want to appropriately label each word in the input text as a noun,\\nverb, preposition, and so on. An example of this is shown in Figure 9-2. While it’s\\nnot the same complexity as building an AI that can answer questions after reading\\na story, it’s a solid first step toward developing an algorithm that can understand\\nthe meaning of how words are used in a sentence. This problem is also interesting\\nbecause it is an instance of a class of problems known as seq2seq, where the goal is\\nto transform an input sequence into a corresponding output sequence. Other famous\\nseq2seq problems include translating text between languages (which we will tackle\\nlater in this chapter),  text summarization, and transcribing speech to text.\\n190 \\n| \\nChapter 9: Models for Sequence Analysis'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 206}, page_content='Figure 9-2. An example of an accurate POS parse of an English sentence\\nAs we discussed, it’s not obvious how we might take a body of text all at once to\\npredict the full sequence of POS tags. Instead, we leverage a trick that is akin to\\nthe way we developed distributed vector representations of words in the previous\\nchapter. The key observation is this: it is not necessary to take into account long-term\\ndependencies to predict the POS of any given word.\\nThe implication of this observation is that instead of using the whole sequence to\\npredict all of the POS tags simultaneously, we can predict each POS tag one at a time\\nby using a fixed-length subsequence. In particular, we utilize the subsequence starting\\nfrom the word of interest and extending n words into the past. This neural n-gram\\nstrategy is depicted in Figure 9-3.\\nFigure 9-3. Using a feed-forward network to perform seq2seq when we can ignore\\nlong-term dependencies\\nSpecifically, when we predict the POS tag for the itℎ word in the input, we use the\\nthe i −n + 1st, i −n + 2nd, ..., itℎ words as the input. We’ll refer to this subsequence\\nas the context window. In order to process the entire text, we’ll start by positioning\\nthe network at the beginning of the text. We’ll then proceed to move the network’s\\ncontext window one word at a time, predicting the POS tag of the rightmost word,\\nuntil we reach the end of the input.\\nLeveraging the word embedding strategy from last chapter, we’ll also use condensed\\nrepresentations of the words instead of one-hot vectors. This will allow us to reduce\\nthe number of parameters in our model and make learning faster.\\nTackling seq2seq with Neural N-Grams \\n| \\n191'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 207}, page_content=\"Implementing a Part-of-Speech Tagger\\nNow that we have a strong understanding of the POS network architecture, we can\\ndive into the implementation. On a high level, the network consists of an input layer\\nthat leverages a three-gram context window. We’ll use word embeddings that are\\n300-dimensional, resulting in a context window of size 900. The feed-forward net‐\\nwork will have two hidden layers of size 512 neurons and 256 neurons, respectively.\\nThen, the output layer will be a softmax calculating the probability distribution of\\nthe POS tag output over a space of 44 possible tags. As usual, we’ll use the Adam\\noptimizer with our default hyperparameter settings, train for a total of 1,000 epochs,\\nand leverage batch-normalization for regularization.\\nThe actual network is extremely similar to networks we’ve implemented in the past.\\nRather, the tricky part of building the POS tagger is in preparing the dataset. We’ll\\nleverage pretrained word embeddings generated from Google News. It includes vec‐\\ntors for 3 million words and phrases and was trained on roughly 100 billion words.\\nWe can use the gensim Python package to read the dataset. Google Colab already has\\ngensim preinstalled. If you are using another machine, you can use pip to install the\\npackage. You will also need to download the Google News data file:\\n$ pip install gensim\\n$ wget https://s3.amazonaws.com/dl4j-distribution/\\n  GoogleNews-vectors-negative300.bin.gz -O googlenews.bin.gz\\nWe can subsequently load these vectors into memory using the following command:\\nfrom gensim.models import KeyedVectors\\nmodel = KeyedVectors.load_word2vec_format('./googlenews.bin.gz',\\n                                          binary=True)\\nThe issue with this operation, however, is that it’s incredibly slow (it can take up to\\nan hour, depending on the specs of your machine). To avoid loading the full dataset\\ninto memory every single time we run our program, especially while debugging code\\nor experimenting with different hyperparameters, we cache the relevant subset of\\nthe vectors to disk using a lightweight database known as LevelDB. To build the\\nappropriate Python bindings (which allow us to interact with a LevelDB instance\\nfrom Python), we simply use the following command:\\n$ pip install leveldb\\nAs we mentioned, the gensim model contains three million words, which is larger\\nthan our dataset. For the sake of efficiency, we’ll selectively cache word vectors for\\nwords in our dataset and discard everything else. To figure out which words we’d like\\nto cache, let’s download the POS dataset from the CoNLL-2000 task.\\n$ wget http://www.cnts.ua.ac.be/conll2000/chunking/train.txt.gz \\n  -O - | gunzip |\\n192 \\n| \\nChapter 9: Models for Sequence Analysis\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 208}, page_content='cut -f1,2 -d\" \" > pos.train.txt\\n$ wget http://www.cnts.ua.ac.be/conll2000/chunking/test.txt.gz \\n  -O - | gunzip |\\n  cut -f1,2 -d \" \" > pos.test.txt\\nThe dataset consists of contiguous text that is formatted as a sequence of rows, where\\nthe first element is a word and the second element is the corresponding part of\\nspeech. Here are the first several lines of the training dataset:\\nConfidence NN\\nin IN\\nthe DT\\npound NN\\nis VBZ\\nwidely RB\\nexpected VBN\\nto TO\\ntake VB\\nanother DT\\nsharp JJ\\ndive NN\\nif IN\\ntrade NN\\nfigures NNS\\nfor IN\\nSeptember NNP\\n, ,\\ndue JJ\\nfor IN\\nrelease NN\\ntomorrow NN\\n...\\nTo match the formatting of the dataset to the gensim model, we’ll have to do some\\npreprocessing. For example, the model replaces digits with \\'#\\' characters, combines\\nseparate words into entities where appropriate (e.g., considering “New_York” as a\\nsingle token instead of two separate words), and utilizes underscores where the raw\\ndata uses dashes. We preprocess the dataset to conform to this model schema with the\\nfollowing code (analogous code is used to process the training data):\\ndef create_pos_dataset(filein, fileout):\\n  dataset = []\\n  with open(filein) as f:\\n    dataset_raw = f.readlines()\\n    dataset_raw = [e.split() for e in dataset_raw\\n                        if len(e.split()) > 0]\\n  counter = 0\\n  while counter < len(dataset_raw):\\n    pair = dataset_raw[counter]\\n    if counter < len(dataset_raw) - 1:\\nImplementing a Part-of-Speech Tagger \\n| \\n193'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 209}, page_content='next_pair = dataset_raw[counter + 1]\\n      if (pair[0] + \"_\" + next_pair[0] in model) and \\\\\\n      (pair[1] == next_pair[1]):\\n        dataset.append([pair[0] + \"_\" + next_pair[0], pair[1]])\\n        counter += 2\\n        continue\\n    word = re.sub(\"\\\\d\", \"#\", pair[0])\\n    word = re.sub(\"-\", \"_\", word)\\n    if word in model:\\n      dataset.append([word, pair[1]])\\n      counter += 1\\n      continue\\n    if \"_\" in word:\\n      subwords = word.split(\"_\")\\n      for subword in subwords:\\n        if not (subword.isspace() or len(subword) == 0):\\n          dataset.append([subword, pair[1]])\\n      counter += 1\\n      continue\\n    dataset.append([word, pair[1]])\\n    counter += 1\\n  with open(fileout, \\'w\\') as processed_file:\\n    for item in dataset:\\n      processed_file.write(\"%s\\\\n\" % (item[0] + \" \" + item[1]))\\n  \\n  return dataset\\ntrain_pos_dataset = create_pos_dataset(\\'./pos.train.txt\\',\\n                                       \\'./pos.train.processed.txt\\')\\ntest_pos_dataset = create_pos_dataset(\\'./pos.test.txt\\',\\n                                      \\'./pos.test.processed.txt\\')\\nNow that we’ve appropriately processed the datasets for use, we can load the words in\\nLevelDB. If the word or phrase is present in the gensim model, we can cache that in\\nthe LevelDB instance. If not, we randomly select a vector to represent to the token,\\nand cache it so that we remember to use the same vector in case we encounter it\\nagain:\\nimport leveldb\\ndb = leveldb.LevelDB(\"./word2vecdb\")\\ncounter = 0\\ndataset_vocab = {}\\ntags_to_index = {}\\nindex_to_tags = {}\\nindex = 0\\nfor pair in train_pos_dataset + test_pos_dataset:\\n194 \\n| \\nChapter 9: Models for Sequence Analysis'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 210}, page_content='if pair[0] not in dataset_vocab:\\n    dataset_vocab[pair[0]] = index\\n    index += 1\\n  if pair[1] not in tags_to_index:\\n    tags_to_index[pair[1]] = counter\\n    index_to_tags[counter] = pair[1]\\n    counter += 1\\nnonmodel_cache = {}\\ncounter = 1\\ntotal = len(dataset_vocab.keys())\\nfor word in dataset_vocab:\\n  \\n  if word in model:\\n    db.Put(bytes(word,\\'utf-8\\'), model[word])\\n  elif word in nonmodel_cache:\\n    db.Put(bytes(word,\\'utf-8\\'), nonmodel_cache[word])\\n  else:\\n    #print(word)\\n    nonmodel_cache[word] = np.random.uniform(-0.25,\\n                                             0.25,\\n                                             300).astype(np.float32)\\n    db.Put(bytes(word,\\'utf-8\\'), nonmodel_cache[word])\\n  counter += 1\\nAfter running the script for the first time, we can just load our data straight from the\\ndatabase if it already exists:\\ndb = leveldb.LevelDB(\"./word2vecdb\")\\nx = db.Get(bytes(\\'Confidence\\',\\'utf-8\\'))\\nprint(np.frombuffer(x,dtype=\\'float32\\').shape)\\n# out: (300,)\\nNext, we build dataset objects for both training and test datasets, which we can\\nuse to generate minibatches for training and testing purposes. Building the dataset\\nobject requires access to the LevelDB db, the dataset, a dictionary tags_to_index\\nthat maps POS tags to indices in the output vector, and a boolean flat get_all that\\ndetermines whether getting the minibatch should retrieve the full set by default:\\nfrom torch.utils.data import Dataset\\nfrom torch.utils.data import DataLoader\\nclass NgramPOSDataset(Dataset):\\n  def __init__(self, db, dataset, tags_to_index, n_grams):\\n    super(NgramPOSDataset, self).__init__()\\n    self.db = db\\n    self.dataset = dataset\\n    self.tags_to_index = tags_to_index\\n    self.n_grams = n_grams\\nImplementing a Part-of-Speech Tagger \\n| \\n195'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 211}, page_content=\"def __getitem__(self, index):\\n    ngram_vector = np.array([])\\n    \\n    for ngram_index in range(index, index + self.n_grams):\\n      word, _ = self.dataset[ngram_index]\\n      vector_bytes = self.db.Get(bytes(word, 'utf-8'))\\n      vector = np.frombuffer(vector_bytes, dtype='float32')\\n      ngram_vector = np.append(ngram_vector, vector)\\n      \\n      _, tag = self.dataset[index + int(np.floor(self.n_grams/2))]\\n      label = self.tags_to_index[tag]\\n    return torch.tensor(ngram_vector, dtype=torch.float32), label\\n  def __len__(self):\\n    return (len(self.dataset) - self.n_grams + 1)\\ntrainset = NgramPOSDataset(db, train_pos_dataset, tags_to_index, 3)\\ntrainloader = DataLoader(trainset, batch_size=4, shuffle=True)\\nFinally, we design our feed-forward network similarly to our approaches in previous\\nchapters. We omit a discussion of the code and refer to the file Ch09_01_POS_Tag‐\\nger.ipynb in the book’s repository.\\nEvery epoch, we manually inspect the model by parsing the sentence: “The woman,\\nafter grabbing her umbrella, went to the bank to deposit her cash.” Within 100 epochs\\nof training, the algorithm achieves over 96% accuracy and nearly perfectly parses the\\nvalidation sentence (it makes the understandable mistake of confusing the possessive\\npronoun and personal pronoun tags for the first appearance of the word “her”).\\nWe’ll conclude this by including the visualizations of our model’s performance using\\nTensorBoard in Figure 9-4.\\nThe POS tagging model was a great exercise, but it was mostly rinsing and repeating\\nconcepts we’ve learned in previous chapters. In the rest of the chapter, we’ll start\\nto think about much more complicated sequence-related learning tasks. To tackle\\nthese more difficult problems, we’ll need to broach brand-new concepts, develop new\\narchitectures, and start to explore the cutting edge of modern deep learning research.\\nWe’ll start by tackling the problem of dependency parsing next.\\n196 \\n| \\nChapter 9: Models for Sequence Analysis\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 212}, page_content='Figure 9-4. TensorBoard visualization of our feed-forward POS tagging model\\nDependency Parsing and SyntaxNet\\nThe framework we used to solve the POS tagging task was rather simple. Sometimes\\nwe need to be much more creative about how we tackle seq2seq problems, especially\\nas the complexity of the problem increases. In this section, we’ll explore strategies\\nthat employ creative data structures to tackle difficult seq2seq problems. As an illus‐\\ntrative example, we’ll explore the problem of dependency parsing.\\nDependency Parsing and SyntaxNet \\n| \\n197'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 213}, page_content='The idea behind building a dependency parse tree is to map the relationships between\\nwords in a sentence. Take, for example, the dependency in Figure 9-5. The words “I”\\nand “taxi” are children of the word “took,” specifically as the subject and direct object\\nof the verb, respectively.\\nFigure 9-5. An example of a dependency parse, which generates a tree of relationships\\nbetween words in a sentence\\nOne way to express a tree as a sequence is by linearizing it. Let’s consider the\\nexamples in Figure 9-6. Essentially, if you have a graph with a root R, and children\\nA (connected by edge r_a), B (connected by edge r_b), and C (connected by edge\\nr_c), we can linearize the representation as (R, r_a, A, r_b, B, r_c, C). We can\\neven represent more complex graphs. Let’s assume, for example, that node B actually\\nhas two more children named D (connected by edge b_d) and E (connected by edge\\nb_e). We can represent this new graph as (R, r_a, A, r_b, [B, b_d, D, b_e, E],\\nr_c, C).\\nFigure 9-6. We linearize two example trees: the diagrams omit edge labels for the sake of\\nvisual clarity\\nUsing this paradigm, we can take our example dependency parse and linearize it, as\\nshown in Figure 9-7.\\n198 \\n| \\nChapter 9: Models for Sequence Analysis'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 214}, page_content='1 Nivre, Joakim. “Incrementality in Deterministic Dependency Parsing.” Proceedings of the Workshop on Incre‐\\nmental Parsing: Bringing Engineering and Cognition Together. Association for Computational Linguistics,\\n2004; Chen, Danqi, and Christopher D. Manning. “A Fast and Accurate Dependency Parser Using Neural\\nNetworks.” EMNLP. 2014.\\nFigure 9-7. Linearization of the dependency parse tree example\\nOne interpretation of this seq2seq problem would be to read the input sentence and\\nproduce a sequence of tokens as an output that represents the linearization of the\\ninput’s dependency parse. It’s not particularly clear, however, how we might port\\nour strategy from the previous section, where there was a clear one-to-one mapping\\nbetween words and their POS tags. Moreover, we could easily make decisions about\\na POS tag by looking at the nearby context. For dependency parsing, there’s no clear\\nrelationship between how words are ordered in the sentence and how tokens in\\nthe linearization are ordered. It also seems like dependency parsing tasks us with\\nidentifying edges that may span a significantly large number of words. Therefore, at\\nfirst glance, it seems like this setup directly violates our assumption that we need not\\ntake into account any long-term dependencies.\\nTo make the problem more approachable, we instead reconsider the dependency\\nparsing task as finding a sequence of valid “actions” that generates the correct\\ndependency parse. This technique, known as the arc-standard system, was first\\ndescribed by Nivre in 2004 and later leveraged in a neural context by Chen and\\nManning in 2014.1 In the arc-standard system, we start by putting the first two words\\nof the sentence in the stack and maintaining the remaining words in the buffer, as\\nshown in Figure 9-8.\\nDependency Parsing and SyntaxNet \\n| \\n199'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 215}, page_content='Figure 9-8. Three options in the arc-standard system: shift a word from the buffer to the\\nstack, draw an arc from the right element to the left element (left arc), or draw an arc\\nfrom the left element to the right element (right arc)\\nAt any step, we can take one of three possible classes of actions:\\nShift\\nMove a word from the buffer to the front of the stack.\\nLeft arc\\nCombine the two elements at the front of the stack into a single unit where the\\nroot of the rightmost element is the parent node and the root of leftmost element\\nis the child node.\\nRight arc\\nCombine the two elements at the front of the stack into a single unit where the\\nroot of the left element is the parent node, and the root of right element is the\\nchild node.\\nWe note that while there is only one way to perform a shift, the arc actions can be\\nof many flavors, each differentiated by the dependency label assigned to the arc that\\nis generated. That being said, we’ll simplify our discussions and illustrations in this\\nsection by considering each decision as a choice among three actions (rather than\\ntens of actions).\\n200 \\n| \\nChapter 9: Models for Sequence Analysis'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 216}, page_content='We terminate this process when the buffer is empty and the stack has one element\\nin it (which represents the full dependency parse). To illustrate this process in its\\nentirety, we illustrate a sequence of actions that generates the dependency parse for\\nour example input sentence in Figure 9-9.\\nFigure 9-9. A sequence of actions that results in the correct dependency parse; we omit\\nlabels\\nIt’s not too difficult to reformulate this decision-making framework as a learning\\nproblem. At every step, we take the current configuration, and we vectorize the\\nconfiguration by extracting a large number of features that describe the configuration\\n(words in specific locations of the stack/buffer, specific children of the words in these\\nlocations, part of speech tags, etc.). During train time, we can feed this vector into\\na feed-forward network and compare its prediction of the next action to take to a\\ngold-standard decision made by a human linguist. To use this model in the wild, we\\ncan take the action that the network recommends, apply it to the configuration, and\\nuse this new configuration as the starting point for the next step (feature extraction,\\naction prediction, and action application). This process is shown in Figure 9-10.\\nDependency Parsing and SyntaxNet \\n| \\n201'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 217}, page_content='Figure 9-10. A neural framework for arc-standard dependency parsing \\nTaken together, these ideas form the core for Google’s SyntaxNet, the state-of-the-art\\nopen source implementation for dependency parsing. Delving into the nitty-gritty\\naspects of implementation is beyond the scope of this text, but we refer you to the\\nopen source repository, which contains an implementation of Parsey McParseface,\\nthe most accurate publicly reported English language parser as of the publication of\\nthis text.\\n202 \\n| \\nChapter 9: Models for Sequence Analysis'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 218}, page_content='Beam Search and Global Normalization\\nIn the previous section, we described a naive strategy for deploying SyntaxNet in\\npractice. The strategy was purely greedy; that is, we selected prediction with the\\nhighest probability without being concerned that we might potentially paint ourselves\\ninto a corner by making an early mistake. In the POS example, making an incorrect\\nprediction was largely inconsequential. This is because each prediction could be\\nconsidered a purely independent subproblem (the results of a given prediction do not\\naffect the inputs of the next step).\\nThis assumption no longer holds in SyntaxNet, because our prediction at\\nstep n affects the input we use at step n + 1. This implies that any mistake we make\\nwill influence all later decisions. Moreover, there’s no good way of “going backward”\\nand fixing mistakes when they become apparent.\\nGarden path sentences are an extreme case of where this is important. Consider\\nthe following sentence: “The complex houses married and single soldiers and their\\nfamilies.” The first glance pass-through is confusing. Most people interpret “complex”\\nas an adjective “houses” as a noun, and “married” as a past tense verb. This makes\\nlittle semantic sense though, and starts to break down as the rest of the sentence is\\nread. Instead, we realize that “complex” is a noun (as in a military complex) and that\\n“houses” is a verb. In other words, the sentence implies that the military complex\\ncontains soldiers (who may be single or married) and their families. A greedy version\\nof SyntaxNet would fail to correct the early parse mistake of considering “complex”\\nas an adjective describing the “houses,” and therefore would fail on the full version of\\nthe sentence.\\nTo remedy this shortcoming, we use a strategy known as beam search, illustrated in\\nFigure 9-11. We generally leverage beam searches in situations like SyntaxNet, where\\nthe output of our network at a particular step influences the inputs used in future\\nsteps. The basic idea behind beam search is that instead of greedily selecting the most\\nprobable prediction at each step, we maintain a beam of the most likely hypothesis\\n(up to a fixed beam size b) for the sequence of the first k actions and their associated\\nprobabilities. Beam searching can be broken up into two major phases: expansion\\nand pruning.\\nBeam Search and Global Normalization \\n| \\n203'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 219}, page_content='Figure 9-11. Using beam search (with beam size 2) while deploying a trained SyntaxNet\\nmodel\\nDuring the expansion step, we take each hypothesis and consider it as a possible input\\nto SyntaxNet. Assume SyntaxNet produces a probability distribution over a space\\nof A  total actions. We then compute the probability of each of the b A  possible\\nhypotheses for the sequence of the first k + 1 actions. Then, during the pruning step,\\nwe keep only the b hypothesis out of the b A  total options with the largest probabili‐\\nties. As Figure 9-11 illustrates, beam searching enables SyntaxNet to correct incorrect\\npredictions post facto by entertaining less probable hypotheses early that might turn\\nout to be more fruitful later in the sentence. In fact, digging deeper into the illustrated\\n204 \\n| \\nChapter 9: Models for Sequence Analysis'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 220}, page_content='2 Andor, Daniel, et al. “Globally Normalized Transition-Based Neural Networks.” arXiv preprint\\narXiv:1603.06042 (2016).\\n3 Ibid.\\nexample, a greedy approach would have suggested that the correct sequence of moves\\nwould have been a shift followed by a left arc. In reality, the best (highest probability)\\noption would have been to use a left arc followed by a right arc. Beam searching with\\nbeam size 2 surfaces this result.\\nThe full open source version takes this a full step further and attempts to bring\\nthe concept of beam searching to the process of training the network. As Andor\\net al. described in 2016,2 this process of global normalization provides both strong\\ntheoretical guarantees and clear performance gains relative to local normalization in\\npractice. In a locally normalized network, our network is tasked with selecting the\\nbest action given a configuration. The network outputs a score that is normalized\\nusing a softmax layer. This is meant to model a probability distribution over all\\npossible actions, provided the actions performed thus far. Our loss function attempts\\nto force the probability distribution to the ideal output (i.e., probability 1 for the\\ncorrect action and 0 for all other actions). The cross-entropy loss does a spectacular\\njob of ensuring this for us.\\nIn a globally normalized network, our interpretation of the scores is slightly different.\\nInstead of putting the scores through a softmax to generate a per-action probability\\ndistribution, we instead add up all the scores for a hypothesis action sequence. One\\nway of ensuring that we select the correct hypothesis sequence is by computing this\\nsum over all possible hypotheses and then applying a softmax layer to generate a\\nprobability distribution. We could theoretically use the same cross-entropy loss func‐\\ntion as we used in the locally normalized network. The problem with this strategy,\\nhowever, is that there is an intractably large number of possible hypothesis sequences.\\nEven considering an average sentence length of 10 and a conservative total number\\nof 15 possible actions—1 shift and 7 labels for each of the left and right arcs—this\\ncorresponds to 1,000,000,000,000,000 possible hypotheses.\\nTo make this problem tractable, as shown in Figure 9-12, we apply a beam search,\\nwith a fixed beam size, until we either (1) reach the end of the sentence, or (2) the\\ncorrect sequence of actions is no longer contained on the beam. We then construct\\na loss function that tries to push the “gold standard” action sequence (highlighted in\\nblue) as high as possible on the beam by maximizing its score relative to the other\\nhypotheses. While we won’t dive into the details of how we might construct this loss\\nfunction here, we refer you to the original paper by Andor et al. in 2016.3 The paper\\nalso describes a more sophisticated POS tagger that uses global normalization and\\nbeam search to significantly increase accuracy (compared to the POS tagger we built\\nearlier in the chapter).\\nBeam Search and Global Normalization \\n| \\n205'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 221}, page_content='Figure 9-12. Coupling training and beam search can make global normalization in\\nSyntaxNet tractable\\nA Case for Stateful Deep Learning Models\\nWhile we’ve explored several tricks to adapt feed-forward networks to sequence\\nanalysis, we’ve yet to truly find an elegant solution to sequence analysis. In the\\nPOS tagger example, we made the explicit assumption that we can ignore long-term\\ndependencies. We were able to overcome some of the limitations of this assumption\\nby introducing the concepts of beam searching and global normalization, but even\\nstill, the problem space was constrained to situations in which there was a one-to-one\\nmapping between elements in the input sequence to elements in the output sequence.\\nFor example, even in the dependency parsing model, we had to reformulate the prob‐\\nlem to discover a one-to-one mapping between a sequence of input configurations\\nwhile constructing the parse tree and arc-standard actions.\\nSometimes, however, the task is far more complicated than finding a one-to-one\\nmapping between input and output sequences. For example, we might want to\\ndevelop a model that can consume an entire input sequence at once and then\\nconclude if the sentiment of the entire input was positive or negative. We’ll build\\na simple model to perform this task later in the chapter. We may want an algorithm\\nthat consumes a complex input (such as an image) and generate a sentence, one word\\nat a time, describing the input. We may event want to translate sentences from one\\nlanguage to another (e.g., from English to French). In all of these instances, there’s\\nno obvious mapping between input tokens and output tokens. Instead, the process is\\nmore like the situation in Figure 9-13.\\n206 \\n| \\nChapter 9: Models for Sequence Analysis'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 222}, page_content='Figure 9-13. The ideal model for sequence analysis can store information in memory\\nover long periods of time, leading to a coherent “thought” vector that it can use to\\ngenerate an answer\\nThe idea is simple. We want our model to maintain some sort of memory over the\\nspan of reading the input sequence. As it reads the input, the model should be able\\nto modify this memory bank, taking into account the information that it observes. By\\nthe time it has reached the end of the input sequence, the internal memory contains\\na “thought” that represents the key pieces of information, that is, the meaning, of the\\noriginal input. We should then, as shown in Figure 9-13, be able to use this thought\\nvector to either produce a label for the original sequence or produce an appropriate\\noutput sequence (translation, description, abstractive summary, etc.).\\nThe concept here isn’t something we’ve explored in any of the previous chapters.\\nFeed-forward networks are inherently “stateless.” After it’s been trained, the feed-\\nforward network is a static structure. It isn’t able to maintain memories between\\ninputs, or change how it processes an input based on inputs it has seen in the past.\\nTo execute this strategy, we’ll need to reconsider how we construct neural networks\\nto create deep learning models that are “stateful.” To do this, we’ll have to return to\\nhow we think about networks on an individual neuron level. In the next section, we’ll\\nexplore how recurrent connections (as opposed to the feed-forward connections we\\nhave studied this far) enable models to maintain state as we describe a class of models\\nknown as recurrent neural networks (RNNs).\\nRecurrent Neural Networks\\nRNNs were first introduced in the 1980s, but have regained popularity recently due\\nto several intellectual and hardware breakthroughs that have made them tractable to\\ntrain. RNNs are different from feed-forward networks because they leverage a special\\ntype of neural layer, known as recurrent layers, that enable the network to maintain\\nstate between uses of the network.\\nRecurrent Neural Networks \\n| \\n207'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 223}, page_content='Figure 9-14 illustrates the neural architecture of a recurrent layer. All of the neurons\\nhave both (1) incoming connections emanating from all of the neurons of the\\nprevious layer and (2) outgoing connections leading to all of the neurons to the\\nsubsequent layer. We notice here, however, that these aren’t the only connections\\nthat neurons of a recurrent layer have. Unlike a feed-forward layer, recurrent layers\\nalso have recurrent connections, which propagate information between neurons of\\nthe same layer. A fully connected recurrent layer has information flow from every\\nneuron to every other neuron in its layer (including itself). Thus a recurrent layer\\nwith r neurons has a total of r2 recurrent connections.\\nFigure 9-14. A recurrent layer contains recurrent connections, that is to say, connections\\nbetween neurons that are located in the same layer\\nTo better understand how RNNs work, let’s explore how one functions after it’s been\\nappropriately trained. Every time we want to process a new sequence, we create a\\nfresh instance of our model. We can reason about networks that contain recurrent\\nlayers by dividing the lifetime of the network instance into discrete time steps. At\\neach time step, we feed the model the next element of the input. Feed-forward\\nconnections represent information flow from one neuron to another where the data\\nbeing transferred is the computed neuronal activation from the current time step.\\nRecurrent connections, however, represent information flow where the data is the\\nstored neuronal activation from the previous time step. Thus, the activations of the\\nneurons in a recurrent network represent the accumulating state of the network\\ninstance. The initial activations of neurons in the recurrent layer are parameters of\\nour model, and we determine the optimal values for them just like we determine the\\noptimal values for the weights of each connection during the process of training.\\n208 \\n| \\nChapter 9: Models for Sequence Analysis'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 224}, page_content='It turns out that, given a fixed lifetime (say t time steps) of an RNN instance,\\nwe can actually express the instance as a feed-forward network (albeit irregularly\\nstructured). This clever transformation, illustrated in Figure 9-15, is often referred\\nto as “unrolling” the RNN through time. Let’s consider the example RNN in the\\nfigure. We’d like to map a sequence of two inputs (each dimension 1) to a single\\noutput (also of dimension 1). We perform the transformation by taking the neurons\\nof the single recurrent layer and replicating them it t times, once for each time step.\\nWe similarly replicate the neurons of the input and output layers. We redraw the\\nfeed-forward connections within each time replica just as they were in the original\\nnetwork. Then we draw the recurrent connections as feed-forward connections from\\neach time replica to the next (since the recurrent connections carry the neuronal\\nactivation from the previous time step).\\nFigure 9-15. We can run an RNN through time to express it as a feed-forward network\\nthat we can train using backpropagation\\nWe can also now train the RNN by computing the gradient based on the unrolled\\nversion. This means that all of the backpropagation techniques that we used for\\nfeed-forward networks also apply to training RNNs. We do run into one issue,\\nhowever. After every batch of training examples we use, we need to modify the\\nweights based on the error derivatives we calculate. In our unrolled network, we\\nhave sets of connections that all correspond to the same connection in the original\\nRNN. The error derivatives calculated for these unrolled connections, however, are\\nnot guaranteed to be (and, in practice, probably won’t be) equal. We can circumvent\\nthis issue by averaging or summing the error derivatives over all the connections that\\nbelong to the same set. This allows us to utilize an error derivative that considers\\nall of the dynamics acting on the weight of a connection as we attempt to force the\\nnetwork to construct an accurate output.\\nRecurrent Neural Networks \\n| \\n209'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 225}, page_content='4 Kilian, Joe, and Hava T. Siegelmann. “The dynamic universality of sigmoidal neural networks.” Information\\nand computation 128.1 (1996): 48-56.\\nThe Challenges with Vanishing Gradients\\nOur motivation for using a stateful network model hinges on this idea of capturing\\nlong-term dependencies in the input sequence. It seems reasonable that an RNN\\nwith a large memory bank (i.e., a significantly sized recurrent layer) would be able\\nto summarize these dependencies. In fact, from a theoretical perspective, Kilian and\\nSiegelmann demonstrated in 1996 that the RNN is a universal functional represen‐\\ntation.4 In other words, with enough neurons and the right parameter settings, an\\nRNN can be used to represent any functional mapping between input and output\\nsequences.\\nThe theory is promising, but it doesn’t necessarily translate to practice. While it is\\nnice to know that it is possible for an RNN to represent any arbitrary function, it is\\nmore useful to know whether it is practical to teach the RNN a realistic functional\\nmapping from scratch by applying gradient descent algorithms. If it turns out to be\\nimpractical, we’ll be in hot water, so it will be useful for us to be rigorous in exploring\\nthis question. Let’s start our investigation by considering the simplest possible RNN,\\nshown in Figure 9-16, with a single input neuron, a single output neuron, and a fully\\nconnected recurrent layer with one neuron.\\nFigure 9-16. A single neuron, fully connected recurrent layer (both compressed and\\nunrolled) for the sake of investigating gradient-based learning algorithms\\n210 \\n| \\nChapter 9: Models for Sequence Analysis'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 226}, page_content='Let’s start off simple. Given nonlinearity f, we can express the activation ℎt  of\\nthe hidden neuron of the recurrent layer at time step t as follows, where i t  is the\\nincoming logit from the input neuron at time step t:\\nℎt = f win\\nt i t + wrec\\nt −1 ℎt −1\\nLet’s try to compute how the activation of the hidden neuron changes in response to\\nchanges to the input logit from k time steps in the past. In analyzing this component\\nof the backpropagation gradient expressions, we can start to quantify how much\\n“memory” is retained from past inputs. We start by taking the partial derivative and\\napply the chain rule:\\n∂ℎt\\n∂i t −k = f′ win\\nt i t + wrec\\nt −1 ℎt −1\\n∂\\n∂i t −k win\\nt i t + wrec\\nt −1 ℎt −1\\nBecause the values of the input and recurrent weights are independent of the input\\nlogit at time step t −k, we can further simplify this expression:\\n∂ℎt\\n∂i t −k = f′ win\\nt i t + wrec\\nt −1 ℎt −1 wrec\\nt −1 ∂ℎt −1\\n∂i t −k\\nBecause we care about the magnitude of this derivative, we can take the absolute\\nvalue of both sides. We also know that for all common nonlinearities (the tanh,\\nlogistic, and ReLU nonlinearities), the maximum value of f′  is at most 1. This leads\\nto the following recursive inequality:\\n∂ℎt\\n∂i t −k\\n≤wrec\\nt −1 · ∂ℎt −1\\n∂i t −k\\nWe can continue to expand this inequality recursively until we reach the base case, at\\nstep t −k:\\n∂ℎt\\n∂i t −k\\n≤wrec\\nt −1 · ... · wrec\\nt −k · ∂ℎt −k\\n∂i t −k\\nWe can evaluate this partial derivative similarly to how we proceeded previously:\\nℎt −k = f win\\nt −k i t −k + wrec\\nt −k −1 ℎt −k −1\\nThe Challenges with Vanishing Gradients \\n| \\n211'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 227}, page_content='∂ℎt −k\\n∂i t −k = f′ win\\nt −k i t −k + wrec\\nt −k −1 ℎt −k −1\\n∂\\n∂i t −k win\\nt −k i t −k\\n+ wrec\\nt −k −1 ℎt −k −1\\nIn this expression, the hidden activation at time t −k −1 is independent of the value\\nof the input at t −k. Thus we can rewrite this expression as:\\n∂ℎt −k\\n∂i t −k = f′ win\\nt −k i t −k + wrec\\nt −k −1 ℎt −k −1 win\\nt −k\\nFinally, taking the absolute value on both sides and again applying the observation\\nabout the maximum value of f′ , we can write:\\n∂ℎt −k\\n∂i t −k\\n≤win\\nt −k\\nThis results in the final inequality (which we can simplify because we constrain the\\nconnections at different time steps to have equal value):\\n∂ℎt\\n∂i t −k\\n≤wrec\\nt −1 · ... · wrec\\nt −k · win\\nt −k\\n= wrec\\nk · win\\nThis relationship places a strong upper bound on how much a change in the input\\nat time t −k can impact the hidden state at time t. Because the weights of our model\\nare initialized to small values at the beginning of training, the value of this derivative\\napproaches zero as k increases. In other words, the gradient quickly diminishes\\nwhen it’s computed with respect to inputs several time steps into the past, severely\\nlimiting our model’s ability to learn long-term dependencies. This issue is commonly\\nreferred to as the problem of vanishing gradients, and it severely impacts the learning\\ncapabilities of vanilla RNNs. In order to address this limitation, we will spend the\\nnext section exploring an extraordinarily influential twist on recurrent layers known\\nas long short-term memory.\\n212 \\n| \\nChapter 9: Models for Sequence Analysis'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 228}, page_content='Long Short-Term Memory Units\\nTo combat the problem of vanishing gradients, Sepp Hochreiter and Jürgen Schmid‐\\nhuber introduced the long short-term memory (LSTM) architecture. The basic princi‐\\nple behind the architecture was that the network would be designed for the purpose\\nof reliably transmitting important information many time steps into the future. The\\ndesign considerations resulted in the architecture shown in Figure 9-17.\\nFigure 9-17. The architecture of an LSTM unit, illustrated at a tensor (designated by\\narrows) and operation (designated by the inner blocks) level\\nFor the purposes of this discussion, we’ll take a step back from the individual neuron\\nlevel and start talking about the network as collection tensors and operations on ten‐\\nsors. As the figure indicates, the LSTM unit is composed of several key components.\\nOne of the core components of the LSTM architecture is the memory cell, a tensor\\nrepresented by the bolded loop in the center of the figure. The memory cell holds\\ncritical information that it has learned over time, and the network is designed to\\neffectively maintain useful information in the memory cell over many time steps.\\nAt every time step, the LSTM unit modifies the memory cell with new information\\nwith three different phases. First, the unit must determine how much of the previous\\nmemory to keep. This is determined by the keep gate, shown in detail in Figure 9-18.\\nLong Short-Term Memory Units \\n| \\n213'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 229}, page_content='Figure 9-18. Architecture of the keep gate of an LSTM unit\\nThe basic idea of the keep gate is simple. The memory state tensor from the previous\\ntime step is rich with information, but some of that information may be stale (and\\ntherefore might need to be erased). We figure out which elements in the memory\\nstate tensor are still relevant and which elements are irrelevant by trying to compute\\na bit tensor (a tensor of zeros and ones) that we multiply with the previous state. If a\\nparticular location in the bit tensor holds a 1, it means that location in the memory\\ncell is still relevant and ought to be kept. If that particular location instead holds a\\n0, it means that the location in the memory cell is no longer relevant and ought to\\nbe eased. We approximate this bit tensor by concatenating the input of this time step\\nand the LSTM unit’s output from the previous time step and applying a sigmoid layer\\nto the resulting tensor. A sigmoidal neuron, as you may recall, outputs a value that\\nis either very close to 0 or very close to 1 most of the time (the only exception is\\nwhen the input is close to 0). As a result, the output of the sigmoidal layer is a close\\napproximation of a bit tensor, and we can use this to complete the keep gate.\\nOnce we’ve figured out what information to keep in the old state and what to erase,\\nwe’re ready to think about what information we’d like to write into the memory state.\\nThis part of the LSTM unit is known as the write gate, and it’s depicted in Figure 9-19.\\nThis is broken down into two major parts. The first component is figuring out\\nwhat information we’d like to write into the state. This is computed by the tanh\\nlayer to create an intermediate tensor. The second component is figuring out which\\ncomponents of this computed tensor we actually want to include into the new state\\nand which we want to toss before writing. We do this by approximating a bit vector of\\n0’s and 1’s using the same strategy (a sigmoidal layer) as we used in the keep gate. We\\nmultiply the bit vector with our intermediate tensor and then add the result to create\\nthe new state vector for the LSTM.\\n214 \\n| \\nChapter 9: Models for Sequence Analysis'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 230}, page_content='Figure 9-19. Architecture of the write gate of an LSTM unit\\nAt every time step, we’d like the LSTM unit to provide an output. While we could\\ntreat the state vector as the output directly, the LSTM unit is engineered to provide\\nmore flexibility by emitting an output tensor that is an “interpretation” or external\\n“communication” of what the state vector represents. The architecture of the output\\ngate is shown in Figure 9-20. We use a nearly identical structure as the write gate: (1)\\nthe tanh layer creates an intermediate tensor from the state vector, (2) the sigmoid\\nlayer produces a bit tensor mask using the current input and previous output, and (3)\\nthe intermediate tensor is multiplied with the bit tensor to produce the final output.\\nFigure 9-20. Architecture of the output gate of an LSTM unit\\nSo why is this better than using a raw RNN unit? The key observation is how\\ninformation propagates through the network when we unroll the LSTM unit through\\ntime. The unrolled architecture is shown in Figure 9-21. At the top, we can observed\\nthe propagation of the state vector, whose interactions are primarily linear through\\ntime. The result is that the gradient that relates an input several time steps in the\\npast to the current output does not attenuate as dramatically as in the vanilla RNN\\nLong Short-Term Memory Units \\n| \\n215'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 231}, page_content='architecture. This means that the LSTM can learn long-term relationships much\\nmore effectively than our original formulation of the RNN.\\nFigure 9-21. Unrolling an LSTM unit through time\\n216 \\n| \\nChapter 9: Models for Sequence Analysis'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 232}, page_content='Finally, we want to understand how easy it is to generate arbitrary architectures with\\nLSTM units. How “composable” are LSTMs? Do we need to sacrifice flexibility to use\\nLSTM units instead of a vanilla RNN? Just as we can we can stack RNN layers to\\ncreate more expressive models with more capacity, we can stack LSTM units, where\\nthe input of the second unit is the output of the first unit, the input of the third unit\\nis the output of the second, and so on. Figure 9-22 shows how this works with a\\nmulticellular architecture made of two LSTM units. This means that anywhere we use\\na vanilla RNN layer, we can easily substitute an LSTM unit.\\nFigure 9-22. Composing LSTM units as one might stack recurrent layers in a neural\\nnetwork\\nNow that we have overcome the issue of vanishing gradients and understand the\\ninner workings of LSTM units, we’re ready to dive into the implementation of our\\nfirst RNN models.\\nLong Short-Term Memory Units \\n| \\n217'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 233}, page_content=\"PyTorch Primitives for RNN Models\\nPyTorch provides seceral primitives that we can use out of the box in order to build\\nRNN models. First, we havetorch.nn.RNNCell objects that represent either an RNN\\nlayer or an LSTM unit:\\nimport torch.nn as nn\\ncell_1 = nn.RNNCell(input_size = 10,\\n                    hidden_size = 20,\\n                    nonlinearity='tanh')\\ncell_2 = nn.LSTMCell(input_size = 10,\\n                     hidden_size = 20)\\ncell_3 = nn.GRUCell(input_size = 10,\\n                    hidden_size = 20)\\nThe RNNCell abstraction represents a vanilla recurrent neuron layer, while the\\nLSTMCell represents an implementation of the LSTM unit. PyTorch also includes\\na variation of the LSTM unit known as the Gated Recurrent Unit (GRU), proposed in\\n2014 by Yoshua Bengio’s group. The critical initialization variable for all of these cells\\nis the size of the hidden state vector or hidden_size.\\nIn addition to the primitives, PyTorch provides multilayer RNN and LSTM classes for\\nstacking layers. If we want to stack recurrent units or layers, we can use the following:\\nmulti_layer_rnn = nn.RNN(input_size = 10,\\n                         hidden_size = 20,\\n                         num_layers = 2,\\n                         nonlinearity = 'tanh')\\nmulti_layer_lstm = nn.LSTM(input_size = 10,\\n                           hidden_size = 20,\\n                           num_layers = 2)\\nWe can also use the dropout parameter to apply dropout to the inputs and outputs\\nof an LSTM with specified keep probability. If the dropout parameter is nonzero, the\\nmodel introduces a dropout layer on the outputs of each LSTM layer except the last\\nlayer, with dropout probability equal to dropout:\\nmulti_layer_rnn = nn.RNN(input_size = 10,\\n                         hidden_size = 20,\\n                         num_layers = 2,\\n                         nonlinearity = 'tanh',\\n                         batch_first = False,\\n                         dropout = 0.5)\\nmulti_layer_lstm = nn.LSTM(input_size = 10,\\n                           hidden_size = 20,\\n                           num_layers = 2,\\n218 \\n| \\nChapter 9: Models for Sequence Analysis\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 234}, page_content='batch_first = False,\\n                           dropout = 0.5)\\nAs shown here, the multilayer RNN and LSTM classes also provide a batch_first\\nparameter. If batch_first equals True, then the input and output tensors are pro‐\\nvided as (batch, seq, feature) instead of (seq, batch, feature). Note that this\\ndoes not apply to hidden or cell states. The default value of batch_first is False. See\\nthe PyTorch documentation for details.\\nFinally, we instantiate an RNN by calling the PyTorch LSTM constructor:\\ninput = torch.randn(5, 3, 10) # (time_steps, batch, input_size)\\nh_0 = torch.randn(2, 3, 20) # (n_layers, batch_size, hidden_size)\\nc_0 = torch.randn(2, 3, 20) # (n_layers, batch_size, hidden_size)\\nrnn = nn.LSTM(10, 20, 2) # (input_size, hidden_size, num_layers)\\noutput_n, (hn, cn) = rnn(input, (h_0, c_0))\\nThe result of calling rnn is a tensor representing the outputs of the RNN, output_n,\\nalong with the final state vectors for each layer. The first tensor, hn, contains the\\nhidden state vectors for each layer that holds the outputs of the Output Gates at\\ntime, n. The second tensor, cn, contains the state vectors for the memory cells of each\\nlayer, which is the output of the write gates. Both hn and cn are of size (n_layers,\\nbatch_size, hidden_size).\\nNow that we have an understanding of the tools at our disposal in constructing RNNs\\nin PyTorch, we’ll build our first LSTM in the next section, focused on the task of\\nsentiment analysis.\\nImplementing a Sentiment Analysis Model\\nIn this section, we attempt to analyze the sentiment of movie reviews taken from the\\nLarge Movie Review Dataset. This dataset consists of 50,000 reviews from IMDb, each\\nof which is labeled as having positive or negative sentiment. We use a simple LSTM\\nmodel leveraging dropout to learn how to classify the sentiment of movie reviews.\\nThe LSTM model will consume the movie review one word at a time. Once it has\\nconsumed the entire review, we’ll use its output as the basis of a binary classification\\nto map the sentiment to be “positive” or “negative.”\\nLet’s start off by loading the dataset with the PyTorch library Torchtext, which comes\\npreinstalled with Google Colab. If you’re running on another machine, you can install\\nTorchtext by running the following command:\\n$ pip install torchtext\\nOnce we’ve installed the package, we can download the dataset and define a token‐\\nizer. Torchtext provides many natural language processing (NLP) datasets and token‐\\nizers through the torchtext.datasets and torchtext.data.utils submodules,\\nImplementing a Sentiment Analysis Model \\n| \\n219'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 235}, page_content=\"respectively. We’ll use the built-in IMDb dataset and standard 'basic_english'\\ntokenizer provided by PyTorch.\\nfrom torchtext.datasets import IMDB\\nfrom torchtext.data.utils import get_tokenizer\\n# Load dataset\\ntrain_iter = IMDB(split=('train'))\\n# Define tokenizer and build vocabulary\\ntokenizer = get_tokenizer('basic_english')\\nUntil now, we’ve been using map-style datasets from PyTorch. Torchtext returns NLP\\ndatasets as iterable-style datasets, which are more appropriate for streaming data.\\nNext, we need to create a vocabulary based on the training dataset and prune the\\nvocabulary to include only the 30,000 most common words. Then, we need to pad\\neach input sequence up to a length of 500 words, and process the labels.\\nfrom torchtext.vocab import build_vocab_from_iterator\\ndef yield_tokens(data_iter):\\n    for _, text in data_iter:\\n        yield tokenizer(text)\\n# build vocab from iterator and add a list of any special tokens\\ntext_vocab = build_vocab_from_iterator(yield_tokens(train_iter),\\n                                       specials=['<unk>', '<pad>'])\\ntext_vocab.set_default_index(text_vocab['<unk>'])\\nAs shown, Torchtext provides a function, build_vocab_from_iterator, to create\\na vocabulary. However, this function expects a list of tokens as input, where\\nnext(train_iter)would return a tuple (label_string, review_string). To satisfy\\nthis requirement, we define a function to yield tokens as the dataset is iterated.\\nFinally, we add special tokens for unknown and padding, and set the default.\\nNext, we need to actually prune the vocabulary and pad the review sequences, as\\nwell as convert the label strings, 'neg' or 'pos', to numbers. We accomplish this by\\ndefining a pipeline function for both the labels and review strings:\\ndef text_pipeline(x, max_size=512):\\n   text = tokenizer(x)\\n   \\n   # reduce vocab size\\n   pruned_text = []\\n   for token in text:\\n     if text_vocab.get_stoi()[token] >= 30000:\\n       token = '<unk>'\\n     pruned_text.append(token)\\n   \\n   # pad sequence or truncate\\n   if len(pruned_text) <= max_size:\\n220 \\n| \\nChapter 9: Models for Sequence Analysis\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 236}, page_content=\"pruned_text += ['<pad>'] * (max_size - len(pruned_text))\\n   else:\\n     pruned_text = pruned_text[0:max_size]\\n   return text_vocab(pruned_text)\\nlabel_pipeline = lambda x: (0 if (x == 'neg') else 1)\\nThe text_pipeline function converts the inputs to 500-dimensional vectors. Each\\nvector corresponds to a movie review where the itℎ component of the vector corre‐\\nsponds to the index of the itℎ word of the review in our global dictionary of 30,000\\nwords. To complete the data preparation, we create a special Python class designed to\\nserve minibatches of a desired size from the underlying dataset.\\nWe can use the built-in DataLoader class from PyTorch to sample the dataset in\\nbatches. Before we do so, we need to define a function, collate_batch, that will tell\\nthe DataLoader how to preprocess each batch:\\ndef collate_batch(batch):\\n  label_list, text_list = [], []\\n  for label, review in batch:\\n    label_list.append(label_pipeline(label))\\n    text_list.append(text_pipeline(review))\\n  return (torch.tensor(label_list, dtype=torch.long),\\n          torch.tensor(text_list, dtype=torch.int32))\\nThe collate_batch function simply runs the labels and review strings through\\neach respective pipeline and returns the batch as a tuple of tensors (labels_batch,\\nreviews_batch). Once the collate_fn is defined, we simply load the dataset\\nusing the IMDb constructor, and configure the dataloaders using the DataLoader\\nconstructor:\\nfrom torch.utils.data import DataLoader\\ntrain_iter, val_iter = IMDB(split=('train','test'))\\ntrainloader = DataLoader(train_iter,\\n                         batch_size = 4,\\n                         shuffle=False,\\n                         collate_fn=collate_batch)\\nvalloader = DataLoader(val_iter,\\n                       batch_size = 4,\\n                       shuffle=False,\\n                       collate_fn=collate_batch)\\nWe use the torchtext.datasets.IMDB Python class to serve both the training and\\nvalidation sets we’ll use while training our sentiment analysis model.\\nNow that the data is ready to go, we’ll begin to construct the sentiment analysis\\nmodel, step by step. First, we’ll want to map each word in the input review to a word\\nvector. To do this, we’ll utilize an embedding layer, which, as you may recall from\\nImplementing a Sentiment Analysis Model \\n| \\n221\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 237}, page_content=\"Chapter 8, is a simple lookup table that stores an embedding vector that corresponds\\nto each word.\\nUnlike in previous examples, where we treated the learning of the word embeddings\\nas a separate problem (i.e., by building a Skip-Gram model), we’ll learn the word\\nembeddings jointly with the sentiment analysis problem by treating the embedding\\nmatrix as a matrix of parameters in the full problem. We accomplish this by using the\\nPyTorch primitives for managing embeddings (remember that input represents one\\nfull minibatch at a time, not just one movie review vector):\\nimport torch.nn as nn\\nembedding = nn.Embedding(\\n                      num_embeddings=30000,\\n                      embedding_dim=512,\\n                      padding_idx=text_vocab.get_stoi()['<pad>'])\\nWe then take the result of the embedding layer and build an LSTM with dropout\\nusing the primitives we saw in the previous section. The implementation of the LSTM\\ncan be achieved as follows:\\nclass TextClassifier(nn.Module):\\n  def __init__(self):\\n    super(TextClassifier,self).__init__()\\n    self.layer_1 = nn.Embedding(\\n                      num_embeddings=30000,\\n                      embedding_dim=512,\\n                      padding_idx=1)                      \\n    self.layer_2 = nn.LSTMCell(input_size=512, hidden_size=512)\\n    self.layer_3 = nn.Dropout(p=0.5)\\n    self.layer_4 = nn.Sequential(\\n                      nn.Linear(512, 2),\\n                      nn.Sigmoid(),\\n                      nn.BatchNorm1d(2))\\n    \\n  def forward(self, x):\\n    x = self.layer_1(x)\\n    x = x.permute(1,0,2)\\n    h = torch.rand(x.shape[1], 512)\\n    c = torch.rand(x.shape[1], 512)\\n    for t in range(x.shape[0]):\\n      h, c = self.layer_2(x[t], (h,c))\\n      h = self.layer_3(h)\\n    return self.layer_4(h)\\nWe top it all off using a batch-normalized hidden layer, identical to the ones we’ve\\nused time and time again in previous examples. Stringing all of these components\\ntogether, we can build the model by calling TextClassifier:\\nmodel = TextClassifier()\\n222 \\n| \\nChapter 9: Models for Sequence Analysis\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 238}, page_content='We omit the other boilerplate involved in setting up summary statistics, saving\\nintermediate snapshots, and creating the session because it’s identical to the other\\nmodels we’ve built in this book (see the GitHub repository). We can then run and\\nvisualize the performance of our model using TensorBoard (Figure 9-23).\\nFigure 9-23. Training cost, validation cost, and accuracy of our movie review sentiment\\nmodel\\nImplementing a Sentiment Analysis Model \\n| \\n223'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 239}, page_content='At the beginning of training, the model struggles slightly with stability, and toward\\nthe end of the training, the model clearly starts to overfit as training cost and\\nvalidation cost significantly diverge. At its optimal performance, however, the model\\nperforms rather effectively and generalizes to approximately 86% accuracy on the test\\nset. Congratulations! You’ve built your first RNN.\\nSolving seq2seq Tasks with Recurrent Neural Networks\\nNow that we’ve built a strong understanding of RNNs, we’re ready to revisit the\\nproblem of seq2seq. We started off this chapter with an example of a seq2seq task:\\nmapping a sequence of words in a sentence to a sequence of POS tags. Tackling this\\nproblem was tractable because we didn’t need to take into account long-term depen‐\\ndencies to generate the appropriate tags. But there are several seq2seq problems, such\\nas translating between languages or creating a summary for a video, where long-term\\ndependencies are crucial to the success of the model. This is where RNNs come in.\\nThe RNN approach to seq2seq looks a lot like the autoencoder we discussed in the\\nprevious chapter. The seq2seq model is composed of two separate networks. The\\nfirst network is known as the encoder network. The encoder network is a recurrent\\nnetwork (usually one that uses LSTM units) that consumes the entire input sequence.\\nThe goal of the encoder network is to generate a condensed understanding of the\\ninput and summarize it into a singular thought represented by the final state of the\\nencoder network. Then we use a decoder network, whose starting state is initialized\\nwith the final state of the encoder network, to produce the target output sequence\\ntoken by token. At each step, the decoder network consumes its own output from the\\nprevious time step as the current time step’s input. The entire process is visualized in\\nFigure 9-24.\\n224 \\n| \\nChapter 9: Models for Sequence Analysis'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 240}, page_content='5 Kiros, Ryan, et al. “Skip-Thought Vectors.” Advances in neural information processing systems. 2015.\\nFigure 9-24. How we use an encoder/decoder recurrent network schema to tackle\\nseq2seq problems\\nIn this this setup, we are attempting to translate an English sentence into French.\\nWe tokenize the input sentence and use an embedding (similar to our approach in\\nthe sentiment analysis model we built in the previous section), one word at a time\\nas an input to the encoder network. At the end of the sentence, we use a special\\n“end-of-sequence” (EOS) token to indicate the end of the input sequence to the\\nencoder network. Then we take the hidden state of the encoder network and use that\\nas the initialization of the decoder network. The first input to the decoder network is\\nthe EOS token, and the output is interpreted as the first word of the predicted French\\ntranslation. From that point onward, we use the output of the decoder network as\\nthe input to itself at the next time step. We continue until the decoder network emits\\nan EOS token as its output, at which point we know that the network has completed\\nproducing the translation of the original English sentence. We’ll dissect the practical,\\nopen source implementation of this network (with a couple of enhancements and\\ntricks to improve accuracy) later in this chapter.\\nThe seq2seq RNN architecture can also be reappropriated for the purpose of learning\\ngood embeddings of sequences. For example, Kiros et al. in 2015 invented the notion\\nof a skip-thought vector,5 which borrowed architectural characteristics from both the\\nautoencoder framework and the Skip-Gram model discussed in Chapter 8. The skip-\\nthought vector was generated by dividing a passage into a set of triplets consisting\\nSolving seq2seq Tasks with Recurrent Neural Networks \\n| \\n225'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 241}, page_content='of consecutive sentences. The authors utilized a single encoder network and two\\ndecoder networks, as shown in Figure 9-25.\\nFigure 9-25. The skip-thought seq2seq architecture to generate embedding representa‐\\ntions of entire sentences\\nThe encoder network consumed the sentence for which we wanted to generate a\\ncondensed representation (which was stored in the final hidden state of the encoder\\nnetwork). Then came the decoding step. The first of the decoder networks would\\ntake that representation as the initialization of its own hidden state and attempt\\nto reconstruct the sentence that appeared prior to the input sentence. The second\\ndecoder network would instead attempt the sentence that appeared immediately after\\nthe input sentence. The full system was trained end to end on these triplets, and once\\ncompleted, could be used to generate seemingly cohesive passages of text in addition\\nto improve performance on key sentence-level classification tasks.\\n226 \\n| \\nChapter 9: Models for Sequence Analysis'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 242}, page_content='Here’s an example of story generation, excerpted from the original paper:\\nshe grabbed my hand .\\n\"come on . \"\\nshe fluttered her back in the air .\\n\"i think we\\'re at your place . I ca n\\'t come get you . \"\\nhe locked himself back up\\n\" no . she will . \"\\nkyrian shook his head\\nNow that we’ve developed an understanding of how to leverage RNNs to tackle\\nseq2seq problems, we’re almost ready to try to build our own. Before we get there,\\nhowever, we’ve got one more major challenge to tackle, and we’ll address it head-on\\nin the next section when we discuss the concept of attentions in seq2seq RNNs.\\nAugmenting Recurrent Networks with Attention\\nLet’s think harder about the translation problem. If you’ve ever attempted to learn\\na foreign language, you’ll know that there are several helpful steps when trying to\\ncomplete a translation. First, it’s helpful to read the full sentence to understand the\\nconcept you would like to convey. Then you write out the translation one word at\\na time, each word following logically from the word you wrote previously. But one\\nimportant aspect of translation is that as you compose the new sentence, you often\\nrefer back to the original text, focusing on specific parts that are relevant to your\\ncurrent translation. At each step, you are paying attention to the most relevant parts\\nof the original “input” so you can make the best decision about the next word to put\\non the page.\\nRecall our approach to seq2seq. By consuming the full input and summarizing it\\ninto a “thought” inside its hidden state, the encoder network effectively achieves\\nthe first part of the translation process. By using the previous output as its current\\ninput, the decoder network achieves the second part of the translation process. This\\nphenomenon of attention has yet to be captured by our approach to seq2seq, and this\\nis the final building block we’ll need to engineer.\\nCurrently, the sole input to the decoder network at a given time step t is its output\\nat time step t −1. One way to give the decoder network some vision into the original\\nsentence is by giving the decoder access to all of the outputs from the encoder\\nnetwork (which we previously had completely ignored). These outputs are interesting\\nto us because they represent how the encoder network’s internal state evolves after\\nseeing each new token. A proposed implementation of this strategy is shown in\\nFigure 9-26. This attempt falls short because it fails to dynamically select the most\\nrelevant parts of the input to focus on.\\nAugmenting Recurrent Networks with Attention \\n| \\n227'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 243}, page_content='6 Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. “Neural Machine Translation by Jointly Learning to\\nAlign and Translate.” arXiv preprint arXiv:1409.0473 (2014).\\nFigure 9-26. An attempt at engineering attentional abilities in a seq2seq architecture\\nThis approach has a critical flaw, however. The problem here is that at every time\\nstep, the decoder considers all of the outputs of the encoder network in the exact\\nsame way. However, this is clearly not the case for a human during the translation\\nprocess. We focus on different aspects of the original text when working on different\\nparts of the translation. The key realization here is that it’s not enough to merely\\ngive the decoder access to all the outputs. Instead, we must engineer a mechanism by\\nwhich the decoder network can dynamically pay attention to a specific subset of the\\nencoder’s outputs.\\nWe can fix this problem by changing the inputs to the concatenation operation, using\\nthe proposal in Bahdanau et al. 2015 as inspiration.6 Instead of directly using the\\nraw outputs from the encoder network, we perform a weighting operation on the\\nencoder’s outputs. We leverage the decoder network’s state at time t −1 as the basis\\nfor the weighting operation.\\n228 \\n| \\nChapter 9: Models for Sequence Analysis'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 244}, page_content='The weighting operation is illustrated in Figure 9-27. First we create a scalar (a\\nsingle number, not a tensor) relevance score for each of the encoder’s outputs. The\\nscore is generated by computing the dot product between each encoder output and\\nthe decoder’s state at time t −1. We then normalize these scores using a softmax\\noperation. Finally, we use these normalized scores to individually scale the encoder’s\\noutputs before plugging them into the concatenation operation. The key here is that\\nthe relative scores computed for each encoder output signify how important that\\nparticular encoder output is to the decision for the decoder at time step t. In fact,\\nas we’ll see later, we can visualize which parts of the input are most relevant to the\\ntranslation at each time step by inspecting the output of the softmax.\\nFigure 9-27. A modification to our original proposal that enables a dynamic attentional\\nmechanism based on the hidden state of the decoder network in the previous time step\\nArmed with this strategy for engineering attention into seq2seq architectures, we’re\\nfinally ready to get our hands dirty with an RNN model for translating English\\nsentences into French. But before we jump in, it’s worth noting that attentions are\\nincredibly applicable in problems that extend beyond language translation. Atten‐\\ntions can be important in speech-to-text problems, where the algorithm learns to\\ndynamically pay attention to corresponding parts of the audio while transcribing\\nthe audio into text. Similarly, attentions can be used to improve image captioning\\nalgorithms by helping the captioning algorithm focus on specific parts of the input\\nimage while writing out the caption. Anytime particular parts of the input are highly\\nAugmenting Recurrent Networks with Attention \\n| \\n229'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 245}, page_content='correlated to correctly producing corresponding segments of the output, attentions\\ncan dramatically improve performance.\\nDissecting a Neural Translation Network\\nState-of-the-art neural translation networks use a number of different techniques and\\nadvancements that build on the basic seq2seq encoder-decoder architecture. Atten‐\\ntion, as detailed in the previous section, is an important and critical architectural\\nimprovement. In this section, we will dissect a fully implemented neural machine\\ntranslation system, complete with the data processing steps, building the model,\\ntraining it, and eventually using it as a translation system to convert English phrases\\nto French phrases.\\nThe pipeline used in training and eventually using a neural machine translation\\nsystem is similar to that of most machine learning pipelines: gather data, prepare\\nthe data, construct the model, train the model, evaluate the model’s progress, and\\neventually use the trained model to predict or infer something useful. We review each\\nof these steps here.\\nWe first gather the data from the International Workshop on Spoken Language\\nTranslation (IWSLT2016) repository, which houses large corpora used in training\\ntranslation systems. For our use case, we’ll be using the English-to-French data. Note\\nthat if we want to be able to translate to or from different languages, we would have\\nto train a model from scratch with the new data. We then preprocess our data into\\na format that is easily usable by our models during training and inference time.\\nThis will involve some amount of cleaning and tokenizing the sentences in each of\\nthe English and French phrases. What follows now is a set of techniques used in\\npreparing the data, and later we will present the implementations of the techniques.\\nThe first step is to parse sentences and phrases into formats that are more compatible\\nwith the model by tokenization. This is the process by which we discretize a partic‐\\nular English or French sentence into its constituent tokens. For instance, a simple\\nword-level tokenizer will consume the sentence “I read.” to produce the array [\"I”,\\n“read”, “.\"], or it would consume the French sentence “Je lis.” to produce the array [\"Je”,\\n“lis”, “.\"].\\nA character-level tokenizer may break the sentence into individual characters or into\\npairs of characters like [\"I”, \" “, “r”, “e”, “a”, “d”, “.\"] and [\"I “, “re”, “ad”, “.\"], respectively.\\nOne kind of tokenization may work better than the other, and each has its pros and\\ncons. For instance, a word-level tokenizer will ensure that the model produces words\\nthat are from some dictionary, but the size of the dictionary may be too large to\\nefficiently choose from during decoding. This is in fact a known issue and something\\nthat we’ll address in the coming discussions.\\n230 \\n| \\nChapter 9: Models for Sequence Analysis'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 246}, page_content='On the other hand, the decoder using a character-level tokenization may not produce\\nintelligible outputs, but the total dictionary that the decoder must choose from is\\nmuch smaller, as it is simply the set of all printable ASCII characters. In this tutorial,\\nwe use a word-level tokenization, but we encourage you to experiment with different\\ntokenizations to observe the effects this has. It is worth noting that we must also add a\\nspecial EOS character, to the end of all output sequences because we need to provide\\na definitive way for the decoder to indicate that it has reached the end of its decoding.\\nWe can’t use regular punctuation because we cannot assume that we are translating\\nfull sentences. Note that we do not need EOS characters in our source sequences\\nbecause we are feeding these in preformatted and do not need an EOS character for\\nourselves to denote the end of our source sequence.\\nThe next optimization involves further modifying how we represent each source\\nand target sequence, and we introduce a concept called bucketing. This is a method\\nemployed primarily in sequence-to-sequence tasks, especially machine translation,\\nthat helps the model efficiently handle sentences or phrases of different lengths.\\nWe first describe the naive method of feeding in training data and illustrate the\\nshortcomings of this approach. Normally, when feeding in encoder and decoder\\ntokens, the length of the source sequence and the target sequence is not always equal\\nbetween pairs of examples. For example, the source sequence may have length X, and\\nthe target sequence may have length Y. It may seem that we need different seq2seq\\nnetworks to accommodate each (X, Y) pair, yet this immediately seems wasteful and\\ninefficient. Instead, we can do a little better if we pad each sequence up to a certain\\nlength, as shown in Figure 9-28, assuming we use a word-level tokenization and that\\nwe’ve appended EOS tokens to our target sequences.\\nFigure 9-28. Naive strategy for padding sequences\\nThis step saves us the trouble of having to construct a different seq2seq model for\\neach pair of source and target lengths. However, this introduces a different issue: if\\nthere were a very long sequence, it would mean that we would have to pad every\\nother sequence up to that length. This would make a short sequence padded to the\\nend take as much computational resources as a long one with few pad tokens, which\\nis wasteful and could introduce a major performance hit to our model. We could\\nDissecting a Neural Translation Network \\n| \\n231'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 247}, page_content='consider breaking up every sentence in the corpus into phrases such that the length of\\neach phrase does not exceed a certain maximum limit, but it’s not clear how to break\\nthe corresponding translations. This is where bucketing helps us.\\nBucketing is the idea that we can place encoder and decoder pairs into buckets of\\nsimilar size, and only pad up to the maximum length of sequences in each respective\\nbucket. For instance, we can denote a set of buckets, [(5, 10), (10, 15), (20, 25), (30,\\n40)], where each tuple in the list is the maximum length of the source sequence and\\ntarget sequence, respectively. Borrowing the preceding example, we can place the\\npair of sequences ([\"I”, “read”, “.\"], [\"Je”, “lis”, “.”, “EOS\"]) in the first bucket, as the\\nsource sequence is smaller than 5 tokens and the target sequence is smaller than 10\\ntokens. We would then place the ([\"See”, “you”, “in”, “a”, “little”, “while\"], [\"A”, “tout”,\\n“a”, “l’heure”, “EOS\"]) in the second bucket, and so on. This technique allows us\\nto compromise between the two extremes, where we need to pad only as much as\\nnecessary, as shown in Figure 9-29.\\nFigure 9-29. Padding sequences with buckets\\nUsing bucketing shows a considerable speedup during training and test time, and\\nallows developers and frameworks to write very optimized code to leverage the fact\\nthat any sequence from a bucket will have the same size and pack the data together in\\nways that allow even further GPU efficiency.\\nWith the sequences properly padded, we need to add one additional token to the\\ntarget sequences: a GO token. This GO token will signal to the decoder that decoding\\nneeds to begin, at which point it will take over and begin decoding.\\nThe last improvement we make in the data preparation side is to reverse the source\\nsequences. Researchers found that doing so improved performance, and this has\\nbecome a standard trick to try when training neural machine translation models.\\nThis is a bit of an engineering hack, but consider the fact that our fixed-size neural\\nstate can hold only so much information, and information encoded while processing\\nthe beginning of the sentence may be overwritten while encoding later parts of the\\nsentence. In many language pairs, the beginning of sentences is harder to translate\\n232 \\n| \\nChapter 9: Models for Sequence Analysis'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 248}, page_content=\"than the end of sentences, so this hack of reversing the sentence improves translation\\naccuracy by giving the beginning of the sentence the last say on what final state is\\nencoded. With these ideas in place, the final sequences look as they do in Figure 9-30.\\nFigure 9-30. Final padding scheme with buckets, reversing the inputs, and adding the\\nGO token\\nWith these techniques described, we can now detail the implementation. First, we\\nload the dataset, then we define our tokenizers and vocabularies. We do not define\\nthe word embeddings here, as we will train our model to compute them. PyTorch’s\\nTorchtext library supports IWSLT2016 in torch.text.datasets:\\nfrom torchtext.datasets import IWSLT2016\\ntrain_iter = IWSLT2016(split=('train'),\\n                       language_pair=('en','fr'))\\nThe dataset constructor returns an iterable-style dataset that can retrieve English and\\nFrench sentence pairs with next(train_iter). We’ll use this iterable-style dataset to\\ncreate bucketed datasets for batching later in our code.\\nFor now, let’s also define our tokenizers and vocabularies for each language. PyTorch\\noffers a get_tokenizer function that operates on common tokenizers. Here, we’ll\\nuse the spacy tokenizer for each language. You may need to download the spacy\\nlanguage files first:\\npip install -U spacy\\npython -m spacy download en_core_web_sm\\npython -m spacy download fr_core_news_sm\\nOnce we have the language files, we can create the tokenizers as follows:\\nfrom torchtext.data.utils import get_tokenizer\\ntokenizer_en = get_tokenizer('spacy',language='en_core_web_sm')\\ntokenizer_fr = get_tokenizer('spacy',language='fr_core_news_sm')\\nDissecting a Neural Translation Network \\n| \\n233\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 249}, page_content=\"Next, we will create our vocabularies for English and French using PyTorch’s\\nbuild_vocab_from_iterator function. This function takes tokens from an iterable-\\nstyle dataset from a single language and creates a vocabulary. Since our dataset has\\nboth English and French sentences, we create a yield_tokens function to return only\\nthe English or French tokens, and pass this into build_vocab_from_iterator:\\ndef yield_tokens(data_iter, language):\\n    if language == 'en':\\n      for data_sample in data_iter:\\n          yield tokenizer_en(data_sample[0])\\n    else:\\n      for data_sample in data_iter:\\n        yield tokenizer_fr(data_sample[1])\\nUNK_IDX, PAD_IDX, GO_IDX, EOS_IDX = 0, 1, 2, 3\\nspecial_symbols = ['<unk>', '<pad>', '<go>', '<eos>']\\n# Create Vocabs\\ntrain_iter = IWSLT2016(root='.data', split=('train'),\\n                             language_pair=('en', 'fr'))\\nvocab_en = build_vocab_from_iterator(\\n                  yield_tokens(train_iter, 'en'),\\n                  min_freq=1,\\n                  specials=special_symbols,\\n                  special_first=True)\\ntrain_iter = IWSLT2016(root='.data', split=('train'),\\n                             language_pair=('en', 'fr'))\\nvocab_fr = build_vocab_from_iterator(\\n                  yield_tokens(train_iter, 'fr'),\\n                  min_freq=1,\\n                  specials=special_symbols,\\n                  special_first=True)\\nNotice that we need to reload the train_iter before building the French vocabulary\\nto restart the iterable-style dataset. We also add in the special tokens and their indices.\\nNow that we have the dataset, tokenizers, and vocabularies, we need to create func‐\\ntions to preprocess the tokens and generate batches of bucketed data. First let’s define\\na process_tokens function to apply the improvements we discussed earlier:\\ndef process_tokens(source, target, bucket_sizes):\\n  # find bucket_index\\n  for i in range(len(bucket_sizes)+2):\\n    # truncate if we exhauset list of buckets\\n    if i >= len(bucket_sizes):\\n      bucket = bucket_sizes[i-1]\\n      bucket_id = i-1\\n      if len(source) > bucket[0]:\\n        source = source[:bucket[0]]\\n234 \\n| \\nChapter 9: Models for Sequence Analysis\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 250}, page_content=\"if len(target) > (bucket[1]-2):\\n        target = target[:bucket[1]-2]\\n      break\\n    bucket = bucket_sizes[i]\\n    if (len(source) < bucket[0]) and ((len(target)+1) < bucket[1]):\\n      bucket_id = i\\n      break\\n  \\n  source += ((bucket_sizes[bucket_id][0] - len(source)) * ['<pad>'])\\n  source = list(reversed(source))\\n  target.insert(0,'<go>')\\n  target.append('<eos>')\\n  target += (bucket_sizes[bucket_id][1] - len(target)) * ['<pad>']\\n  return vocab_en(source), vocab_fr(target), bucket_id\\nIn this function, we pass in the variable size lists of source and target tokens, plus\\na list of bucket sizes. First, we decide on the smallest bucket size that will fit both\\nthe source and target token lists. Then, we process the source tokens by padding and\\nreversing the sequence as described earlier. For the target tokens, we add a <go> token\\nto the beginning and add an <eos> token to the end, then pad to the bucket size.\\nWhen determining the smallest bucket size, we accounted for the two added tokens\\n<go> and <eos>.\\nNow we have a function that takes lists of source and target tokens and prepares\\nthem appropriately. Next, we need to collect a single batch of data for our model and\\ntraining loop. To do this, we will use the built-in PyTorch Dataset and DataLoader\\nclasses.\\nWe are going to separate Dataset and DataLoader for each bucket size. This\\napproach will enable us to use the built-in feature of the DataLoader for random\\nbatching and parallel processing.\\nFirst we create a BucketedDataset class by subclassing PyTorch’s Dataset class. Since\\nthis will be a map-style dataset, we’ll need to define the __getitem__ and __len__\\nmethods for data access:\\nfrom torch.utils.data import Dataset\\nclass BucketedDataset(Dataset):\\n  def __init__(self, bucketed_dataset, bucket_size):\\n    super(BucketedDataset, self).__init__()\\n    self.length = len(bucketed_dataset)\\n    self.input_len = bucket_size[0]\\n    self.target_len = bucket_size[1]\\n    self.bucketed_dataset = bucketed_dataset\\n  def __getitem__(self, index):\\nDissecting a Neural Translation Network \\n| \\n235\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 251}, page_content='return (torch.tensor(self.bucketed_dataset[index][0],\\n                         dtype=torch.float32),\\n            torch.tensor(self.bucketed_dataset[index][1],\\n                         dtype=torch.float32))\\n  def __len__(self):\\n    return self.length\\nbucketed_datasets = []\\nfor i, dataset in enumerate(datasets):\\n  bucketed_datasets.append(BucketedDataset(dataset,\\n                                           bucket_sizes[i]))\\nWe create a list of BucketedDataset objects in bucketed_datasets, one for each\\nbucket size. The BucketedDataset constructor also converts our vocabulary integers\\nto PyTorch tensors so we can pass them into our model later.\\nNext we use the PyTorch’s DataLoader class to create dataloaders for each dataset in\\nbucketed_datasets. Since we created Dataset objects, we get the batching capabili‐\\nties of the DataLoader class without writing any additional code:\\nfrom torch.utils.data import DataLoader\\ndataloaders = []\\nfor dataset in bucketed_datasets:\\n  dataloaders.append(DataLoader(dataset,\\n                                batch_size=32,\\n                                shuffle=True))\\nThe dataloaders list hold a dataloader for each bucket size, so when we run our\\ntraining or test loops, we will select a bucket size (randomly for training) and use the\\ncorresponding dataloader to pull a batch of encoders and decoder inputs:\\nfor epoch in range(n_epochs):\\n  # exhaust all dataloaders randomly\\n  # keep track of when we used up all values\\n  dataloader_sizes = []\\n  for dataloader in dataloaders:\\n    dataloader_sizes.append(len(dataloader))\\n  \\n  while np.array(dataloader_sizes).sum() != 0:\\n    bucket_id = torch.randint(0,len(bucket_sizes),(1,1)).item()\\n    if dataloader_sizes[bucket_id] == 0:\\n      continue\\n    source, target = next(iter(dataloaders[bucket_id]))\\n    dataloader_sizes[bucket_id] -= 1\\n    loss = train(encoder_inputs,\\n                 decoder_inputs,\\n                 target_weights,\\n                 bucket_id)\\n236 \\n| \\nChapter 9: Models for Sequence Analysis'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 252}, page_content='We measure the loss incurred during prediction time, as well as keep track of other\\nrunning metrics:\\nloss += step_loss / steps_per_checkpoint current_step += 1\\nLastly, every so often, as dictated by a global variable, we will carry out a number of\\ntasks. First, we print statistics for the previous batch, such as the loss, the learning\\nrate, and the perplexity. If we find that the loss is not decreasing, the model may have\\nfallen into a local optima. To assist the model in escaping this, we anneal the learning\\nrate so that it won’t make large leaps in any particular direction. At this point, we also\\nsave a copy of the model and its weights and activations to disk.\\nThis concludes the high-level details of training and using the models. We have\\nlargely abstracted away the fine details of the model itself. For more, see the book’s\\nrepository.\\nWith this, we’ve successfully completed a full tour of the implementation details of\\na fairly sophisticated neural machine translation system. Production systems have\\nadditional tricks that are not as generalizable, and these systems are trained on huge\\ncompute servers to ensure that state-of-the-art performance is met.\\nFor reference, this exact model was trained on eight NVIDIA Telsa M40 GPUs for\\nfour days. We show plots for the perplexity in Figures 9-31 and 9-32, and show the\\nlearning rate anneal over time as well. In Figure 9-31, we see that after 50,000 epochs,\\nthe perplexity decreases from about 6 to 4, which is a reasonable score for a neural\\nmachine translation system. In Figure 9-32, we observe that the learning rate almost\\nsmoothly declines to 0. This means that by the time we stopped training, the model\\nwas approaching a stable state.\\nFigure 9-31. Plot of perplexity on training data over time\\nDissecting a Neural Translation Network \\n| \\n237'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 253}, page_content='Figure 9-32. Plot of learning rate over time\\nTo showcase the attentional model more explicitly, we can visualize the attention that\\nthe decoder LSTM computes while translating a sentence from English to French.\\nIn particular, we know that as the encoder LSTM is updating its cell state in order\\nto compress the sentence into continuous vector representations, it also computes\\nhidden states at every time step. We know that the decoder LSTM computes a\\nconvex sum over these hidden states, and we can think of this sum as the attention\\nmechanism; when there is more weight on a particular hidden state, we can interpret\\nthat as the model is paying more attention to the token inputted at that time step.This\\nis exactly what we visualize in Figure 9-33.\\nThe English sentence to be translated is on the top row, and the resulting French\\ntranslation is on the first column. The lighter a square is, the more attention the\\ndecoder paid to that particular column when decoding that row element. That is,\\nthe (i, j)th element in the attention map shows the amount of attention that was paid\\nto the jth token in the English sentence when translating the ith token in the French\\nsentence.\\nWe can immediately see that the attention mechanism seems to be working quite\\nwell. Large amounts of attention are generally being placed in the right areas, even\\nthough there is slight noise in the model’s prediction. It is possible that adding layers\\nto the network would help produce crisper attention. One impressive aspect is that\\nthe phrase “the European Economic” is translated in reverse in French as the “zone\\néconomique européenne,” and as such, the attention weights reflect this flip. These\\nkinds of attention patterns may be even more interesting when translating from\\nEnglish to a different language that does not parse smoothly from left to right.\\n238 \\n| \\nChapter 9: Models for Sequence Analysis'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 254}, page_content='Figure 9-33. Visualizing the weights of the convex sum when the decoder attends over\\nhidden states in the encoder\\nWith one of the most fundamental architectures understood and implemented, we\\nnow move forward to study exciting new developments with RNNs and begin a foray\\ninto more sophisticated learning.\\nSelf-Attention and Transformers\\nEarlier, we discussed a form of attention that was first presented in Bahdanau et al.\\nin 2015. Specifically, we used a simple, feed-forward neural network to calculate the\\nalignment score of each encoder hidden state with the decoder state at the current\\ntime step. In this section, we’ll discuss a different form of attention called scaled\\ndot product attention, its use in self-attention, and the transformer, a recent language\\nmodeling breakthrough. Transformer-based models have primarily replaced LSTM,\\nand have been proven to be superior in quality for many sequence-to-sequence\\nproblems.\\nDot product attention is really as simple as it sounds—this method calculates align‐\\nment scores as the dot product between each encoder hidden state st. These weights\\nare used in the calculation of the context vector, which is a convex sum (via softmax)\\nof the encoder hidden states. Why use the dot product to measure alignment? As we\\nlearned in Chapter 1, the dot product of two vectors can be expressed as a product\\nof the norms of the two vectors and the cosine of the angle between them. As the\\nangle between the two vectors goes to zero, the cosine goes to one. Also, recall from\\ntrigonometry that cosine has the range 1 to –1 when the input angle is between 0\\ndegrees and 180 degrees, which is the only part of the domain of the angle we need\\nto consider. The dot product has the nice property that, as the angle between two\\nSelf-Attention and Transformers \\n| \\n239'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 255}, page_content='7 Vaswani et. al. “Attention Is All You Need.” arXiv Preprint arXiv:1706.03762 2017.\\nvectors gets smaller, the dot product gets larger. This allows us to use the dot product\\nas a natural measure of similarity.\\nIn 2017, Vaswani et al.7 introduced a modification to the preexisting dot product\\nattention framework via the inclusion of a scaling factor—the square root of the\\ndimension of the hidden states. Vaswani et al. acknowledge the fact that, as hidden\\nstate representations get larger and larger in terms of dimension, we expect to see\\nsignificantly more instances of high magnitude dot products. To understand the rea‐\\nsoning behind the inclusion of this scaling factor, assume, for the sake of argument,\\neach index of ℎi is drawn independently and identically distributed from a mean zero,\\nunit variance random variable. Let’s compute the expectation and variance of their\\ndot product:\\nE stTℎi = ∑j = 1\\nk\\nE st, j * ℎi, j\\n=∑j = 1\\nk\\nE st, j E ℎi, j\\n= 0\\nVar stTℎi = ∑j = 1\\nk\\nVar st, j * ℎi, j\\n= ∑j = 1\\nk\\nE st, j\\n2 * ℎi, j\\n2\\n−E st, j * ℎi, j\\n2\\n= ∑j = 1\\nk\\nE st, j\\n2\\nE ℎi, j\\n2\\n= ∑j = 1\\nk\\n1\\n= k\\nLet’s review the steps that got us to these conclusions regarding the expectation and\\nvariance. The first equality in the expectation is due to the linearity of expectation,\\nsince the dot product can be expressed as a sum of the product of each index.\\nThe second equality comes from the fact that the two random variables in each\\nexpectation are independent, so we can separate the expectation of the product into a\\nproduct of expectations. The final step follows directly from the fact that each of these\\nindividual expectations are zero.\\nThe first equality in the variance is due to the linearity of variance when the individ‐\\nual terms are all independent. The second equality is just the definition of variance.\\nThe third equality uses a result from our calculation of the expectation of the dot\\nproduct (we can separate out the square of the expectation into the product of\\nsquares of expectations, where each individual expectation is zero). Additionally, the\\nexpectation of the product of squares can be split up into a product of expectations\\n240 \\n| \\nChapter 9: Models for Sequence Analysis'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 256}, page_content='of squares, since the square of each random variable is independent of the squares of\\nall other random variables. The second to last equality comes from the fact that the\\nexpectation of the square of each random variable is just the variance of the random\\nvariable (since the expectation of each random variable is zero). The final equality\\nfollows directly.\\nWe see that the expectation of the dot product is zero, while its variance is k,\\nthe dimension of the hidden representation. Thus, as the dimension increases, the\\nvariance increases—this implies a higher probability of seeing high-magnitude dot\\nproducts.\\nUnfortunately, with the presence of more high-magnitude dot products comes\\nsmaller gradients due to the softmax function. Although we won’t derive it here, this\\nmakes a lot of intuitive sense—think back to the use of softmax in neural networks\\nfor classification problems. As the neural network gets more and more confident in a\\ncorrect prediction (i.e., a high logit value for the true index), the gradient gets smaller\\nand smaller. The scaling factor introduced by Vaswani et al. reduces the magnitude of\\ndot products, leading to larger gradients and better learning.\\nNow that we’ve covered scaled dot product attention, we turn our attention to\\nself-attention. In the previous sections, we saw attention through the context of\\nmachine translation where we are given a training set of sentences that are in English\\nand French, and the goal is to be able to translate unseen English sentences to\\nFrench. In this specific class of problems, there exists a direct supervision through\\nthe target French sentences. However, attention can also be used in a completely\\nself-contained manner. The intuition is that, given a sentence in English, we may be\\nable to perform more insightful sentiment analysis, more effective machine reading,\\nand better understanding via learning relationships between tokens within sentences\\nor paragraphs.\\nThe transformer, our final topic for this section, utilizes both scaled dot product\\nattention and self-attention. The transformer architecture (Vaswani et al. 2017) has\\nboth encoder and decoder architectures, where there exists self-attention within both\\nthe encoder and decoder, as well as standard attention between the encoder and\\ndecoder. The self-attention layers in the encoders and decoders allow each to attend\\nto all positions prior to the current position in their respective architectures. The\\nstandard attention allows the decoder to attend to each encoder hidden state, as\\ndescribed earlier.\\nSelf-Attention and Transformers \\n| \\n241'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 257}, page_content='Summary\\nIn this chapter, we’ve delved deep into the world of sequence analysis. We’ve ana‐\\nlyzed how we might hack feed-forward networks to process sequences, developed a\\nstrong understanding of RNNs, and explored how attentional mechanisms can enable\\nincredible applications ranging from language translation to audio transcription.\\nSequence analysis is a field that ranges problems not only in natural language, but\\nalso topics in finance, such as time-series analysis of returns of financial assets. Any\\nfield that involves longitudinal analyses, or analyses across time, could use the appli‐\\ncations of sequence analysis described in this chapter. We advise you to really deepen\\nyour understanding of sequence analysis via implementation across different fields\\nand by comparing the results of the techniques presented for natural language with\\nthe state-of-the-art in each field. There are also situations in which the techniques\\npresented here may not be the most appropriate modeling choice, and we advise\\nyou to think deeply about why the modeling assumptions made here may not apply\\nbroadly. Sequence analysis is a powerful tool that has a place in almost all technical\\napplications, not just natural language.\\n242 \\n| \\nChapter 9: Models for Sequence Analysis'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 258}, page_content='CHAPTER 10\\nGenerative Models\\nGenerative models attempt to understand the latent, or underlying, process that\\nproduces the data we see. For example, when breaking down images of digits in the\\nMNIST dataset, we can interpret some attributes of the underlying process generating\\neach image as the digit itself (a discrete variable ranging from zero through nine),\\nthe orientation or angle at which it will be drawn, the size of the resulting image,\\nthe thickness of the lines, and some noise component (all of which are continuous\\nvariables). So far, we’ve been concerned with discriminative models, either in the\\nregression or classification setting. In the classification setting, discriminative models\\ntake as input an example such as an image from the MNIST dataset and attempt\\nto determine the most likely digit category, from zero through nine, that the input\\nbelongs to. Generative models instead attempt to fully model the data distribution,\\nand in the process may implicitly try to learn some of the features mentioned previ‐\\nously to generate images that look as if they were originally from the MNIST dataset.\\nNote that generative modeling is a harder problem than discriminative modeling, as\\na discriminative model may, for example, need to learn only a few features well to\\ndistinguish between different digits in the MNIST dataset to a satisfactory degree.\\nGenerative models come in many varieties, and in this chapter, we provide a glimpse\\ninto a vast research landscape that has begun to blossom only in the past decade.\\n243'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 259}, page_content='1 Goodfellow et al. “Generative Adversarial Networks.” arXiv Preprint arXiv:1406.2661. 2014.\\nGenerative Adversarial Networks\\nGenerative Adversarial Networks, or GANs for short, are a form of generative model\\ndesigned to produce realistic samples of entities, such as images, from noise. They\\nwere introduced by Goodfellow et al. in 2014.1 For the remainder of this section, we\\nwill assume we are working with an image dataset such as MNIST or CIFAR-10. The\\noriginal GAN architecture is broken down into two neural networks: the discrimina‐\\ntor and the generator.\\nThe generator takes in samples from some noise distribution, such as a multivariate\\nGaussian distribution, and outputs an image. The discriminator is tasked with pre‐\\ndicting whether this image was produced by the generator or was sampled from the\\noriginal dataset. As the generator gets better and better at producing images that\\nlook real, the discriminator has a harder time determining whether a given image\\nwas produced by the generator or sampled from the dataset. We can think of these\\ntwo networks as participating in a game, competing against each other to develop.\\nEach network evolves until the generator can eventually produce images that look as\\nif they were drawn directly from the original dataset, and the discriminator cannot\\ndistinguish between the two sets of images, i.e., predicts that any image is from the\\ndataset with probability 1\\n2.\\nMore rigorously, we define the data distribution to be pdata x . Although we can\\nnever really know the true data distribution, in practice we generally think of it as\\nbeing approximated well enough by the dataset we have on hand (pdata x  is just a\\nuniform distribution over all of the images present in the dataset and zero likelihood\\nassociated with all images that are not in the dataset).\\nWe additionally define the distribution parametrized by the generator to be pg x .\\nThe random variable x represents an entity such as an image, a collection of pixels\\nthat can each be thought of as their own random variables. The generator, which\\nwe also refer to as G, defines pg x  by mapping samples from the noise distribution,\\nwhich we will refer to as p(z), to the data space, which consists of all possible images\\n(not just those in the dataset). It is important to keep in mind that G itself is a\\ndeterministic function, but implicitly defines a distribution by acting on the noise\\ndistribution. Note that this distribution is implicit because we can generate samples\\nfrom it only via G(z), rather than being an explicit distribution we can work with\\ndirectly and query an image for its likelihood. Figure 10-1 shows the typical GAN\\narchitecture.\\n244 \\n| \\nChapter 10: Generative Models'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 260}, page_content='Figure 10-1. The discriminator determines whether any input image was sampled from\\nthe dataset or generator. The generator’s goal is to trick the discriminator into believing\\nits images were sampled from the dataset.\\nAn optimal generator for a given dataset would also parametrize pdata x , as this\\nwould perfectly confuse even the best discriminator. In other words, if the generator\\nparametrizes the exact same distribution as that of the dataset and it is equally likely\\nto sample from either the generator or the dataset, then no discriminator would be\\nable to tell where the query originated from, as both are always equally likely. We\\nformalize this intuition in the next paragraph.\\nThinking back to Chapter 2, given a generator that parametrizes the same distribu‐\\ntion as the dataset, we have p x y = generator = p x y = dataset , ∀x, where y is a\\nBernoulli random variable over the two options: generator or dataset. Note that we\\nuse pg x  and p x y = generator  interchangeably, and pdata x  and p x y = dataset\\ninterchangeably, since they mean the same thing. The latter option of each allows\\nus to keep in mind we are working with conditional probabilities. Again, assuming\\nthat sampling from the generator and sampling from the dataset are equally likely,\\nor p y = generator = p y = dataset , we can use Bayes’ Rule to obtain the equality:\\np y = generator x = p y = dataset x , ∀x. Since there are only two options, as y is\\na Bernoulli random variable, we are left with the perfectly confused discriminator\\nalluded to earlier that predicts any image to be sampled from the dataset with\\nprobability 1\\n2.\\nKnowing our end goal, we can now go about designing an objective function for\\ntraining our generator and discriminator in tandem. In the original GAN paper, the\\nobjective presented was:\\nV G, D = Ex ∼pdata x log D x\\n+ Ez ∼p z log 1 −D G z\\nG(z) represents the mapping from the noise distribution to the data space described\\nearlier, and D(x) represents the score assigned to the input image. D(x) is interpreted\\nas the probability that the input image was drawn from the dataset. Of course,\\nthe discriminator D would like to maximize this objective—this corresponds with\\nGenerative Adversarial Networks \\n| \\n245'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 261}, page_content='assigning high probabilities to images drawn from the dataset rather than images\\nproduced by the generator G. G, on the other hand, would like to minimize this\\nobjective, since that corresponds with producing realistic images, or even images that\\nlook exactly like those from the dataset, that confuse D and cause it to return a high\\nscore for these generator-produced images. This idea of maximizing the objective for\\none network and minimizing the objective for the other is termed minimax, and the\\noptimization procedure looks like this:\\nminGmaxDEx ∼pdata x log D x\\n+ Ez ∼p z log 1 −D G z\\nThe paper goes on to show that, for a fixed generator G, the optimal discriminator\\ntrained under this objective would output the following score:\\npdata x\\npdata x + pg x\\nfor a given image x. First, we consider why this should even describe the behav‐\\nior of an optimal discriminator given a fixed generator. Before we get into the\\n“why,” it’s important to keep in mind that D can be alternatively represented as\\npθ y = dataset x , or the discriminator’s belief that the image was drawn from the\\ndataset. Here θ represents the parameters, or weights, of D. When we perform an\\nupdate operation such as gradient descent, θ represents the set of weights that is\\nbeing updated. It is important to keep in mind that this distribution is distinct from\\np y = dataset x  mentioned earlier—the latter is the true probability that a given\\nimage was sampled from the dataset.\\nThe optimal discriminator can never know the exact origin of the image unless\\nit is impossible for the generator to have produced the image, i.e., pg x = 0. We\\ncan quantify the uncertainty in the discriminator’s prediction as a function of the\\nimage’s likelihood under the data distribution, or pdata x , and the image’s likelihood\\nunder the distribution defined by G, or pg x . If the image’s likelihood under the\\ndistribution defined by the generator is less than that of the data distribution, it\\nmakes sense that the optimal discriminator should be swayed accordingly and should\\nscore the image closer to one than zero.\\nNote that a quick back-of-the-envelope check shows that this prop‐\\nerty is true for the score \\npdata x\\npdata x + pg x . But why is this the exact\\nproportion by which the property is true? Let’s take a more con‐\\ncrete look at the score \\npdata x\\npdata x + pg x  and determine why this is the\\noptimal function of the two probabilities.\\n246 \\n| \\nChapter 10: Generative Models'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 262}, page_content='Taking some inspiration from our discussion regarding the perfectly confused dis‐\\ncriminator, we can alternatively express the proposed optimal discriminator score in\\nterms of conditional probabilities:\\np x y = dataset\\np x y = dataset + p x y = generator\\nAdditionally, \\nmaking \\nthe \\nsame \\nassumption \\nregarding \\nequal \\nlikelihood\\nof \\nsampling \\nfrom \\nthe \\ndataset \\nversus \\nsampling \\nfrom \\nthe \\ngenerator\\n(p y = dataset = p y = generator = 0 . 5), we can get to a much more interpretable\\nrepresentation of the optimal score:\\nD* x =\\np x y = dataset\\np x y = dataset + p x y = generator\\n=\\np x y = dataset * p y = dataset\\np x y = dataset * p y = dataset + p x y = generator * p y = generator\\n= p x, y = dataset\\np x\\n= p y = dataset x\\nThe denominator in the third equality is a result of having marginalized out y.\\nThe final result is just the conditional probability of having sampled from the\\ndataset given the input image. It makes sense that the optimal discriminator,\\npθ* y = dataset x , should strive to match the true probability that the input image\\nwas drawn from the dataset, p y = dataset x .\\nNow, we consider why the minimax objective defined earlier is maximized by\\np y = dataset x , or the true conditional probability of having drawn from the data‐\\nset given an image x, under the assumption of a fixed generator. Let’s take a closer\\nlook at the objective and try to reformulate it in a more informative manner that may\\nprovide us with some insight:\\nV G, D = Ex ∼pdata x log D x\\n+ Ez ∼p z log 1 −D G z\\n= Ex ∼p x y = dataset log pθ y = dataset x\\n+ Epφ x y = generator log 1 −pθ y = dataset x\\n= Ex ∼p x y = dataset log pθ y = dataset x\\n+ Epφ x y = generator log pθ y = generator x\\nAs usual, we have formulated the objective in terms of conditional probabilities. To\\nget from the first equality to the second, we note that taking the expectation with\\nrespect to the noise distribution p(z) and then applying a function such as G to each\\nGenerative Adversarial Networks \\n| \\n247'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 263}, page_content='sample is equivalent to just taking the expectation with respect to the distribution\\nover the data space defined by G’s mapping. This is similar in spirit to a concept we\\ndiscussed in Chapter 2, where random variables can be functions of other random\\nvariables. Also note the addition of the letter φ starting from the second line—this\\nletter represents the parameters, or weights, of G.\\nTaking a closer look at the final expression, we start to see an awful lot of similarities\\nbetween the objective and the concepts of entropy and cross entropy introduced in\\nChapter 2. It turns out that we can manipulate the objective slightly without affecting\\nthe best θ here to obtain a sum of the negatives of two cross-entropy terms:\\nθ* = argminθV G, D\\n= argminθEx ∼p x y = dataset log pθ y = dataset x\\n+ Epφ x y = generator log pθ y = generator x\\n= argminθ −H p x, y = dataset , pθ x, y = dataset\\n−H p x, y = generator , pθ x, y = generator\\nAs discussed in Chapter 2, the cross entropy between two distributions is mini‐\\nmized when the two distributions are exactly the same—here we are doing the\\nequivalent by simply maximizing the negative cross entropy instead. Thus, θ ach‐\\nieves the optimal set of weights θ* when pθ x, y = dataset = p x, y = dataset  and\\npθ x, y = generator = p x, y = generator .\\nAs our final step, we’d like to show that at θ*, pθ* y = dataset x = p y = dataset x\\nas promised. We already know that pθ* x, y = dataset = p x, y = dataset  from our\\nprior work. Dividing by p(x) on both sides leaves us with the desired result.\\nSo far, we have assumed a fixed G, and shown various properties regarding the\\noptimal D. Unfortunately, we can’t assume a fixed G in practice, as we must train the\\ngenerator as well as the discriminator. But now that we have shown some properties\\nregarding the optimal D, we can begin to talk about the properties G must satisfy\\nto achieve the global optimum—a generator that can perfectly confuse even the\\noptimal discriminator. If we assume an optimal discriminator and plug in its score\\npdata x\\npdata x + pg x  to the objective V(G,D), we obtain an objective that is solely dependent\\non the parameters, or weights, of G:\\nC G = Ex ∼pdata x log\\npdata x\\npdata x + pg x\\n+ Ex ∼pg x log 1 −\\npdata x\\npdata x + pg x\\n= Ex ∼pdata x log\\npdata x\\npdata x + pg x\\n+ Ex ∼pg x log\\npg x\\npdata x + pg x\\n248 \\n| \\nChapter 10: Generative Models'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 264}, page_content='2 Kingma et al. “Auto-Encoding Variational Bayes.” arXiv Preprint arXiv:1312.6114. 2014.\\n= Ex ∼p x y = dataset log\\np x y = dataset\\np x y = dataset + pφ x y = generator\\n+ Ex ∼pφ x y = generator log\\npφ x y = generator\\np x y = dataset + pφ x y = generator\\nWe can now minimize this objective by optimizing over the generator weights φ.\\nWe refer you to the original GAN paper for the rigorous derivation. However, as\\none might expect by now, it turns out that the optimal distribution G represents,\\nor pg* x , is equal to pdata x , ∀x. This matches our original intuition regarding the\\nperfectly confused discriminator and shows that the objective function proposed in\\nthe original GAN paper does indeed theoretically converge to this global optimum.\\nNow that we have an optimal generator and discriminator, how do we perform image\\ngeneration? All we need to do is sample from our noise distribution p(z) and run\\neach sample through the generator. The generator, being optimal, should produce\\nimages that look as if they were drawn from the dataset itself. It may come as a\\nsurprise to you that the discriminator is no longer needed in this phase—but it\\nhas served its purpose. The discriminator played a key role in competing with the\\ngenerator, each evolving until the latter could produce images that perfectly confused\\nthe discriminator.\\nNote that unlike the standard interpretation of generative modeling, z does not\\nrepresent a set of latent variables from which the data is generated. z simply plays\\nthe role of being a random variable distributed as one of our standard distributions,\\nsuch as a uniform distribution or a standard multivariate Gaussian distribution,\\nwhich are easy to sample from. G, when fully trained and optimal, is a complex,\\ndifferentiable function that transforms samples from p(z) into samples from pdata x ,\\nwhich approximates p(x). In the next section, we will see the parallels between G(z)\\nand the reparametrization trick, which also allows us to sample from a distribution by\\ntransforming samples (via a differentiable function) from a distribution that is easier\\nto sample from.\\nVariational Autoencoders\\nIn parallel to the introduction of GANs, Kingma and Welling introduced the Varia‐\\ntional Autoencoder, or VAE for short, in their seminal paper, “Auto-Encoding Var‐\\niational Bayes,” from 2014.2 The idea behind the VAE is more strongly rooted in\\nprobabilistic modeling than the aforementioned GAN. The VAE assumes there exists\\na set of unobserved latent variables, which we denote as z, that generate the data\\nwe see, which we denote as x. More formally, we say there exists a joint probability\\ndistribution p(x,z) over the latent variables z and the observed data x that factors as\\nVariational Autoencoders \\n| \\n249'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 265}, page_content='p x z p z  (see Figure 10-2). Thinking back to Chapter 2, this factorization is quite\\nintuitive. Given the predefined roles of z and x, the universe in which z takes on some\\nvalue and x is generated from this setting of z makes much more sense than the other\\nway around.\\nFigure 10-2. z represents the latent variables from which every instance of x is generated.\\nThe arrow pointing from z to x signifies this relationship.\\nx could represent any sort of continuous or discrete data, including images. We\\nadditionally know the domain of x due to our knowledge of the dataset. z, on the\\nother hand, is much more elusive. We have no idea what z looks like, so we make\\nsome initial assumptions about it. For example, we may assume that it initially takes\\nthe form of a Gaussian distribution, i.e., p(z) is Gaussian. Again, thinking back to\\nChapter 2, we say that p(z), or our prior on z, is Gaussian.\\nWhenever we think about such a data-generation process, some natural probabilistic\\nquestions (should) come to mind. For example, what is the distribution p z x , or\\nthe posterior of z having known x? As we observe data, our beliefs regarding the\\nunderlying parameters often change. Take the coin flip experiment from Chapter 2\\nas an example. We initially assumed a 50-50 chance of flipping heads, where the\\n50-50 can be thought of as our latent parameter α—the parameter dictating the data\\ngeneration procedure of sequences of heads and tails. This is a little simplified—in\\nreality, we initially have a distribution over α, the probability of flipping heads, which\\nis our prior distribution. Of course, the domain of the prior is the range [0,1], where\\nit is logical to design the prior p α  such that p α = 0 . 5  is larger than all other\\nsettings of α. As we observed sequences of flips, we updated our prior via Bayes’\\nTheorem. In a similar manner, we initially assume p(z) to be a Gaussian distribution\\nwith some mean and variance; but as we observe data, we recalculate our belief in the\\nform of a posterior, p(z|x) (see Figure 10-3).\\nAnother question naturally comes to mind: what is the distribution p(x|z), or the\\nlikelihood of the data x given a certain setting of the latent variables z? In the coin\\nflip setting, p(x|z) is easy to think about. Due to our complete knowledge of the\\nexperiment, we know the probability of any sequence is just the product of the\\nprobability of each flip, which is directly defined by z. In more intricate settings\\nsuch as images, however, we can assume the relationship between the data x and the\\nlatent variables z is much more complicated than that. For example, when looking\\nat images, it is clear that the value of a given pixel is quite affected by the values of\\nits neighboring pixels and sometimes even by pixels much farther than one might\\nthink. The simple independence assumption we have for coin flips will not suffice for\\nour purposes. This is just one reason why we can’t simply use a method like Bayes’\\n250 \\n| \\nChapter 10: Generative Models'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 266}, page_content='Theorem to learn a posterior over z—it requires much more knowledge regarding the\\nsystem than what is immediately available to us.\\nFigure 10-3. Here we have the coin flip experiment, where the prior is designed such that\\n0.5 has the highest likelihood. Once we see a series of heads and tails, the posterior shifts\\nto the right due to there being more heads than tails.\\nIn variational autoencoders, we encode these distributions as neural networks, which\\ncan be seen as complex, nonlinear functions that can accurately model the relation‐\\nships between latent variables z and the observed data x. We denote the neural\\nnetwork that outputs a distribution over the data given a setting of the latent vari‐\\nables, also termed the decoder, as pθ x z , where θ represents the weights of the\\nneural network. In other words, the setting of θ, in addition to the predetermined\\narchitecture of the neural network, completely define the model’s belief of the true\\ndistribution p x z . We optimize θ to achieve a setting that is closest to that of the\\ntrue distribution.\\nWe additionally encode the posterior over z, or p(z|x), as a neural network. We denote\\nthis neural network, termed the encoder, as qφ z x . Similarly to the decoder, we\\noptimize φ to achieve a setting that is closest to that of the true posterior.\\nKingma and Welling made some key observations that made the variational autoen‐\\ncoder a practical means for generative modeling (Figure 10-4). The first was that\\nthe evidence lower bound (ELBO for short), which is a lower bound on the true\\nlog likelihood of the data p x , could be reformulated in a way that allowed for\\ntractable optimization over the encoder and decoder parameters. The second was a\\nreparametrization trick that enabled the computation of a low variance estimate of\\nthe gradient with respect to the parameters of the encoder, φ. Although this may\\nsound like a lot of jargon right now, we will go into each of these key observations in\\nmuch more detail and concretely motivate the encoder-decoder architecture.\\nVariational Autoencoders \\n| \\n251'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 267}, page_content='Figure 10-4. The overall VAE architecture presented in Kingma and Welling. Note that\\nboth z and the image after the decoder are both samples from the encoder distribution\\nand decoder distribution, respectively.\\nLet’s assume we have observed some data x, where each individual example can be\\ndenoted as x i . Note that we are still under the assumption that there exist some set\\nof latent variables z generating the data we’ve seen. We split our analysis over the\\nobserved data into one over each individual example x i . We know there exists a\\ntrue posterior over the latent variables p z x i , but we have no idea what that true\\nposterior is. We assume it can be approximated by some distribution over the latent\\nvariables qφ z x i , where q is a family of distributions in which optimization is\\nmuch easier but complex enough to accurately model the true posterior. An example\\nwould be a multilayer neural network, which, as we’ve already seen, can be efficiently\\noptimized via gradient descent and can represent complex, nonlinear functions. Note\\nthat each example x i  we’ve seen has some true probability of occurrence, which we\\ncan write as p x i . We instead work with log p x i , since this allows us to do some\\nconvenient decomposition into terms we’ve encountered before and doesn’t affect the\\nverity of the optimization process:\\nlog p x i\\n= log p x i , z −log p z x i\\n= log p x i , z −log p z x i\\n+ log qφ z x i\\n−log qφ z x i\\n= Eqφ z x i\\nlog p x i , z −log p z x i\\n+ log qφ z x i\\n−log qφ z x i\\n= Eqφ z x i\\nlog\\np x i , z\\nqφ z x i\\n+ Eqφ z x i\\nlog\\nqφ z x i\\np z x i\\n= ELBO + KL qφ z x i\\np z x i\\nThe first step is to express the marginal likelihood of the individual example x i  as\\na function of the example itself and the latent factors z. As we learned earlier, the\\nmarginal likelihood can be broken down into a quotient of the joint distribution\\np z, x i  and the conditional distribution p z x i . The log function allows us to\\n252 \\n| \\nChapter 10: Generative Models'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 268}, page_content='separate this quotient into a difference between the logs of the two terms. In the\\nsecond step, we use a little trick that allows us to conveniently insert the approximate\\nposterior into the equality—adding and subtracting the same term shouldn’t affect\\nthe equality. In the third step, we insert an expectation with respect to the approx‐\\nimate posterior. Why is this allowed? Well, a priori we know that log p x i  is a\\nconstant. It is just the log of the probability of the example occurring under the true\\ndistribution, which is fixed. Thus, taking the expectation on both sides doesn’t change\\nanything about the left side of the equation, since the expectation of a constant is just\\nthe constant itself. On the right side, we have now gotten closer to expressing the log\\nof the marginal likelihood in terms that we’ve seen before. In the second to last step,\\nwe combine logs back into quotients and use the linearity of expectation to arrive at a\\nsum of two terms: (1) the KL divergence between the approximate posterior and the\\ntrue posterior, and (2) the ELBO, or the evidence lower bound.\\nBy now, you may have noticed that the form of the KL divergence is slightly different\\nthan what we encountered in Chapter 2. Recall the standard KL divergence presented\\nearlier, where the true distribution was p(x) and its approximation was q(x). The\\nKL divergence we defined was the difference between the cross entropy of the two\\ndistributions and the entropy of the true distribution, which was expressed as follows:\\nEp x\\nlog p x\\nq x\\nWe can see that the KL divergence in this derivation is the exact opposite. The\\nexpectation is with respect to the approximate posterior rather than the true poste‐\\nrior, and the numerator and denominator are flipped. Essentially what we see is\\nEq x\\nlog q x\\np x  instead of Ep x\\nlog p x\\nq x . We call this the reverse KL divergence,\\nsince the roles of the model and the truth have been switched, and is the quantity\\nwe attempt to minimize in VAEs. Although this does not have as clean a physical\\ninterpretation as the standard KL, note that the reverse KL divergence is just a type\\nof KL divergence and retains all the properties we discussed in Chapter 2. Thus,\\noptimizing the reverse KL divergence still achieves a unique global minimum of zero\\nwhen q x = p x , ∀x, so it is a valid objective to be optimizing over as it reaches\\nits unique minimum when the approximate posterior is exactly the same as the true\\nposterior.\\nThe reality, however, is that the true posterior p z x i  is still unknown to us. As a\\nresult, we can’t directly minimize any KL divergence with the true posterior. This is\\nwhere the ELBO plays a key role. As we discussed earlier, log p x i  is a constant.\\nThus, minimizing the reverse KL divergence is the same as maximizing the ELBO.\\nThe name evidence lower bound should make more sense now—as we maximize this\\nterm, it provides a better and better lower bound on the true log probability of the\\nexample. If we can develop a methodology for maximizing the ELBO efficiently, we\\nVariational Autoencoders \\n| \\n253'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 269}, page_content='should be well on our way to developing a generative model. Let’s reformulate the\\nELBO into terms that might be easier to work with:\\nEqφ z x i\\nlog\\np x i , z\\nqφ z x i\\n= Eqφ z x i\\nlog p x i , z −log qφ z x i\\n= Eqφ z x i\\nlog p x i z + log p z −log qφ z x i\\n= Eqφ z x i\\nlog p x i z\\n+ Eqφ z x i\\nlog p z −log qφ z x i\\n= Eqφ z x i\\nlog p x i z\\n−Eqφ z x i\\nlog\\nqφ z x i\\np z\\n= −KL qφ z x i\\np z\\n+ Eqφ z x i\\nlog p x i z\\nAt this point, we can start to see the beginnings of an architecture and an optimiza‐\\ntion procedure for maximizing the ELBO. For example, the first term is just the\\nreverse KL divergence between the approximate posterior and the prior, which we\\nalready assumed to be a Gaussian distribution. We can use a neural network, or\\nencoder, to represent the approximate posterior. The reverse KL divergence acts as\\na regularization term on the approximate posterior, since maximizing the negative\\nof the reverse KL is the same as minimizing the reverse KL. Regularization prevents\\nthe approximate posterior from straying too far from the prior distribution. This is\\ndesirable since we have witnessed only a single example, and thus we don’t want our\\nbelief over the latent variables to shift too much from our prior. The second term\\nis the expected true log likelihood of the example given a setting of latent variables\\nz, where z is sampled from the approximate posterior. Wanting to maximize this\\nquantity with respect to φ is intuitively reasonable. This influences the approximate\\nposterior to assign higher likelihoods to settings of z that, in turn, explain the input\\nexample x i  as well as possible. The balancing act between regularization, which\\nprevents overfitting, and maximum likelihood estimation, which on its own would\\nreach an optimum where qφ z x i  is just a point mass over the setting of z that best\\ndescribes x i , is a classic optimization procedure you’ve likely encountered in many\\ndata science and machine learning problems.\\nHowever, as noted earlier, we unfortunately don’t have access to the true conditional\\ndistribution p x z . Instead, we attempt to learn it using a second neural network—\\nthe decoder. We denote the parameters of the decoder as θ and let the decoder repre‐\\nsent the distribution pθ x z . In summary, we perform the following optimization\\nprocedure:\\nφ*, θ* = argmaxφ, θ −KL qφ z x i\\np z\\n+ Eqφ z x i\\nlog pθ x i z\\n254 \\n| \\nChapter 10: Generative Models'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 270}, page_content='We’ve already discussed why this is a valid optimization procedure for the encoder\\nparameters φ, assuming that pθ x z = p x z . Of course, this assumption is not\\nsatisfied at the beginning of training. However, as training progresses and θ becomes\\nmore and more optimal, we eventually arrive at the desired theoretical optimization.\\nBut the question still remains: why is this a valid optimization procedure for θ? If we\\nassume the encoder represents the true posterior distribution, we’d want to maximize\\nthe likelihood of recovering the original example x i  from our encoder samples z.\\nOf course, just like the optimization of φ, our assumption about the approximate\\nposterior is not satisfied at the beginning of training—but as training progresses and\\ntwo networks improve jointly, we hope to eventually reach our goal.\\nThis leads us into how to actually carry out the optimization. For θ, it turns out we\\ncan use standard minibatch gradient descent techniques directly:\\n∇θ −KL qφ z x i\\np z\\n+ Eqφ z x i\\nlog pθ x i z\\n= ∇θ −KL qφ z x i\\np z\\n+ ∇θEqφ z x i\\nlog pθ x i z\\n= ∇θEqφ z x i\\nlog pθ x i z\\n= Eqφ z x i\\n∇θ log pθ x i z\\n≈1\\nn ∑j = 1\\nn\\n∇θ log pθ x i z = zj\\nThe first equality arises from the fact that the gradient of a sum of terms is equal to\\nthe sum of the gradients of each of the terms. Since the first term is not a function of\\nθ, its gradient with respect to θ is 0, leading us to the second equality. From there we\\nhave the standard minibatch gradient estimate derivation.\\nThe optimization with respect to φ is not as simple. If we try to do the same for φ as\\nwe did for θ, we run into an unforeseen issue:\\n∇φ −KL qφ z x i\\np z\\n+ Eqφ z x i\\nlog pθ x i z\\n= ∇φ −KL qφ z x i\\np z\\n+ ∇φEqφ z x i\\nlog pθ x i z\\n= ∇φ −KL qφ z x i\\np z\\n+ ∇φ∫qφ z x i\\nlog pθ x i z dz\\n= ∇φ −KL qφ z x i\\np z\\n+ ∫∇φqφ z x i\\nlog pθ x i z dz\\nIn the last step, we can’t express the second term as an expectation. This is because\\nthe gradient is with respect to the parameters of the distribution from which we are\\nVariational Autoencoders \\n| \\n255'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 271}, page_content='sampling. We can’t simply switch the order of the expectation and gradient as we did\\nfor θ. To get around this, we make the following observation:\\n∇φqφ z x i\\n= ∇φqφ z x i *\\nqφ z x i\\nqφ z x i\\n= qφ z x i *\\n∇φqφ z x i\\nqφ z x i\\n= qφ z x i ∇φ log qφ z x i\\nWith a bit of calculus and algebra, we have derived an equivalent form for the\\ngradient. If we substitute this reformulation into the step we were stuck on:\\n= ∇φ −KL qφ z x i\\np z\\n+ ∫qφ z x i ∇φ log qφ z x i\\nlog pθ x i z dz\\n= ∇φ −KL qφ z x i\\np z\\n+ Eqφ z x i\\n∇φ log qφ z x i\\nlog pθ x i z\\n≈∇φ −KL qφ z x i\\np z\\n+ 1\\nn ∑j = 1\\nn\\n∇φ log qφ z = zj x i\\nlog pθ x i z = zj\\nWe can now use standard minibatch gradient estimation techniques to optimize our\\nobjective with respect to φ. The observation we made is a well-known technique in\\nthe machine learning community termed the log trick. We will see this technique used\\nagain later in the chapter on reinforcement learning when we introduce the policy\\ngradient method.\\nNow that we have fully dissected the first observation that Kingma and Welling made,\\nwe now move to the second: the computation of a low variance estimate of the\\ngradient with respect to φ. As we mentioned earlier, the log trick allows us to estimate\\nthis gradient. However, this estimate has been shown to be of high variance. This\\nmeans that if we were to run trials where, in each trial, we draw a few samples zj\\nfrom the approximate posterior and estimate the gradient with respect to φ, we would\\nexpect to see vastly different estimates of the gradient across trials. Of course, this is\\nundesirable, as we’d like trials for the same input example to be consistent with each\\nother to have any confidence in our training procedure. We could try to ameliorate\\nthis by drawing many samples from the approximate posterior for each example, but\\nthis becomes computationally prohibitive for relatively little gain.\\nKingma and Welling proposed an alternative method to the log trick for getting\\naround the issue of taking the gradient with respect to the weights of the network\\nparametrizing distribution from which we are sampling. This method is called the\\nreparametrization trick, and it allows us to compute a low variance estimate of the\\ngradient, as opposed to the log trick. Why this is the case is beyond the scope of this\\n256 \\n| \\nChapter 10: Generative Models'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 272}, page_content='text, but we refer you to the vast amount of academic literature that exists on this and\\nsimilar topics.\\nThe reparametrization trick involves assuming the approximate posterior takes on\\nsome form, such as a multivariate Gaussian distribution, and then expressing this\\ndistribution as a function of another distribution that has no dependence on the\\nweights of the encoder. Let’s assume that qφ z x i  takes on the form N z; μφ, σφ\\n2I .\\nThis represents a multivariate Gaussian distribution where each component zi is\\nindependent of all other components and zi ∼N μφ, i, σφ, i\\n2\\n, ∀i. We use φ in the sub‐\\nscript to explicitly show the approximate posterior’s dependence on the parameters of\\nthe encoder through its mean and variance vectors, which are defined by the encoder.\\nIn its current form, we run into the issue of not being able to switch the order of the\\nexpectation and the gradient that we encountered earlier. Using the reparametrization\\ntrick, we can rewrite the sampling procedure as:\\nz ∼N μφ, σφ\\n2I\\nz = μφ + σφ * ϵ, ϵ ∼N 0, I\\nWe highly encourage you to work out why the sampling procedure can be rewritten\\nin this manner using the definition of the Gaussian distribution. It will be easier to\\nconsider the univariate case first, where X is a standard Gaussian random variable,\\nand then show Y = c*X is a Gaussian random variable with mean zero and variance\\nc2. Then, consider the general univariate case where X is any Gaussian random\\nvariable, and show Y = X + c is a Gaussian random variable with mean E[X] + c\\nand variance Var(X). Putting these steps together will get you to the reformulated\\nsampling procedure described previously.\\nIn summary, we have expressed the approximate posterior as a function of a distribu‐\\ntion that is independent of φ, along with a mean vector and a standard deviation\\nvector that are dependent on φ. We term the random variable ϵ an auxiliary random\\nvariable. Plugging this reformulation into our troublesome gradient expression from\\nearlier:\\n∇φEqφ z x i\\nlog pθ x i z\\n= ∇φEϵ ∼N 0, I\\nlog pθ x i gφ ϵ\\n= Eϵ ∼N 0, I ∇φ log pθ x i gφ ϵ\\n≈1\\nn ∑j = 1\\nn\\n∇φ log pθ x i gφ ϵj\\nWhere gφ ϵ = μφ + σφ * ϵ. We rewrote z as gφ ϵ  to explicitly show that the depend‐\\nence on the encoder parameters is now only through the deterministic function\\nVariational Autoencoders \\n| \\n257'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 273}, page_content='applied to the sampling distribution, rather than the sampling distribution itself. This\\nallows us to switch the order of the expectation and the gradient seamlessly, thereby\\nlending it to standard minibatch gradient estimation techniques.\\nHow does this change manifest itself in the encoder architecture? Earlier, when\\nusing the log trick, we could directly parametrize the approximate posterior via the\\nencoder. Now, we instead have the encoder, for each example x i , output a vector of\\nmeans μφ, a vector of standard deviations σφ, and sample ϵ from a standard Gaussian\\ndistribution that is completely separate from the encoder-decoder VAE architecture.\\nNote that the reparametrization technique comes with its own restrictions—we must\\nassume a form for the approximate posterior, in this case a Gaussian, that allows us\\nto define a differentiable function such as gφ. However, there’s no guarantee the true\\nposterior is Gaussian—it is most likely a complex distribution that cannot be repre‐\\nsented as functions of our standard distributions. This is a trade-off we must make to\\nachieve a low variance gradient estimate for tractable optimization (Figure 10-5).\\nFigure 10-5. What the encoder looks like after the inclusion of reparametrization. It\\nreturns a mean and standard deviation vector, which we can combine with ϵ to generate\\nthe setting of z. The purpose of the circle versus rectangles is to show that the only\\nsampling is happening for ϵ, completely independent of the encoder architecture. The\\nmean and standard deviation vectors are produced deterministically from the input\\nimage. In addition, z is deterministic once we know the value of ϵ.\\nNote that the training procedure for a VAE is quite simple—the beast was in the\\nmotivation and mathematics behind the architecture and optimization. All we need\\nto do is:\\n1. Sample an example x i  from the dataset.\\n1.\\n2. Run x i  through the encoder network to generate a vector of means μφ and a\\n2.\\nvector of standard deviations σφ.\\n3. Sample ϵ and calculate the result of gφ ϵ .\\n3.\\n258 \\n| \\nChapter 10: Generative Models'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 274}, page_content='4. Run the result through the decoder network, which now represents the distribu‐\\n4.\\ntion pθ x z = gφ ϵ .\\n5. Query this distribution with our initial example x i and take the log of the\\n5.\\nresulting likelihood. This will be our decoder loss. If you took multiple samples\\nof ϵ in step 3, run the above procedure for each sample, and average to get the\\ndecoder loss.\\n6. Sum the decoder loss with −KL qφ z x i\\np z , the encoder loss, to get a final\\n6.\\nloss. Use the negative of the final loss in the next step since we want to maximize\\nit instead of minimize it.\\n7. Perform classical SGD/minibatch gradient descent to update φ and θ.\\n7.\\nNow that we have covered how to train a VAE, how do we utilize it as a generative\\nmodel once it is trained? Note that we initially defined the generative process as\\np x z p z , where we start with some setting of the latent variables z sampled from\\nthe prior distribution and map z to an instance x in the data space via the conditional\\nlikelihood. We’ve already learned this generative process in the form of pθ x z , or\\nthe decoder, and assumed the prior distribution p z  to be a multivariate standard\\nGaussian at the beginning. To generate samples from a VAE, we sample zi from the\\nprior distribution p(z), pass this sample through the decoder so it now represents the\\ndistribution pθ x z = zi , and finally sample xi from pθ x z = zi . Note that we no\\nlonger need the approximate posterior at this step—however, it played a key role in\\nthe training of the decoder and is still useful in understanding how our latent variable\\ndistribution shifts after witnessing an example from the dataset.\\nImplementing a VAE\\nIn this section, we will build a VAE from scratch in PyTorch. We will additionally\\nprovide some example training and testing code on the famous MNIST digits dataset.\\nBefore we begin, here is a list of the packages you will need to reproduce this section\\non your own:\\nimport torch\\nfrom torch.distributions.multivariate_normal \\\\\\n  import MultivariateNormal\\nimport torch.nn as nn\\nfrom torchvision import datasets, transforms\\nfrom torchvision.utils import save_image\\nimport torch.optim as optim\\nLet’s start with the encoder. As we discussed in the previous section, the encoder is\\na neural network that outputs a vector of means and a vector of standard deviations.\\nEach index represents a univariate Gaussian, and the entire vector represents a mul‐\\ntivariate Gaussian where each component is independent from the others. Though\\nImplementing a VAE \\n| \\n259'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 275}, page_content='we are working with image data, for the sake of simplicity we convert each image\\ninto a vector by flattening it at the start. This allows us to apply standard, fully\\nconnected layers on the input. Since each image in the MNIST dataset is of size 28 ×\\n28, each resulting representation is a 784-dimensional vector. We also need to decide\\non the number of components, or latent variables, we will use to represent the latent\\nspace. We can treat the number of components as a hyperparameter—if we notice\\nthat the decoder log likelihoods of input examples are consistently low even after a\\nsignificant amount of training, this may indicate an approximate posterior that is not\\nexpressive enough. Increasing the number of components and retraining in this case\\nis advisable.\\nHere is example code for an encoder:\\n# Encoder layers (Gaussian MLP)\\nD_in, H, D_out = 784, 200, 20\\ninput_layer = nn.Linear(D_in, H)\\nhidden_layer_mean = nn.Linear(H, D_out)\\nhidden_layer_var = nn.Linear(H, D_out)\\nFor the sake of simplicity, we leave out nonlinearities between the layers for now. Our\\nencoder consists of two levels of layers. The first level operates on the input, embed‐\\nding the vector into a lower dimensional representation. The second level operates on\\nthe 200-d representation and consists of two independent layers: one for determining\\nthe means of each of the univariate Gaussian components, and one for determining\\nthe standard deviations of each of the univariate Gaussian components. Here, we\\nuse 20 components. As we stated earlier, we assume qφ z x i  takes the form of a\\nmultivariate Gaussian, where each component is independent of the others. Note that\\nattempting to learn a full covariance matrix is computationally prohibitive (amongst\\nother concerns), as its size grows quadratically with the number of components.\\nHere is example code for a decoder:\\n# Decoder layers (Bernoulli MLP for MNIST data)\\nrecon_layer = nn.Linear(D_out, H)\\nrecon_output = nn.Linear(H, D_in)\\nAgain, we leave out the nonlinearities for the sake of simplicity. The decoder operates\\non the sampled z, which we know is a 20-d vector. The rest of the decoder architec‐\\nture is symmetrical to the encoder, and outputs a distribution over the input data.\\nAlthough not in the code just yet, there is a final sigmoid layer that will be applied\\nto the output of the recon_output layer which, recall, squashes each input dimension\\ninto the range (0,1). Since we are working with the discrete MNIST dataset where\\neach pixel is represented as either a zero or a one, the output of the final sigmoid\\nlayer is used to represent a Bernoulli distribution for each pixel. Recall the Bernoulli\\ndistribution from Chapter 2, represented as Ber(p), where p is the probability of\\nreturning a one and 1 – p is the the probability of returning a zero.\\n260 \\n| \\nChapter 10: Generative Models'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 276}, page_content='More formally, we have that the decoder likelihood distribution pθ x z  can be\\nrewritten as a product over each pixel:\\npθ x z = ∏j = 1\\n784 pθ xj z\\nwhere p xj z = Ber decoder z j\\nNote that decoder(z) represents the 784-d vector after applying the sigmoid layer. For\\na given pixel xj\\ni  in the input example x i , we’d like its corresponding probability p\\nto be close to one if xj\\ni = 1, and its corresponding probability p to be close to zero\\nif xj\\ni = 0. As you may recall from the previous section, we work with log pθ x z ,\\nwhich reduces to ∑j = 1\\n784\\nlog pθ xj z .\\nNow, we can put the encoder and decoder together into a single VAE architecture:\\nclass VAE(nn.Module):\\n  def __init__(self, D_in, H, D_out):\\n    super(VAE, self).__init__()\\n    self.D_in, self.H, self.D_out = D_in, H, D_out\\n    # Encoder layers (Gaussian MLP)\\n    self.input_layer = nn.Linear(D_in, H)\\n    self.hidden_layer_mean = nn.Linear(H, D_out)\\n    self.hidden_layer_var = nn.Linear(H, D_out)\\n    \\n    # Decoder layers (Bernoulli MLP for MNIST data)\\n    self.recon_layer = nn.Linear(D_out, H)\\n    self.recon_output = nn.Linear(H, D_in)\\n    self.tanh = nn.Tanh()\\n    self.sigmoid = nn.Sigmoid()\\n    \\n  def encode(self, inp):\\n    h_vec = self.input_layer(inp)\\n    h_vec = self.sigmoid(h_vec)\\n    means = self.hidden_layer_mean(h_vec)\\n    log_vars = self.hidden_layer_var(h_vec)\\n    return means, log_vars\\n  def decode(self, means, log_vars):\\n    # Reparametrization trick\\n    std_devs = torch.pow(2,log_vars)**0.5\\n    aux = MultivariateNormal(torch.zeros(self.D_out), \\\\\\n    torch.eye(self.D_out)).sample()\\n    sample = means + aux * std_devs\\n    \\n    # Reconstruction\\n    h_vec = self.recon_layer(sample)\\n    h_vec = self.tanh(h_vec)\\n    output = self.sigmoid(self.recon_output(h_vec))\\nImplementing a VAE \\n| \\n261'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 277}, page_content='return output\\n  def forward(self, inp):\\n    means, log_vars = self.encode(inp)\\n    output = self.decode(means, log_vars)\\n    return output, means, log_vars\\n  def reconstruct(self, sample):\\n    h_vec = self.recon_layer(sample)\\n    h_vec = self.tanh(h_vec)\\n    output = self.sigmoid(self.recon_output(h_vec))\\n    return output\\nThe call to encode is followed by the call to decode in the forward function. Note that\\ndecode uses only a single sample from the approximate posterior, as we found that a\\nsingle sample is sufficient for the MNIST dataset, but this can be easily modified to\\nwork for multiple samples. To calculate the reverse KL, the forward function returns\\nthe results of the encode call in addition to the decoder likelihood distribution.\\nHere is example code for computing the loss:\\ndef compute_loss(inp, recon_inp, means, log_vars):\\n  # Calculate reverse KL divergence\\n  # (formula provided in Kingma and Welling)\\n  kl_loss = -0.5 * torch.sum(1 + log_vars\\n                            - means ** 2 - torch.pow(2,log_vars))\\n  # Calculate BCE loss\\n  loss = nn.BCELoss(reduction=\"sum\")\\n  recon_loss = loss(recon_inp, inp)\\n  return kl_loss + recon_loss\\nWe recommend you take a look at the PyTorch documentation for nn.BCELoss and\\nverify that it is indeed computing the negative log likelihood of the input example\\nx i : −∑j = 1\\n784\\nlog pθ xj\\ni z . We also recommend you verify that the kl_loss term is\\nthe reverse KL divergence between two Gaussian distributions as derived in Kingma\\nand Welling. Returning the sum of the negative log likelihood and the reverse KL\\ndivergence as a final loss term gets us to the end of step 6 from the previous section.\\nFinally, for some training code:\\nD_in, H, D_out = 784, 500, 20\\nvae = VAE(D_in, H, D_out)\\nvae.to(\"cpu\")\\ndef train():\\n  vae.train()\\n  optimizer = optim.Adam(vae.parameters(), lr=1e-3)\\n  train_loader = torch.utils.data.DataLoader(\\n      datasets.MNIST(\\'../data\\',\\n                     train=True,\\n262 \\n| \\nChapter 10: Generative Models'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 278}, page_content='download=True,\\n                     transform=transforms.ToTensor()),\\n                     batch_size=100,\\n                     shuffle=True)\\n  \\n  epochs = 10\\n  for epoch in range(epochs):\\n    for batch_idx, (data, _) in enumerate(train_loader):\\n      optimizer.zero_grad()\\n      data = data.view((100,784))\\n      output, means, log_vars = vae(data)\\n      loss = compute_loss(data, output, means, log_vars)\\n      loss.backward()\\n      optimizer.step()\\n      if (batch_idx * len(data)) % 10000 == 0:\\n        print(\\n            \\'Train Epoch: {} [{}/{} ({:.0f}%)]\\\\tLoss: {:.6f}\\' \\\\\\n            .format(\\n            epoch, batch_idx * len(data), len(train_loader.dataset),\\n            100. * batch_idx / len(train_loader), loss.item()))\\n  torch.save(vae.state_dict(), \"vae.%d\" % epoch)\\nHere, we train the VAE for 10 epochs, saving the state of the VAE at the end of each\\nepoch. Note that we set some hyperparameters fixed here, such as the learning rate\\nof the optimizer and the number of latent variables. We recommend writing some\\nvalidation code, in addition to the training code presented here, to select the best\\nhyperparameter settings.\\nFinally, how can we test the generative capabilities of our fully trained VAE? We know\\nthat the generative process can be written as p(z)p(x|z), where we first draw a sample\\nzj from our prior, run the sample through the decoder so the decoder’s likelihood\\ndistribution now represents pθ x z = zj , and sample xj from this distribution. Here\\nis the code that puts this logic into action:\\ndef test():\\n  dist = MultivariateNormal(torch.zeros(D_out), torch.eye(D_out))\\n  vae = VAE(D_in, H, D_out)\\n  vae.load_state_dict(torch.load(\"vae.%d\" % 9))\\n  vae.eval()\\n  outputs = []\\n  \\n  for i in range(100):\\n    sample = dist.sample()\\n    outputs.append(vae.reconstruct(sample).view((1,1,28,28)))\\n  outputs = torch.stack(outputs).view(100,1,28,28)\\n  save_image(outputs, \"prior_reconstruct_100.png\", nrow=10)\\nThe for loop generates 100 samples from the approximate posterior, and for each of\\nthose samples, a sample from the corresponding decoder likelihood distribution over\\nImplementing a VAE \\n| \\n263'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 279}, page_content='the input data. The last couple of lines of code allow us to save the samples in a 10 ×\\n10 grid, depicted in Figure 10-6.\\nFigure 10-6. 100 samples from a VAE trained on the MNIST dataset for 10 epochs.\\nThough the images are a bit blurry, we can make out digits in most of the samples.\\nWith more complex architectures such as RNNs, hyperparameter tuning, and longer\\ntraining times, we will surely see even better results. In the next section, we intro‐\\nduce a slightly different take on generative models that has recently been achieving\\npopularity.\\nScore-Based Generative Models\\nIn this section, we approach generative modeling through a slightly different lens\\nthan what we have encountered so far. In an optimally trained GAN, we first sample\\nfrom some noise distribution p(z) and run this sample zi through a generator G,\\nwhich deterministically transforms zi into a sample xi from the true data distribution\\n(where we approximate the true data distribution p(x) using our dataset, pdata x ).\\nThough G itself is a deterministic function, G(z) is a random variable distributed as\\nthe true data distribution. In summary, we have implicitly defined a distribution over\\nour domain via the generator’s action on samples from p(z), and a way of sampling\\nfrom the true data distribution via a simpler distribution p(z), such as a multivariate\\nGaussian distribution.\\n264 \\n| \\nChapter 10: Generative Models'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 280}, page_content='VAEs are more explicit in their probabilistic modeling. We define z to be a set of\\nlatent variables that generate the data we see, x. We explicitly learn a conditional\\ndistribution over the data pθ x z  via the decoder, which we can sample from. In\\nan optimally trained VAE, pθ x z = p x z , is the true conditional likelihood of the\\ndata. To generate data using an optimally trained VAE, we first sample a setting of\\nthe latent variables from p(z) and run this sample zi through the decoder, which now\\nparametrizes the distribution p x z = zi . This is an explicit probability distribution\\nwe can now sample from.\\nNote that although GANs and VAEs themselves are quite distinct, both of their\\narchitectures and actions involve an additional distribution p(z) (whether that is\\na noise distribution in GANs or a prior over latent variables in VAEs). Is there a\\nway of sampling from the true data distribution without the additional distribution?\\nScore-based generative models attempt to do just that.\\nOne method of sampling from a probability distribution is an iterative process\\ncalled Langevin dynamics. This process is actually an instance of a class of algo‐\\nrithms referred to as Markov Chain Monte Carlo (MCMC) algorithms. Motivating\\nMCMC algorithms and proving why they sample from probability distributions in an\\nunbiased manner are beyond the scope of this section, but we refer you to the vast\\namount of academic literature that exists on this topic.\\nLangevin dynamics follows the process defined as follows:\\nx i + 1 = x i + η∇x log p x i\\n+\\n2ηϵ, ϵ ∼N 0, I\\nx i  here represents a sample from p(x), and this dynamics equation shows us how to\\ngenerate the next sample x i + 1  given our current sample.\\nNote that if we were to remove the Gaussian noise component at the end of the\\ndynamics equation, we would just be following the gradient to a maximum of p(x),\\ni.e., performing gradient ascent with some step-size η. The intuition behind this\\ndynamics equation is that the addition of the noise component prevents us from\\nsimply reaching the maximum x and instead allows us to explore regions with high\\nprobability, thereby exploring regions of low probability less (Figure 10-7). Again,\\nwhy this produces samples from p(x) in an unbiased manner is beyond the scope of\\nthis text, but we highly encourage you to learn more from the academic literature.\\nScore-Based Generative Models \\n| \\n265'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 281}, page_content='Figure 10-7. We use f here to represent a Gaussian distribution with mean (and also\\nmaximum) at the origin. Each of the contours represents locations with equal likelihood.\\nAs we can see from the diagram, the gradient points directly toward the maximum, but\\nadding a bit of noise allows us to explore and sample from high-density regions without\\nconverging to the maximum.\\nAlthough we use the gradient of the log probability instead of the gradient of the\\nprobability, the value(s) of x that maximizes log p x  is the same as the value(s)\\nof x that maximizes p(x) due to the log’s concaveness. More generally, the log’s\\nconcaveness also preserves the ordering relationships between all possible values of x,\\ni.e., if p x1 ≥p x2 , then log p x1 ≥log p x2 , and vice versa. For that reason, as we\\nsaw in “Implementing a VAE” on page 259, these sorts of optimization processes tend\\nto not be affected meaningfully by the inclusion of the log.\\nHowever, the main issue with Langevin dynamics, as we’ve encountered before with\\nother generative models, is that we don’t know p(x), let alone the gradient of its log!\\nBut there may be a way to model ∇x log p x , which we call p x ’s score function,\\ndirectly. This would allow us to simply plug the score directly into the Langevin\\n266 \\n| \\nChapter 10: Generative Models'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 282}, page_content='dynamics equation and draw samples from p(x) as if we knew p(x) all along. This is\\nthe idea of score-based generative modeling.\\nFor a moment, let’s forget the problem of sampling from an unknown distribution\\np(x) and instead consider the problem of learning p(x). From now until the end\\nof this section, we will consider only the problems of learning and sampling from\\ncontinuous probability distributions. In the same vein of explicitly learning approxi‐\\nmate probability distributions like in VAEs, we can try to approximate p(x) with a\\nlearned version pθ x , where θ represents the parameters of the learned model. What\\nwe envision is a learned function, such as a neural network, that takes as input an\\nexample x and outputs a likelihood pθ x . However, there is no way to ensure that\\n∫pθ x dx = 1, which is a necessary condition of any probability distribution.\\nInstead, we settle for learning what we call an unnormalized probability distribution\\nqθ x . This is a function that takes an example x and outputs an unnormalized likeli‐\\nhood. We can, in theory, represent the normalized probability distribution pθ x  via\\nqθ x\\nZ θ , where Z θ = ∫qθ x dx. Unfortunately, this integral is generally intractable and\\nhas no closed form solution. Of course, there are exceptions to the rule. For example,\\nZ θ = σ * 2π for a univariate Gaussian distribution, where θ = μ, σ  are the mean\\nand standard deviation of the Gaussian. But if we’d like to model more expressive\\ndistributions via a neural network, for example, it is almost always impossible to\\ntractably calculate Z θ , which we will also refer to as the partition function.\\nHow can we go about learning such an unnormalized probability distribution?\\nResearchers have presented many approaches for learning qθ x  throughout the\\nhistory of machine learning and inference, but one particular method starts to bridge\\nthe gap between learning an unnormalized probability distribution and sampling\\nfrom its normalized version via a process like Langevin dynamics. Score matching, or\\nthe idea of learning qθ x  via minimizing the difference between the score function\\nof qθ x  and the score function of the true distribution p x , was first proposed by\\nHyvarinen in 2005.\\nHere, we show that minimizing the difference as stated is equivalent to minimizing\\nthe difference between the score function of pθ x  and the score function of p x :\\n∇x log qθ x = ∇x log pθ x * Z θ\\n= ∇x log pθ x + ∇x log Z θ\\n= ∇x log pθ x\\nIt turns out that the score function of qθ x  is the same as the score function of\\npθ x , ∀x. This is because the log first separates the product of qθ x  and the partition\\nfunction into a sum of logs, and finally the gradient with respect to x eliminates the\\nScore-Based Generative Models \\n| \\n267'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 283}, page_content='log of the partition function, since this term is solely dependent on the weights θ\\nand is not a function of x itself. Thus, the optimal θ that minimizes the proposed\\ndifference is equivalent to the optimal θ that minimizes the difference in scores\\nbetween pθ x  and p(x). The following is the optimization procedure, which we call\\nexplicit score matching:\\nJ θ = Ep x\\n1\\n2 ∇x log qθ x −∇x log p x\\n2\\n2\\nθ* = argminθJ θ\\nThe reason for the leading 1\\n2 is to simplify the resulting gradient (cancels out with\\nthe 2 that will be pulled down from the square of the norm). Note that we have\\ncompletely removed the dependence on the partition function in our analysis, and\\nwe now have a way to (1) learn an unnormalized distribution qθ x , and (2) calculate\\nthe score of pθ x i  via our neural network. For item 1, in the case where we find\\na setting θ that results in J θ = 0, pθ x  and p(x) are the same for all x since their\\ngradients are the same for all x. Of course, in general, two functions that have the\\nsame gradients everywhere can still be different functions by being off from each\\nother by a nonzero constant. However, in our case, these two functions cannot be off\\nby a nonzero constant since they are both probability distributions that must sum\\nto one. Thus, we have a valid optimization procedure for learning an unnormalized\\ndistribution that, when normalized, should approximate the true distribution well.\\nTo perform item 2, in theory all we would need to do is first run our example x i\\nthrough our neural network to get qθ x i , take the log of qθ x i , and backpropagate\\nthis result through our network all the way back to the input. We’ve already shown\\nthat the resultant score is equivalent to the score of pθ x i . Going forward, we will\\nrefer to the score function of pθ x  (and qθ x ) as Ψθ x , and the score function\\nof p x  as Ψ x . Using our new notation, we rewrite the explicit score matching\\nobjective as:\\nθ* = argminθEp x\\n1\\n2 Ψθ x −Ψ x\\n2\\n2\\nAlthough we have gotten around the issue of the partition function, we still have no\\nidea what Ψ x  is. Hyvarinen, in 2005, in addition to proposing the notion of explicit\\nscore matching, proved an amazing property regarding explicit score matching (satis‐\\nfied under certain weak regularity conditions):\\nEp x\\n1\\n2 Ψθ x −Ψ x\\n2\\n2 = Ep x\\n1\\n2 Ψθ x\\n2\\n2 + ∑i = 1\\nd\\n∇xiΨθ, i x + c\\n268 \\n| \\nChapter 10: Generative Models'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 284}, page_content='Where Ψθ, i x = ∇xi log pθ x —the score function is just a length d vector (assum‐\\ning x is of d dimensions), where each index i corresponds with the partial derivative\\nof the log probability with respect to xi. c is a constant that has no dependence on θ,\\nso it can simply be ignored during optimization. This is a method the community has\\ncome to know as implicit score matching.\\nNote that the equivalent expression has no dependence on the true probability distri‐\\nbution, and thus we can directly optimize θ, using it as we would any other objective.\\nOnce we learn the optimal θ, all we need to do to perform generative modeling is:\\n1. Follow the methodology presented earlier for calculating the score of pθ x i :\\n1.\\nrun the example through our learned network, take the log of the result, and\\nbackpropagate all the way to the input.\\n2. Sample ϵ from N(0,I).\\n2.\\n3. Plug in the results of steps 1 and 2 into the Langevin dynamics equation to obtain\\n3.\\nthe next sample, x i + 1 .\\n4. Repeat steps 1 through 3 with x i + 1 .\\n4.\\nThis procedure allows us to draw samples from pθ x , which, as shown earlier, should\\napproximate p(x) well once the network has been trained.\\nCan we do better than implicit score matching? For one, implicit score match‐\\ning requires us to calculate second-order gradients, as can be seen from the\\n∑i = 1\\nd\\n∇xiΨθ, i x  term in the implicit score matching objective. This can be quite\\ncomputationally expensive depending on the size of x. In a framework such as\\nPyTorch, this would require first calculating the first-order gradient through stan‐\\ndard means such as backpropagation and then looping through each xi manually\\nto compute its second-order gradient. In the next section, we will cover denoising\\nautoencoders and denoising score matching, which modify the objective and allow us\\nto get around these complexity issues.\\nDenoising Autoencoders and Score Matching\\nBefore explaining the connection between denoising autoencoders and score match‐\\ning, we first motivate the denoising autoencoder architecture. In Chapter 9,\\nwe learned about autoencoders through the lens of representation learning. We\\nused autoencoders to compress high-dimensional data, such as images, into low-\\ndimensional representations that preserved the information, or useful features,\\nnecessary to reconstruct the original data. We additionally showed, through our\\nexperiments on MNIST, that we were able to reconstruct the data quite well and\\nwe generally saw, for instances of a given digit, clustering of its low-dimensional\\nrepresentations. This implies that if we were to train a standard classifier on these\\nDenoising Autoencoders and Score Matching \\n| \\n269'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 285}, page_content=\"low-dimensional representations, with the label being their original digit categories,\\nwe’d expect to see great accuracy.\\nHowever, depending on the data we try to compress, it turns out that, at times, our\\ncompressions aren’t able to capture useful features. In other words, when we use our\\ntrained autoencoder on real-world images outside of our sample that may be slightly\\ncorrupted, rotated, shifted, or captured under various light settings, our ability to\\nclassify these images using their low-dimensional representations takes a large dip.\\nIdeally, we would like our learned representations to be invariant to such noise. In\\n2008, Vincent proposed denoising autoencoders as a method for combating the issues\\nwe see with standard autoencoders. Denoising autoencoders first corrupt the original\\ninput data with noise, run the corrupted input through a standard autoencoder, and\\nfinally attempt to reconstruct the original input (Figure 10-8). The original paper\\nused a corruption scheme that randomly zeroed out some portion of the input, but\\nacknowledged that a variety of corruption schemes could be used instead. Intuitively,\\nthe representations learned from such a procedure should be much more robust to\\nthe challenges presented by real-world images. Indeed, the experiments on MNIST by\\nVincent in 2008 showed that, under various data augmentations such as rotation and\\nbackground noise, the denoising autoencoder performed significantly better than the\\nstandard autoencoder in terms of classification accuracy.\\nFigure 10-8. The denoising autoencoder architecture is the same as that of the standard\\nautoencoder, except instead of minimizing the reconstruction error between y and the\\ninput x', we minimize the reconstruction error between y and the original x.\\nFollowing Vincent 2011, which first noticed the connection between denoising AEs\\nand score matching, we instead define the corruption scheme to be the addition of\\nGaussian noise to the original data. Formally, we have that p x  represents the true\\ndistribution of the data, pdata x  represents the distribution of the data using our\\ntraining set, and pσ x′ x  represents the conditional distribution of the corrupted\\ndata given the original data. In particular:\\npσ x′ x = N x′; x, σ2I\\n270 \\n| \\nChapter 10: Generative Models\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 286}, page_content='Where the mean of the distribution is the original data and the subscript σ represents\\nthe standard deviation of the Gaussian noise applied to the original data. Note that x’\\nand x are defined over the same domain (all possible images, for example). We can\\nnow calculate the distribution over the corrupted data:\\npσ x′ = ∑xpσ x′ x p x\\n≈∑xpσ x′ x pdata x\\n= 1\\nn ∑i = 1\\nn\\npσ x′ x = x i\\nwhich is the empirical average over the conditional probabilities using each data\\npoint from our dataset as the reference. This follows naturally from letting the true\\ndistribution be approximated by the distribution defined by the dataset (same as how\\nthis was defined in “Generative Adversarial Networks” on page 244).\\nIn 2011, Vincent explored the possibility of using pσ x′  as the reference instead\\nof p x  as we do in explicit score matching. The reasoning for this is that pσ x′\\ncan be viewed as a continuous approximation to the true distribution p x . The\\napproximation defined by pdata x  is unbiased, but is unfortunately discontinuous\\neverywhere x is not present in the dataset due to being a uniform distribution over all\\nimages in the dataset, with a likelihood of zero everywhere else. Of course, as σ gets\\nlarger, pσ x′  is seen as a less and less faithful approximation to p x , so we’d like to\\nwork with small σ’s.\\nVincent 2011 first proposed explicit score matching using pσ x′  as the reference:\\nJ θ = Epσ x′\\n1\\n2 ∇x′ log pθ x′ −∇x′ log pσ x′\\n2\\n2\\nθ* = argminθJ θ\\nNote that the same reasoning for why this is a valid optimization procedure for\\npθ x  is the same as in the previous section—the only difference here is the reference\\ndistribution we are trying to match. Vincent 2011 actually goes an extra step and\\nshows that this optimization procedure is equivalent to:\\nJDSM θ = Epσ x, x′\\n1\\n2 ∇x′ log pθ x′ −∇x′ log pσ x′ x\\n2\\n2\\nθDSM\\n*\\n= argminθJDSM θ\\nAlthough we won’t show the proof here and refer you to Vincent 2011 for the full\\ndetails, it does utilize the log trick we described in “Implementing a VAE” on page\\nDenoising Autoencoders and Score Matching \\n| \\n271'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 287}, page_content='259. We refer to optimizing this objective as denoising score matching, or DSM for\\nshort, and as we will show soon, it serves as the connection to denoising AEs.\\nWe know that pσ x′ x = N x′; x, σ2I , and now compute the gradient of its log:\\n∇x′ log pσ x′ x = ∇x′ log\\n1\\n2π d σ2I\\ne\\n−x′ −x T x′ −x\\n2σ2\\n= ∇x′ log\\n1\\n2π d σ2I\\n+ ∇x′ log e\\n−x′ −x T x′ −x\\n2σ2\\n= −\\n1\\n2σ2∇x′ x′ −x T x′ −x\\n= −\\n1\\n2σ2 ∇x′x′Tx′ −2∇x′x′Tx + ∇x′xTx\\n=\\n1\\nσ2 x −x′\\nLet’s break down the math. The first equality is simply the definition of a Gaussian\\ndistribution with mean x and variance σ2I. The second equality is a result of the log\\nbreaking up the product into a sum of logs, and the gradient of a sum being the sum\\nof gradients. In the third equality, we see the first term has been removed since it is\\nnot a function of x’, and thus its gradient is zero. Additionally, the log of e raised to\\nany power is just the power itself, since log as used here has base e. Finally, we expand\\nout the dot product of x’ – x with itself and apply the gradient to each individual term\\nof the resulting sum. Note that we can simply rewrite −x′Tx −xTx′ as −2x′Tx since\\nthe two terms are transposes of each other and result in the same scalar. We refer you\\nto an amazing text called The Matrix Cookbook by KB Petersen and Michael Syskind\\nPedersen, which can serve as a guide to evaluating these gradients (plus more) and\\narrive at the final equality. The intuition for the gradient of x′Tx′ is that it is the\\nanalog of the derivative of the square of a variable from single-variable calculus.\\nFor the final step, we will show that optimizing the objective for denoising score\\nmatching is equivalent to optimizing the objective for denoising AEs. To recap, a\\ndenoising AE has the same architecture as that of a standard AE—the only difference\\nis in the input data and the training objective. The training objective of the denoising\\nAE looks like:\\nJDAE θ = Epσ x, x′\\ndecode encode x′\\n−x\\n2\\n2\\nθDAE\\n*\\n= argminθJDAE θ\\n272 \\n| \\nChapter 10: Generative Models'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 288}, page_content='Note that the parameters, or weights, of both decode() and encode() are encompassed\\nby θ. To summarize, we must show that θDAE\\n*\\n and θDSM\\n*\\n defined earlier are equivalent\\nfor some form of the unnormalized likelihood. Once again, following Vincent 2011,\\nwe define the denoising autoencoder as an encoder consisting of a single fully con‐\\nnected layer followed by a sigmoid layer and a decoder consisting solely of a single\\nfully connected layer. Additionally, we add the constraint that the two fully connected\\nlayers are weight-tied so that they are transposes of each other. The training objective\\ncan now be specified as, where θ = W, b, c :\\nJDAE θ = Epσ x, x′\\nW T Wx′ + b + c −x\\n2\\n2\\n= 2σ4 * Epσ x, x′\\n1\\n2σ4 W T Wx′ + b + c −x\\n2\\n2\\n= 2σ4 * Epσ x, x′\\n1\\n2\\n1\\nσ2 W T Wx′ + b + c −x′ −1\\nσ2 x −x′\\n2\\n2\\nYou may notice that our algebraic manipulation has led to the appearance of\\n∇x′ log pσ x′ x . All we need to do now is find a form for the unnormalized like‐\\nlihood whose gradient with respect to x’ is 1\\nσ2 W T Wx′ + b + c −x′ .\\nAs it turns out, if we define the unnormalized likelihood qθ x′  to be\\n−1\\nσ2 cTx −1\\n2\\nx\\n2\\n2 + ∑j = 1\\nd\\nsoftplus W j\\nTx + bj  and plug in this expression to the\\ndenoising score matching objective, we are left with an objective that is just\\n1\\n2σ4JDAE θ . We refer you to Vincent 2011 to see why this is the case.\\nOptimizing this new objective with respect to θ is no different from optimizing a\\ndenoising autoencoder. This is because σ is a positive constant and has no depend‐\\nence on θ, thus only scaling the magnitude of the resulting gradient rather than\\naffecting its direction. In summary, we have found that training a denoising AE is the\\nsame as optimizing the denoising score matching objective, where the unnormalized\\nlikelihood takes the form specified in the previous paragraph. More simply, the\\nweights of a trained denoising AE would be the same as those of an unnormalized\\nlikelihood specified by −1\\nσ2 cTx −1\\n2\\nx\\n2\\n2 + ∑j = 1\\nd\\nsoftplus W j\\nTx + bj  and trained\\nvia denoising score matching.\\nAll we would need to do to perform generative modeling using a denoising AE is:\\n1. Fully train the denoising AE by minimizing JDAE θ .\\n1.\\n2. For a given x i , calculate its score by evaluating 1\\nσ2 decode encode x i\\n−x i .\\n2.\\n3. Sample ϵ from N(0,I).\\n3.\\nDenoising Autoencoders and Score Matching \\n| \\n273'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 289}, page_content='4. Plug in the results of 2 and 3 into the Langevin dynamics equation to obtain the\\n4.\\nnext sample x i + 1 .\\n5. Repeat steps 2 through 4 with x i + 1 .\\n5.\\nThough we’ve gotten around the issue of needing to calculate second-order gradients\\nby using this method, there is still the issue of being able to sample only from the\\nnoisy approximation of p(x). More recent work builds off of concepts from both\\nimplicit score matching and denoising score matching to achieve even stronger and\\nmore realistic generative capabilities. We highly recommend you explore the litera‐\\nture further, as most of the prerequisite material has been covered in these sections.\\nSummary\\nIn summary, we have learned a great deal about generative models. We covered\\nthe motivation and mathematics behind GANs, VAEs, and a few forms of score\\nmatching, and even implemented a VAE from scratch. We also learned about the\\nsimilarities and differences between these methods. For example, a GAN implicitly\\nmodels a complex distribution that we can sample from via its generator, while a\\nVAE explicitly learns distributions but is slightly more restrictive in the complexity\\nof distributions it can model. Implicit score matching, similarly to GANs, allowed\\nus to sample from complex distributions via Langevin dynamics (without the use of\\nan additional noise distribution p(z)), but having to compute second-order gradients\\nled us to the development of the denoising score matching and its connection with\\npre-existing denoising AEs. Additionally, VAEs took on the strongest probabilistic\\nmodeling approach of the three by defining a set of latent variables and explicitly\\nlearning an approximate posterior, given an input example, and a likelihood function,\\ngiven a setting of latent variables. In contrast, for GANs, the additional variable\\nz’s purpose is solely as an intermediate for sampling. Although all of these models\\ntackle generative modeling from distinct perspectives and motivations, they have all\\nproduced strong results and have laid a solid groundwork for current and future\\nresearch.\\n274 \\n| \\nChapter 10: Generative Models'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 290}, page_content='CHAPTER 11\\nMethods in Interpretability\\nOverview\\nThe field of interpretability is broad and can be uniquely applied to a variety of\\ntasks. Simply put, interpretability defines a model’s ability to “explain” its decision\\nmaking to a third party. There are many modern architectures that do not have this\\ncapability just by construction. A neural network, for example, is a prime example\\nof one of these modern architectures. The term “opaque” is often used to describe\\nneural networks, both in media and in literature. This is because, without post hoc\\ntechniques to explain the final classification or regression result of a neural network,\\nthe data transformations occurring within the trained model are unclear and difficult\\nfor the end user to interpret. All we know is that we fed in an example and out\\npopped a result. Although we can examine the learned weights of a neural network,\\nthe composition of all of these weights is an extremely complex function. This makes\\nit difficult to tell what part of the input ended up contributing the most to the final\\nresult.\\nA variety of post hoc methodologies have been designed to explain the output of a\\nneural network—saliency mapping is a prime example. Saliency mapping measures\\nthe gradient of the output of a trained model with respect to the input. By the\\ndefinition of the gradient, the input positions with the highest magnitude gradients\\nwould affect the output value, or class in the case of classification, the most when\\ntheir values are changed slightly. Saliency mapping thus interprets the set of positions\\n(and their respective values) with the highest magnitude gradients as the part of the\\ninput that contributes the most to the final result.\\nHowever, this is not the be-all and end-all of interpretability. One issue with saliency\\nmapping is that it can be a bit noisy, especially when we consider the gradient at the\\nindividual pixel level for tasks like image classification. Additionally, if the input is\\n275'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 291}, page_content='categorical in nature rather than continuous, e.g., one-hot encodings for sentences,\\nthe gradient with respect to the input isn’t interpretable in itself since the input space\\nis discontinuous.\\nFurther, as mentioned earlier, the task at hand is often key for determining what\\nmakes sense as a valid method of interpretability. We will expound on this more in\\nthe sections to come.\\nOftentimes, interpretability comes at the expense of performance. Building interpret‐\\nability into a model often adds some bias (the bias in bias-variance trade-off) by\\nmaking simplifying model assumptions, e.g., in vanilla linear regression, where we\\nassume a linear relationship between the features and the target variable. These\\nsimplifying assumptions, however, are what make the relationship between the input\\nfeatures and the target variable much clearer in a vanilla linear regression as opposed\\nto a complex, neural architecture.\\nThis all begs the question: why do we care about interpretability in the first place? In\\na world that is becoming increasingly dominated by technology, complex algorithms,\\nand machine learning, the ability to explain decision making is imperative. Especially\\nin fields such as medicine, where patient’s lives are on the line, or in finance, where\\npeoples’ financial livelihoods are at stake, the ability to explain a model’s decision\\nmaking is a key step toward widespread adoption. In the next section, we will cover\\nsome classical models that have strong notions of interpretability built into their\\ndesign.\\nDecision Trees and Tree-Based Algorithms\\nMost classical data science and machine learning methodologies have some built-in\\nform of interpretability. Tree-based algorithms are a clear example of this. Decision\\ntrees are designed to classify an input based on a series of conditional statements,\\nwhere each node in the tree is associated with a conditional statement. To understand\\nhow a trained tree-based model is making a decision, all we must do for any given\\ninput is follow the correct branch at each node in the tree (Figure 11-1).\\n276 \\n| \\nChapter 11: Methods in Interpretability'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 292}, page_content='Figure 11-1. A decision tree trained to classify bird species. Given a set of bird features,\\nfollow the right “Yes” or “No” branch at each node to reach a final classification.\\nMore complex tree-based algorithms, such as the random forest algorithm, which is\\ncomposed of an ensemble of large decision trees, are also interpretable. For example,\\nin the case of classification, random forest algorithms function by running a given\\ninput through each decision tree and then taking the majority output class amongst\\nthe decision trees as the final output (or an average in the case of regression). By\\nthe algorithm’s construction, we know exactly how random forest came to a final\\nconclusion regarding the input.\\nIn addition to interpretability at the individual example level, decision trees and\\ntheir more complex ensembles have built-in metrics for feature importance at the\\nglobal level. For example, when a decision tree is being trained, it must determine\\nwhich feature to split on and the threshold(s) of that feature at which to split. In the\\nclassification regime, one methodology to do this is to calculate the information gain\\nby splitting on a proposed feature at a proposed threshold. To frame our thinking,\\nlet’s think of the possible training labels as the domain of a discrete probability\\ndistribution, where the probability of each label is the frequency with which that label\\nappears in the training dataset (Figure 11-2).\\nDecision Trees and Tree-Based Algorithms \\n| \\n277'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 293}, page_content='Figure 11-2. Label probabilities.\\nThinking back to Chapter 2, a metric that summarizes the uncertainty within a\\nprobability distribution is the entropy of the distribution. When given a proposed\\nfeature and associated threshold(s) to split on, we can split the training data popula‐\\ntion into at least two separate groups based on which branch we would follow for\\neach input example. Each subgroup now has its own distribution over the possible\\nlabels, and we take the difference between the training dataset’s entropy and the\\nweighted sum of each subgroup’s entropy to calculate the information gain, where the\\nweight is proportional to the number of elements in each subgroup. The feature and\\nassociated threshold(s) with the highest information gain at each branching point are\\nthe optimal split.\\nWhy does this work? Although we won’t do a rigorous proof here, consider the\\nproblem where we have a molecular dataset with a binary label, for example, indicat‐\\ning whether each compound is toxic or not, and we’d like to build a classifier to\\npredict compound toxicity. Also assume that one of the features associated with each\\ncompound is a binary feature of whether the molecule contains a phenol functional\\ngroup or not. The phenol functional group is both quite toxic and is a common cause\\nof toxicity in compounds, so splitting on this feature would lead to two well-separated\\nsubgroups.\\nThe positive subgroup, which contains compounds with phenol functional groups,\\nis likely to have few false positives due to the phenol’s level of toxicity. The negative\\nsubgroup, which contains compounds without phenol functional groups, is likely\\nto have few false negatives due to phenol being a common cause of toxicity. Thus,\\neach subgroup’s associated entropy is quite low since the true label distribution over\\ncompounds in each subgroup is quite concentrated over a single label. The sum\\nof their weighted entropies removed from the entire dataset’s associated entropy\\ndemonstrates a significant information gain (Figure 11-3).\\n278 \\n| \\nChapter 11: Methods in Interpretability'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 294}, page_content='Figure 11-3. The original dataset can be broken down into 30% toxic and 70% nontoxic,\\nwhere a true label of 1 indicates toxicity and 0 otherwise. Breaking up the n examples\\ninto two subgroups based on containing phenol greatly concentrates the true probability\\nover a single label in each subgroup.\\nThis checks out well with our a priori knowledge of the phenol group—due to\\nboth its widespread nature in toxic compounds and its level of toxicity, we would\\nhave expected it to be a great feature for toxicity classification. The way we select\\nfeatures and their splits in decision trees is actually the same way we approach greedy\\nalgorithms in the more general algorithmic framework. Greedy algorithms select the\\nmost optimal local action at each decision point, and depending on the properties\\nof the problem, the composition of these locally optimal actions leads to the global\\noptimum. Decision trees similarly select the feature and split that locally lead to\\nthe largest gain in some metric at each decision point. For example, we just used\\ninformation gain for toxicity classification, and although we showed the result of\\njust one split, assuming splitting on the phenol trait leads to the highest information\\ngain, we perform this same greedy procedure at each junction of every level in the\\ntree. However, it turns out that the problem of finding the globally optimal decision\\ntree for a dataset is NP-complete, which for the purposes of this text means that it\\nis computationally very difficult. The best we can do to approach the problem in a\\nDecision Trees and Tree-Based Algorithms \\n| \\n279'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 295}, page_content='tractable manner is the greedy approach, although it does not provably lead to the\\nglobal optimum.\\nFor each feature we split on in a tree, there exists an associated information gain with\\nthat feature. The order of importance of each feature is simply a list of the features\\nsorted by their information gain. If we have a random forest rather than a single\\ndecision tree, we average the information gain for each feature across all of the trees\\nin the forest and sort using the mean. Note that there is no extra work required in\\ncalculating the information gain, since we use information gain to train the individual\\ndecision trees in the first place. Thus, we have both example-level interpretability and\\na global understanding of feature importance for free in tree-based algorithms.\\nLinear Regression\\nA quick background on linear regression: given a set of features and a target variable,\\nour goal is to find the “best” linear combination of features that approximates the\\ntarget variable. Implicit in this model is the assumption that the input features are\\nlinearly related to the target variable. We define “best” as the set of coefficients that\\nresults in the linear combination with the lowest root mean squared error when\\ncompared against the ground truth:\\ny = β · x + ϵ, ϵ ∼N 0, σ2\\nWhere β represents the vector of coefficients. Our built-in, global notion of feature\\nimportance follows directly from this. The features that correspond with the coeffi‐\\ncients with the highest magnitude are, globally, the most important features in the\\nregression.\\nHow about an example-level notion of feature importance? Recall that to get a\\nprediction for a given example, we take the dot product between the example and the\\nlearned coefficients. Logically, the feature associated with the feature-coefficient prod‐\\nuct that contributes the most, in magnitude, to the final result is the feature that is\\nmost important for prediction. Without much effort, we have both an example-level\\nand global-level notion of interpretability more or less built into linear regression.\\nHowever, linear regression has some unaddressed issues when considering feature\\nimportance. For example, when there exist significant correlations between the fea‐\\ntures in a multivariate regression, it is often difficult for the model to disentangle\\nthe effects of these correlated features on the output. In “SHAP” on page 292, we\\nwill describe Shapley values, which were designed to measure the marginal, unbiased\\nimpact of a given feature on the output in such cases.\\n280 \\n| \\nChapter 11: Methods in Interpretability'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 296}, page_content='Methods for Evaluating Feature Importance\\nFor models where feature importance isn’t built in, researchers have developed a\\nvariety of methods over the years to evaluate it. In this section, we will discuss a few\\nthat are used in the industry, in addition to their benefits and shortcomings.\\nPermutation Feature Importance\\nThe idea behind permutation feature importance is quite simple: assume we have\\na trained neural model f and a set of features U that f has been trained on. We’d\\nlike to understand the impact that an individual feature s has on the predictions\\nof f. One way to do this is to randomly rearrange the values that s takes on in the\\ndataset amongst all of the examples and measure the resulting decrease in predictive\\naccuracy. If the feature s did not add much predictive accuracy in the first place,\\nwe should see that the predictive accuracy of f decreases minimally when using the\\npermuted samples. Inversely, if feature s was predictive of the output in the first place,\\nwe should see a large drop in predictive accuracy upon permuting the values of s in\\nthe dataset. In essence, if the feature s was originally strongly correlated with the true\\nlabels, randomizing the values of s would break this strong correlation and nullify its\\neffectiveness at predicting the true label.\\nUnfortunately, as with all interpretability methods, this one is not perfect. Imagine\\nthe scenario in which our target is ice cream sales in a given region and two fea‐\\ntures in U are the readings of two temperature sensors within a one-mile radius of\\neach other. We’d expect that each of these features is independently quite predictive\\nof ice cream sales due to the seasonality of our target. However, if we were to\\nperform the permutation methodology presented previously on this dataset, we’d\\ncounterintuitively get a low feature importance for both of these features. Why is\\nthis the case? Although each of these features is strongly predictive of the target,\\nthey are also strongly correlated due to the close proximity of the two temperature\\nsensors. Additionally, permuting only one of these features at a time to compute its\\nimportance means that the other is kept intact, preserving most of the predictive\\ninformation contained within the two features. Thus, we’d see little change in the\\npredictive performance of f for both features, leading us to believe that the weather is\\nnot predictive of ice cream sales.\\nThe moral of the story here is that we must always keep in mind correlations between\\nfeatures in our dataset. It is good data science and machine learning practice to\\nunderstand the relationships between the features themselves before running these\\nfeatures through any sort of predictive modeling algorithm, simple or complex. One\\nway to do this is to plot each of the z-scored features against each other to get a visual\\nidea of feature correlation.\\nMethods for Evaluating Feature Importance \\n| \\n281'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 297}, page_content='Partial Dependence Plots\\nPartial dependence plots, or PDPs for short, measure the marginal impact that a\\nsubset of features included in the model has on the output. As previously discussed,\\nmeasuring this marginal impact in an unbiased manner is difficult for complex\\nneural models. In the case of regression, we can represent the trained neural network\\n(or any other complex, uninterpretable model) as a function f that takes as input\\na set of features U and outputs a value in the reals. Imagine that, as a user of this\\nmodel, you are looking for an interpretability method that can measure the marginal\\nimpact of any subset of features S on the output of f. That is, if we are given an\\narbitrary setting of feature set S, we would like to calculate the expected output of\\nthe function f conditioned on this setting. The expectation of f is taken over U \\\\ S,\\nthe rest of the features in U (conditioned on the known setting of S). Intuitively, we\\nhave marginalized out the feature subset U \\\\ S and have the output of a new function\\nf’ that takes as input only the feature set S. If we carry out this process for enough\\nsettings of S, we can learn the patterns of how f’ changes as the feature set S changes.\\nFor example, assume the output is the number of vehicles on the road in a given\\nregion and our feature set S consists of a single feature: precipitation levels in that\\nregion. The features that make up U \\\\ S could be variables like time of day, geograph‐\\nical location, population density, etc. By running the above process for a range of\\nprecipitation levels, we can estimate the number of vehicles we’d expect to see on the\\nroad at each level and observe the trend as precipitation levels get higher or lower.\\nPlotting this trend is what gives us a PDP.\\nA couple of important notes: the first is that we do not plan on actually learning f’,\\nbut rather estimating it using our trained model f. Learning f’ itself would require\\nretraining for every potential subset S to be explained, which is exponential in the\\nnumber of features and thus intractable. The second is that it is currently unclear how\\nwe would go about computing the expectation of f taken over U \\\\ S. As we will soon\\nsee, the PDP methodology addresses this second point. Before diving into the weeds,\\nhere is a simple yet concrete mathematical formulation of the process we have just\\ndescribed:\\nfâ S = EU ∖S S f U ∖S, S\\nAs stated, the conditional expectation is a bit tricky to estimate. So far in the text,\\nwe have approximated expectations in an unbiased manner via empirical averages. To\\nestimate a conditional expectation, however, we are further constrained by the fact\\nthat we must take only the average over samples that contain the exact setting of S in\\nquestion. Unfortunately, the only samples we have from the underlying distribution\\nover U are contained within the dataset we are provided with. And in the common\\ncase that the features of U are continuous, the likelihood that we see the exact setting\\n282 \\n| \\nChapter 11: Methods in Interpretability'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 298}, page_content='of S in question even once in the dataset is exceedingly low. To get around this, PDP\\nmakes an independence assumption that allows us to use the entire dataset directly to\\nestimate this expectation:\\nfâ S = EU ∖S S f U ∖S, S\\n= EU ∖S f U ∖S, S\\n≈1\\nn ∑i = 1\\nn\\nf U ∖S i, S\\nWhere n is the number of samples in the dataset. PDP assumes that the features in\\nS are independent from the features in U \\\\ S. This assumption allows us to use all\\nof the training samples indiscriminately for computing the expectation since, under\\nthis assumption, the sampling of U \\\\ S is independent from the setting of S anyway.\\nWe now have a concrete method for estimating the marginal impact of any arbitrary\\nsubset of features S on the output of f.\\nIf there are significant correlations between the features in S and those in U \\\\ S,\\nthen our generated PDP is likely not reflective of the true marginal effect of S on the\\noutput due to bias in our sampling assumption. Essentially, we would be taking the\\naverage over many samples that are very unlikely to occur, which means that we (1)\\ncan’t expect f to generate meaningful outputs on these samples, and (2) are taking the\\naverage over the outputs for samples that do not accurately reflect the relationships in\\nthe underlying distribution over U.\\nThe second concern is likely pretty clear, but to illustrate the first, imagine that you\\nhave trained a neural network to completion on the MNIST dataset. Now, I find\\nan image of a dog online and run this image through the network. By chance, it\\nturns out that the network returns a 9 with high confidence—should we trust these\\nresults? Since the input image is completely out of the distribution of images that the\\nmodel expects to see, we can’t trust the model to generalize to this extent. Although\\nour situation with PDP is a bit less extreme, it is analogous—we have essentially\\ncreated these unlikely, out-of-distribution “franken-samples” and are expecting f to\\nproduce meaningful outputs on these samples. The independence assumption that\\nPDP makes is an inherent limitation of the method, again since the only samples\\nwe have are those from the dataset. Additionally, PDPs are often used to analyze the\\nimpact of small subsets of features (≤2), since humans can interpret only up to three\\ndimensions visually. Regardless, PDPs can be an effective method for visualizing\\ntrends between subsets of input features and the output of a complex model.\\nExtractive Rationalization\\nExtractive rationalization, or the selection of concise portions of the input that retain\\nmost, or all, of the relevant information necessary to predict a property, is a built-in\\nform of interpretability at the example level. In this section, we will review the\\nExtractive Rationalization \\n| \\n283'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 299}, page_content='1 Lei et al. “Rationalizing Neural Predictions.” arXiv Preprint arXiv:1606.04155. 2016.\\nmethodology of the paper “Rationalizing Neural Predictions,”1 which attempts to\\ndo this in the natural language space. In this paper, the task at hand is property\\nprediction: given a textual review, predict some properties regarding the text. The\\npaper specifically worked with a beer review dataset, where each review consisted of\\nsome text along with an appearance score, a smell score, and a palate score.\\nThe high-performing but uninterpretable method would be to train a classic property\\npredictor using a recurrent architecture, followed by a vanilla regression neural net‐\\nwork that takes as input the final embedding produced by the recurrent architecture,\\nas shown in Figure 11-4.\\nFigure 11-4. Depicted is the classical property predictor, where x is an encoding of the\\noriginal sentence, h(x) is the hidden state produced by the recurrent architecture after\\nreaching the end of x, and y is the result of a standard feed-forward neural architecture.\\nThe goal of this paper is to additionally generate rationales, or selected, concise\\nportions of the input text that are most relevant to the property being predicted,\\nwhile limiting the hit to performance. This is why this method of rationalization is\\nreferred to as “extractive”—it works by extracting relevant portions of the input. You\\nmight be wondering why there is an emphasis on conciseness. If there were no limit\\nor penalty on the conciseness of the rationale produced by the model, there would\\nbe no reason for the model to just return the entire input, which is a trivial solution.\\nOf course, all of the information necessary for predicting the output is within the\\nrationale if the rationale is the entire input.\\nHow do we modify the structure of the proposed property predictor to also produce\\nrationales as a built-in mechanism? This paper proposed a two-network approach,\\nwhere the first network is termed the generator and the second network is termed the\\nencoder. The generator is an RNN responsible for selecting the rationale, while the\\nencoder is an RNN responsible for predicting the property given solely the rationale,\\n284 \\n| \\nChapter 11: Methods in Interpretability'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 300}, page_content='not the entire input. The logic behind this is that, given the right objective function,\\nthe generator will have to learn to select meaningful portions of the input text to\\nbe able to accurately predict the ground truth rating. The generator parameterizes a\\ndistribution over all possible binary masks that can be applied to the input, where\\na 1 indicates that the word should be included in the rationale, and a 0 indicates\\notherwise. Figure 11-5 shows the proposed two-step architecture, where the encoder\\nis just the single-step property predictor diagrammed earlier, and z represents a\\nbinary mask sampled from the generator.\\nFigure 11-5. The generator parametrizes a distribution over masks z given input x,\\nwhich we sample from to get the input to the encoder. The encoder follows the same\\nstructure as that of the classical property predictor depicted earlier.\\nMore formally, we represent the input text x as a vector, where xi represents the\\ntoken at position i. The generator parameterizes the distribution p(z|x), where z is\\na vector consisting of individual Bernoulli random variables zi, which each take on\\nthe value 1 if xi is to be included in the rationale, and 0 otherwise. Note that z is\\nthe same length as x, which changes depending on x. How exactly do we represent\\nthis distribution? A first step is to make a reasonable conditional independence\\nassumption, which is that all zi are mutually independent of each other conditioned\\non x: p z x = ∏i = 1\\nn\\np zi z1, ..., zi −1, x = ∏i = 1\\nn\\np zi x . This is a very reasonable\\nassumption because all of the information regarding whether xi should be in the\\nrationale or not should be contained within x itself (the token xi and its surrounding\\ncontext). Converting this to neural net speak, we can implement this by applying a\\nfully connected layer followed by a sigmoid activation to each final hidden state hi of\\nthe generator independently to get the probability of zi taking on value 1, as we will\\nsee soon.\\nBefore going into the specifics of the objective function, we’ll describe the architec‐\\ntures of the generator and the encoder in more detail. The generator and encoder are\\nboth recurrent architectures, where the recurrent unit could be an LSTM or a GRU.\\nAs stated in the previous paragraph, the generator produces a hidden unit hi for each\\nExtractive Rationalization \\n| \\n285'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 301}, page_content='token xi. The final embedding for a token consists of two intermediate embeddings:\\nthe first intermediate embedding is the result of a forward pass through the tokens,\\nwhile the second intermediate embedding is the result of a backward pass through the\\ntokens. More formally, we have:\\nℎi = f ℎi −1, xi\\nℎi = f ℎi + 1, xi\\nℎi = concat ℎi, ℎi\\nWhere f  and f  correspond to two independent recurrent units, the former trained\\non the forward pass and the latter trained on the backward pass. From this formula‐\\ntion, we can see that the final embedding is bidirectional, incorporating information\\nfrom the entire context of a token rather than information in solely one direction.\\nThe paper then applies a single, fully connected layer and sigmoid to each embedding\\nto generate an independent Bernoulli random variable for each token:\\np zi x = σ wz · ℎi + bz\\nThe encoder is also a recurrent architecture, but is designed to be a regressive\\narchitecture due to its purpose of predicting the rating associated with the text. For\\nthis reason, the encoder can be designed the same way we design the vanilla property\\npredictor alluded to earlier in the section.\\nSo, what is the right objective function for training the two networks in tandem?\\nIn addition to any constraints we may want to have on the rationales the generator\\nproduces, we must also ensure that the predictor is accurate. If the predictor were\\nnot accurate, there would be no reason for the generator to produce meaningful\\nrationales. Putting this all together into a mathematical formulation, we have the\\nfollowing objective function:\\nθ*, φ* = argminθ, φL θ, φ\\nL θ, φ =\\n∑\\nx, y ∈D\\nEz ∼genθ x cost x, y, z\\ncost x, y, z = λ1 * z + λ2 * ∑\\nt\\nzt −zt −1 +\\nencφ x, z −y 2\\n2\\nWhere λ1 and λ2 are hyperparameters we can tune during validation. The cost\\nfunction used in the paper additionally contains a continuity penalty, which is higher\\n286 \\n| \\nChapter 11: Methods in Interpretability'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 302}, page_content='when the rationale is interspersed throughout the text rather than one contiguous\\nblock. We want to minimize the sum of the expected cost for each training example,\\nwhere the rationales are drawn according to the generator distribution. Calculating\\nthe expected cost exactly is computationally prohibitive due to the number of config‐\\nurations of z growing exponentially with the length of x, so we’d instead like to be\\nable to approximate the gradient of the expected cost via some empirical, sampled\\nestimate.\\nThis is feasible for the gradient of the cost function with respect to the parameters of\\nthe encoder, but when we try to do this for the generator, we run into a similar issue\\nas we did when we first tried optimizing the VAE encoder:\\n∇θEz ∼genθ x cost x, y, z\\n= ∑zcost x, y, z * ∇θpθ z x\\nNote that the cost function is only indirectly a function of θ via sampling from\\nthe generator, and thus can be treated as a constant. We can’t re-express this as an\\nexpectation since the gradient is with respect to the distribution from which we are\\nsampling from. This paper uses the log trick, which we also introduced in the section\\non VAEs, to resolve this issue:\\n∑\\nz\\ncost x, y, z * ∇θpθ z x\\n= ∑\\nz\\ncost x, y, z * pθ z x * ∇θ log pθ z x\\n= Ez ∼genθ x cost x, y, z * ∇θ log pθ z x\\nThe gradient of the cost function with respect to the parameters of the encoder is just:\\n∇φEz ∼genθ x cost x, y, z\\n= ∑\\nz\\npθ z x * ∇φcost x, y, z\\n= Ez ∼genθ x ∇φcost x, y, z\\nWhich resembles the standard empirical estimate of the expected gradient when\\nperforming SGD or minibatch gradient descent.\\nHow do we go about training these two networks in tandem? It might be easier to\\nconsider a single training example for starters. We first select a training example at\\nrandom from the dataset, where a training example consists of a text review and\\nan associated rating, and feed the text review to the generator. The generator, which\\nnow represents a probability distribution over all possible binary masks given the\\ninput text review, can be sampled from by sampling each zi independently due to our\\nExtractive Rationalization \\n| \\n287'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 303}, page_content='2 Ribeiro et al. “Why Should I Trust You? Explaining the Predictions of Any Classifier.” arXiv Preprint\\narXiv:1602.04938. 2016.\\nconditional independence claim from earlier. Each sampled binary mask represents a\\npossible rationale, which we then feed to the encoder for prediction. After obtaining\\nthe result of the encoder for each rationale, we have all the information we need\\nto calculate the cost function for each rationale. This will suffice for updating the\\nweights of the encoder, but to update the weights of the generator we will also need to\\nkeep track of the log likelihood of the rationale, or log pθ zk x  for each sampled zk.\\nNow that we have a mechanism for training, how do we translate this to validating\\nand testing our model? During the validation and testing phases, instead of sampling\\nbinary masks from the generator, we select the most likely binary mask according to\\nthe generator probability distribution. To select the most likely binary mask, all we\\nneed to do is select the most likely zi for each xi in our input test review x, again due\\nto our conditional independence assumption from earlier. This is a very reasonable\\napproach to testing, since this is how we would determine the intended rationale\\nwhen using this model in the real world.\\nYou may have noticed some parallels to the concept of attention. After all, the\\ngenerated binary mask can be thought of as a vector of weights we use to multiply the\\nfeature vectors that make up the input text review, where these weights are either 0 or\\n1, rather than some continuous weighting scheme implemented in standard attention.\\nIndeed, the authors of this paper mention that their approach can be viewed as a\\nform of “hard” attention, where we completely mask out or input tokens of the input\\naccording to a probability distribution rather than computing a weighted average\\nof the feature vectors in the input. You might be wondering why hard attention\\nmakes more sense in this case rather than the “soft” attention schemes presented in\\nthe previous section. In this case, hard attention schemes make more sense because\\nfractional weights on words in a sentence are hard to interpret as a measure of\\nimportance, while selecting a strict subset of words in the text as the explanation for a\\nrating is much more interpretable.\\nLIME\\nLIME, or Local Interpretable Model-agnostic Explanations,2 is an interpretability\\ntechnique that is applied to a trained model rather than a built-in feature of the model\\nitself. LIME is a per-example interpretability method, meaning that it generates a\\nsimple, local explanation of the underlying model’s potentially complex behavior. It is\\nalso model agnostic, meaning that the structure of the underlying model itself does\\nnot matter when applying LIME.\\n288 \\n| \\nChapter 11: Methods in Interpretability'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 304}, page_content='Before describing the methodology of LIME, the authors take some time to delineate\\na few characteristics they believe to be necessary components of any explainer. The\\nfirst is that it should be interpretable, meaning that the explainer should provide a\\n“qualitative relationship between input variables and response” that is easy for the\\nuser to understand. Even if the features used in the original model are uninterpreta‐\\nble, the explainer must use features that a human can interpret. For example, in an\\napplication of natural language processing, even if the underlying model utilizes a\\ncomplex word embedding for any given word, the explainer must use features that a\\nhuman can understand, such as the original words themselves.\\nThe second characteristic is local fidelity, which means that the explainer must\\nbehave similarly to the underlying model within some vicinity of the chosen example.\\nWe might ask, why local and not global fidelity? Global fidelity, however, as the\\npaper notes, is quite difficult to achieve and would require drastic advances in the\\nfield—much of the field of interpretability would be solved if global fidelity could be\\nachieved. Thus, we settle for local fidelity.\\nThe third is for the explainer to be model agnostic, which, as we explained earlier,\\nmeans that the structure of the underlying model itself should not matter. The\\nunderlying model can range from a linear regression model to a complex convolu‐\\ntional neural architecture, and the explainer should still be able to satisfy the other\\nthree characteristics. Being model agnostic allows for flexibility in the structure of\\nthe underlying model, which is desirable as this doesn’t necessitate changes in the\\nstructure of the explainer.\\nThe fourth and final characteristic is global perspective, which is to select explana‐\\ntions for a subset of examples that is representative of the model’s behavior. This helps\\nbuild user trust in the model.\\nNow we will take some time to develop the methodology of LIME. As stated, the\\nfeatures of the original model may not be interpretable to a human (and usually aren’t\\nfor most complex models), so the features used by the explainer will be different from\\nthose used by the underlying model. The features used by the explainer could be indi‐\\nvidual words in an NLP task, or functional groups in a chemical property prediction\\ntask—units, or interpretable components, that the end user can understand easily.\\nThus, any example when converted to the feature space of the explainer becomes a\\nbinary vector, where each index is associated with a distinct interpretable component\\n(such as a functional group). A one at any index i indicates the presence of the\\nassociated interpretable component in the original example, and a zero indicates a\\nlack of that component in the original example. Following the notation used in the\\nreferenced paper, we denote x ∈ℝd to be the original feature representation of the\\nexample to be explained and xâ ∈0, 1 d′ to be the representation acted upon by the\\nexplainer, where d’ is the number of interpretable components.\\nLIME \\n| \\n289'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 305}, page_content='Further, the paper defines G to be a class of potentially interpretable models, such\\nas linear regression or random forest, and an explainer to be an instance g ∈G. g\\nacts on an instance x’ and returns a value in the range of the underlying model. We\\ndenote the underlying model to be f, which acts on an instance x and is a function\\nfrom ℝd\\nℝ in the case of regression or a function from ℝd to the range [0,1]\\nin the case of classification, where f returns a probability distribution. Additionally,\\nthe paper defines a proximity measure, or kernel, πx z  around the instance x. This\\nfunction can be defined in a multitude of ways—most implementations of LIME use\\nan exponential kernel that attains a maximum value at x and decreases exponentially\\nas one gets farther and farther from x.\\nAt a high level, LIME attempts to find the explanation g* that minimizes a loss\\nfunction that looks like:\\ng* = argming ∈GL f, g, x + ω g\\nWhere L(f,g,x) is a measure of the unfaithfulness of g in modeling f around the\\ninstance in question x, and ω g  is a measure of the complexity of g. Thus, minimiz‐\\ning their sum results in an optimal explainer g* that has the desired characteristics of\\nlocal fidelity and interpretability described earlier.\\nHow do we measure the unfaithfulness of a potential explainer? The paper’s method‐\\nology is to sample an instance z’ from the vicinity of x’, convert z’ back to an example\\nz in the original feature space, and compute the difference between f(z) and g(z’). The\\ndifference represents the loss for that sample—if g(z’) is far from f(z), then it is not\\nfaithful to the model’s predictions at that point. We can then weight this loss using the\\nkernel πx z , which increasingly discounts the loss as the sample z gets further and\\nfurther from the original example x. Putting this together, the loss function looks like:\\nL f, g, x = ∑z, zâπx z * f z −g zâ\\n2\\nHow do we achieve the samples z’ used in this loss function? The paper samples from\\nthe vicinity of x’ by selecting a subset of the x’ nonzero components, where each\\nsubset is chosen uniformly at random, and setting all other indices of the sample to\\nzero (Figure 11-6).\\n290 \\n| \\nChapter 11: Methods in Interpretability'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 306}, page_content=\"Figure 11-6. x can be thought of as some high-dimensional input such as an image,\\nwhile each index of x’ is associated with some interpretable feature, where a 1 denotes\\nthe existence of that feature in x. The sampling procedure selects some subset of nonzero\\nindices in x’ to keep nonzero in each of w’ and z', which are then mapped back to the\\noriginal input space.\\nLIME then maps these samples z’ back to samples z from the original feature space so\\nwe can measure the fidelity of the explainer via f(z) – g(z’).\\nLIME also takes into account the complexity of the explainer via ω g , which enforces\\nthe interpretability aspect of viable explainers. In the specific case where G represents\\nthe class of linear models, the paper uses a version of ω that places a hard limit on the\\nnumber of nonzero weights in g:\\nω g = ∞* 1\\nwg\\n0 > K\\nWhere ω g  represents g’s weight vector, the L0 norm counts the number of nonzero\\nelements in ω g , and 1[*] is the indicator function, which evaluates to 1 if the\\ncondition within the function is satisfied, and 0 otherwise. The result is that ω g\\nattains a value of infinity when ω g  has more than K nonzero elements, and is 0\\notherwise. This ensures that the chosen ω g  will have at most K nonzero elements,\\nLIME \\n| \\n291\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 307}, page_content='3 Lundberg et al. “A Unified Approach to Interpreting Model Predictions.” arXiv Preprint arXiv:1705.07874.\\n2017.\\nsince one can always do better than any proposed ω g  that has more than K nonzero\\nelements by simply zeroing out weights until there are at most K nonzero weights.\\nThis regularization approach is likely different from regularization approaches you\\nhave encountered in the past, such as the L1 or L2 norm on the weight vector. In\\nfact, to optimize the objective function defined in the paper, the authors utilize an\\nalgorithm they term K-LASSO, which involves first selecting K features via LASSO\\nand then performing standard least squares optimization.\\nAfter performing LIME, we are left with an optimal explainer g, which is a linear\\nmodel with at most K nonzero weights in this case. Now we must check if g satisfies\\nthe goals that the authors set out from the beginning of the paper. First, g must be\\ninterpretable. Since we chose a relatively simple class of explainer models G, which\\nwere linear models in this example, all we need to explain the behavior of the model\\naround the chosen example x are the values of the (at most) K nonzero weights of\\ng. The interpretable components associated with the nonzero weights are considered\\nto be most important for prediction in that locality. In terms of local fidelity, our\\noptimization procedure helps to ensure local fidelity by minimizing the least squares\\nloss between the explainer’s predictions and the model’s predictions. However, there\\ndo exist limitations; for example, the paper notes that if the underlying model is\\nhighly nonlinear even within a short vicinity of the example we are explaining, our\\nlinear explainer won’t be able to do the model’s local behavior justice. With regards\\nto being model agnostic, note that methodology of LIME does not concern itself\\nwith the underlying model’s structure. All LIME needs to function are predictions\\nf(z) from the underlying model. And finally, to achieve a global perspective, we\\ncan select examples that are representative of the model’s behavior and display their\\nexplanations to the user.\\nSHAP\\nSHAP, or Shapley Additive Explanations,3 are similarly a per-prediction interpretabil‐\\nity method for complex models. The paper that introduces the methodology of SHAP\\nfirst provides a framework that the authors feel unifies a variety of interpretability\\nmethods in the field. This framework is termed additive feature attribution, where\\nall instances of this framework utilize a linear explanation model that acts on binary\\nvariables:\\ng x′ = φ0 + ∑i = 1\\nM\\nφixi′\\n292 \\n| \\nChapter 11: Methods in Interpretability'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 308}, page_content='Where M is the number of binary variables. For example, LIME, when using a class\\nof linear explainer models, follows this framework exactly, since each example to\\nbe explained is first converted to a binary vector over interpretable components. It\\nturns out that, in the additive feature attribution framework, there exists a unique\\nsolution in this class that has three desirable properties: local accuracy, missingness,\\nand consistency. Before discussing the unique solution, we will describe the three\\nproperties in more detail.\\nThe first is local accuracy, which states that the explainer model must match the\\nunderlying model exactly at the example being interpreted. This is understandable as\\na desirable property, since it is reasonable that at least the example being interpreted\\nshould be explained perfectly. It’s important to note that not all interpretability frame‐\\nworks necessarily follow this property. For example, the explainer that is generated by\\nLIME, as presented in its original paper and described in the previous section, need\\nnot be locally accurate in the way that the authors of SHAP define local accuracy. This\\nwill be discussed further near the end of this section. Mathematically, local accuracy\\nin SHAP is defined as:\\nf x = g xâ = φ0 + ∑i = 1\\nM\\nφixi′\\nNote that x’ is a simplified feature vector, where each feature in x’ is a binary variable\\nthat represents the presence or absence of a complex feature in the original input\\nspace. The second desirable property is missingness, which states that if x’ contains\\nfeatures equal to zero, the weights associated with those features in the explainer\\nmodel should also be zero. This is also understandable as a desirable property, since\\nthere would be no influence of a feature with value of zero on the output under a\\nlinear explainer g, and thus no need to assign a nonzero weight to that feature in the\\nexplainer.\\nAnd finally, the third desirable property is consistency. This property states that if the\\nunderlying model changes such that a feature in the explainer space either increases\\nor keeps its contribution constant, regardless of the values of the other features in the\\nexplainer space when compared to the original model, the explainer weight associated\\nwith that feature should be larger for the changed underlying model as compared to\\nthe original. That was a mouthful, so we represent it more precisely in mathematical\\nnotation:\\nIf f′ ℎx z′\\n−f′ ℎx z′ ∖i\\n≥f ℎx z′\\n−f ℎx z′ ∖i\\n, ∀z′, then φi f′, x\\n≥φi f, x\\nWhere h is the function that maps inputs from the interpretable space back to the\\noriginal input space. Why is consistency a desirable property? For the new model, the\\nSHAP \\n| \\n293'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 309}, page_content='delta between the existence of the corresponding, more complex feature in the input\\nspace and its absence is greater than or equal to the delta for the old model, regardless\\nof all other feature settings. Thus, it makes sense that we should attribute at least as\\nlarge a weight to it in the explainer for the new model as compared to the old model,\\nsince its existence is clearly more important for the new model.\\nAs mentioned, for each underlying model f there exists a unique g within the additive\\nattribution framework that also satisfies all three properties listed. Although we won’t\\nshow this here, this result is one that follows from earlier results in cooperative\\ngame theory, where the learned weights are called Shapley values. Shapley values\\nwere originally defined to quantify per-example feature importance in multivariate\\nlinear regression models where individual features have significant correlations. This\\nis an important problem, especially in the setting of significant correlations due to\\nambiguity between which features are the most predictive. It could be the case that a\\nfeature A is correlated with the target y, but when factoring in feature B it turns out\\nthat feature A provides only negligible additional value (i.e., predictions don’t change\\nsignificantly, test statistics remain relatively constant, etc.). On the other hand, feature\\nB may provide significant predictive power both in the individual case and in the case\\nwhere feature A is included.\\nDetermining the relative importance of feature A and B in a vanilla multivariate\\nregression that includes both features is difficult due to their non-negligible correla‐\\ntion. Shapley values tease out these relationships and compute the true marginal\\nimpact of a given feature, as we will soon see. Here is the formula for Shapley values,\\nwhere i represents the feature in question:\\nφi = ∑S ∈F ∖i\\nS ! *\\nF\\n−S −1 !\\nF !\\n* fS ∪i xS ∪i\\n−fS xS\\nWe will now break down this formula. Intuitively, the Shapley value for an individual\\nfeature is computed by first taking the difference between the predictions for a\\nmodel trained over a subset of features S plus the feature in question i included, and\\npredictions for a model trained over the same subset of features S but with feature\\ni withheld. The final Shapley value is a weighted sum of these differences over all\\npossible subsets of features S.\\nTo find these differences, we can first train a multivariate linear regression model fS\\nthat only uses some subset of features S, and then train a second multivariate linear\\nregression model fS ∪i  that uses the subset of features S ∪i . Let the example we\\nare explaining be denoted as x, and xA denote the portion of x that corresponds to\\nsome feature subset A. The difference fS ∪i xS ∪i  – fS xS  represents how much\\nthe prediction changes when we include the feature i. Additionally, note that the\\nformula is a sum over all possible feature subsets, which means that if the computed\\nShapley value for feature i is high, the difference between including feature i and not\\n294 \\n| \\nChapter 11: Methods in Interpretability'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 310}, page_content='including feature i was likely substantial for a majority of possible feature subsets.\\nThis result signifies that feature i generally adds significant predictive power regard‐\\nless of the features in S, which is captured by the high Shapley value. In the example\\nprovided earlier, we’d find that feature B has a higher Shapley value than feature A.\\nAdditionally, the intuition behind the weighting scheme is that it achieves a more\\nunbiased result for feature importance, since subsets of a given size occur either more\\nor less frequently than subsets of a different size in the set of all subsets. The number\\nof subsets of a given size is computed using the choose function, a concept from\\ncounting and probability. When this is inverted and used as a weighting scheme,\\nthe result of a subset whose size occurs more frequently in the set of all possible\\nsubsets is weighted less than, for example, a feature subset consisting of only a single\\nfeature other than the feature in question. As stated earlier, we won’t prove why this is\\nunbiased in full, but we hope that this makes the intuition clearer.\\nYou may notice that computing exact Shapley regression values for any reasonable\\nnumber of features is intractable. This would involve training a regression model\\non all possible subsets of features, where the number of subsets of features (and\\nthus models to train) grows exponentially with the number of features. We instead\\nresort to approximation via sampling to help. Given an example x to explain and a\\nregression model fS trained on some subset of features S, we can compute fS ∖i  by\\ntaking an expectation of fS with respect to the distribution of feature i conditioned\\non x’s setting of features S ∖i :\\nfS ∖i = Ep xi xS ∖i\\nfS xS ∖i , xi\\nWhere the bold represents that xS ∖i  is being treated as a known entity taken\\nfrom x, the example being explained, while xi is being treated as an unknown, i.e.,\\nis a random variable rather than taking on the value provided by x. As has been\\na common theme throughout this book, we can approximate expectations like the\\npreceding one in an unbiased manner by sampling and averaging:\\nEp xi xS ∖i\\nfS xS ∖i , xi\\n= Ep xi fS xS ∖i , xi\\n≈1\\nn ∑\\nj = 1\\nn\\nfS xS ∖i , xi\\nj\\nYou may have noticed the parallels between the procedure we just described and the\\nestimation procedure described in “Partial Dependence Plots” on page 282. In fact,\\nthese are doing the exact same thing—notice that we again assume independence\\nSHAP \\n| \\n295'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 311}, page_content='between feature subset S ∖i  and feature i, which allows us to use all samples of\\nfeature i from the dataset indiscriminately.\\nThe authors propose SHAP values in the general case, where SHAP values are given\\nby the formula:\\nφi f, x = ∑z′ ⊆x′\\nz′ ! * M −z′ −1 !\\nM !\\nf ℎx z′\\n−f ℎx z′ ∖i\\nWhere z’ is a subset of the nonzero components of x’. Additionally, z′ ∖i  rep‐\\nresents setting feature i in the interpretable space equal to zero. Note that if\\nfeature i is already zero in the input x’, then the formula outputs zero as well\\nsince f ℎx z′\\n= f ℎx z′ ∖i\\n, ∀z′ ⊆x′. This quick check shows that the formula\\nindeed satisfies the property of missingness. The vector consisting of SHAP values for\\neach feature in x’ completely defines the optimal explainer model g in the additive\\nattribution framework, where optimal signifies that g satisfies all three properties\\ndefined earlier: local accuracy, consistency, and missingness. Right off the bat, we\\ncan see the parallels between the proposed SHAP values and the Shapley values from\\nmultivariate regression. Additionally, we can use the same sampling procedure to\\nestimate SHAP values.\\nAs discussed, LIME is in the additive attribution framework. In the original LIME\\npaper, the optimal explainer model g was selected via a specialized optimization\\nprocedure that first selected k features to have a nonzero contribution and then\\nperformed standard least squares optimization to achieve the final weights of g. Due\\nto these heuristics, including the choice of kernel πx z , there is no guarantee that\\nthe explainer selected using the procedure presented in the original LIME paper will\\nsatisfy the SHAP criteria of local accuracy, missingness, and consistency.\\nHowever, the optimization procedure presented in the LIME paper does achieve an\\nexplainer that satisfies the criteria for explainer models proposed in LIME; recall\\nthe concepts of being interpretable, having local fidelity, being model agnostic, and\\nachieving global perspective from the previous section. We point this out specifically\\nto show that different groups of knowledgeable individuals don’t necessarily have\\nthe exact same idea of what it means for an explainer to be interpretable, and that\\ninterpretability as a concept has evolved over time.\\nIt turns out that in LIME, there exists an exact form to the proximity measure πx z ,\\nω, and loss function L such that when minimized, results in an optimal explainer g\\nthat satisfies all three SHAP criteria for interpretability:\\n296 \\n| \\nChapter 11: Methods in Interpretability'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 312}, page_content=\"ω g = 0\\nπx′ z′ =\\nM −1\\nM choose z′ * z′ * M −z′\\nL f, g, π = ∑\\nz′ ∈Z\\nf ℎx z′\\n−g z′\\n2 * πx′ z′\\nWe can optimize this loss function using a weighted least squares optimization to\\nobtain the unique optimal g. Note that the kernel here is distinct in interpretation\\nfrom the kernel choices presented in the original LIME paper. Instead of the kernel\\ndecreasing in value as the samples get farther from the example being explained, the\\nSHAP kernel is symmetric. This can be verified by examining the output of the kernel\\nwhen |z'| = k and when |z'| = M – k. In fact, from just looking at the formula, we can\\nsee that the value of the kernel isn’t even dependent on x’.\\nIn conclusion, SHAP values unify several existing interpretability methods by first\\ndefining the additive attribution framework, which is shared amongst these methods,\\nand second by proving the existence of a unique optimal explainer within this\\nframework that satisfies three desirable properties.\\nSummary\\nAlthough interpretability often comes in a variety of forms, they are all designed\\nwith the end goal of being able to explain model behavior. We learned that not\\nevery model is interpretable by construction, and even those that are might only be\\nsuperficially so. For example, although vanilla linear regression seems to be quite\\ninterpretable by design, correlations between features can muddle this initially clear\\npicture. Additionally, we learned about interpretability methods that are built into the\\nmodel itself, such as extractive rationalization, and post hoc interpretability methods\\nsuch as LIME and SHAP. The right form of interpretability will often depend on\\nthe domain—for example, using gradient-based methods for image classification\\nmay make sense, but not so much in language problems. The soft attention scheme\\ndiscussed in previous chapters may not be as desirable for sentiment analysis as, say,\\nthe hard selection methodology presented in our section on extractive rationalization.\\nAnd finally, we learned about how interpretability does not carry the exact same\\nmeaning across the board, even in research—note our discussion on the differences\\nbetween the optimal explainers generated by LIME and SHAP. We hope that this\\nchapter served as a fruitful foray into the vast landscape of interpretability research.\\nSummary \\n| \\n297\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 313}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 314}, page_content='CHAPTER 12\\nMemory Augmented Neural Networks\\nMostafa Samir\\nSo far we’ve seen how effective an RNN can be at solving a complex problem like\\nmachine translation. However, we’re still far from reaching its full potential! In\\nChapter 9 we mentioned that it’s theoretically proven that the RNN architecture is a\\nuniversal functional representer; a more precise statement of the same result is that\\nRNNs are Turing complete. This simply means that given proper wiring and adequate\\nparameters, an RNN can learn to solve any computable problem, which is basically\\nany problem that can be solved by a computer algorithm or, equivalently, a Turing\\nmachine.\\nNeural Turing Machines\\nThough theoretically possible, it’s extremely difficult to achieve that kind of uni‐\\nversality in practice. This difficulty stems from the fact that we’re looking at an\\nimmensely huge search space of possible wirings and parameter values of RNNs,\\na space so vastly large for gradient descent to find an appropriate solution for any\\narbitrary problem. However, in this chapter we’ll start exploring some approaches at\\nthe edge of research that will allow us to start tapping into that potential.\\nLet’s think for a while about a very simple reading comprehension question like the\\nfollowing:\\nMary travelled to the hallway. She grabbed the milk glass there.\\nThen she travelled to the office, where she found an apple\\nand grabbed it.\\nHow many objects is Mary carrying?\\n299'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 315}, page_content=\"The answer is so trivial: it’s two. But what actually happened in our brains that\\nallowed us to come up with the answer so trivially? If we thought about how we could\\nsolve that comprehension question using a simple computer program, our approach\\nwould probably go like this:\\n1. allocate a memory location for a counter\\n2. initialize counter to 0\\n3. for each word in passage\\n    3.1. if word is 'grabbed'\\n        3.1.1. increment counter\\n4. return counter value\\nIt turns out that our brains tackle the same task in a similar way to that simple\\ncomputer program. Once we start reading, we start allocating memory (just as our\\ncomputer program) and store the pieces of information we receive. We start by\\nstoring that location of Mary, which after the first sentence is the hallway. In the\\nsecond sentence we store the objects Mary is carrying, and by now it’s only a glass of\\nmilk. Once we see the third sentence, our brain modifies the first memory location\\nto point to the office. By the end of the fourth sentence, the second memory location\\nis modified to include both the milk and the apple. When we finally encounter\\nthe question, our brains quickly query the second memory location and count the\\ninformation there, which turns out to be two. In neuroscience and cognitive psychol‐\\nogy, such a system of transient storing and manipulation of information is called a\\nworking memory, and it’s the main inspiration behind the line of research we’ll be\\ndiscussing in the rest of this chapter.\\nIn 2014, Graves et al. from Google DeepMind started this line of work in a paper\\ncalled “Neural Turing Machines” in which they introduced a new neural architecture\\nwith the same name, a Neural Turing Machine (NTM), that consists of a controller\\nneural network (usually an RNN) with an external memory that resembles the brain’s\\nworking memory. For the close resemblance between the working memory model\\nand the computer model we just saw, Figure 12-1 shows that the same resemblance\\nholds for the NTM architecture, with the external memory in place of the RAM, the\\nread/write heads in place of the read/write buses, and the controller network in place\\nof the CPU, except for the fact that the controller learns its program, unlike the CPU,\\nwhich is fed its program. Figure 12-1 has a single read head and a single write head,\\nbut an NTM can have several in practice.\\n300 \\n| \\nChapter 12: Memory Augmented Neural Networks\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 316}, page_content='Figure 12-1. Comparing the architecture of a modern-day computer, which is fed its\\nprogram (left) to an NTM that learns its program (right)\\nIf we thought about NTMs in light of our earlier discussion of RNN’s Turing com‐\\npleteness, we’ll find that augmenting the RNN with an external memory for transient\\nstorage prunes a large portion out of that search space, as we now don’t care about\\nexploring RNNs that can both process and store the information; we’re just looking\\nfor the RNNs that can process the information stored outside of them. This pruning\\nof the search space allows us to start tapping into some of the RNN potentials that\\nwere locked away before augmenting it with a memory, evident by the variety of tasks\\nthat the NTM could learn: from copying input sequences after seeing them, to emu‐\\nlating N-gram models, to performing a priority sort on data. We’ll even see by the end\\nof the chapter how an extension to the NTM can learn to do reading comprehension\\ntasks like the one we saw earlier, with nothing more than a gradient-based search.\\nAttention-Based Memory Access\\nTo be able to train an NTM with a gradient-based search method, we need to make\\nsure that the whole architecture is differentiable so that we can compute the gradient\\nof some output loss with respect to the model’s parameters that process the input.\\nThis property is called end-to-end-differentiable, with one end being the inputs and\\nthe other the outputs. If we attempted to access the NTM’s memory in the same way\\na digital computer accesses its RAM, via discrete values of addresses, the discreteness\\nof the addresses would introduce discontinuities in gradients of the output, and\\nAttention-Based Memory Access \\n| \\n301'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 317}, page_content='hence we would lose the ability to train the model with a gradient-based method.\\nWe need a continuous way to access the memory while being able to “focus” on a\\nspecific location in it. This kind of continuous focusing can be achieved via attention\\nmethods.\\nInstead of generating a discrete memory address, we let each head generate a normal‐\\nized softmax attention vector with the same size as the number of memory locations.\\nWith this attention vector, we’ll be accessing all the memory locations at the same\\ntime in a blurry manner, with each value in the vector telling us how much we’re\\ngoing to focus on the corresponding location, or how likely we’re going to access it.\\nFor example, to read a vector at a time step t out of our N × W NTM’s memory\\nmatrix denoted by Mt (where N is the number of locations and W is the size of the\\nlocation), we generate an attention vector, or a weighting vector wt of size N, and our\\nread vector can be calculated via the product:\\nrt = Mt\\n⊤wt\\nwhere ⊤ denotes the matrix transpose operation. Figure 12-2 shows how with the\\nweights attending to a specific location, we can retrieve a read vector that approxi‐\\nmately contains the same information as the content of that memory location.\\nFigure 12-2. A demonstration of how a blurry attention-based reading can retrieve a\\nvector containing approximately the same information as in the focused-on location\\nA similar attention weighting method is used for the write head: a weighting vector\\nwt is generated and used for erasing specific information from the memory, as\\nspecified by the controller in an erase vector et that has W values between 0 and\\n1 specifying what to erase and what to keep. Then we use the same weighting for\\nwriting to the erased memory matrix some new information, also specified by the\\ncontroller in a write vector vt containing W values:\\n302 \\n| \\nChapter 12: Memory Augmented Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 318}, page_content='Mt = Mt −1 ∘E −wtet⊤+ wtvt⊤\\nwhere E is a matrix of ones and ∘ is element-wise multiplication. Similar to the\\nreading case, the weighting wt tells us where to focus our erasing (the first term of the\\nequation) and writing operations (the second term).\\nNTM Memory Addressing Mechanisms\\nNow that we understand how NTMs access their memories in a continuous manner\\nvia attention weighting, we’re left with how these weightings are generated and what\\nforms of memory addressing mechanisms they represent. We can understand that\\nby exploring what NTMs are expected to do with their memories, and based on\\nthe model they are mimicking (the Turning machine), we expect them to be able to\\naccess a location by the value it contains, and to be able to go forward or backward\\nfrom a given location.\\nThe first mode of behavior can be achieved with an access mechanism that we’ll call\\ncontent-based addressing. In this form of addressing, the controller emits the value\\nthat it’s looking for, which we’ll call a key kt, then it measures its similarity to the\\ninformation stored in each location and focuses the attention on the most similar\\none. This kind of weighting can be calculated via:\\n C(M,k, β) = \\nexp βD M, k\\n∑i = 0\\nN\\nexp βD M i , k\\nwhere D is some similarity measure, like the cosine similarity. The equation is noth‐\\ning more than a normalized softmax distribution over the similarity scores. There\\nis, however, an extra parameter β that is used to attenuate the attention weights\\nif needed. We call that the key strength. The main idea behind that parameter is\\nthat for some tasks, the key emitted by the controller may not be close to any of\\nthe information in the memory, which would result in seemingly uniform attention\\nweights. Figure 12-3 shows how the key strength allows the controller to learn how to\\nattenuate such uniform attention to be more focused on a single location that is the\\nmost probable; the controller then learns what value of the strength to emit with each\\npossible key it emits.\\nNTM Memory Addressing Mechanisms \\n| \\n303'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 319}, page_content='Figure 12-3. An indecisive key with unit strength results in a nearly uniform attention\\nvector; increasing the strength for keys like that focuses the attention on the most\\nprobable location\\nTo move forward and backward in the memory, we first need to know where we\\nare we standing now, and such information is located in the access weighting from\\nthe last time step wt −1. So to preserve the information about our current location\\nwith the new content-based weighting wtc we just got, we interpolate between the two\\nweighting using a scalar gt that lies between 0 and 1:\\n wt\\ng = gtwtc + 1 −gt wt −1\\nWe call gt the interpolation gate, and it’s also emitted by the controller to control the\\nkind of information we want to use in the current time step. When the gate’s value\\nis close to 1, we favor the addressing given by content lookup. However, when it’s\\nclose to 0, we tend to pass the information about our current location through and\\nignore the content-based addressing. The controller learns to use this gate so that, for\\nexample, it could set it to 0 when iteration through consecutive locations is desired\\nand information about the current location is crucial. The type of information the\\ncontroller chooses to gate through is denoted by the gated weighting wt\\ng.\\nTo start moving around the memory we need a way to take our current gated weight‐\\ning and shift the focus from one location to another. This can be done by convoluting\\nthe gated weighting with a shift weighting st, also emitted by the controller. This shift\\n304 \\n| \\nChapter 12: Memory Augmented Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 320}, page_content='weighting is a normalized softmax attention vector of size n + 1, where n is an even\\ninteger specifying the number of possible shifts around the focused-on location in\\nthe gated weighting; for example, if it has a size of 3, then there are two possible\\nshifts around a location: one forward and one backward. Figure 12-4 shows how a\\nshift weighting can move around the focused-on location in gated weighting. The\\nshifting occurs by convoluting the gated weighting by the shift weighting in pretty\\nmuch the same way we convoluted images with feature maps back in Chapter 7. The\\nonly exception is how we handle the case when the shift weightings go outside the\\ngated weighting. Instead of using padding like we did before, we use a rotational\\nconvolution operator where overflown weights get applied to the values at the other\\nend of the gated weighting, as shown in the middle panel of Figure 12-4. This\\noperation can be expressed element-wise as:\\nwt i = ∑j = 0\\nst\\nwt\\ng i +\\nst −1\\n2\\n−j\\nmod N st j\\nFigure 12-4. A shift weighting focused on the right shifts the gated weighting one location\\nto the right (left). Rotational convolution on a left-focused shift weighting, shifting the\\ngated weighting to the left (middle). A nonsharp centered shift weighting keeps the gated\\nweighting intact but disperses it (right).\\nWith the introduction of the shifting operation, our heads’ weightings can now move\\naround the memory freely forward and backward. However, a problem occurs if at\\nany time the shift weighting is not sharp enough. Because of the nature of the convo‐\\nlution operation, a nonsharp shift weighting (as in the right panel of Figure 12-4)\\ndisperses the original gated weightings around its surroundings and results in a\\nless-focused shifted weighting. To overcome that blurring effect, we run the shifted\\nweightings through one last operation: a sharpening operation. The controller emits\\none last scalar γt ≥1 that sharpens the shifted weightings via:\\nwt =\\nwt\\nγt\\n∑i = 0\\nN\\nwt i γt\\nNTM Memory Addressing Mechanisms \\n| \\n305'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 321}, page_content='1 Source: Graves et al. “Neural Turing Machines.” (2014)\\nStarting from interpolation down to the final weighting vector out of sharpening, this\\nprocess constitutes the second addressing mechanism of NTMs: the location-based\\nmechanism. Using a combination of both addressing mechanisms, an NTM is able to\\nutilize its memory to learn to solve various tasks. One of these tasks that would allow\\nus to get a deeper look into the NTM in action is the copy task shown in Figure 12-5.\\nIn this task, we present the model with a sequence of random binary vectors that\\nterminate with a special end symbol. We then request the same input sequence to be\\ncopied to the output.\\nFigure 12-5. An NTM trained on the copy task1\\nThe visualization shows how at the input time, the NTM starts writing the inputs\\nstep-by-step into consecutive locations in the memory. In the output time, the NTM\\ngoes back at the first written vector and iterates through the next locations to read\\nand return the previously written input sequence. The original NTM paper contains\\nseveral other visualizations of NTMs trained on different problems that are worth\\nchecking. These visualizations demonstrate the architecture’s ability to utilize the\\naddressing mechanisms to adapt to and learn to solve various tasks.\\nWe’ll suffice with our current understanding of NTMs and skip its implementation.\\nInstead, we will spend the rest of the chapter exploring the drawbacks of NTMs and\\nhow the novel architecture of the differentiable neural computer (DNC) was able\\nto overcome these drawbacks. We’ll conclude our discussion by implementing that\\nnovel architecture on simple reading comprehension tasks like the one we saw earlier.\\n306 \\n| \\nChapter 12: Memory Augmented Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 322}, page_content='Differentiable Neural Computers\\nDespite the power of NTMs, they have a few limitations regarding their memory\\nmechanisms. The first of these limitations is that NTMs have no way to ensure\\nthat no interference or overlap between written data would occur. This is due to\\nthe nature of the “differentiable” writing operation in which we write new data\\neverywhere in the memory to some extent specified by the attention. Usually, the\\nattention mechanisms learn to focus the write weightings strongly on a single mem‐\\nory location, and the NTM converges to a mostly interference-free behavior, but that’s\\nnot guaranteed.\\nHowever, even when the NTM converges to an interference-free behavior, once a\\nmemory location has been written to, there’s no way to reuse that location again, even\\nwhen the data stored in it becomes irrelevant. The inability to free and reuse memory\\nlocations is the second limitation of the NTM architecture. This results in new data\\nbeing written to new locations that are likely to be contiguous, as we saw with the\\ncopy task. This contiguous writing fashion is the only way for an NTM to record\\nany temporal information about the data being written: consecutive data is stored in\\nconsecutive locations. If the write head jumped to another place in the memory while\\nwriting some consecutive data, a read head won’t be able to recover the temporal\\nlink between the data written before and after the jump: this constitutes the third\\nlimitation of NTMs.\\nIn October 2016, Graves et al. from DeepMind published in Nature a paper titled,\\n“Hybrid Computing Using a Neural Network with Dynamic External Memory,” in\\nwhich they introduced a new memory-augmented neural architecture called differen‐\\ntiable neural computer (DNC) that improves on NTMs and addresses the limitations\\nwe just discussed. Similar to NTMs, DNCs consists of a controller that interacts with\\nan external memory. The memory consists of N words of size W, making up an\\nN × W matrix we’ll be calling  M. The controller takes in an input vector of size X\\nand the R vectors of size W read from memory in the previous step, where R is the\\nnumber of read heads. The controller then processes them through a neural network\\nand returns two pieces of information:\\n• An interface vector that contains all the necessary information to query the\\nmemory (i.e., write and read from it)\\n• A pre-output vector of size Y\\nThe external memory then takes in the interface vector, performs the necessary\\nwriting through a single write head, then reads R new vectors from the memory.\\nIt returns the newly read vectors to the controller to be added with the pre-output\\nvector, producing the final output vector of size Y.\\nDifferentiable Neural Computers \\n| \\n307'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 323}, page_content='Figure 12-6 summarizes the operation of the DNC that we just described. We can see\\nthat unlike NTMs, DNCs keep other data structures alongside the memory itself to\\nkeep track of the state of the memory. As we’ll shortly see, with these data structures\\nand some clever new attention mechanisms, DNCs are able to successfully overcome\\nNTM’s limitations.\\nFigure 12-6. An overview of DNC’s architecture and operation\\nTo make the whole architecture differentiable, DNCs access the memory through\\nweight vectors of size N whose elements determine how much the heads focus on\\neach memory location. There are R weightings for the read heads wt\\nr, 1, ⋯, wt\\nr, R\\nwhere t denotes the time step. On the other hand, there’s one write weighting wtw for\\nthe single write head. Once we obtain these weightings, we can modify the memory\\nmatrix and get updated via:\\nMt = Mt −1 ∘E −wtwet⊤+ wtwvt⊤\\nand et, vt are the erase and write vectors we saw earlier with NTMs, coming from the\\ncontroller through the interface vector as instructions about what to erase from and\\nwrite to the memory.\\nAs soon as we get the updated memory matrix Mt, we can read out the new read\\nvectors rt\\n1, rt\\n2, ⋯, rtR using the following equation for each read weighting:\\nrti = Mt\\n⊤wt\\nr, i\\n308 \\n| \\nChapter 12: Memory Augmented Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 324}, page_content='Up until now, it seems that there’s nothing different from how NTMs write to and\\nread from memory. However, the differences will start to show up when we discuss\\nthe attention mechanisms DNCs use to obtain their access weightings. While they\\nboth share the content-based addressing mechanism C(M, k, β) defined earlier, DNCs\\nuse more sophisticated mechanisms to attend more efficiently to the memory.\\nInterference-Free Writing in DNCs\\nThe first limitation we discussed for NTMs was their inability to ensure an\\ninterference-free writing behavior. An intuitive way to address this issue is to design\\nthe architecture to focus strongly on a single, free memory location and not wait for\\nNTM to learn to do so. To keep track of which locations are free and which are busy,\\nwe need to introduce a new data structure that can hold this kind of information.\\nWe’ll call it the usage vector.\\nThe usage vector ut is a vector of size N, where each element holds a value between 0\\nand 1 that represents how much of the corresponding memory location is used; with\\n0 indicating a completely free location, and 1 indicating a completely used one.\\nThe usage vector initially contains zeros u0 = 0 and gets updated with the usage\\ninformation across the steps. Using this information, it’s clear that the location to\\nwhich the weights should attend most strongly is the one with the least usage value.\\nTo obtain such weighting, we need to first sort the usage vector and obtain the list\\nof location indices in ascending order of the usage; we call such a list a free list and\\ndenote it by φt. Using that free list, we can construct an intermediate weighting called\\nthe allocation weighting at that would determine which memory location should be\\nallocated for new data. We calculate at using:\\nat φt j\\n= 1 −ut φt j\\n∏i = 1\\nj −1ut φt i\\nwhere j ∈1, ⋯, N\\nThis equation may look incomprehensible at first glance. A good way to\\nunderstand it is to work through it with a numerical example, for example,\\nwhen ut = 1, 0 . 7, 0 . 2, 0 . 4 . We’ll leave the details for you to go through. In the\\nend, you should arrive at the allocation weighting being at = 0, 0 . 024, 0 . 8, 0 . 12 . As\\nwe go through the calculations, we’ll begin to understand how this formula works:\\nthe 1 −ut φt j makes the location weight proportional to how free it is. By noticing\\nthat the product ∏i = 1\\nj −1ut φt j  gets smaller and smaller as we iterate through the\\nfree list (because we keep multiplying small values between 0 and 1), we can see that\\nthis product decreases the location weight even more as we go from the least used\\nlocation to the most used one, which finally results in the least used location having\\nthe largest weight, while the most used one gets the smallest weight. So we’re able to\\nguarantee the ability to focus on a single location by design without the the need to\\nInterference-Free Writing in DNCs \\n| \\n309'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 325}, page_content='hope for the model to learn it on its own from scratch; this means more reliability as\\nwell as faster training time.\\nWith the allocation weighting at and lookup weighting ctw we get from the content-\\nbased addressing mechanism ctw = C Mt −1, kt\\nw, βt\\nw , where kt\\nw, βt\\nw are the lookup\\nkey and the lookup strength we receive through the interface vector, we can now\\nconstruct our final write weighting:\\n wtw = gtw gtaat + 1 −gta ctw\\nwhere gtw and gta are values between 0 and 1 and are called the write and allocation\\ngates, which we also get from the controller through the interface vector. These gates\\ncontrol the writing operation, with gtw determining if any writing is going to happen\\nin the first place, and gta specifying whether we’ll write to a new location using the\\nallocation weighting or modify an existing value specified by the lookup weighting.\\nDNC Memory Reuse\\nWhat if while we calculate the allocation weighting we find that all locations are used,\\nor in other words, ut = 1? This means that the allocation weightings will turn out all\\nzeros and no new data can be allocated to memory. This raises the need for the ability\\nto free and reuse the memory.\\nIn order to know which locations can be freed and which cannot, we construct a\\nretention vector ψt of size N that specifies how much of each location should be\\nretained and not get freed. Each element of this vector takes a value between 0 and 1,\\nwith 0 indicating that the corresponding location can be freed, and 1 indicating that it\\nshould be retained. This vector is calculated using:\\nψt = ∏i = 1\\nR\\n1 −ft\\niwt −1\\nr, i\\nThis equation is basically saying that the degree to which a memory location should\\nbe freed is proportional to how much is read from it in the last time steps by the\\nvarious read heads (represented by the values of the read weightings wt −1\\nr, i ). However,\\ncontinuously freeing a memory location once its data is read is not generally prefera‐\\nble as we might still need the data afterward. We let the controller decide when to free\\nand when to retain a location after reading by emitting a set of R free gates ft\\n1, ⋯, ft\\nR\\nthat have a value between 0 and 1. This determines how much freeing should be done\\nbased on the fact that the location was just read from. The controller will then learn\\nhow to use these gates to achieve the behavior it desires.\\n310 \\n| \\nChapter 12: Memory Augmented Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 326}, page_content='Once the retention vector is obtained, we can use it to update the usage vector to\\nreflect any freeing or retention made via:\\nut = ut −1 + wt −1\\nw\\n−ut −1 ∘wt −1\\nw\\n∘ψt\\nThis equation can be read as follows: a location will be used if it has been retained (its\\nvalue in ψt ≈1) and either it’s already in use or has just been written to (indicated by\\nits value in ut −1 + wt −1\\nw\\n). Subtracting the element-wise product ut −1 ∘wt −1\\nw\\n brings\\nthe whole expression back between 0 and 1 to be a valid usage value in case the\\naddition between the previous usage got the write weighting past 1.\\nBy doing this usage update step before calculating the allocation, we can introduce\\nsome free memory for possible new data. We’re also able to use and reuse a limited\\namount of memory efficiently and overcome the second limitation of NTMs.\\nTemporal Linking of DNC Writes\\nWith the dynamic memory management mechanisms that DNCs use, each time a\\nmemory location is requested for allocation, we’re going to get the most unused\\nlocation, and there’ll be no positional relation between that location and the location\\nof the previous write. With this type of memory access, NTM’s way of preserving\\ntemporal relation with contiguity is not suitable. We’ll need to keep an explicit record\\nof the order of the written data.\\nThis explicit recording is achieved in DNCs via two additional data structures along‐\\nside the memory matrix and the usage vector. The first is called a precedence vector pt,\\nan N-sized vector considered to be a probability distribution over the memory loca‐\\ntions, with each value indicating how likely the corresponding location was the last\\none written to. The precedence is initially set to p0 = 0 and gets updated in the\\nfollowing steps via:\\npt = 1 −∑i = 1\\nN\\nwtw i pt −1 + wtw\\nUpdating is done by first resetting the previous values of the precedence with a reset\\nfactor that is proportionate to how much writing was just made to the memory\\n(indicated by the summation of the write weighting’s components). Then the value\\nof write weighting is added to the reset value so that a location with a large write\\nweighting (that is the most recent location written to) would also get a large value in\\nthe precedence vector.\\nThe second data structure we need to record temporal information is the link\\nmatrix Lt. The link matrix is an N × N matrix in which the element Lt i, j  has a\\nTemporal Linking of DNC Writes \\n| \\n311'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 327}, page_content='value between 0,1, indicating how likely it is that location i was written after location\\nj. This matrix is also initialized to zeros, and the diagonal elements are kept at zero\\nthroughout the time Lt i, i = 0, as it’s meaningless to track if a location was written\\nafter itself when the previous data has already been overwritten and lost. However,\\neach other element in the matrix is updated using:\\nLt i, j = 1 −wtw i −wtw j Lt −1 i, j + wtw i pt −1 j\\nThe equation follows the same pattern we saw with other update rules: first the\\nlink element is reset by a factor proportional to how much writing had been done\\non locations i, j. Then the link is updated by the correlation (represented here by\\nmultiplication) between the write weighting at location i and the previous precedence\\nvalue of location j. This eliminates NTM’s third limitation; now we can keep track of\\ntemporal information no matter how the write head hops around the memory.\\nUnderstanding the DNC Read Head\\nOnce the write head has finished updating the memory matrix and the associated\\ndata structures, the read head is now ready to work. Its operation is simple: it needs\\nto be able to look up values in the memory and be able to iterate forward and back‐\\nward in temporal ordering between data. The lookup ability can simply be achieved\\nwith content-based addressing: for each read head i, we calculate an intermediate\\nweighting ct\\nr, i = C Mt, kt\\nr, i, βt\\nr, i , where kt\\nr, 1, ⋯, kt\\nr, R and βt\\nr, 1, ⋯, βt\\nr, R are two sets\\nof R read keys and strengths received from the controller in the interface vector.\\nTo achieve forward and backward iterations, we need to make the weightings go a\\nstep ahead or back from the location they recently read from. We can achieve that\\nfor the forward iteration by multiplying the link matrix by the last read weightings.\\nThis shifts the weights from the last read location to the location of the last write\\nspecified by the link matrix and constructs an intermediate forward weighting for\\neach read head i: ft\\ni = Ltwt −1\\nr, i . Similarly, we construct an intermediate backward\\nweighting by multiplying the transpose of the link matrix by the last read weight‐\\nings bt\\ni = Lt −1\\n⊤\\nwt −1\\nr, i .\\nWe can now construct the new read weightings for each read using the following rule:\\nwt\\nr, i = πti 1 bt\\ni + πti 2 cti + πti 3 ft\\ni\\nwhere πt\\n1, ⋯, πtR are called the read modes. Each of these are a softmax distribution\\nover three elements that come from the controller on the interface vector. Its three\\nvalues determine the emphasis the read head should put on each read mechanism:\\n312 \\n| \\nChapter 12: Memory Augmented Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 328}, page_content='backward, lookup, and forward, respectively. The controller learns to use these modes\\nto instruct the memory on how data should be read.\\nThe DNC Controller Network\\nNow that we’ve figured out the internal workings of the external memory in the\\nDNC architecture, we’re left with understanding how the controller that coordinates\\nall the memory operations work. The controller’s operation is simple: in its heart\\nthere’s a neural network (recurrent or feed-forward) that takes in the input step along\\nwith the read-vectors from the last step and outputs a vector whose size depends on\\nthe architecture we chose for the network. Let’s denote that vector by N(χt), where\\nN denotes whatever function is computed by the neural network, and χt denotes the\\nconcatenation of the input step and the last read vectors χt = xt; rt −1\\n1\\n; ⋯; rt −1\\nR\\n. This\\nconcatenation of the last read vectors serves a similar purpose as the hidden state in a\\nregular LSTM: to condition the output on the past.\\nFrom that vector emitted from the neural network, we need two pieces of informa‐\\ntion. The first one is the interface vector ζt. As we saw, the interface vector holds\\nall the information for the memory to carry out its operation. We can look at\\nthe ζt vector as a concatenation of the individual elements we encountered before, as\\ndepicted in Figure 12-7.\\nFigure 12-7. The interface vector decomposed to its individual components\\nBy summing up the sizes along the components, we can consider the ζt vector as\\none big vector of size R × W + 3W + 5R + 3. So in order to obtain that vector from\\nthe network output, we construct a learnable N\\n× R × W + 3W + 5R + 3  weights\\nmatrix Wζ, where N  is the size of the network’s output, such that:\\nζt = WζN χt\\nBefore passing that ζt vector to the memory, we need to make sure that each compo‐\\nnent has a valid value. For example, all the gates as well as the erase vector must have\\nvalues between 0 and 1, so we pass them through a sigmoid function to ensure that\\nrequirement:\\net = σ et , ft\\ni = σ ft\\ni , gta = σ gta , gtw = σ gtw\\nwhere σ z =\\n1\\n1 + e−z\\nThe DNC Controller Network \\n| \\n313'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 329}, page_content='Also, all the lookup strengths need to have a value larger than or equal to 1, so we\\npass them through a oneplus function first:\\nβt\\nr, i = oneplus βt\\nr, i , βt\\nw = oneplus βt\\nw\\nwhere oneplus z = 1 + log 1 + ez\\nAnd finally, the read modes must have a valid softmax distribution:\\nπti = softmax πti\\nwhere softmax z =\\nez\\n∑jezj\\nBy these transformations, the interface vector is now ready to be passed to the mem‐\\nory; and while it guides the memory in its operations, we’ll be needing a second piece\\nof information from the neural network, the pre-output vector vt. This is a vector of\\nthe same size of the final output vector, but it’s not the final output vector. By using\\nanother learnable N\\n× Y weights matrix Wy, we can obtain the pre-output via:\\nvt = WyN χt\\nThis pre-output vector gives us the ability to condition our final output not just on\\nthe network output, but also on the recently read vectors rt from memory. Via a third\\nlearnable R × W × Y weights matrix Wr, we can get the final output as:\\nyt = vt + Wr rt\\n1; ⋯; rtR\\nGiven that the controller knows nothing about the memory except for the word\\nsize W, an already learned controller can be scaled to a larger memory with more\\nlocations without any need for retraining. Also, the fact that we didn’t specify any\\nparticular structure for the neural network or any particular loss function makes\\nDNC a universal architecture that can be applied to a variety of tasks and learning\\nproblems.\\nVisualizing the DNC in Action\\nOne way to see DNC’s operation in action is to train it on a simple task that would\\nallow us to look at the weightings and the parameters’ values and visualize them in\\nan interpretable way. For this simple task, we’ll use the copy problem we already saw\\nwith NTMs, but in a slightly modified form.\\n314 \\n| \\nChapter 12: Memory Augmented Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 330}, page_content='Instead of trying to copy a single sequence of binary vectors, our task here will be to\\ncopy a series of such sequences. In Figure 12-8, (a) shows the single sequence input.\\nAfter processing such single sequence input and copying the same sequence to the\\noutput, the DNC would have finished its program, and its memory would be reset\\nin a way that will not allow us to see how it can dynamically manage it. Instead we’ll\\ntreat a series of such sequences, shown in Figure 12-8 (b), as a single input.\\nFigure 12-8. Single sequence input versus a series of input sequences\\nFigure 12-9 shows a visualization of the DNC operation after being trained on a\\nseries of length 4 where each sequence contains five binary vectors and an end mark.\\nThe DNC used here has only 10 memory locations, so there’s no way it can store\\nall 20 vectors in the input. A feed-forward controller is used to ensure that nothing\\nwould be stored in a recurrent state, and only one read head is used to make the\\nvisualization more clear. These constraints should force the DNC to learn how to\\ndeallocate and reuse memory to successfully copy the whole input, and indeed it\\ndoes.\\nWe can see in that visualization how the DNC is writing each vector of the five in a\\nsequence into a single memory location. As soon as the end mark is seen, the read\\nhead starts reading from these locations in the exact same order of writing. We can\\nsee how both the allocation and free gates alternate in activation between writing and\\nreading phases of each sequence in the series. From the usage vector chart at the\\nbottom, we can also see how after a memory location is written to, its usage becomes\\nexactly 1, and how it drops to 0 just after reading from that location, indicating that it\\nwas freed and can be reused again.\\nVisualizing the DNC in Action \\n| \\n315'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 331}, page_content='Figure 12-9. Visualization of the DNC operation on the copy problem\\nThis visualization is part of the open source implementation of the DNC architecture\\nby Mostafa Samir. In the next section we’ll learn the important tips and tricks that\\nwill allow us to implement a simpler version of DNC on the reading comprehension\\ntasks.\\n316 \\n| \\nChapter 12: Memory Augmented Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 332}, page_content='Implementing the DNC in PyTorch\\nImplementing the DNC architecture is essentially a direct application of the math we\\njust discussed. So with the full implementation in the code repository associated with\\nthe book, we’ll just be focusing on the tricky parts and introduce some new PyTorch\\npractice while we’re at it.\\nThe main part of the implementation resides in the mem_ops.py file where all of the\\nattention and access mechanisms are implemented. This file is then imported to be\\nused with the controller. Two operations that might be a little tricky to implement are\\nthe link matrix update and the allocation weighting calculation. Both of these opera‐\\ntions can be naively implemented with for loops, but using for loops in creating a\\ncomputational graph is generally not a good idea. Let’s take the link matrix update\\noperation first and see how it looks with a loop-based implementation:\\ndef Lt(L, wwt, p, N):\\n    L_t = torch.zeros((N,N), dtype=torch.float32)\\n    for i in range(N):\\n        for j in range(N):\\n            if i == j:\\n                continue\\n            mask = torch.zeros((N,N), dtype=torch.float32)\\n            mask[i,j] = 1.0\\n        \\n            link_t = (1 - wwt[i] - wwt[j]) * L[i,j] + \\\\\\n                      wwt[i] * p[j]\\n            L_t += mask * link_t\\n    return L_t\\nAfter that computational graph is fully defined, it’s then fed with concrete values and\\nexecuted. With that in mind, we can see, as depicted in Figure 12-10, how in most\\nof the iterations of the for loop, a new set of nodes representing the loop body gets\\nadded in the computational graph. So for  N memory locations, we end up with\\nN 2 −N identical copies of the same nodes, each for each iteration, each taking up\\na chunk of our RAM and needing its own time to be processed before the next can\\nbe. When N is a small number, say 5, we get 20 identical copies, which is not so\\nbad. However, if we want to use a larger memory, like with N = 256, we get 65,280\\nidentical copies of the nodes, which is catastrophic for both the memory usage and\\nthe execution time.\\nImplementing the DNC in PyTorch \\n| \\n317'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 333}, page_content='Figure 12-10. The computational graph of the link matrix update operation built with\\nthe for loop implementation\\nOne possible way to overcome such an issue is vectorization. In vectorization, we\\ntake an array operation that is originally defined in terms of individual elements and\\nrewrite it as an operation on the whole array at once. For the link matrix update, we\\ncan rewrite the operation as:\\nLt =\\n1 −wtw ⊕wtw ∘Lt −1 + wtwpt −1 ∘1 −I\\nWhere I is the identity matrix, and the product wtwpt −1 is an outer product. To\\nachieve this vectorization, we define a new operator, the pairwise-addition of vectors,\\ndenoted by ⊕. This new operator is simply defined as:\\n u ⊕v =\\nu1 + v1 ⋯u1 + vn\\n⋮\\n⋱\\n⋮\\nun + v1 ⋯un + vn\\n318 \\n| \\nChapter 12: Memory Augmented Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 334}, page_content='This operator adds a little bit to the memory requirements of the implementation,\\nbut not as much as the case in the loop-based implementation. With this vectorized\\nreformulation of the update rule, we rewrite a more memory- and time-efficient\\nimplementation:\\ndef Lt(L, wwt, p, N):\\n    \"\"\"\\n    returns the updated link matrix given the previous one along\\n    with the updated write weightings and the previous precedence\\n    vector\\n    \"\"\"\\n    def pairwise_add(v):\\n        \"\"\"\\n        returns the matrix of pairs - adding the elements of v to\\n        themselves\\n        \"\"\"\\n        n = v.shape[0]\\n        # a NxN matrix of duplicates of u along the columns\\n        V = v.repeat(1,n)  \\n        return V + V\\n    # expand dimensions of wwt and p to make matmul behave as outer\\n    # product\\n    wwt = torch.unsqueeze(wwt, 1)\\n    p = torch.unsqueeze(p, 0)\\n    I = torch.eye(N, dtype=torch.float32)\\n    return (((1 - pairwise_add(wwt)) * L +\\n             torch.matmul(wwt, p)) * (1 - I))\\nA similar process could be made for the allocation weightings rule. Instead of having\\na single rule for each element in the weighting vector, we can decompose it into a few\\noperations that work on the whole vector at once:\\n1. While sorting the usage vector to get the free list, we also grab the sorted usage\\nvector itself.\\n2. We calculate the cumulative product vector of the sorted usage. Each element of\\nthat vector is the same as the product term in our original element-wise rule.\\n3. We multiply the cumulative product vector by (1 – the sorted usage vector).\\nThe resulting vector is the allocation weighting but in the sorted order, not the\\noriginal order of the memory location.\\n4. For each element of that out-of-order allocation weighting, we take its value and\\nput it in the corresponding index in the free list. The resulting vector is now the\\ncorrect allocation weighting that we want.\\nFigure 12-11 summarizes this process with a numerical example.\\nImplementing the DNC in PyTorch \\n| \\n319'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 335}, page_content='Figure 12-11. The vectorized process of calculating the allocation weightings\\nIt may seem that we still need loops for the sorting operation in step 1 and for reor‐\\ndering the weights in step 4, but fortunately PyTorch provides symbolic operations\\nthat would allow us to carry out these operations without the need for a Python loop.\\nFor sorting we’ll be using torch.topk. This operation takes a tensor and a number\\nk, and returns both the sorted top k values in descending order and the indices of\\nthese values. To get the sorted usage vector in ascending order, we need to get the top\\nN values of the negative of the usage vector. We can bring back the sorted values to\\ntheir original signs by multiplying the resulting vector by −1:\\nsorted_ut, free_list = torch.topk(-1*ut, N)\\nsorted_ut *= -1\\nFor reordering the allocation weights, we first create an empty tensor array of size\\nN to be the container of the weights in their correct order, and then put the values\\nin their correct places using the instance method scatter(indices, values). This\\nmethod takes in its second argument a tensor, and scatters the values along its first\\n320 \\n| \\nChapter 12: Memory Augmented Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 336}, page_content='dimension across the array, with the first argument being a list of indices of the loca‐\\ntions to which we want to scatter the corresponding values. In our case here, the first\\nargument is the free list, and the second is the out-of-order allocation weightings.\\nOnce we get the array with the weights in the correct places, we use another instance\\nmethod, pack(), to wrap up the whole array into a Tensor object:\\nempty_at = torch.empty(N)\\na_t = empty_at.scatter(0, free_list, out_of_location_at)\\nThe last part of the implementation that requires looping is the controller loop itself\\n—the loop that goes over each step of the input sequence to process it. Because\\nvectorization works only when operations are defined element-wise, the controller’s\\nloop can’t be vectorized. Fortunately, PyTorch still provides us with a method to\\nescape Python’s for loops and their massive performance hit; this method is the\\nsymbolic loop. A symbolic loop works like most of our symbolic operations:  instead\\nof unrolling the actual loop into the graph, it defines a node that would be executed as\\na loop when the graph is executed.\\nWe’ll leave the symbolic loop implementation in PyTorch up to the reader. More\\ninformation on how you can use symbolic loops in PyTorch can be found in the\\ntorch.fx documentation.\\nThe TensorFlow implementation of our symbolic loop can be found in the\\ntrain_babi.py file in the code repository.\\nTeaching a DNC to Read and Comprehend\\nEarlier in the chapter, when we were talking about neural n-grams, we said that it’s\\nnot of the complexity of an AI that can answer questions after reading a story. Now\\nwe have reached the point where we can build such a system because this is exactly\\nwhat DNCs do when applied on the bAbI dataset.\\nThe bAbI dataset is a synthetic dataset consisting of 20 sets of stories, questions on\\nthose stories, and their answers. Each set represents a specific and unique task of\\nreasoning and inference from text. In the version we’ll use, each task contains 10,000\\nquestions for training and 1,000 questions for testing. For example, the following\\nstory (from which the passage we saw earlier was adapted) is from the lists-and-sets\\ntask where the answers to the questions are lists/sets of objects mentioned in the\\nstory:\\n1 Mary took the milk there.\\n2 Mary went to the office.\\n3 What is Mary carrying?   milk 1\\n4 Mary took the apple there.\\n5 Sandra journeyed to the bedroom.\\n6 What is Mary carrying?   milk,apple 1 4\\nTeaching a DNC to Read and Comprehend \\n| \\n321'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 337}, page_content='This is taken directly from the dataset, and as you can see, a story is organized\\ninto numbered sentences that start from 1. Each question ends with a question\\nmark, and the words that directly follow the question mark are the answers. If an\\nanswer consists of more than one word, the words are separated by commas. The\\nnumbers that follow the answers are supervisory signals that point to the sentences\\nthat contain the answers’ words.\\nTo make the tasks more challenging, we’ll discard these supervisory signals and let\\nthe system learn to read the text and figure out the answer on its own. Following\\nthe DNC paper, we’ll preprocess our dataset by removing all the numbers and punc‐\\ntuation except for “?” and “.”, bringing all the words to lowercase, and replacing the\\nanswer words with dashes “-” in the input sequence. After this we get 159 unique\\nwords and marks (lexicons) across all the tasks, so we’ll encode each lexicon as a\\none-hot vector of size 159, no embeddings, just the plain words directly. Finally, we\\ncombine all of the 200,000 training questions to train the model jointly on them, and\\nwe keep each task’s test questions separate to test the trained model afterward on each\\ntask individually. This whole process is implemented in the preprocess.py file in the\\ncode repository.\\nTo train the model, we randomly sample a story from the encoded training data,\\npass it through the DNC with an LSTM controller, and get the corresponding output\\nsequence. We then measure the loss between the output sequence and the desired\\nsequence using the softmax cross-entropy loss, but only on the steps that contain\\nanswers. All the other steps are ignored by weighting the loss with a weights vector\\nthat has 1 at the answer’s steps and 0 elsewhere. This process is implemented in the\\ntrain_babi.py file.\\nAfter the model is trained, we test its performance on the remaining test questions.\\nOur metric will be the percentage of questions the model failed to answer in each\\ntask. An answer to a question is the word with the largest softmax value in the output,\\nor the most probable word. A question is considered to be answered correctly if all\\nof its answer’s words are the correct words. If the model failed to answer more than\\n5% of a task’s questions, we consider that the model failed on that task. The testing\\nprocedure is found in the test_babi.py file.\\nAfter training the model for about 500,000 iterations (caution—it takes a long time!),\\nwe can see that it’s performing pretty well on most of the tasks. At the same time, it’s\\nperforming badly on more difficult tasks like pathfinding, where the task is to answer\\nquestions about how to get from one place to another. The following report compares\\nour model’s results to the mean values reported in the original DNC paper:\\n322 \\n| \\nChapter 12: Memory Augmented Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 338}, page_content=\"Task                       Result            Paper's Mean\\n---------------------------------------------------------\\nsingle supporting fact     0.00%             9.0±12.6%\\ntwo supporting facts       11.88%            39.2±20.5%\\nthree supporting facts     27.80%            39.6±16.4%\\ntwo arg relations          1.40%             0.4±0.7%\\nthree arg relations        1.70%             1.5±1.0%\\nyes no questions           0.50%             6.9±7.5%\\ncounting                   4.90%             9.8±7.0%\\nlists sets                 2.10%             5.5±5.9%\\nsimple negation            0.80%             7.7±8.3%\\nindefinite knowledge       1.70%             9.6±11.4%\\nbasic coreference          0.10%             3.3±5.7%\\nconjunction                0.00%             5.0±6.3%\\ncompound coreference       0.40%             3.1±3.6%\\ntime reasoning             11.80%            11.0±7.5%\\nbasic deduction            45.44%            27.2±20.1%\\nbasic induction            56.43%            53.6±1.9%\\npositional reasoning       39.02%            32.4±8.0%\\nsize reasoning             8.68%             4.2±1.8%\\npath finding               98.21%            64.6±37.4%\\nagents motivations         2.71%             0.0±0.1%\\n---------------------------------------------------------\\nMean Err.                  15.78%            16.7±7.6%\\nFailed (err. > 5%)         8              11.2±5.4\\nSummary\\nIn this chapter, we’ve explored the cutting edge of deep learning research with NTMs\\nand DNCs, culminating with the implementation of a model that can solve an\\ninvolved reading comprehension task.\\nIn the final chapter of this book, we’ll begin to explore a very different space of\\nproblems known as reinforcement learning. We’ll build an intuition for this new class\\nof tasks and develop an algorithmic foundation to tackle these problems using the\\ndeep learning tools we’ve developed thus far.\\nSummary \\n| \\n323\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 339}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 340}, page_content='1 Mnih, Volodymyr, et al. “Human-Level Control Through Deep Reinforcement Learning.” Nature 518.7540\\n(2015): 529-533.\\nCHAPTER 13\\nDeep Reinforcement Learning\\nNicholas Locascio\\nIn this chapter, we’ll discuss reinforcement learning, which is a branch of machine\\nlearning that deals with learning via interaction and feedback. Reinforcement learn‐\\ning is essential to building agents that can not only perceive and interpret the world,\\nbut also take action and interact with it. We will discuss how to incorporate deep\\nneural networks into the framework of reinforcement learning and discuss recent\\nadvances and improvements in this field.\\nDeep Reinforcement Learning Masters Atari Games\\nThe application of deep neural networks to reinforcement learning had a major\\nbreakthrough in 2014, when the London startup DeepMind astonished the machine\\nlearning community by unveiling a deep neural network that could learn to play\\nAtari games with superhuman skill. This network, termed a deep Q-network (DQN)\\nwas the first large-scale successful application of reinforcement learning with deep\\nneural networks. DQN was so remarkable because the same architecture, without\\nany changes, was capable of learning 49 different Atari games, despite each game\\nhaving different rules, goals, and game-play structure. To accomplish this feat, Deep‐\\nMind brought together many traditional ideas in reinforcement learning while also\\ndeveloping a few novel techniques that proved key to DQN’s success. Later in this\\nchapter, we will implement DQN, as described in the Nature paper, “Human-Level\\nControl Through Deep Reinforcement Learning.”1 But first, let’s take a dive into\\nreinforcement learning (Figure 13-1).\\n325'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 341}, page_content='2 This image is from the OpenAI Gym DQN agent that we build in this chapter: Brockman, Greg, et al.\\n“OpenAI Gym.” arXiv preprint arXiv:1606.01540 (2016). https://gym.openai.com\\nFigure 13-1. A deep reinforcement learning agent playing Breakout2\\nWhat Is Reinforcement Learning?\\nReinforcement learning, at its essentials, is learning by interacting with an environ‐\\nment. This learning process involves an agent, an environment, and a reward signal.\\nThe agent chooses to take an action in the environment, for which the agent is\\nrewarded accordingly. The way in which an actor chooses actions is called a policy.\\nThe agent wants to increase the reward it receives, and so must learn an optimal\\npolicy for interacting with the environment (Figure 13-2).\\nReinforcement learning is different from the other types of learning that we have\\ncovered thus far. In traditional supervised learning, we are given data and labels,\\nand are tasked with predicting labels given data. In unsupervised learning, we are\\ngiven just data and are tasked with discovering underlying structure in this data. In\\n326 \\n| \\nChapter 13: Deep Reinforcement Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 342}, page_content='reinforcement learning, we are given neither data nor labels. Our learning signal is\\nderived from the rewards given to the agent by the environment.\\nFigure 13-2. Reinforcement learning setup\\nReinforcement learning is exciting to many in the AI community because it is a\\ngeneral-purpose framework for creating intelligent agents. Given an environment and\\nsome rewards, the agent learns to interact with that environment to maximize its total\\nreward. This type of learning is more in line with how humans develop. Yes, we can\\nbuild a pretty good model to classify dogs from cats with extremely high accuracy\\nby training on thousands of images. But you won’t find this approach used in any\\nelementary schools. Humans interact with their environment to learn representations\\nof the world that they can use to make decisions.\\nFurthermore, reinforcement learning applications are at the forefront of many\\ncutting-edge technologies, including self-driving cars, robotic motor control, game\\nplaying, air-conditioning control, ad placement optimization, and stock market trad‐\\ning strategies.\\nAs an illustrative exercise, we’ll be tackling a simple reinforcement learning and\\ncontrol problem called pole balancing. In this problem, there is a cart with a pole that\\nis connected by a hinge, so the pole can swing around the cart. There is an agent that\\ncan control the cart, moving it left or right. There is an environment, which rewards\\nthe agent when the pole is pointed upward, and penalizes the agent when the pole\\nfalls over (Figure 13-3).\\nWhat Is Reinforcement Learning? \\n| \\n327'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 343}, page_content='3 This image is from our OpenAI Gym Policy Gradient agent that we build in this chapter.\\nFigure 13-3. A simple reinforcement learning agent: balancing a pole3\\nMarkov Decision Processes\\nOur pole-balancing example has a few important elements, which we formalize as a\\nMarkov decision process (MDP). These elements are:\\nState\\nThe cart has a range of possible places on the x-plane where it can be. Similarly,\\nthe pole has a range of possible angles.\\nAction\\nThe agent can take action by moving the cart either left or right.\\nState transition\\nWhen the agent acts, the environment changes: the cart moves and the pole\\nchanges angle and velocity.\\nReward\\nIf an agent balances the pole well, it receives a positive reward. If the pole falls,\\nthe agent receives a negative reward.\\nAn MDP is defined as the following:\\n• S, a finite set of possible states\\n• A, a finite set of actions\\n• P r, s′ s, a , a state transition function\\n• R, reward function\\nMDPs offer a mathematical framework for modeling decision making in a given\\nenvironment. Figure 13-4 shows an example, with circles representing the states of\\n328 \\n| \\nChapter 13: Deep Reinforcement Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 344}, page_content='the environment, diamonds representing actions that can be taken, and the edges\\nfrom diamonds to circles representing the transition from one state to the next. The\\nnumbers along these edges represent the probability of taking a certain action, and\\nthe numbers at the end of the arrows represent the reward given to the agent for\\nmaking the given transition.\\nFigure 13-4. Example of an MDP\\nAs an agent takes action in an MDP framework, it forms an episode. An episode\\nconsists of series of tuples of states, actions, and rewards. Episodes run until the\\nenvironment reaches a terminal state, like the “Game Over” screen in Atari games,\\nor when the pole hits the ground in our pole-cart example. The following equation\\nshows the variables in an episode:\\ns0, a0, r0 , s1, a1, r1 , ... sn, an, rn\\nIn pole-cart, our environment state can be a tuple of the position of the cart and the\\nangle of the pole, like so: (xcart, θpole).\\nPolicy\\nMDP’s aim is to find an optimal policy for our agent. Policies are how our agent acts\\nbased on its current state. Formally, policies can be represented as a function π that\\nchooses the action a that the agent will take in state s.\\nMarkov Decision Processes \\n| \\n329'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 345}, page_content='The objective of our MDP is to find a policy to maximize the expected future return:\\nmaxπ E R0 + R1 + ... Rt π\\nIn this objective, R represents the future return of each episode. Let’s define exactly\\nwhat future return means.\\nFuture Return\\nFuture return is how we consider the rewards of the future. Choosing the best\\naction requires consideration of not only the immediate effects of that action, but\\nalso the long-term consequences. Sometimes the best action actually has a negative\\nimmediate effect, but a better long-term result. For example, a mountain-climbing\\nagent that is rewarded by its altitude may actually have to climb downhill to reach a\\nbetter path to the mountain’s peak.\\nTherefore, we want our agents to optimize for future return. To do that, the agent\\nmust consider the future consequences of its actions. For example, in a game of Pong,\\nthe agent receives a reward when the ball passes into the opponent’s goal. However,\\nthe actions responsible for this reward (the inputs that position the racquet to strike\\na scoring hit) happen many time steps before the reward is received. The reward for\\neach of those actions is delayed.\\nWe can incorporate delayed rewards into our overall reward signal by constructing a\\nreturn for each time step that takes into account future rewards as well as immediate\\nrewards. A naive approach for calculating future return for a time step may be a\\nsimple sum like so:\\n  Rt = ∑k = 0\\nT\\nrt + k\\nWe can calculate all returns, R, where R = R0, R1, ...Ri, ...Rn , with the following\\ncode:\\ndef calculate_naive_returns(rewards):\\n\"\"\" Calculates a list of naive returns given a \\n    list of rewards.\"\"\"\\n    total_returns = np.zeros(len(rewards))\\n    total_return = 0.0\\n    for t in range(len(rewards), 0):\\n        total_return = total_return + reward\\n        total_returns[t] = total_return\\n    return total_returns\\nThis naive approach successfully incorporates future rewards so the agent can learn\\nan optimal global policy. This approach values future rewards equally to immedi‐\\nate rewards. However, this equal consideration of all rewards is problematic. With\\n330 \\n| \\nChapter 13: Deep Reinforcement Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 346}, page_content='infinite time steps, this expression can diverge to infinity, so we must find a way\\nto bind it. Furthermore, with equal consideration at each time step, the agent can\\noptimize for a future reward, and we would learn a policy that lacks any sense of\\nurgency or time sensitivity in pursuing its rewards.\\nInstead, we should value future rewards slightly less in order to force our agents to\\nlearn to get rewards quickly. We accomplish this with a strategy called discounted\\nfuture return.\\nDiscounted Future Return\\nTo implement discounted future return, we scale the reward of a current state by the\\ndiscount factor, γ, to the power of the current time step. In this way, we penalize\\nagents that take many actions before receiving positive reward. Discounted rewards\\nbias our agent to prefer receiving the reward in the immediate future, which is\\nadvantageous to learning a good policy. We can express the reward as follows:\\nRt = ∑k = 0\\nT\\nγtrt + k + 1\\nThe discount factor, γ, represents the level of discounting we want to achieve, and\\ncan be between 0 and 1. High γ means little discounting, low γ provides much\\ndiscounting. A typical γ hyperparameter setting is between 0.99 and 0.97.\\nWe can implement discounted return like so:\\ndef discount_rewards(rewards, gamma=0.98):\\n    discounted_returns = [0 for _ in rewards]\\n    discounted_returns[-1] = rewards[-1]\\n    for t in range(len(rewards)-2, -1, -1): # iterate backwards\\n        discounted_returns[t] = rewards[t] + \\n          discounted_returns[t+1]*gamma\\n    return discounted_returns\\nExplore Versus Exploit\\nReinforcement learning is fundamentally a trial-and-error process. In such a frame‐\\nwork, an agent afraid to make mistakes can prove to be highly problematic. Consider\\nthe following scenario. A mouse is placed in the maze shown in Figure 13-5. Our\\nagent must control the mouse to maximize reward. If the mouse gets the water, it\\nreceives a reward of +1; if the mouse reaches a poison container (red), it receives a\\nreward of -10; if the mouse gets the cheese, it receives a reward of +100. Upon receiv‐\\ning reward, the episode is over. The optimal policy involves the mouse successfully\\nnavigating to the cheese and eating it.\\nExplore Versus Exploit \\n| \\n331'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 347}, page_content='Figure 13-5. A predicament that many mice find themselves in\\nIn the first episode, the mouse takes the left route, steps on a trap, and receives a -10\\nreward. In the second episode, the mouse avoids the left path, since it resulted in such\\na negative reward, and drinks the water immediately to its right for a +1 reward. After\\ntwo episodes, it would seem that the mouse has found a good policy. It continues\\nto follow its learned policy on subsequent episodes and achieves the moderate +1\\nreward reliably. Since our agent utilizes a greedy strategy—always choosing the mod‐\\nel’s best action—it is stuck in a policy that is a local maximum.\\nTo prevent such a situation, it may be useful for the agent to deviate from the model’s\\nrecommendation and take a suboptimal action in order to explore more of the envi‐\\nronment. So instead of taking the immediate right turn to exploit the environment to\\nget water and the reliable +1 reward, our agent may choose to take a left turn and\\nventure into more treacherous areas in search of a more optimal policy. Too much\\nexploration, and our agent fails to optimize any reward. Not enough exploration can\\nresult in our agent getting stuck in a local minimum. This balance of explore versus\\nexploit is crucial to learning a successful policy.\\n332 \\n| \\nChapter 13: Deep Reinforcement Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 348}, page_content='ϵ-Greedy\\nOne strategy for balancing the explore-exploit dilemma is called ϵ-greedy. ϵ-greedy is\\na simple strategy that involves making a choice at each step to either take the agent’s\\ntop recommended action or take a random action. The probability that the agent\\ntakes a random action is the value known as ϵ.\\nWe can implement ϵ-greedy like so:\\ndef epsilon_greedy_action(action_distribution,\\n                          epsilon=1e-1):\\n    action_distribution = action_distribution.detach().numpy()\\n    if random.random() < epsilon:\\n        return np.argmax(np.random.random(\\n           action_distribution.shape))\\n    else:\\n        return np.argmax(action_distribution)\\nAnnealed ϵ-Greedy\\nWhen training a reinforcement learning model, oftentimes we want to do more\\nexploring in the beginning since our model knows little of the world. Later, once\\nour model has seen much of the environment and learned a good policy, we want\\nour agent to trust itself more to further optimize its policy. To accomplish this, we\\ncast aside the idea of a fixed ϵ, and instead anneal it over time, having it start low\\nand increase by a factor after each training episode. Typical settings for annealed\\nϵ-greedy scenarios include annealing from 0.99 to 0.1 over 10,000 scenarios. We can\\nimplement annealing like so:\\ndef epsilon_greedy_action_annealed(action_distribution,\\n                                   percentage,\\n                                   epsilon_start=1.0,\\n                                   epsilon_end=1e-2):\\n    action_distribution = action_distribution.detach().numpy()\\n    annealed_epsilon = (epsilon_start*(1.0-percentage) +\\n                        epsilon_end*percentage)\\n    if random.random() < annealed_epsilon:\\n        return np.argmax(np.random.random(\\n          action_distribution.shape))\\n    else:\\n        return np.argmax(action_distribution)\\nExplore Versus Exploit \\n| \\n333'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 349}, page_content='4 Sutton, Richard S., et al. “Policy Gradient Methods for Reinforcement Learning with Function Approxima‐\\ntion.” NIPS. Vol. 99. 1999.\\nPolicy Versus Value Learning\\nSo far we’ve defined the setup of reinforcement learning, discussed discounted future\\nreturn, and looked at the trade-offs of explore versus exploit. What we haven’t\\ntalked about is how we’re actually going to teach an agent to maximize its reward.\\nApproaches to this fall into two broad categories: policy learning and value learning.\\nIn policy learning, we are directly learning a policy that maximizes reward. In value\\nlearning, we are learning the value of every state + action pair. If you were trying to\\nlearn to ride a bike, a policy learning approach would be to think about how pushing\\non the right pedal while you were falling to the left would course-correct you. If you\\nwere trying to learn to ride a bike with a value learning approach, you would assign a\\nscore to different bike orientations and actions you can take in those positions. We’ll\\nbe covering both in this chapter, so let’s start with policy learning.\\nIn typical supervised learning, we can use stochastic gradient descent to update our\\nparameters to minimize the loss computed from our network’s output and the true\\nlabel. We are optimizing the expression:\\narg minθ ∑i log p yi ∣xi; θ\\nIn reinforcement learning, we don’t have a true label, only reward signals. However,\\nwe can still use SGD to optimize our weights using something called policy gradients.4\\nWe can use the actions the agent takes, and the returns associated with those actions,\\nto encourage our model weights to take good actions that lead to high reward, and to\\navoid bad ones that lead to low reward. The expression we optimize for is:\\narg minθ −∑iRi log p yi ∣xi; θ\\nwhere yi is the action taken by the agent at time step t and where Ri is our discounted\\nfuture return. A In this way, we scale our loss by the value of our return, so if the\\nmodel chose an action that led to negative return, this would lead to greater loss.\\nFurthermore, if the model is confident in that bad decision, it would get penalized\\neven more, since we are taking into account the log probability of the model choosing\\nthat action. With our loss function defined, we can apply SGD to minimize our loss\\nand learn a good policy.\\n334 \\n| \\nChapter 13: Deep Reinforcement Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 350}, page_content=\"Pole-Cart with Policy Gradients\\nWe’re going to implement a policy-gradient agent to solve pole-cart, a classic rein‐\\nforcement learning problem. We will be using an environment from the OpenAI\\nGym created just for this task.\\nOpenAI Gym\\nThe OpenAI Gym is a Python toolkit for developing reinforcement agents. OpenAI\\nGym provides an easy-to-use interface for interacting with a variety of environments.\\nIt contains over one hundred open source implementations of common reinforce‐\\nment learning environments. OpenAI Gym speeds up development of reinforcement\\nlearning agents by handling everything on the environment simulation side, allowing\\nresearchers to focus on their agent and learning algorithms. Another benefit of\\nOpenAI Gym is that researchers can fairly compare and evaluate their results with\\nothers because they can all use the same standardized environment for a task. We’ll be\\nusing the pole-cart environment from OpenAI Gym to create an agent that can easily\\ninteract with this environment.\\nCreating an Agent\\nTo create an agent that can interact with an OpenAI environment, we’ll define a class\\nPGAgent, which will contain our model architecture, model weights, and hyperpara‐\\nmeters:\\nfrom torch import optim\\nclass PGAgent(object):\\n    def __init__(self, state_size, num_actions,\\n                 hidden_size,\\n                 learning_rate=1e-3,\\n                 explore_exploit_setting= \\\\\\n                 'epsilon_greedy_annealed_1.0->0.001'):\\n        self.state_size = state_size\\n        self.num_actions = num_actions\\n        self.hidden_size = hidden_size\\n        self.learning_rate = learning_rate\\n        self.explore_exploit_setting = \\\\\\n                        explore_exploit_setting\\n        self.build_model()\\n    def build_model(self):\\n      self.model = torch.nn.Sequential(\\n        nn.Linear(self.state_size, self.hidden_size),\\n        nn.Linear(self.hidden_size, self.hidden_size),\\n        nn.Linear(self.hidden_size, self.num_actions),\\n        nn.Softmax(dim=0))\\nPole-Cart with Policy Gradients \\n| \\n335\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 351}, page_content=\"def train(self, state, action_input, reward_input):\\n        state = torch.tensor(state).float()\\n        action_input = torch.tensor(action_input).long()\\n        reward_input = torch.tensor(reward_input).float()\\n        self.output = self.model(state)\\n        # Select the logits related to the action taken\\n        logits_for_actions = self.output.gather(1,\\n                                           action_input.view(-1,1))\\n        self.loss = -torch.mean(\\n            torch.log(logits_for_actions) * reward_input)\\n        self.loss.backward()\\n        self.optimizer = optim.Adam(self.model.parameters())\\n        self.optimizer.step()\\n        self.optimizer.zero_grad()\\n        return self.loss.item()\\n        \\n    def sample_action_from_distribution(self,\\n                                        action_distribution,\\n                                        epsilon_percentage):\\n        # Choose an action based on the action probability\\n        # distribution and an explore vs exploit\\n        if self.explore_exploit_setting == 'greedy':\\n              action = epsilon_greedy_action(action_distribution,\\n                                             0.00)\\n        elif self.explore_exploit_setting == 'epsilon_greedy_0.05':\\n              action = epsilon_greedy_action(action_distribution,\\n                                             0.05)\\n        elif self.explore_exploit_setting == 'epsilon_greedy_0.25':\\n              action = epsilon_greedy_action(action_distribution,\\n                                             0.25)\\n        elif self.explore_exploit_setting == 'epsilon_greedy_0.50':\\n              action = epsilon_greedy_action(action_distribution,\\n                                             0.50)\\n        elif self.explore_exploit_setting == 'epsilon_greedy_0.90':\\n              action = epsilon_greedy_action(action_distribution,\\n                                             0.90)\\n        elif self.explore_exploit_setting == \\\\\\n          'epsilon_greedy_annealed_1.0->0.001':\\n              action = epsilon_greedy_action_annealed(\\n                  action_distribution,\\n                  epsilon_percentage, 1.0,0.001)\\n        elif self.explore_exploit_setting == \\\\\\n          'epsilon_greedy_annealed_0.5->0.001':\\n              action = epsilon_greedy_action_annealed(\\n                  action_distribution,\\n                  epsilon_percentage, 0.5, 0.001)\\n        elif self.explore_exploit_setting == \\\\\\n          'epsilon_greedy_annealed_0.25->0.001':\\n              action = epsilon_greedy_action_annealed(\\n336 \\n| \\nChapter 13: Deep Reinforcement Learning\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 352}, page_content='action_distribution,\\n                  epsilon_percentage, 0.25, 0.001)\\n        return action\\n    def predict_action(self, state, epsilon_percentage):\\n        action_distribution = self.model(\\n                                 torch.from_numpy(state).float())\\n        action = self.sample_action_from_distribution(\\n            action_distribution, epsilon_percentage)\\n        return action\\nBuilding the Model and Optimizer\\nLet’s break down some important functions. In build_model(), we define our model\\narchitecture as a three-layer neural network. The model returns a layer of three\\nnodes, each representing the model’s action probability distribution. In build_train\\ning(), we implement our policy gradient optimizer. We express our objective loss\\nas we talked about, scaling the model’s prediction probability for an action with the\\nreturn received for taking that action, and summing these all up to form a minibatch.\\nWith our objective defined, we can use torch.optim.AdamOptimizer, which will\\nadjust our weights according to the gradient to minimize our loss.\\nSampling Actions\\nWe define the predict_action function, which samples an action based on the mod‐\\nel’s action probability distribution output. We support the various sampling strategies\\nthat we talked about to balance explore versus exploit, including greedy, ϵ greedy, and\\nϵ greedy annealing.\\nKeeping Track of History\\nWe’ll be aggregating our gradients from multiple episode runs, so it will be useful to\\nkeep track of state, action, and reward tuples. To this end, we implement an episode\\nhistory and memory:\\nclass EpisodeHistory(object):\\n    def __init__(self):\\n        self.states = []\\n        self.actions = []\\n        self.rewards = []\\n        self.state_primes = []\\n        self.discounted_returns = []\\n    def add_to_history(self, state, action, reward,\\n      state_prime):\\n        self.states.append(state)\\n        self.actions.append(action)\\nPole-Cart with Policy Gradients \\n| \\n337'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 353}, page_content=\"self.rewards.append(reward)\\n        self.state_primes.append(state_prime)\\nclass Memory(object):\\n    def __init__(self):\\n        self.states = []\\n        self.actions = []\\n        self.rewards = []\\n        self.state_primes = []\\n        self.discounted_returns = []\\n    def reset_memory(self):\\n        self.states = []\\n        self.actions = []\\n        self.rewards = []\\n        self.state_primes = []\\n        self.discounted_returns = []\\n    def add_episode(self, episode):\\n        self.states += episode.states\\n        self.actions += episode.actions\\n        self.rewards += episode.rewards\\n        self.discounted_returns += episode.discounted_returns\\nPolicy Gradient Main Function\\nLet’s put this all together in our main function, which will create an OpenAI Gym\\nenvironment for CartPole, make an instance of our agent, and have our agent interact\\nwith and train on the CartPole environment:\\n# Configure Settings\\n#total_episodes = 5000\\ntotal_episodes = 16\\ntotal_steps_max = 10000\\nepsilon_stop = 3000\\ntrain_frequency = 8\\nmax_episode_length = 500\\nrender_start = -1\\nshould_render = False\\nexplore_exploit_setting = 'epsilon_greedy_annealed_1.0->0.001'\\nenv = gym.make('CartPole-v0')\\nstate_size = env.observation_space.shape[0]  # 4 for\\n                                              # CartPole-v0\\nnum_actions = env.action_space.n  # 2 for CartPole-v0\\nsolved = False\\nagent = PGAgent(state_size=state_size,\\n                num_actions=num_actions,\\n338 \\n| \\nChapter 13: Deep Reinforcement Learning\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 354}, page_content=\"hidden_size=16,\\n                explore_exploit_setting= \\\\\\n                  explore_exploit_setting)\\nepisode_rewards = []\\nbatch_losses = []\\nglobal_memory = Memory()\\nsteps = 0\\nfor i in range(total_episodes):\\n  state = env.reset()\\n  episode_reward = 0.0\\n  episode_history = EpisodeHistory()\\n  epsilon_percentage = float(min(i/float(epsilon_stop), 1.0))\\n  for j in range(max_episode_length):\\n      action = agent.predict_action(state, epsilon_percentage)\\n      state_prime, reward, terminal, _ = env.step(action)\\n      \\n      episode_history.add_to_history(\\n          state, action, reward, state_prime)\\n      state = state_prime\\n      episode_reward += reward\\n      steps += 1\\n      \\n      if j == (max_episode_length - 1):\\n            terminal = True\\n            \\n      if terminal:\\n          episode_history.discounted_returns = \\\\\\n            discount_rewards(episode_history.rewards)\\n          global_memory.add_episode(episode_history)\\n          # every 8th episode train the NN\\n          # train on all actions from episodes in memory, \\n          # then reset memory\\n          if np.mod(i, train_frequency) == 0:\\n            reward_input = global_memory.discounted_returns\\n            action_input = global_memory.actions\\n            state = global_memory.states\\n            # train step\\n            batch_loss = agent.train(state, action_input, \\n                                     reward_input)\\n              # print(f'Batch loss: {batch_loss}')\\n              # batch_losses.append(batch_loss)\\n            global_memory.reset_memory()\\n          episode_rewards.append(episode_reward)\\n          if i % 10 == 0:\\nPole-Cart with Policy Gradients \\n| \\n339\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 355}, page_content=\"mean_rewards = torch.mean(torch.tensor(\\n                                         episode_rewards[:-10]))\\n              if mean_rewards > 10.0:\\n                  solved = True\\n              else:\\n                  solved = False\\n              print(f'Solved: {solved} Mean Reward: {mean_rewards}')\\n          break # stop playing if terminal\\n          \\n  print(f'Episode[{i}]: {len(episode_history.actions)} \\\\\\n          actions {episode_reward} reward')\\nThis code will train a CartPole agent to successfully and consistently balance the pole.\\nPGAgent Performance on Pole-Cart\\nFigure 13-6 is a chart of the average reward of our agent at each step of training.\\nWe try out 8 different sampling methods, and achieve best results with ϵ greedy\\nannealing from 1.0 to 0.001.\\nFigure 13-6. Explore-exploit configurations affect how fast and how well learning occurs\\n340 \\n| \\nChapter 13: Deep Reinforcement Learning\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 356}, page_content='Notice how, across the board, standard ϵ greedy does very poorly. Let’s talk about why\\nthis might be. With a high ϵ set to 0.9, we are taking a random action 90% of the\\ntime. Even if the model learns to execute the perfect actions, we’ll still be using these\\nonly 10% of the time. On the other end, with a low ϵ of 0.05, we are taking what our\\nmodel believes to be optimal actions the vast majority of the time. This performance\\nis a bit better, but gets stuck in a local reward minimum because it lacks the ability to\\nexplore other strategies. So neither ϵ greedy of 0.05 nor 0.9 gives us great results. The\\nformer places too much emphasis on exploration, and the latter, too little. This is why\\nϵ annealing is such a powerful sampling strategy. It allows the model to explore early\\nand exploit late, which is crucial to learning good policies.\\nTrust-Region Policy Optimization\\nTrust-region policy optimization, or TRPO for short, is a framework that ensures\\npolicy improvement while preventing the policy from shifting too much during each\\ntraining step. TRPO has been empirically shown to outperform many of its fellow\\npolicy gradient and policy iteration methods, allowing researchers to effectively\\nlearn complex, nonlinear policies (often parametrized by large neural networks) that\\nweren’t previously possible through gradient-based methods. In this section, we will\\nmotivate TRPO and describe its objective in more detail.\\nThe idea of preventing the policy from shifting too much during each training step\\nis not a new one—most regularized optimization procedures do this indirectly by\\npenalizing the norm of the parameters, for example, globally ensuring the norm of\\nthe parameters doesn’t get too high. Of course, in cases where regularized optimiza‐\\ntion can also be formulated as constrained optimization (where there are explicit\\nbounds on the norm of the parameter vector), such as L2-regularized linear regres‐\\nsion, we have a direct equivalence to the idea of preventing the policy from shifting\\ntoo much during each training step. The per-step change in the norm of the param‐\\neters is bounded by the range of the constraint, since all possible parameter values\\nmust fall in this range. For those interested, I would recommend looking further into\\nthe equivalence between Tikhonov and Ivanov regularization in linear regression.\\nPreventing the policy from shifting too much during each training step has the\\nstandard effect of regularized optimization: it promotes stability in training, which is\\nideal in preventing overfitting to new data. How do we define a shift in the policy?\\nPolicies are simply discrete probability distributions over the action space given a\\nstate, πθ a s , for which we can use notions of dissimilarity, introduced in Chapter 2.\\nThe original TRPO paper introduced a bound on the average KL divergence (over all\\npossible states) between the current policy and the new policy.\\nNow that we’ve introduced the constraint portion of TRPO’s constrained optimiza‐\\ntion, we will motivate and define the objective function.\\nTrust-Region Policy Optimization \\n| \\n341'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 357}, page_content='Let’s recap and introduce some terminology:\\nη π = Es0, a0, s1, a1, ... ∑t = 0\\n∞\\nγtr st\\nQπ st, at = Est + 1, at + 1, ... ∑l = 0\\n∞\\nγlr st + l\\nVπ st  = Eat, st + 1, at + 1... ∑l = 0\\n∞\\nγlr st + l\\nAπ s, a = Qπ s, a −Vπ s\\nρπ s = ∑i = 0\\n∞\\nγiP st = s\\nThe first term is η π , which represents the expected discounted reward. We saw\\nthe finite-time horizon version of the term inside the expectation earlier when we\\ndiscussed future discounted reward. Instead of looking at a single trajectory here, we\\ntake the expectation over all possible trajectories as defined by our policy π. As usual,\\nwe can estimate this expectation via an empirical average by sampling trajectories\\nusing π. The second term, which will be discussed in more detail in “Q-Learning and\\nDeep Q-Networks” on page 347, is the Q-function Qπ st, at , which looks very similar\\nto the previous term but is instead defined as the expected discounted return from\\ntime t, given we are in some state st and perform a defined action at in that state.\\nWe again calculate the expectation using our policy π. Note that the time t doesn’t\\nactually matter all too much since we only consider an infinite time horizon and the\\nexpected discounted return from t rather than from the beginning of the trajectory.\\nThe third term is Vπ st , or the value function at a particular state at time t. The\\nvalue function can actually be more concisely written as Vπ st = Eat Qπ st, at , or\\nthe expectation of the Q-function with respect to π at st . In essence, the Q-function\\nsupposes that we take a defined action at in state st, while the value function leaves at\\nas a variable. Thus, to get the value function, all we need to do is take the expectation\\nof the Q-function with respect to the distribution over at knowing the current state\\nst. The result is the weighted average of the Q-function, where the weights are\\nπ at st . In essence, this term captures the average future discounted return we’d\\nexpect to see starting in some state st.\\nThe fourth term is Aπ s, a , or the advantage function. Note that we have dropped\\nthe time t by now for the reasons mentioned earlier. The intuition for the advantage\\nfunction is that it quantifies, under a fixed policy π, the benefit of letting trajectories\\nplay out after taking a particular action a in the current state s, over simply letting\\ntrajectories play out from the current state s completely unconstrained. Even more\\nconcisely, it defines how much better, or worse, in the long run it is to initially take\\naction a in state s compared to the average.\\n342 \\n| \\nChapter 13: Deep Reinforcement Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 358}, page_content='The final term, or the unnormalized discounted visitation frequency, reintroduces the\\ntime term t. This term is a function of the probability of being in state s at each\\ntime t from the start to infinity. This term will be important in our definition of the\\nobjective function. The original TRPO paper chose to optimize the model parameters\\nby maximizing this objective function:\\nLθold θ = ∑sρθold s ∑aπθ a s Aθold s, a\\nθnew = argmaxθLθold θ\\nAlthough we won’t fully show the derivation behind this objective as it is quite math‐\\nematically involved and beyond the scope of this text, we provide some intuition. Let’s\\nfirst examine this term: ∑aπθ a s Aθold s, a , assuming a fixed state s. For the sake\\nof argument, let’s replace θ with θold as our proposed policy’s parameters, which also\\nrepresents our current policy’s parameters:\\n∑aπθold a s Aπθold s, a = Ea ∼πθold a s Aπθold s, a\\n= Ea ∼πθold a s Qπθold s, a\\n−Ea ∼πθold a s Vπθold s\\n= Ea ∼πθold a s Qπθold s, a\\n−Vπθold s\\n= Vπθold s −Vπθold s\\n= 0\\nWhat have we shown here? Earlier, we talked about how Aπθold s, a  defines how\\nmuch better or worse it is, under the current policy, to take action a in state s\\ncompared to what we expect to see starting from state s unconstrained. Here, we\\nshowed that if we average each of these advantages weighted by the current policy’s\\ndistribution, we are left with zero average advantage over the current policy—this\\nmakes a lot of intuitive sense, since the proposed policy and the current policy are\\nthe exact same. We don’t expect to see any performance gain by replacing the current\\npolicy with itself.\\nNow, if we replace θ with a different proposed policy’s parameters θalt, the above\\nderivation leads us to:\\nEa ∼πθalt a s Qπθold s, a\\n−Vπθold s\\nTrust-Region Policy Optimization \\n| \\n343'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 359}, page_content='This is as far as simplification will take us, since the actions in the first term are no\\nlonger distributed as the current policy and we can’t make the simplification that led\\nus to the penultimate step. If we evaluate this expression and we receive a positive\\nresult, we can interpret the result as representing a positive average advantage from\\nfollowing the proposed policy compared to following the current policy, directly\\ntranslating to a performance gain for this specific state s by replacing the current\\npolicy with the proposed policy.\\nNote that we have only been considering a specific state s. But even\\nif we see a performance gain for some state, it might be the case\\nthat that state only rarely shows up. This leads us to the inclusion\\nof the term ∑sρθold s , which quantifies how often we see a given\\nstate. We can actually rewrite this as an expectation even though\\nthis is an unnormalized distribution—all we’d need to do is factor\\nout the normalizing constant, which is also a constant from the\\nperspective of θ since the normalizing constant is solely a function\\nof θold.\\nKeep in mind that ∑sρθold s  is evaluated using the current policy rather than the\\nproposed policy; this is because, as noted in the paper, the complex dependency this\\nintroduces on θ when optimizing the alternative objective (which uses ∑sρθ s ) with\\nrespect to θ makes the optimization process difficult. Additionally, the paper proves\\nthat the first-order gradient matches that of the alternative objective anyway, allowing\\nus to make this substitution without introducing a biased gradient estimate. We won’t\\nshow this here, however, as it is beyond the scope of the text.\\nPutting everything together, we have the following constrained optimization\\nobjective:\\nθ* = argmaxθ∑sρθold s ∑aπθ a s Aθold s, a\\ns . t . Avg. KL θold, θ ≤δ\\nwhere the average KL divergence denotes the expected KL divergence between\\npolicies over all states. This is what we call the trust region, and it represents the\\nparameter settings that lie close enough to the current parameter setting, mitigate\\ntraining instability, and mitigate overfitting. How do we go about optimizing this\\nobjective? The inner summation looks like an expectation with respect to πθ a, s , but\\nall we have is our current setting of parameter values θold. In the standard setting,\\nor on-policy setting, we are sampling from the same policy we are optimizing, so\\nwe can use classic policy gradient optimization for this. However, TRPO can be\\nmodified to work in the off-policy setting as well, where the policy we are sampling\\nfrom is different from the policy we are optimizing. Generally, the reason for this\\n344 \\n| \\nChapter 13: Deep Reinforcement Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 360}, page_content='distinction is that we may have a behavior policy, the policy we are sampling from\\nthat may be more exploratory in nature, while we learn the target policy, which is to\\nbe optimized. In the off-policy setting, since we are sampling actions from a different\\ndistribution q(a|s) (the behavior policy) from πθ a s  (the target policy), we instead\\nuse the following constrained optimization objective:\\nθ* = argmaxθ∑sρθold s ∑a\\nπθ a s\\nq a s Aθold s, a\\ns . t . Avg. KL θold, θ ≤δ\\nThe addition of q(a|s) accounts for the fact that we are sampling from a separate\\nbehavior policy. We can think about this more concretely in terms of expectations:\\n∑aπθ a s Aθold s, a = ∑a\\nq a s\\nq a s πθ a s Aθold s, a\\n= ∑aq a s\\nπθ a s\\nq a s Aθold s, a\\n= Eq a s\\nπθ a s\\nq a s Aθold s, a\\nNote that the left side of the first equality can be written as an expectation of the\\nadvantage with respect to the target policy. In a few algebraic manipulations, we\\nwere able to convert our original objective into an equivalent objective, but with an\\nexpectation that is taken with respect to the behavior policy. This is ideal because we\\nare sampling from the behavior policy and can thus use standard minibatch gradient\\ndescent techniques to optimize this objective (adding in the constraint on the KL\\ndivergence makes this a bit more complicated than just standard gradient descent).\\nAnd finally, we have already seen methods for sampling from the unnormalized\\nprobability distribution ρold s  for the outer expectation, amongst others that exist in\\nacademic literature.\\nProximal Policy Optimization\\nOne issue with TRPO is that its optimization is relatively complicated due to the\\ninclusion of the average KL divergence term and involves second-order optimization\\nto perform. Proximal policy optimization, or PPO for short, is an algorithm that tries\\nto retain the benefits of TRPO without the complicated optimization. PPO proposes\\nthe following objective instead:\\nJ θ = E min\\nπθ a s\\nπθold a s Aθold s, a , clip\\nπθ a s\\nπθold a s , 1 −ϵ, 1 + ϵ Aθold s, a\\nProximal Policy Optimization \\n| \\n345'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 361}, page_content='θ* = argmaxθJ θ\\nNote that we no longer have a complex constraint, but rather an extra term built\\ninto the optimization objective. The clip function represents an upper limit and a\\nlower limit on the ratio between the target policy and the behavior policy, where any\\nratio above or below these limits is set equal to the corresponding limit. Note the\\ninclusion of the minimum between the original and the clipped, which prevents us\\nfrom making extreme updates and keeps us from overfitting.\\nAs stated in the paper introducing PPO, it’s important to notice that the objective\\nfor TRPO and PPO have the same gradient at θ = θold. This is the case in at least\\nthe on-policy setting, where we have a single policy from which we are sampling\\nand optimizing (i.e., no distinction between the behavior and target policy). Let’s\\ntake a closer look at why this is the case. To do this, we first need to reformulate\\nTRPO’s constrained optimization objective as an equivalent regularized optimization\\nobjective (recall from early in the previous section), which we can do according to the\\ntheory. The objective looks like:\\nJ TRPO θ = E\\nπθ a s\\nπθold a s A s, a −β * KL πθold a s\\nπθ a s\\nNotice that we can separate the expression within the expectation into a difference\\nof expectations due to the linearity of expectation. If we first consider the second\\nexpectation, or the KL term, we’ll notice that this term is minimized at θ = θold, since\\nthe reference distribution is parametrized using θold. Thus, the gradient at this setting\\nis zero, since we have already reached the global minimum. We are left with only the\\ngradient of the first expectation:\\n∇θE\\nπθ a s\\nπθold a s A s, a\\nLooking to the objective for PPO, we notice that at θ = θold, the ratio between the\\ntwo policies is one, eliminating the need for the clip term. Thus, we are left with a\\nminimum over two equivalent terms, which simplifies to the expectation over a single\\nterm. The gradient comes out to exactly what we just saw for the TRPO objective:\\n∇θE\\nπθ a s\\nπθold a s A s, a\\nWe have shown that PPO has the same gradient as TRPO in the select on-policy\\nsetting, and is additionally much easier to optimize in practice. PPO has also shown\\n346 \\n| \\nChapter 13: Deep Reinforcement Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 362}, page_content='strong empirical results on a variety of tasks, and has become widely used in the field\\nof deep RL.\\nQ-Learning and Deep Q-Networks\\nQ-learning is in the category of reinforcement learning called value learning. Instead\\nof directly learning a policy, we will be learning the value of states and actions.\\nQ-learning involves learning a function, a Q-function, which represents the quality of\\na state, action pair. The Q-function, defined Q(s, a), is a function that calculates the\\nmaximum discounted future return when action a is performed in state s.\\nThe Q-value represents our expected long-term rewards, given we are at a state,\\nand take an action, and then take every subsequent action perfectly (to maximize\\nexpected future reward). This can be expressed formally as:\\nQ* st, at = maxπE ∑i = t\\nT\\nγiri\\nA question you may be asking is, how can we know Q-values? It is difficult, even\\nfor humans, to know how good an action is, because you need to know how you are\\ngoing to act in the future. Our expected future returns depend on what our long-term\\nstrategy is going to be. This seems to be a bit of a chicken-and-egg problem. In order\\nto value a state, action pair, you need to know all the perfect subsequent actions. And\\nin order to know the best actions, you need to have accurate values for a state and\\naction.\\nThe Bellman Equation\\nWe solve this dilemma by defining our Q-values as a function of future Q-values.\\nThis relation is called the Bellman equation, and it states that the maximum future\\nreward for taking action is the current reward plus the next step’s max future reward\\nfrom taking the next action a’:\\nQ* st, at = E rt + γ maxa′ Q* st + 1, a′\\nThis recursive definition allows us to relate between Q-values.\\nAnd since we can now relate between Q-values past and future, this equation conven‐\\niently defines an update rule. Namely, we can update past Q-values to be based on\\nfuture Q-values. This is powerful because there exists a Q-value we know is correct:\\nthe Q-value of the very last action before the episode is over. For this last state, we\\nknow exactly that the next action led to the next reward, so we can perfectly set the\\nQ-values for that state. We can use the update rule, then, to propagate that Q-value to\\nthe previous time step:\\nQ-Learning and Deep Q-Networks \\n| \\n347'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 363}, page_content='Qj\\nQj + 1\\nQj + 2\\n...\\nQ*\\nThis updating of the  Q-value is known as value iteration.\\nOur first Q-value starts out completely wrong, but this is perfectly acceptable. With\\neach iteration, we can update our Q-value via the correct one from the future. After\\none iteration, the last Q-value is accurate, since it is just the reward from the last state\\nand action before episode termination. Then we perform our Q-value update, which\\nsets the second-to-last Q-value. In our next iteration, we can guarantee that the last\\ntwo Q-values are correct, and so on and so forth. Through value iteration, we will be\\nguaranteed convergence on the ultimate optimal Q-value.\\nIssues with Value Iteration\\nValue iteration produces a mapping between state and action pairs with correspond‐\\ning Q-values, and we are constructing a table of these mappings, or a Q-table. Let’s\\nbriefly talk about the size of this Q-table. Value iteration is an exhaustive process\\nthat requires a full traversal of the entire space of state, action pairs. In a game like\\nBreakout, with 100 bricks that can be either present or not, with 50 positions for the\\npaddle to be in, and 250 positions for the ball to be in, and 3 actions, we have already\\nconstructed a space that is far, far larger than the sum of all computational capacity of\\nhumanity. Furthermore, in stochastic environments, the space of our Q-table would\\nbe even larger, and possibly infinite. With such a large space, it will be intractable for\\nus to find all of the Q-values for every state, action pair. Clearly this approach is not\\ngoing to work. How else are we going to do Q-learning?\\nApproximating the Q-Function\\nThe size of our Q-table makes the naive approach intractable for any nontoy prob‐\\nlem. However, what if we relax our requirement for an optimal Q-function? If\\ninstead, we learn approximations of the Q-function, we can use a model to estimate\\nour Q-function. Instead of having to experience every state, action pair to update our\\nQ-table, we can learn a function that approximates this table, and even generalizes\\noutside of its own experience. This means we won’t have to perform an exhaustive\\nsearch through all possible Q-values to learn a Q-function.\\nDeep Q-Network\\nThis was the main motivation behind DeepMind’s work on deep Q-network (DQN).\\nDQN uses a deep neural network that takes an image (the state) in to estimate the\\nQ-value for all possible actions.\\n348 \\n| \\nChapter 13: Deep Reinforcement Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 364}, page_content='Training DQN\\nWe would like to train our network to approximate the Q-function. We express this\\nQ-function approximation as a function of our model’s parameters, like this:\\nQθ s, a ∣θ ∼Q* s, a\\nRemember, Q-learning is a value-learning algorithm. We are not learning a policy\\ndirectly, but rather we are learning the values of each state, action pair, regardless if\\nthey are good or not. We have expressed our model’s Q-function approximation as\\nQtheta, and we would like this to be close to the future expected reward. Using the\\nBellman equation from earlier, we can express this future expected reward as:\\nRt* = rt + γ maxa′ Q st + 1, a′ θ\\nOur objective is to minimize the difference between our Q’s approximation, and the\\nnext Q value:\\nminθ ∑e ∈E ∑t = 0\\nT\\nQ st, at θ −Rt*\\nExpanding this expression gives us our full objective:\\nminθ ∑e ∈E ∑t = 0\\nT\\nQ st, at θ −rt + γ maxa′ Q st + 1, a′ θ\\nThis objective is fully differentiable as a function of our model parameters, and we\\ncan find gradients to use in stochastic gradient descent to minimize this loss.\\nLearning Stability\\nOne issue you may have noticed is that we are defining our loss function based on\\nthe difference of our model’s predicted Q-value of this step and the predicted Q-value\\nof the next step. In this way, our loss is doubly dependent on our model parameters.\\nWith each parameter update, the Q-values are constantly shifting, and we are using\\nshifting Q-values to do further updates. This high correlation of updates can lead to\\nfeedback loops and instability in our learning, where our parameters may oscillate\\nand make the loss diverge.\\nWe can employ a couple of simple engineering hacks to remedy this correlation\\nproblem: namely, target Q-network and experience replay.\\nQ-Learning and Deep Q-Networks \\n| \\n349'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 365}, page_content='Target Q-Network\\nInstead of updating a single network frequently with respect to itself, we can reduce\\nthis codependence by introducing a second network, called the target network. Our\\nloss function features to instances of the Q-function, Q st, at θ  and  Q st + 1, a′ θ .\\nWe are going to have the first Q be represented as our prediction network, and\\nour second Q will be produced by the target Q-network. The target Q-network is\\na copy of our prediction network that lags in its parameter updates. We update\\nthe target Q-network to equal the prediction network only every few batches. This\\nprovides much needed stability to our Q-values, and we can now properly learn a\\ngood Q-function.\\nExperience Replay\\nThere is yet another source of irksome instability to our learning: the high correla‐\\ntions of recent experiences. If we train our DQN with batches drawn from recent\\nexperience, these action, state pairs are all going to be related to one another. This\\nis harmful because we want our batch gradients to be representative of the entire\\ngradient, and if our data is not representative of the data distribution, our batch\\ngradient will not be an accurate estimate of the true gradient.\\nSo, we have to break up this correlation of data in our batches. We can do this\\nusing something called experience replay. In experience replay, we store all of the\\nagent’s experiences as a table, and to construct a batch, we randomly sample from\\nthese experiences. We store these experiences in a table as si, ai, ri, si + 1  tuples.\\nFrom these four values, we can compute our loss function, and thus our gradient to\\noptimize our network.\\nThis experience replay table is more of a queue than a table. The experiences an agent\\nsees early in training may not be representative of the experiences a trained agent\\nfinds itself in later, so it is useful to remove old experiences from our table.\\nFrom Q-Function to Policy\\nQ-learning is a value learning paradigm, not a policy learning algorithm. This means\\nwe are not directly learning a policy for acting in our environment. But can’t we\\nconstruct a policy from what our Q-function tells us? If we have learned a good\\nQ-function approximation, this means we know the value of every action for every\\nstate. We could then trivially construct an optimal policy in the following way: look\\nat our Q-function for all actions in our current state, choose the action with the\\nmax Q-value, enter a new state, and repeat. If our Q-function is optimal, our policy\\nderived from it will be optimal. With this in mind, we can express the optimal policy\\nas follows:\\n350 \\n| \\nChapter 13: Deep Reinforcement Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 366}, page_content='π s; θ = arg maxa′ Q* s, a′; θ\\nWe can also use the sampling techniques we discussed earlier to make a stochastic\\npolicy that sometime deviates from the Q-function recommendations to vary the\\namount of exploration our agent does.\\nDQN and the Markov Assumption\\nDQN is still a Markov decision process that relies on the Markov assumption, which\\nassumes that the next state si + 1 depends only on the current state si and action ai,\\nand not on any previous states or actions. This assumption doesn’t hold true for many\\nenvironments where the game’s state cannot be summed up in a single frame. For\\nexample, in Pong, the ball’s velocity (an important factor in successful game play)\\nis not captured in any single game frame. The Markov assumption makes modeling\\ndecision processes much simpler and reliable, but often at a loss of modeling power.\\nDQN’s Solution to the Markov Assumption\\nDQN solves this problem by utilizing state history. Instead of processing one game\\nframe as the game’s state, DQN considers the past four game frames as the game’s\\ncurrent state. This allows DQN to utilize time-dependent information. This is a bit\\nof an engineering hack, and we will discuss better ways of dealing with sequences of\\nstates at the end of this chapter.\\nPlaying Breakout with DQN\\nLet’s pull all of what we learned together and actually go about implementing DQN to\\nplay Breakout. We start out by defining our DQNAgent:\\n# DQNAgent\\nclass DQNAgent(object):\\n    def __init__(self, num_actions,\\n                 learning_rate=1e-3, history_length=4,\\n                 screen_height=84, screen_width=84,\\n                 gamma=0.99):\\n        self.num_actions = num_actions\\n        self.learning_rate = learning_rate\\n        self.history_length = history_length\\n        self.screen_height = screen_height\\n        self.screen_width = screen_width\\n        self.gamma = gamma\\n        self.build_prediction_network()\\n        self.build_target_network()\\n        #self.build_training()\\nQ-Learning and Deep Q-Networks \\n| \\n351'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 367}, page_content='def build_prediction_network(self):\\n        self.model_predict = nn.Sequential(\\n          nn.Conv2d(4, 32, kernel_size=8 , stride=4),\\n          nn.Conv2d(32, 64, kernel_size=4, stride=2),\\n          nn.Conv2d(64, 64, kernel_size=3, stride=1),\\n          nn.Flatten(),\\n          nn.Linear(3136, 512),\\n          nn.Linear(512, self.num_actions)\\n          )\\n    def build_target_network(self):\\n        self.model_target = nn.Sequential(\\n          nn.Conv2d(4, 32, kernel_size=8 , stride=4),\\n          nn.Conv2d(32, 64, kernel_size=4, stride=2),\\n          nn.Conv2d(64, 64, kernel_size=3, stride=1),\\n          nn.Flatten(),\\n          nn.Linear(3136, 512),\\n          nn.Linear(512, self.num_actions)\\n          )\\n    def sample_and_train_pred(self, replay_table, batch_size):\\n        s_t, action, reward, s_t_plus_1, terminal = \\\\\\n                   replay_table.sample_batch(batch_size)\\n        # given state_t, find q_t (predict_model) and \\n        #  q_t+1 (target_model)\\n        # do it in batches\\n        # Find q_t_plus_1\\n        input_t = torch.from_numpy(s_t_plus_1).float()\\n        model_t = self.model_target.float()\\n        q_t_plus_1 = model_t(input_t)\\n        \\n        terminal = torch.tensor(terminal).float()\\n        max_q_t_plus_1, _ = torch.max(q_t_plus_1, dim=1)\\n        reward = torch.from_numpy(reward).float()\\n        target_q_t = (1. - terminal) * self.gamma * \\\\\\n                     max_q_t_plus_1 + reward\\n        # Find q_t, and q_of_action\\n        input_p = torch.from_numpy(s_t).float()\\n        model_p = self.model_predict.float()\\n        q_t = model_p(input_p)\\n        action = torch.from_numpy(action)\\n        action_one_hot = nn.functional.one_hot(action,\\n                                               self.num_actions)\\n        q_of_action = torch.sum(q_t * action_one_hot)\\n        # Compute loss\\n        self.delta = (target_q_t - q_of_action)\\n        self.loss = torch.mean(self.delta)\\n352 \\n| \\nChapter 13: Deep Reinforcement Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 368}, page_content='# Update predict_model gradients (only)\\n        self.optimizer = optim.Adam(self.model_predict.parameters(),\\n                                    lr = self.learning_rate)\\n        self.loss.backward()\\n        self.optimizer.step()\\n        return q_t\\n    def predict_action(self, state, epsilon_percentage):\\n        input_p = torch.from_numpy(state).float().unsqueeze(dim=0)\\n        model_p = self.model_predict.float()\\n        action_distribution = model_p(input_p)\\n        # sample from action distribution\\n        action = epsilon_greedy_action_annealed(\\n                                       action_distribution.detach(),\\n                                       epsilon_percentage)\\n        return action\\n     \\n    def process_state_into_stacked_frames(self,\\n                                          frame,\\n                                          past_frames,\\n                                          past_state=None):\\n        full_state = np.zeros((self.history_length,\\n                              self.screen_width,\\n                              self.screen_height))\\n        if past_state is not None:\\n            for i in range(len(past_state)-1):\\n                full_state[i, :, :] = past_state[i+1, :, :]\\n            full_state[-1, :, :] = self.preprocess_frame(frame,\\n                                                 (self.screen_width,\\n                                                  self.screen_height)\\n                                                 )\\n        else:\\n            all_frames = past_frames + [frame]\\n            for i, frame_f in enumerate(all_frames):\\n                full_state[i, :, :] = self.preprocess_frame(frame_f,\\n                                                 (self.screen_width,\\n                                                  self.screen_height)\\n                                                  )\\n        return full_state\\n    def to_grayscale(self, x):\\n        return np.dot(x[...,:3], [0.299, 0.587, 0.114])\\n    def preprocess_frame(self, im, shape):\\n        cropped = im[16:201,:] # (185, 160, 3)\\n        grayscaled = self.to_grayscale(cropped) # (185, 160)\\n        # resize to (84,84)\\n        resized = np.array(Image.fromarray(grayscaled).resize(shape))\\n        mean, std = 40.45, 64.15\\nQ-Learning and Deep Q-Networks \\n| \\n353'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 369}, page_content='frame = (resized-mean)/std\\n        return frame\\nThere is a lot going on in this class, so let’s break it down in the following sections.\\nBuilding Our Architecture\\nWe build our two Q-networks: the prediction network and the target Q-network.\\nNotice how they have the same architecture definition, since they are the same\\nnetwork, with the target Q just having delayed parameter updates. Since we are\\nlearning to play Breakout from pure pixel input, our game state is an array of pixels.\\nWe pass this image through three convolution layers, and then two fully connected\\nlayers to produce our Q-values for each of our potential actions.\\nStacking Frames\\nYou may notice that our state input is actually of size [None, self.history_length,\\nself.screen_height, self.screen_width]. Remember, in order to model and\\ncapture time-dependent state variables like speed, DQN uses not just one image,\\nbut a group of consecutive images, also known as a history. Each of these consec‐\\nutive images is treated as a separate channel. We construct these stacked frames\\nwith the helper function process_state_into_stacked_frames(self, frame,\\npast_frames, past_state=None).\\nSetting Up Training Operations\\nOur loss function is derived from our objective expression from earlier in this\\nchapter:\\nminθ ∑e ∈E ∑t = 0\\nT\\nQ st, at θ −rt + γ maxa′ Q st + 1, a′ θ\\nWe want our prediction network to equal our target network, plus the return at the\\ncurrent time step. We can express this in pure PyTorch code as the difference between\\nthe output of our prediction network and the output of our target network. We use\\nthis gradient to update and train our prediction network, using AdamOptimizer.\\nUpdating Our Target Q-Network\\nTo ensure a stable learning environment, we update our target Q-network only once\\nevery four batches. Our update rule for the target Q-network is pretty simple: we\\njust set its weights equal to the prediction network. We do this in the function\\nupdate_target_q_network(self). The optimizer_predict.step() function sets the\\ntarget Q-network’s weights equal to those of the prediction network.\\n354 \\n| \\nChapter 13: Deep Reinforcement Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 370}, page_content='Implementing Experience Replay\\nWe’ve discussed how experience replay can help de-correlate our gradient batch\\nupdates to improve the quality of our Q-learning and subsequent derived policy.\\nLet’s walk though a simple implementation of experience replay. We expose a method\\nadd_episode(self, episode), which takes an entire episode (an EpisodeHistory\\nobject) and adds it to the ExperienceReplayTable. It then checks if the table is full and\\nremoves the oldest experiences from the table.\\nWhen it comes time to sample from this table, we can call sample_batch(self,\\nbatch_size) to randomly construct a batch from our table of experiences:\\nclass ExperienceReplayTable(object):\\n    def __init__(self, table_size=50000):\\n        self.states = []\\n        self.actions = []\\n        self.rewards = []\\n        self.state_primes = []\\n        self.terminals = []\\n        self.table_size = table_size\\n    def add_episode(self, episode):\\n        self.states += episode.states\\n        self.actions += episode.actions\\n        self.rewards += episode.rewards\\n        self.state_primes += episode.state_primes\\n        self.terminals += episode.terminals\\n        self.purge_old_experiences()\\n    def purge_old_experiences(self):\\n        while len(self.states) > self.table_size:\\n            self.states.pop(0)\\n            self.actions.pop(0)\\n            self.rewards.pop(0)\\n            self.state_primes.pop(0)\\n    def sample_batch(self, batch_size):\\n        s_t, action, reward, s_t_plus_1, terminal = [], [], [], \\n                                                    [], []\\n        rands = np.arange(len(self.states))\\n        np.random.shuffle(rands)\\n        rands = rands[:batch_size]\\n        for r_i in rands:\\n            s_t.append(self.states[r_i])\\n            action.append(self.actions[r_i])\\n            reward.append(self.rewards[r_i])\\n            s_t_plus_1.append(self.state_primes[r_i])\\nQ-Learning and Deep Q-Networks \\n| \\n355'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 371}, page_content=\"terminal.append(self.terminals[r_i])\\n        return (np.array(s_t), np.array(action), np.array(reward),\\n                np.array(s_t_plus_1), np.array(terminal))\\nDQN Main Loop\\nLet’s put this all together in our main function, which will create an OpenAI Gym\\nenvironment for Breakout, make an instance of our DQNAgent, and have our agent\\ninteract with and train to play Breakout successfully:\\nlearn_start = 4\\ntotal_episodes = 32\\nepsilon_stop = 32\\ntrain_frequency = 2\\ntarget_frequency = 4\\nbatch_size = 4\\nmax_episode_length = 1000\\nenv = gym.make('Breakout-v4')\\nnum_actions = env.action_space.n\\nsolved = False\\nagent = DQNAgent(num_actions=num_actions,\\n                 learning_rate=1e-4,\\n                 history_length=4,\\n                 gamma=0.98)\\n    \\nepisode_rewards = []\\nq_t_list = []\\nbatch_losses = []\\npast_frames_last_time = None\\nreplay_table = ExperienceReplayTable()\\nglobal_step_counter = 0\\nfor i in range(total_episodes):\\n    # Get initial frame -> state\\n    frame = env.reset() # np.array of shape (210, 160, 3)\\n    # past_frames is a list of past 3 frames (np.arrays)\\n    past_frames = [copy.deepcopy(frame) for _ in range(\\n                                           agent.history_length-1)]\\n    state = agent.process_state_into_stacked_frames(\\n        frame, past_frames, past_state=None) # state is (4,84,84)\\n    \\n    # initialize episode history (s_t, a, r, s_t+1, terminal)\\n    episode_reward = 0.0\\n    episode_history = EpisodeHistory()\\n    epsilon_percentage = float(min(i/float(epsilon_stop), 1.0))\\n    for j in range(max_episode_length):\\n        # predict action or choose random action at first\\n        if global_step_counter < learn_start:\\n356 \\n| \\nChapter 13: Deep Reinforcement Learning\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 372}, page_content='action = np.argmax(np.random.random((agent.num_actions)))\\n        else:\\n          action = agent.predict_action(state, epsilon_percentage)\\n        # take action, get next frame (-> next state), reward, \\n        # and terminal\\n        reward = 0\\n        frame_prime, reward, terminal, _ = env.step(action)\\n        if terminal == True:\\n          reward -= 1\\n          \\n        # get next state from next frame and past frames\\n        state_prime = agent.process_state_into_stacked_frames(\\n                                                   frame_prime,\\n                                                   past_frames,\\n                                                   past_state=state)\\n        # Update past_frames with frame_prime for next time\\n        past_frames.append(frame_prime)\\n        past_frames = past_frames[len(past_frames)- \\\\\\n                            agent.history_length:]\\n        past_frames_last_time = past_frames\\n        # Add to episode history (state, action, reward, \\n        #  state_prime, terminal)\\n        episode_history.add_to_history(\\n                    state, action, reward, state_prime, terminal)\\n        state = state_prime\\n        episode_reward += reward\\n        global_step_counter += 1\\n        \\n        #  Do not train predict_model until we have enough\\n        #   episodes in episode history\\n        if global_step_counter > learn_start:\\n          if global_step_counter % train_frequency == 0:\\n              if(len(replay_table.actions) != 0):\\n                q_t = agent.sample_and_train_pred(replay_table, \\n                                                  batch_size)\\n                q_t_list.append(q_t)\\n                if global_step_counter % target_frequency == 0:\\n                    agent.model_target.load_state_dict(\\n                        agent.model_predict.state_dict())\\n        # If terminal or max episodes reached,\\n        #   add episode_history to replay table\\n        if j == (max_episode_length - 1):\\n            terminal = True\\n        if terminal:\\n            replay_table.add_episode(episode_history)\\n            episode_rewards.append(episode_reward)\\n            break\\nQ-Learning and Deep Q-Networks \\n| \\n357'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 373}, page_content=\"print(f'Episode[{i}]: {len(episode_history.actions)} \\\\\\n              actions {episode_reward} reward')\\nDQNAgent Results on Breakout\\nWe train our DQNAgent for one thousand episodes to see the learning curve. To obtain\\nsuperhuman results on Atari, typical training time runs up to several days. However,\\nwe can see a general upward trend in reward pretty quickly, as shown in Figure 13-7.\\nFigure 13-7. Our DQNAgent gets increasingly better at Breakout during training as it\\nlearns a good value function and also acts less stochastically due to ϵ-greedy annealing\\nImproving and Moving Beyond DQN\\nDQN did a pretty good job back in 2013 in solving Atari tasks, but had some\\nserious shortcomings. DQN’s many weaknesses include that it takes very long to\\ntrain, doesn’t work well on certain types of games, and requires retraining for every\\nnew game. Much of the deep reinforcement learning research of the past few years\\nhas been in addressing these various weaknesses.\\n358 \\n| \\nChapter 13: Deep Reinforcement Learning\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 374}, page_content='5 Sorokin, Ivan, et al. “Deep Attention Recurrent Q-Network.” arXiv preprint arXiv:1512.01693 (2015).\\n6 Mnih, Volodymyr, et al. “Asynchronous Methods for Deep Reinforcement Learning.” International Conference\\non Machine Learning. 2016.\\nDeep Recurrent Q-Networks\\nRemember the Markov assumption? The one that states that the next state relies\\nonly on the previous state and the action taken by the agent? DQN’s solution to the\\nMarkov assumption problem, stacking four consecutive frames as separate channels,\\nsidesteps this issue and is a bit of an ad hoc engineering hack. Why 4 frames and\\nnot 10? This imposed frames history hyperparameter limits the model’s generality.\\nHow do we deal with arbitrary sequences of related data? That’s right: we can use\\nwhat we learned back in Chapter 8 on RNNs to model sequences with deep recurrent\\nQ-networks (DRQNs).\\nDRQN uses a recurrent layer to transfer a latent knowledge of state from one time\\nstep to the next. In this way, the model itself can learn how many frames are informa‐\\ntive to include in its state and can even learn to throw away noninformative ones or\\nremember things from long ago.\\nDRQN has even been extended to include neural attention mechanism, as shown\\nin Sorokin et al.’s 2015 paper, “Deep Attention Recurrent Q-Network” (DAQRN).5\\nSince DRQN is dealing with sequences of data, it can attend to certain parts of\\nthe sequence. This ability to attend to certain parts of the image both improves\\nperformance and provides model interpretability by producing a rationale for the\\naction taken.\\nDRQN has shown to be better than DQN at playing first-person shooter (FPS) games\\nlike DOOM, as well as improving performance on certain Atari games with long time\\ndependencies, like Seaquest.\\nAsynchronous Advantage Actor-Critic Agent\\nAsynchronous advantage actor-critic (A3C) is a new approach to deep reinforcement\\nlearning introduced in the 2016 DeepMind paper, “Asynchronous Methods for Deep\\nReinforcement Learning.”6 Let’s discuss what it is and why it improves upon DQN.\\nA3C is asynchronous, which means we can parallelize our agent across many threads,\\nwhich means orders of magnitude faster training by speeding up our environment\\nsimulation. A3C runs many environments at once to gather experiences. Beyond the\\nspeed increase, this approach presents another significant advantage in that it further\\ndecorrelates the experiences in our batches, because the batch is being filled with the\\nexperiences of numerous agents in different scenarios simultaneously.\\nImproving and Moving Beyond DQN \\n| \\n359'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 375}, page_content='7 Konda, Vijay R., and John N. Tsitsiklis. “Actor-Critic Algorithms.” NIPS. Vol. 13. 1999.\\n8 Jaderberg, Max, et al. “Reinforcement Learning with Unsupervised Auxiliary Tasks.” arXiv preprint\\narXiv:1611.05397 (2016).\\nA3C uses an actor-critic method.7 Actor-critic methods involve learning both a value\\nfunction V st  (the critic) and also a policy π st  (the actor). Early in this chapter,\\nwe delineated two different approaches to reinforcement learning: value learning and\\npolicy learning. A3C combines the strengths of each, using the critic’s value function\\nto improve the actor’s policy.\\nA3C uses an advantage function instead of a pure discounted future return. When\\ndoing policy learning, we want to penalize the agent when it chooses an action that\\nleads to a bad reward. A3C aims to achieve this same goal, but uses advantage instead\\nof reward as its criterion. Advantage represents the difference between the model’s\\nprediction of the quality of the action taken versus the actual quality of the action\\ntaken. We can express advantage as:\\nAt = Q* st, at −V st .\\nA3C has a value function, V(t), but it does not express a Q-function. Instead, A3C\\nestimates the advantage by using the discounted future reward as an approximation\\nfor the Q-function:\\nAt = Rt −V st\\nThese three techniques proved key to A3C’s takeover of most deep reinforcement\\nlearning benchmarks. A3C agents can learn to play Atari Breakout in less than 12\\nhours, whereas DQN agents may take 3 to 4 days.\\nUNsupervised REinforcement and Auxiliary Learning\\nUNREAL is an improvement on A3C introduced in “Reinforcement learning with\\nunsupervised auxiliary tasks\" by Jaderberg et al.,8 who, you guessed it, are from\\nDeepMind.\\nUNREAL addresses the problem of reward sparsity. Reinforcement learning is so\\ndifficult because our agent just receives rewards, and it is hard to determine exactly\\nwhy rewards increase or decrease, which makes learning difficult. Additionally, in\\nreinforcement learning, we must learn a good representation of the world as well as\\na good policy to achieve reward. Doing all of this with a weak learning signal like\\nsparse rewards is quite a tall order.\\n360 \\n| \\nChapter 13: Deep Reinforcement Learning'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 376}, page_content='UNREAL asks the question, what can we learn from the world without rewards? It\\naims to learn a useful world representation in an unsupervised matter. Specifically,\\nUNREAL adds some additional unsupervised auxiliary tasks to its overall objective.\\nThe first task involves the UNREAL agent learning about how its actions affect the\\nenvironment. The agent is tasked with controlling pixel values on the screen by\\ntaking actions. To produce a set of pixel values in the next frame, the agent must\\ntake a specific action in this frame. In this way, the agent learns how its actions affect\\nthe world around it, enabling it to learn a representation of the world that takes into\\naccount its own actions.\\nThe second task involves the UNREAL agent learning reward prediction. Given a\\nsequence of states, the agent is tasked with predicting the value of the next reward\\nreceived. The intuition behind this is that if an agent can predict the next reward, it\\nprobably has a pretty good model of the future state of the environment, which will\\nbe useful when constructing a policy.\\nAs a result of these unsupervised auxiliary tasks, UNREAL is able to learn around 10\\ntimes faster than A3C in the Labyrynth game environment. UNREAL highlights the\\nimportance of learning good world representations and how unsupervised learning\\ncan aid in weak learning signal or low-resource learning problems like reinforcement\\nlearning.\\nSummary\\nIn this chapter, we covered the fundamentals of reinforcement learning, including\\nMDPs, maximum discounted future rewards, and explore versus exploit. We also\\ncovered various approaches to deep reinforcement learning, including policy gradi‐\\nents and deep Q-networks, and touched on some recent improvements on DQN and\\nnew developments in deep reinforcement learning.\\nReinforcement learning is essential to building agents that can not only perceive and\\ninterpret the world, but also take action and interact with it. Deep reinforcement\\nlearning has made major advancements toward this goal, successfully producing\\nagents capable of mastering Atari games, safely driving automobiles, trading stocks\\nprofitably, controlling robots, and more.\\nSummary \\n| \\n361'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 377}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 378}, page_content=\"Index\\nSymbols\\n2D convolutions, 130\\n__init__ method, 134\\nϵ-Greedy strategy, 333\\nA\\naction, 328\\nactor-critic method, 360\\nAdaDelta optimizer, 115\\nAdaGrad algorithm, 111\\nAdam algorithm, 113-115, 135, 143\\nadditive feature attribution, 292\\nadvantage function, 342, 360\\naffine transformation, 139\\nagent, 326\\nAlexNet, 120, 147\\nalgebra (see linear algebra)\\nalgorithms\\nAdaDelta algorithm, 115\\nAdaGrad algorithm, 111\\nAdam algorithm, 113-115, 135, 143\\nBroyden–Fletcher–Goldfarb–Shanno algo‐\\nrithm, 110\\ngreedy algorithms, 279\\nMarkov Chain Monte Carlo algorithm, 265\\nneural style algorithm, 152\\nrandom forest algorithm, 277\\ntree-based algorithms, 276-280\\nallocation weightings, 309, 317\\nannealed ϵ-Greedy strategy, 333\\narc-standard system, 199\\nartificial neural networks, 48\\nartistic styles, replicating, 152\\nasynchronous advantage actor-critic agent\\n(A3C), 359\\nAtari games, 325\\nattention-based memory access, 301-303\\nattentional mechanisms, 227-230\\naudiograms, 154\\nautoencoders\\narchitecture, 160, 178\\ndenoising, 171-173, 269-274\\nk-Sparse autoencoders, 176\\nin PyTorch, 161-170\\nsparsity, 174-177\\nVariational Autoencoders, 249-259\\nauxiliary random variables, 257\\naxons, 46\\nB\\nbAbI dataset, 321\\nbackpropagation, 61-63\\nbackward() function, 84, 86\\nbasis, 9\\nbatch gradient descent, 63\\nbatch normalization, 137-139\\nBatchNorm1d constructor, 139\\nBatchNorm2d class, 138\\nBayes' Theorem, 27, 35\\nBayesian view, 18\\nbeam searches, 203-205\\nBellman equation, 347\\nBFGS (Broyden–Fletcher–Goldfarb–Shanno)\\nalgorithm, 110\\nbias, 46\\nblurry attention-based reading, 302\\nBreakout, 351, 358\\n363\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 379}, page_content='broadcasting, 81\\nBroyden–Fletcher–Goldfarb–Shanno (BFGS)\\nalgorithm, 110\\nbucketing, 231\\nbuild_vocab_from_iterator function, 220\\nC\\nCBOW (Continuous Bag of Words model), 179\\ncell bodies, 46\\nCentral Limit Theorem (CLT), 34\\ncharacteristic polynomial, 14\\nCIFAR-10 challenge, 141\\nclassification, 20\\nclassifiers, 20\\nclosed under scalar multiplication and closed\\nunder addition, 9\\nCLT (Central Limit Theorem), 34\\ncode, 160\\ncode examples, obtaining and using, x\\ncolumn space, 7-10\\ncolumn vector, 6\\ncolumn vector interpretation of matrix multi‐\\nplication, 4\\ncomments and questions, xi\\ncomplement, 18\\nCompose transform, 137\\ncompressive embeddings, 160, 175\\ncomputer programs, 40\\nconda package management system, 77\\nconditional probability, 20-21\\nconjugate gradient descent, 109\\nconsistency, 293\\ncontent-based addressing, 303\\nContinuous Bag of Words (CBOW) model, 179\\ncontinuous probability distributions, 32-36\\nconvolutional filters, 152\\nconvolutional neural networks (CNNs)\\napplying to other domains, 154\\nbatch normalization, 137-139\\nCIFAR-10 challenge, 141\\nconvolutional filters, 152\\nfeature selection, 118-120\\nfilters and feature maps, 122-127\\nfull architectural description of, 132-133\\nfull description of, 127-131\\ngroup normalization, 139\\nimage preprocessing pipelines, 136\\ninception of, 120\\nmax pooling, 131\\nMNIST classifier using, 134-135\\nneurons in human vision, 117\\nresidual learning and skip connections,\\n147-149\\nresidual networks with superhuman vision,\\n149-152\\nscaling problems, 121\\nspace required for, 133\\nvisualizing learning in, 143-146\\nconvolutions, 124\\ncopy.deepcopy method, 99\\ncovariance, 27\\ncritical point, 102\\ncross entropy, 31, 36\\ncross-entropy loss metric, 86\\nD\\nDAQRN (Deep Attention Recurrent Q-\\nNetwork), 359\\ndata matrix, 74\\ndata structures and operations\\nmatrix arrays, 1\\nmatrix operations, 3-6\\nmatrix-vector multiplication, 7\\nvector operations, 6\\nDataLoader class (PyTorch), 88-89, 221\\nDataset class (PyTorch), 87-89\\ndataset normalization, 136\\ndecision trees, 276-280\\ndecoders, 161, 224, 251\\nDeep Attention Recurrent Q-Network\\n(DAQRN), 359\\ndeep learning\\napproach to learning, x\\napproach to problem solving, 68\\ncore concepts of, 1\\ndefinition of term, 39\\nprerequisites to learning, ix\\nstateful deep learning models, 206\\nsuccess of, 45\\ndeep Q-networks (DQNs) (see also Q-learning)\\nbuilding architecture, 354\\nDQNAgent results on Breakout, 358\\nexperience replay, 350\\nfrom Q-function to policy, 350\\nimplementing experience replay, 355\\nlearning stability, 349\\nmain loop, 356\\nMarkov assumption and, 351\\n364 \\n| \\nIndex'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 380}, page_content='motivation behind, 348\\nplaying Breakout with, 351\\nremarkable achievements of, 325\\nsetting up training operations, 354\\nsolution to Markov assumption, 351\\nstacking frames, 354\\ntarget Q-network, 350\\ntraining DQNs, 349\\nupdating target Q-network, 354\\ndeep recurrent Q-networks (DRQNs), 359\\ndeep reinforcement learning\\nbasics of, 326\\nexplore-exploit dilemma, 331-333\\nMarkov decision process, 328-331\\nmoving beyond DQNs\\nasynchronous advantage actor-critic\\nagent, 359\\ndeep recurrent Q-networks, 359\\nUNREAL, 360\\npole-cart problem\\nagent creation, 335\\nhistory, 337\\nmodel and optimizer, 337\\nOpenAI Gym, 335\\nPGAgent performance, 340\\npolicy gradient main function, 338\\nsampling actions, 337\\npolicy versus value learning, 334\\nproximal policy optimization, 345\\nQ-learning and deep Q-networks\\napproximating Q-function, 348\\nBellman equation, 347\\nbuilding architecture, 354\\ndeep Q-networks, 348\\nDQN and Markov assumption, 351\\nDQN main loop, 356\\nDQNAgent results on Breakout, 358\\nDQNs solution to Markov assumption,\\n351\\nexperience replay, 350\\nfrom Q-function to policy, 350\\nimplementing experience replay, 355\\nlearning stability, 349\\nplaying Breakout with DQN, 351\\nQ-functions and Q-values, 347\\nsetting up training operations, 354\\nstacking frames, 354\\ntarget Q-network, 350\\ntraining DQNs, 349\\nupdating target Q-network, 354\\nvalue iteration, 348\\ntrust-region policy optimization, 341-345\\nDeepMind, 325\\ndelta rule, 59\\ndendrites, 46\\ndenoising, 171-173, 269\\ndenoising score matching (DSM), 272\\ndensity, 175\\ndependency parsing, 197-202\\ndeterminant, 14\\ndifferentiable neural computers (DNCs)\\nbasics of, 307-309\\ncontroller network, 313\\nimplementing in PyTorch, 317-321\\ninterference-free writing in, 309\\nmemory reuse, 310\\nread heads, 312\\nteaching to read and comprehend, 321-322\\ntemporal linking of writes, 311\\nvisualizing in action, 314-316\\ndimension, 4, 9\\ndimensionality reduction\\nalternatives to, 177\\ndrawbacks of PCA for, 159\\ndiscounted future return, 331\\ndiscrete space, 17\\ndiscriminative models, 243\\ndiscriminators, 244\\ndivergence, 176\\nDNC (see differentiable neural computers)\\nDOOM, 359\\ndot product, 6\\ndot product interpretation of matrix multiplica‐\\ntion, 4\\ndownsampler function, 150\\nDQN (see deep Q-networks)\\ndropout, 23, 73\\nDRQNs (deep recurrent Q-networks), 359\\nE\\neigendecomposition, 187\\neigenvalues, 13-14\\neigenvectors, 13-14\\nELBO (evidence lower bound), 251\\nembedding and representation learning\\nautoencoder architecture, 160\\nautoencoders in PyTorch, 161-170\\ncontext and, 177-179\\nIndex \\n| \\n365'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 381}, page_content='definition of embeddings, 157\\ndenoising, 171-173\\nlower-dimensional representations, 157\\nPCA and SVD, 187-188\\nprincipal component analysis, 158-159\\nSkip-Gram model, 182-186\\nsparsity, 174-177\\nWord2Vec framework, 179-182\\nempirical risk, 75\\nencoders, 160, 224, 251\\nend-of-sequence (EOS) tokens, 225\\nend-to-end-differentiability, 301\\nentropy, 29-32, 36\\nenvironment, 326\\nepochs, 69\\nepsilon greedy strategy, 333\\nerase vector, 302, 308\\nerror surfaces\\nchallenges of spurious local minima, 98-101\\ncritical challenges to optimization, 104-106\\nflat regions in, 101-103\\nlearning rate adaptation\\naccumulating historical gradients, 111\\nchoosing correct learning rate, 111\\ncombining momentum and RMSProp,\\n113-115\\nexponentially weighted moving average\\nof gradients, 112\\nlocal minima in, 96\\nmodel identifiability, 97\\nmomentum-based optimization, 106-109\\nphilosophy behind optimizer selection, 115\\nsecond-order methods, 109\\nevents and probability, 17-20\\nevidence lower bound (ELBO), 251\\nexpansion step, 204\\nexpectation and variance, 24-27\\nexpected discounted reward, 342\\nexperience replay, 350, 355\\nexplainers, 275 (see also interpretability meth‐\\nods)\\nexplicit score matching, 268\\nexplore-exploit dilemma, 331-333\\nexponentially weighted decay, 106\\nextractive rationalization, 283-288\\nF\\nfast-food problem, 55\\nfeature extraction, 119\\nfeature importance, evaluating\\npartial dependence plots, 282\\npermutation feature importance, 281\\nfeature maps, 123-127\\nfeature selection\\nautomating with embeddings, 157\\nshortcomings of, 118-120\\nfeature vectors, 119\\nfeatures, 41\\nfeed-forward neural networks\\nbasics of, 48-51\\nsequence analysis and, 189\\ntraining\\nbackpropagation, 61-63\\ndelta rule and learning rates, 58\\nfast-food problem, 55\\ngradient descent, 57\\ngradient descent with sigmoidal neu‐\\nrons, 60\\npreventing overfitting, 71-73\\nstochastic and minibatch gradient\\ndescent, 63\\ntest sets, validation sets, and overfitting,\\n65-71\\nfilters, 123-127, 152\\nfirst moment, 113\\nfirst-person shooter (FPS) games, 359\\nfor loops, 317\\nfractional max pooling, 132\\nfree list, 309\\nfrequentist view, 18\\nfundamental spaces\\ncolumn space, 7-10\\nnull space, 10-13\\nfuture return, 330\\nG\\nGANs (Generative Adversarial Networks),\\n244-249\\ngarden path sentences, 203\\nGated Recurrent Unit (GRU), 218\\ngated weighting, 304\\nGaussian distributions, 34, 250\\ngeneralization, 66\\nGenerative Adversarial Networks (GANs),\\n244-249\\ngenerative models\\nbasics of, 243\\n366 \\n| \\nIndex'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 382}, page_content='denoising autoencoders and score match‐\\ning, 269-274\\nGenerative Adversarial Networks, 244-249\\nscore-based generative models, 264-269\\nVariational Autoencoders basics, 249-259\\nVariational Autoencoders implementation,\\n259-264\\ngenerators, 244\\ngensim package, 192\\nglobal normalization, 205\\nGO tokens, 232\\nGoogle Colab, 192, 219\\nGoogle News, 192\\ngradient descent\\nbatch, 63\\nchallenges of using, 95\\nchallenges with vanishing gradients,\\n210-212\\nconjugate, 109\\nlearning parameter vectors through, 45\\nminibatch, 65\\nwith sigmoidal neurons, 60\\nstochastic, 64\\ntraining feed-forward networks, 57\\ngradients, 57, 104-106, 334\\nGram matrix, 153\\ngreedy algorithms, 279\\ngreedy pretraining, 95, 203\\ngrid searches, 69\\ngroup normalization, 139\\nGRU (Gated Recurrent Unit), 218\\nH\\nHessian matrix (H), 105\\nhidden layers, 49\\nhistory, 354\\nhuman vision, 117\\nhyperparameter optimization, 69\\nI\\nidentity matrix, 5\\niff (if and only if), 6\\nill-conditioning, 105\\nimage preprocessing pipelines, 136\\nimage recognition (see convolutional neural\\nnetworks)\\nimage whitening, 136\\nImageNet challenge, 120\\nimplicit score matching, 269\\nindependence, 21\\nintelligent agents, 327\\nintelligent machines, 39\\nintensity detectors, 119\\ninterface vector, 307\\ninterpolation gate, 304\\ninterpretability, 174\\ninterpretability methods\\nclassic models\\ndecision trees and tree-based algorithms,\\n276-280\\nlinear regression, 280\\nevaluating feature importance\\npartial dependence plots, 282\\npermutation feature importance, 281\\nextractive rationalization, 283-288\\nLIME, 288-292\\noverview of, 275\\nSHAP, 292-297\\nintersection, 19\\ninversion, 3\\ninverted dropout, 73\\niterable-style datasets, 220\\nJ\\njoint probability distribution, 27\\nK\\nk-Sparse autoencoders, 176\\nkeep gates, 213\\nkernel_size argument, 131, 150\\nkey strength, 303\\nKullback-Leibler (KL) divergence, 31, 36, 176,\\n253\\nL\\nL-BFGS, 110\\nL1 regularization, 72\\nL2 regularization, 72, 139\\nLangevin dynamics, 265\\nlanguage translation, 230-239\\nLarge Movie Review Dataset, 219\\nlayer normalization, 140\\nlearning rate, 58, 139\\nlearning rate adaptation\\nAdaGrad algorithm, 111\\nAdam algorithm, 113-115\\nchoosing correct learning rate, 111\\nIndex \\n| \\n367'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 383}, page_content='RMSProp optimizer, 112\\nLevelDB database, 192\\nlikelihood, 33\\nLIME (Local Interpretable Model-agnostic\\nExplanations), 288-292\\nlinear algebra\\ndata structures and operations, 1-7\\neigenvectors and eigenvalues, 13-14\\nfundamental spaces, 7-13\\nrelationship to deep learning, 1\\nlinear independence, 9\\nlinear neurons, 51\\nlinear perceptrons\\ndefinition of term, 43\\nexpressing as neurons, 47\\nlinear regression, 280\\nlink matrix, 311\\nload_state_dict, 99\\nlocal accuracy, 293\\nLocal Interpretable Model-agnostic Explana‐\\ntions (LIME), 288-292\\nlocal maxima, 102\\nlocal minima, 96\\nlocal normalization, 205\\nlocally invariance, 132\\nlocation-based mechanism, 306\\nlogit, 46\\nlong short-term memory (LSTM) architecture,\\n213-217\\nloop-based implementation, 317\\nloss metric, 86\\nlower-dimensional representations, 157 (see\\nalso embedding and representational learn‐\\ning)\\nLSTM (long short-term memory) architecture,\\n213-217\\nM\\nmachine learning (ML), 41-45\\nmanifolds, 172\\nMarkov assumption, 351\\nMarkov Chain Monte Carlo (MCMC) algo‐\\nrithms, 265\\nMarkov decision process (MDP)\\ndiscounted future return, 331\\nfuture return, 330\\noverview of, 328\\npolicy, 329\\nmatrix arrays, 1\\nmatrix inverse, 12\\nmatrix operations, 3-6\\nmatrix properties, 7-13\\nmatrix-vector multiplication, 7\\nmax future reward, 347\\nmax norm constraints, 73\\nmax pooling layer, 131\\nMCMC (Markov Chain Monte Carlo) algo‐\\nrithms, 265\\nMDP (see Markov decision process)\\nmemory cells, 213\\nmemory constrained learning, 139\\nmemory-augmented neural networks (see also\\nneural networks)\\ndifferentiable neural computers\\nbasics of, 307-309\\ncontroller network, 313\\nimplementing in PyTorch, 317-321\\ninterference-free writing in, 309\\nmemory reuse, 310\\nread heads, 312\\nteaching to read and comprehend,\\n321-322\\ntemporal linking of writes, 311\\nvisualizing in action, 314-316\\nNeural Turing Machines\\nattention-based memory access, 301-303\\nbasics of, 299-301\\nmemory addressing mechanisms,\\n303-306\\nmem_ops.py file, 317\\nminibatch gradient descent, 65\\nmissingness, 293\\nML (machine learning), 41-45\\nMNIST classifiers, 89-92, 134-135\\nmodel identifiability, 97\\nmodel.parameters() method, 99, 108\\nmodel.state_dict() function, 99\\nmodels, 41\\nmomentum, 107\\nmomentum value, 139\\nmomentum-based optimization, 106-109\\nN\\nNCE (noise-contrastive estimation), 180\\nNesterov momentum, 109\\nneural n-gram strategy, 190\\nneural net learning theory, 74\\n368 \\n| \\nIndex'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 384}, page_content='neural networks (see also memory augmented\\nneural networks)\\nartificially intelligent machines, 39\\nfeed-forward neural networks, 48-51, 55-73,\\n189\\nlinear neurons, 51\\nlinear perceptrons expressed as neurons, 47\\nmachine learning, 41-45\\nneurons, 45-47\\nscaling problems, 121\\nsigmoid, tanh, and ReLU neurons, 51-52\\nsoftmax output layers, 54\\ntraditional computer programs, 40\\nneural style algorithm, 152\\nneural translation networks, 230-239\\nNeural Turing Machines (NTMs)\\nattention-based memory access, 301-303\\nbasics of, 299-301\\nmemory addressing mechanisms, 303-306\\nneurons\\nartificial, 46\\nbiological, 45\\nin human vision, 117\\nlinear neurons, 51\\nlinear perceptrons expressed as, 47\\nsigmoid, tanh, and ReLU neurons, 51-52\\nnn module (PyTorch), 84-87, 134\\nnn.BCELoss, 262\\nnoise-contrastive estimation (NCE), 180\\nnonidentifiability, 97\\nnormalization\\nbatch normalization, 137-139\\ndataset normalization, 136\\nglobal normalization, 205\\ngroup normalization, 139\\nlocal normalization, 205\\nNormalize transform, 136\\nNTM (see Neural Turing Machines)\\nnull space, 10-13\\nnum_features argument, 138\\nO\\noff-policy setting, 344\\non-policy setting, 344\\none-hot vector representations, 178\\noneplus function, 314\\nOpenAI Gym, 335\\noptimization\\ncritical challenges to, 104-106\\ndefinition of term, 44\\nlearning rate adaptation, 111-115\\nmomentum-based, 106-109\\nphilosophy behind optimizer selection, 115\\nprimary challenge in, 96\\nsecond-order methods, 109\\nopt_state_dict, 99\\northogonality, 6\\nout_channels argument, 131\\noverfitting\\npreventing, 71-73\\nproblem of, 65-71\\nP\\npack() method, 321\\npadding, 231\\npadding argument, 131\\nparameter vectors, 55\\nparameters() function, 86\\nParsey McParseface, 202\\npart-of-speech (POS) tags\\nimplementing taggers, 192-196\\nproducing with neural n-grams, 190\\npartial dependence plots (PDPs), 282\\npartition function, 267\\npathfinding, 322\\nPCA (see principal component analysis)\\nPDPs (partial dependence plots), 282\\nperceptrons\\ndefinition of term, 43\\nexpressing linear as neurons, 47\\npermutation feature importance, 281\\nPGAgent class, 335\\npole-cart problem\\nagent creation, 335\\nhistory, 337\\nmodel and optimizer, 337\\nOpenAI Gym, 335\\nPGAgent performance, 340\\npolicy gradient main function, 338\\nsampling actions, 337\\npolicy, 326\\npolicy gradients, 334\\npolicy learning, 334\\npolicy optimization\\nproximal policy optimization, 345\\ntrust-region policy optimization , 341-345\\npopulation risk, 75\\nposterior, 18\\nIndex \\n| \\n369'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 385}, page_content=\"PPO (proximal policy optimization), 345\\npre-output vector, 307, 314\\nprecedence vector, 311\\nprefix codes, 30\\npretraining\\ngreedy pretraining, 95, 203\\nimage preprocessing pipelines, 136\\nprincipal component analysis (PCA), 14,\\n158-159, 187-188\\nprior, 18\\nprobability\\nBayes' Theorem, 27\\nconditional probability, 20-21\\ncontinuous probability distributions, 32-36\\nentropy, cross entropy, and KL divergence,\\n29-32\\nevents and probability, 17-20\\nexpectation and variance, 24-27\\nrandom variables, 22-23\\nprobability density functions (PDFs), 32\\nprobability distribution, 17\\nproximal policy optimization (PPO), 345\\npruning step, 204\\nPyTorch\\n2D convolution, 130\\naccessing model parameters, 99\\nautoencoders in, 161-170\\nbatch normalization, 138\\nconverting data to tensors, 137\\nDatasets and DataLoaders, 87-89\\ngradients in, 83\\ngroup normalization, 141\\nimage whitening, 136\\nimplementing DNCs in, 317-321\\ninstalling, 77\\nMNIST classifier, 89-92\\nNCE implementation, 181\\nnn module, 84-87, 134\\noptimization\\nAdaDelta optimizer, 115\\nAdaGrad algorithm, 111\\nAdam optimizer, 115\\nmomentum-based, 108\\nNerestov momentum, 109\\nRMSProp optimizer, 113\\nsupport for second-order methods, 110\\norigins and benefits of, 77\\nprimitives for RNN models, 218\\nresidual block for ResNet34, 149\\nSkip-Gram model, 182-186\\ntensors\\nbasics of, 78\\ntensor attributes, 79\\ntensor init, 78\\ntensor operations, 80-83\\ntorchvision library, 136, 149\\nQ\\nQ-learning (see also deep Q-networks)\\napproximating Q-function, 348\\nBellman equation, 347\\nQ-functions and Q-values, 347\\nQ-tables, 348\\nvalue iteration, 348\\nquestions and comments, xi\\nR\\nrandom forest algorithm, 277\\nrandom variables, 22-23\\nrandom walk, 107\\nrand_state_dict, 100\\nread modes, 312\\nreading and comprehension, 321-322\\nRectified Linear Unit (ReLU) neurons, 52\\nrecurrent neural networks (RNNs)\\nfor sentiment analysis, 219-224\\nfor seq2seq tasks, 224-227\\nfor sequence analysis, 207-209\\nregression, 20, 280\\nregularization, 71\\nreinforcement learning (RL), 325 (see also deep\\nreinforcement learning)\\nReLU (Rectified Linear Unit) neuron, 52\\nreparameterization trick, 251, 256\\nrepresentation learning (see embedding and\\nrepresentation learning)\\nresidual learning\\nsuperhuman vision, 149-152\\nfor very deep networks, 147-149\\nResNet34 architecture, 147\\nResNet34 model, 149, 149\\nretention vector, 310\\nreturn, 330\\nreverse KL divergence, 253\\nreward, 328\\nreward prediction, 361\\nreward signal, 326\\nRL (reinforcement learning), 325\\n370 \\n| \\nIndex\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 386}, page_content='(see also deep reinforcement learning)\\nRMSProp optimizer, 112\\nrotational invariance, 187\\nrow vector, 6\\nS\\nsaddle points, 102\\nsaliency mapping, 275\\nsample space, 17\\nscalar-matrix multiplication, 3\\nscaled dot product attention, 239\\nscaling problems, 121\\nscikit-learn library, 168, 185\\nscore function, 266\\nscore matching, 267, 270-274\\nscore-based generative models, 264-269\\nSeaquest, 359\\nsecond-order methods, 109\\nself-attention, 239-241\\nsentiment analysis models, 219-224\\nseq2seq problems\\nsolving with RNNs, 224-227\\ntackling with neural n-grams, 190\\nsequence analysis\\napplying to multiple fields, 242\\nattentional mechanisms, 227-230\\nbeam search and global normalization,\\n203-205\\ndependency parsing and SyntaxNet,\\n197-202\\nlong short-term memory units, 213-217\\nneural n-grams, 190\\nneural translation networks, 230-239\\npart-of-speech taggers, 192-196\\nPyTorch primitives for RNN models, 218\\nrecurrent neural networks, 207-209\\nself-attention and transformers, 239-241\\nsentiment analysis models, 219-224\\nseq2seq tasks with RNNs, 224-227\\nstateful deep learning models, 206\\nvanishing gradients, 210-212\\nvariable-length inputs, 189\\nSGD (stochastic gradient descent), 64\\nSHAP (Shapley Additive Explanations),\\n292-297\\nshape, 4\\nShapley Additive Explanations (SHAP),\\n292-297\\nshift weighting, 304\\nsigmoid neurons, 51, 60\\nsingular value decomposition (SVD), 187-188\\nsingular values, 187\\nsingularity, 12\\nskip connections, 147-149\\nSkip-Gram model, 182-186\\nskip-thought vectors, 225\\nsoftmax layer, 54\\nspan, 9\\nspanning list, 9\\nsparsity, 174-177\\nspatial extent, 131\\nspurious local minima, 98-101\\nsquare, 4\\nstate, 328\\nstate history, 351\\nstate transition, 328\\nstateful deep learning models, 206\\nstate_dict method, 99\\nsteepest descent, 109\\nstochastic gradient descent (SGD), 64\\nstride argument, 131\\nsupervised learning, 326\\nSVD (singular value decomposition), 187-188\\nsymbolic loops, 321\\nSyntaxNet, 202-205\\nT\\nt-Distributed Stochastic Neighbor Embedding\\n(t-SNE), 145, 185\\ntanh neurons, 52\\ntarget network, 350\\nTensorBoard, 164\\ntensors (PyTorch)\\nbasics of, 78\\ntensor attributes, 79\\ntensor init, 78\\ntensor operations, 80-83\\ntest sets, 65-71\\ntext_pipeline function, 221\\ntokenization, 230\\ntokenizers, 219\\ntorch.load, 99\\ntorch.nn.GroupNorm class, 141\\ntorch.nn.RNNCell objects, 218\\ntorch.optim module (PyTorch), 87\\ntorch.topk operation, 320\\ntorch.utils.tensorboard.SummaryWriter, 163\\ntorchaudio library, 78\\nIndex \\n| \\n371'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 387}, page_content='Torchtext library, 219, 233\\ntorchtext.datasets.IMDB class, 221\\ntorchvision library\\nimage whitening with, 136\\ninstalling, 78\\nResNet34 model, 149\\ntraining (see also error surfaces; feed-forward\\nneural networks)\\nbatch normalization, 137-139\\ndefinition of term, 55\\ngreedy pretraining, 95, 203\\ntraining sets, 68\\ntransformers, 239-241\\ntranslation networks, 230-239\\ntree-based algorithms, 276-280\\nTRPO (trust-region policy optimization),\\n341-345\\ntrust region, 344\\ntrust-region policy optimization (TRPO),\\n341-345\\nTuring complete, 299\\nU\\nuniform distribution, 33\\nunion, 19\\nunnormalized discounted visitation frequency,\\n343\\nunsupervised learning, 326\\nUNsupervised REinforcement and Auxiliary\\nLearning (UNREAL), 360\\nusage vector, 309\\nV\\nvalidation sets, 65-71\\nvalue function, 342\\nvalue iteration, 348\\nvalue learning, 334\\nvanishing gradients, 210-212\\nvariance, 24-27\\nVariational Autoencoders (VAEs)\\nabout, 249-259\\nimplementing, 259-264\\nvector operations, 6\\nvector space, 8\\nvectorization, 50, 178, 318\\nvectors, 2\\nVGGNet architecture, 133, 153\\nvision, 117 (see also convolutional neural net‐\\nworks)\\nvisualizations\\ncomparing PCA to autoencoders, 169\\nusing autoencoders, 168\\nusing CNNs, 143-146\\nusing t-SNE, 185\\nusing TensorBoard, 164, 164\\nW\\nweight decay, 72, 106\\nweighting vector, 302\\nweights, 55\\nwhitening, 136\\nword embeddings (see also embedding and\\nrepresentational learning)\\nSkip-Gram model, 182-186\\nWord2Vec framework for, 179-182\\nword-level tokenization, 231\\nWord2Vec framework, 179-182\\nworking memory, 300\\nwrite gates, 214\\nwrite vector, 302, 308\\n372 \\n| \\nIndex'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 388}, page_content='About the Authors\\nNithin Buduma is one of the first machine learning engineers at XY.ai, a startup\\nbased out of Harvard and Stanford working to help healthcare companies leverage\\ntheir massive datasets.\\nNikhil Buduma is the cofounder and chief scientist of Remedy, a San Francisco-\\nbased company that is building a new system for data-driven primary healthcare.\\nAt the age of 16, he managed a drug discovery laboratory at San Jose State Univer‐\\nsity and developed novel low-cost screening methodologies for resource-constrained\\ncommunities. By the age of 19, he was a two-time gold medalist at the International\\nBiology Olympiad. He later attended MIT, where he focused on developing large-\\nscale data systems to impact healthcare delivery, mental health, and medical research.\\nAt MIT, he cofounded Lean On Me, a national nonprofit organization that provides\\nan anonymous text hotline to enable effective peer support on college campuses,\\nand leverages data to effect positive mental health and wellness outcomes. Today,\\nNikhil spends his free time investing in hard technology and data companies through\\nhis venture fund, Q Venture Partners, and managing a data analytics team for the\\nMilwaukee Brewers baseball team.\\nJoe Papa is the author of the PyTorch Pocket Reference (O’Reilly) and founder of\\nPyTorch Academy and TeachMe.AI. He has over 25 years of experience in research\\nand development and currently leads AI projects as Chief AI Engineer at Mobile\\nInsights. He holds an MSEE and has led AI research teams with PyTorch at Booz\\nAllen Hamilton and Perspecta Labs. Joe has mentored hundreds of data scientists and\\nhas taught more than 7,000 students across the world on Udemy, Packt, and O’Reilly\\nLearning.\\nColophon\\nThe animal on the cover of Fundamentals of Deep Learning is a North Pacific crestfish\\n(Lophotus capellei), also known as the unicornfish. It’s part of the Lophotidae family\\nand lives in the deep waters of the Atlantic and Pacific oceans. Because of their\\nseclusion from researchers, little is known about this fish. Some have been caught,\\nhowever, that are six feet in length.\\nMany of the animals on O’Reilly covers are endangered; all of them are important to\\nthe world. To learn more about how you can help, go to animals.oreilly.com.\\nThe cover image is by Karen Montgomery, based on a black and white engraving\\nfrom Lydekker’s Royal Natural History. The cover fonts are Gilroy Semibold and\\nGuardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad\\nCondensed; and the code font is Dalton Maag’s Ubuntu Mono.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 389}, page_content='Learn from experts. \\nBecome one yourself.\\nBooks | Live online courses  \\nInstant Answers | Virtual events\\nVideos | Interactive learning\\nGet started at oreilly.com. \\n©2022 O’Reilly Media, Inc. O’Reilly is a registered trademark of O’Reilly Media, Inc. | 175'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 390}, page_content='Typical CNN Layer\\nInput\\nConvolutional\\nStage: Affine\\nTransform\\nDetector  \\nStage:  \\nNonlinearity\\nPooling Stage\\nNormalization  \\nStage  \\n(Optional)\\nOutput:  \\nFeature Map'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 391}, page_content='A simple CNN structure\\nCONV: Convolutional kernel layer\\nRELU: Activation function\\nPOOL: Dimension reduction layer\\nFC: Fully connection layer'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 392}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 393}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 394}, page_content='Example:\\n200x200 image\\n40K hidden units\\n~2B parameters!!!\\n- Spatial correlation is local\\n- Waste of resources + we have not enough\\nFully Connected\\nLayer\\ntraining samples anyway..'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 395}, page_content='Locally Connected\\nLayer\\nExample: 200x200 image\\n40K hidden units\\nFilter size: 10x10\\n4M parameters\\nNote: This parameterization is good  when \\ninput image is registered (e.g.,\\nface recognition).'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 396}, page_content='The Convolution operation'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 397}, page_content='Convolutional kernel\\nPadding on the input \\nvolume with zeros in such \\nway that the conv layer \\ndoes not alter the spatial \\ndimensions of the input'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 398}, page_content=\"Example of 'valid' 2-D convolution\\n(without kernel flipping) where a\\n3x4 matrix convolved with a 2x2\\nkernel to output a2x3 matrix\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 399}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 400}, page_content='Convolutional Layer\\nShare the same parameters across  \\ndifferent locations (assuming input is  \\nstationary):\\nConvolutions with learned kernels'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 401}, page_content='Convolutional Layer'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 402}, page_content='Convolutional Layer'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 403}, page_content='Convolutional Layer'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 404}, page_content='Convolutional Layer'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 405}, page_content='Convolutional Layer'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 406}, page_content='Convolutional Layer'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 407}, page_content='Convolutional Layer'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 408}, page_content='Convolutional Layer'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 409}, page_content='Convolutional Layer'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 410}, page_content='Convolutional Layer'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 411}, page_content='Convolutional Layer'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 412}, page_content='Convolutional Layer'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 413}, page_content='Convolutional Layer'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 414}, page_content='Convolutional Layer'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 415}, page_content='Convolutional Layer'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 416}, page_content='Convolutional Layer'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 417}, page_content='Convolutional Layer\\n*\\n-1 0 1\\n-1 0 1\\n-1 0 1\\n='),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 418}, page_content='Learn multiple filters.\\nE.g.: 200x200 image  100\\nFilters\\nFilter size: 10x10  10K\\nparameters\\nConvolutional Layer'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 419}, page_content='Reason 1 : Sparse Connectivity'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 420}, page_content='• Receptive fields of units in deeper layers \\nlarger than shallow layers\\n• Though direct connections are very \\nsparse, deeper layers indirectly connected \\nto most of  the input image\\n• Effect increases with strided convolution \\nor pooling.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 421}, page_content='Input n e u r o n s\\nre p re s e nt i n g a  28x28\\ni m a g e ( s u c h a s f r o m   \\nMNIST dataset)'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 422}, page_content='Every h i d d e n layer n e u ro n\\nh a s a  local receptive field \\no f  region  5 x5 pixels'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 423}, page_content=\"And s o on, t h e first h i d d e n layer is built!\\n(28 - 5 + 1) = 24 x 24 n e u r o n s in t h e h i d d e n layer o n 'valid'\\nconvolution  Size o f t h e h i d d e n layer ca n b e c h a n g e d u s i n g a n o th e r\\nvariant o f convolution\"),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 424}, page_content='Reason 2 : Parameter sharing'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 425}, page_content='A closer look at spatial dimensions:\\n32\\n3\\n32x32x3 image  \\n5x5x3 filter\\n32\\nactivation map\\n1\\n28\\n28\\nconvolve (slide) over all  \\nspatial locations'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 426}, page_content='32\\n3\\n6\\n28\\nactivation maps\\n32\\n28\\nConvolution Layer\\nFor example, if we had 6 5x5 filters, we’ll get 6 separate activation maps:\\nWe stack these up to get a “new image” of size 28x28x6!'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 427}, page_content='Stride'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 428}, page_content='7\\n7x7 input (spatially)  \\nassume 3x3 filter\\n7\\nA closer look at spatial dimensions:'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 429}, page_content='7\\n7x7 input (spatially)  \\nassume 3x3 filter\\n7\\nA closer look at spatial dimensions:'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 430}, page_content='7\\n7x7 input (spatially)  \\nassume 3x3 filter\\n7\\nA closer look at spatial dimensions:'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 431}, page_content='7\\n7x7 input (spatially)  \\nassume 3x3 filter\\n7\\nA closer look at spatial dimensions:'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 432}, page_content='=> 5x5 output\\n7\\n7x7 input (spatially)  \\nassume 3x3 filter\\n7\\nA closer look at spatial dimensions:'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 433}, page_content='7x7 input (spatially)  \\nassume 3x3 filter  \\napplied with stride 2\\n7\\n7\\nA closer look at spatial dimensions:'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 434}, page_content='7x7 input (spatially)  \\nassume 3x3 filter  \\napplied with stride 2\\n7\\n7\\nA closer look at spatial dimensions:'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 435}, page_content='7x7 input (spatially)  \\nassume 3x3 filter  \\napplied with stride 2\\n=> 3x3 output!\\n7\\n7\\nA closer look at spatial dimensions:'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 436}, page_content='7x7 input (spatially)  \\nassume 3x3 filter  \\napplied with stride 3?\\n7\\n7\\nA closer look at spatial dimensions:'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 437}, page_content='7x7 input (spatially)  \\nassume 3x3 filter  \\napplied with stride 3?\\n7\\n7\\nA closer look at spatial dimensions:\\ndoesn’t fit!\\ncannot apply 3x3 filter on  \\n7x7 input with stride 3.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 438}, page_content='N\\nF\\nF\\nN\\nOutput size:\\n(N - F) / stride + 1\\ne.g. N = 7, F = 3:\\nstride 1 => (7 - 3)/1 + 1 = 5\\nstride 2 => (7 - 3)/2 + 1 = 3\\nstride 3 => (7 - 3)/3 + 1 = 2.33 :\\\\'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 439}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 440}, page_content='0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\ne.g. input 7x7\\n3x3 filter, applied with stride 1\\npad with 1 pixel border => what is the output?\\n(recall:)\\n(N - F) / stride + 1\\nZero-Padding: common to the border'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 441}, page_content='e.g. input 7x7\\n3x3 filter, applied with stride 1\\npad with 1 pixel border => what is the output?\\n7x7 output!\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\nZero-Padding: common to the border'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 442}, page_content='e.g. input 7x7\\n3x3 filter, applied with stride 1\\npad with 1 pixel border => what is the output?\\n7x7 output!\\nin general, common to see CONV layers with  \\nstride 1, filters of size FxF, and zero-padding with  \\n(F-1)/2. (will preserve size spatially)\\ne.g. F = 3 => zero pad with 1\\nF = 5 => zero pad with 2\\nF = 7 => zero pad with 3\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\nZero-Padding: common to the border'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 443}, page_content='Examples time:\\nInput volume: 32x32x3\\n10 5x5 filters with stride 1, pad 2\\nOutput volume size: ?'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 444}, page_content='Examples time:\\nInput volume: 32x32x3\\n10 5x5 filters with stride 1, pad 2\\nOutput volume size:\\n(32+2*2-5)/1+1 = 32 spatially, so\\n32x32x10'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 445}, page_content='Examples time:\\nInput volume: 32x32x3\\n10 5x5 filters with stride 1, pad 2\\nNumber of parameters in this layer?'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 446}, page_content='Examples time:\\nInput volume: 32x32x3\\n10 5x5 filters with stride 1, pad 2\\n(+1 for bias)\\nNumber of parameters in this layer?  \\neach filter has 5*5*3 + 1 = 76 params\\n=> 76*10 = 760'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 447}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 448}, page_content='Summary'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 449}, page_content='Common settings:\\nK = (powers of 2, e.g. 32, 64, 128, 512)\\n-\\nF = 3, S = 1, P =1\\n-\\nF = 5, S = 1, P =2\\n-\\nF = 5, S = 2, P = ? (whateverfits)\\n-\\nF = 1, S = 1, P = 0'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 450}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 451}, page_content='Let us assume filter is an “eye” detector.\\nQ.: how can we make the detection robust to  \\nthe exact location of the eye?\\nPooling Layer'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 452}, page_content='By “pooling” (e.g., taking max) filter  \\nresponses at different locations we gain  \\nrobustness to the exact spatial location  of \\nfeatures.\\nPooling Layer'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 453}, page_content='Pooling layer'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 454}, page_content='Pooling'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 455}, page_content='Eﬀect = invariance to small translations of the input\\nPooling'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 456}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 457}, page_content='-\\nmakes the representations smaller and more manageable\\n-\\noperates over each activation map independently'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 458}, page_content='1\\n1\\n2\\n4\\n5\\n6\\n7\\n8\\n3\\n2\\n1\\n0\\n1\\n2\\n3\\n4\\nSingle depth slice\\nx\\ny\\nmax pool with 2x2 filters  \\nand stride 2\\n6\\n8\\n3\\n4\\nMax Pooling'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 459}, page_content='Summary'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 460}, page_content='Pooling Layer: Receptive\\nField Size\\nConv.  \\nlayer\\nhn−1\\nhn\\nhn 1\\nPool.  \\nlayer\\nIf convolutional filters have size KxK and stride 1, and pooling layer  has \\npools of size PxP, then each unit in the pooling layer depends  upon a \\npatch (at the input of the preceding conv. layer) of size: (P+K- 1)x(P+K-1)'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 461}, page_content='Pooling Layer: Receptive\\nField Size\\nConv.  \\nlayer\\nhn−1\\nhn\\nhn 1\\nPool.  \\nlayer\\nIf convolutional filters have size KxK and stride 1, and pooling layer  has \\npools of size PxP, then each unit in the pooling layer depends  upon a \\npatch (at the input of the preceding conv. layer) of size: (P+K- 1)x(P+K-1)'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 462}, page_content='7\\n8\\nConvNets: Typical Stage\\nOne stage (zoom)\\nConvol.\\nPooling'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 463}, page_content='ConvNets: Typical Stage\\nOne stage (zoom)\\nConvol.\\nPooling\\nConceptually similar to: SIFT, HoG, etc.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 464}, page_content='Fully Conn.\\nLayers\\n1st stage\\n2nd stage\\n3rd stage\\nInput  \\nImage\\nClass\\nLabels\\nConvNets: Typical Architecture\\nOne stage (zoom)\\nConvol.\\nPooling\\nWhole system'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 465}, page_content='Conceptually similar to:\\nSIFT → K-Means → Pyramid Pooling →SVM\\nLazebnik et al. “...Spatial Pyramid Matching...” CVPR 2006\\nSIFT → Fisher Vect. → Pooling →SVM\\nSanchez et al. “Image classifcation with F.V.: Theory and practice” IJCV 2012\\nFully Conn.\\nLayers\\nWhole system\\n1st stage\\n2nd stage\\n3rd stage\\nInput  \\nImage\\nClass\\nLabels\\nConvNets: Typical Architecture'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 466}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 467}, page_content='85\\nNxMxM, M small\\nH hidden units /  \\nHx1x1 feature maps\\nFully conn. layer /\\nConv. layer (H kernels of size NxMxM)'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 468}, page_content='H hidden units /  \\nHx1x1 feature maps\\nNxMxM, M small\\nFully conn. layer /\\nConv. layer (H kernels of size NxMxM)\\nFully conn. layer /\\nConv. layer (K kernels of size Hx186x1)\\nK hidden units /  \\nKx1x1 feature maps'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 469}, page_content='87\\nViewing fully connected layers as convolutional layers enables efficient use\\nof convnets on bigger images (no need to slide windows but unroll\\nnetwork over space as needed to re-use computation).\\nCNN\\nInput  \\nImage\\nCNN\\nInput  \\nImage\\nInput  \\nImage\\nTRAINING TIME\\nTEST TIME\\nx\\ny'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 470}, page_content='ConvNets: Test\\nAt test time, run only is forward mode (FPROP).'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 471}, page_content='CONV NETS: EXAMPLES\\n- OCR\\n/ House number\\n& Traffic sign\\nclassification'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 472}, page_content='CONV NETS: EXAMPLES\\n- Texture classification'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 473}, page_content='CONV NETS: EXAMPLES\\n- Pedestrian detection'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 474}, page_content='CONV NETS: EXAMPLES\\n- Scene Parsing'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 475}, page_content='CONV NETS: EXAMPLES\\n- Segmentation 3D volumetric images'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 476}, page_content='102\\nCONV NETS: EXAMPLES\\n- Action recognition from videos'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 477}, page_content='CONV NETS: EXAMPLES\\n- Object detection'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 478}, page_content='Architecture for Classification\\ninput  \\nimage\\nlabel\\nConv. layer: 3x3 filters\\nMax pooling layer: 2x2, stride 2  Fully \\nconnected layer: 4096 hiddens\\n24 Layers in total!!!\\n109\\n64\\n128\\n256\\n512\\n512'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 479}, page_content='Architecture for Classification\\ninput  \\nimage\\nlabel\\n0.1G\\n20G\\n}\\nFLOPS:\\n20G\\n90\\nTOTAL'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 480}, page_content='Architecture for Classification\\ninput  \\nimage\\nlabel\\n123M\\n21M\\n}\\nNr. of parameters:\\n144M\\n91\\nTOTAL'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 481}, page_content='Architecture for Classification\\ninput  \\nimage\\nlabel\\n123M\\n21M\\n}\\nNr. of parameters:\\n144M\\n92\\nTOTAL\\nData augmentation is key to improve generalization:\\n- random translation\\n- left/right flipping\\n- scaling'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 482}, page_content='Optimization\\nSGD with momentum:  \\nLearning rate = 0.01  \\nMomentum = 0.9\\nImproving generalization by:  \\nWeight sharing (convolution)  \\nInput distortions\\nDropout = 0.5\\nWeight decay = 0.0005'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 483}, page_content='Choosing The\\nArchitecture\\nTask dependent\\nCross-validation\\n[Convolution → pooling]* + fully connected layer\\nThe more data: the more layers and the more kernels  \\nLook at the number of parameters at each layer  Look \\nat the number of flops at each layer\\nComputational resources  \\nBe creative :)'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 484}, page_content='How To Optimize\\nSGD (with momentum) usually works very well\\nPick learning rate by running on a subset of the data\\nBottou “Stochastic Gradient Tricks” Neural Networks 2012\\nStart with large learning rate and divide by 2 until loss does not diverge  \\nDecay learning rate by a factor of ~1000 or more by the end of training\\nUse\\nnon-linearity\\nInitialize parameters so that each feature across layers has  \\nsimilar variance. Avoid units in saturation.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 485}, page_content='Improving Generalization\\nWeight sharing (greatly reduce the number of parameters)\\nData augmentation (e.g., jittering, noise injection, etc.)  \\nDropout\\nHinton et al. “Improving Nns by preventing co-adaptation of feature detectors” arxiv  \\n2012\\nWeight decay (L2, L1)\\nSparsity in the hidden units\\nMulti-task (unsupervised learning)'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 486}, page_content='Good To Know\\nCheck gradients numerically by finite differences\\nVisualize features (feature maps need to be uncorrelated)  \\nand have high variance.\\nsamples\\nhidden unit\\nGood training: hidden units are sparse across samples  and \\nacross features.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 487}, page_content='Check gradients numerically by finite differences\\nVisualize features (feature maps need to be uncorrelated)  \\nand have high variance.\\nsamples\\nhidden unit\\nBad training: many hidden units ignore the input and/or  \\nexhibit strong correlations.\\nGood To Know'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 488}, page_content='Check gradients numerically by finite differences\\nVisualize features (feature maps need to be uncorrelated)  \\nand have high variance.\\nVisualize parameters\\ntoo noisy\\ntoo correlated\\nlack structure\\nGood training: learned filters exhibit structure and are uncorrelated.\\nGOOD\\nBAD\\nBAD\\nBAD\\nGood To Know'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 489}, page_content='Check gradients numerically by finite differences\\nVisualize features (feature maps need to be uncorrelated)  \\nand have high variance.\\nVisualize parameters\\nMeasure error on both training and validation set.\\nTest on a small subset of the data and check the error →0.\\nGood To Know'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 490}, page_content='What If It Does Not Work?\\nTraining diverges:\\nLearning rate may be too large → decrease learning rate  \\nBPROP is buggy → numerical gradient checking\\nParameters collapse / loss is minimized but accuracy is low\\nCheck loss function:\\nIs it appropriate for the task you want to solve?\\nDoes it have degenerate solutions? Check “pull-up” term.\\nNetwork is underperforming\\nCompute flops and nr. params. →\\nif too small, make net larger  \\nVisualize hidden units/params → fix optmization\\nNetwork is too slow\\nCompute flops and nr. params. → GPU,distrib. framework, make net  \\nsmaller'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 491}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 492}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 493}, page_content='• By applying convolution and pooling, important features will be \\nobtained (extracted) and it can reduce the complexity of dimension \\nalso the computational speed.\\n• These will be applied to ANN for classification.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 494}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 495}, page_content='RNN'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 496}, page_content='•Recurrent neural networks (RNNs) are tailored to \\nthe processing of sequential data. \\n•An RNN processes a sequence of data by\\nprocessing each element in the sequence one at \\ntime.\\n• An RNN network only has a single hidden layer, but it\\nalso has a memory buffer that stores the output of\\nthis hidden layer for one input and feeds it back into\\nthe hidden layer along with the next input from the\\nsequence.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 497}, page_content='• This recurrent flow of information means that the network\\nprocesses each input within the context generated by\\nprocessing the previous input, which in turn was processed\\nin the context of the input preceding it.\\n• In this way, the information that flows through the recurrent\\nloop encodes contextual information from (potentially) all of\\nthe preceding inputs in the sequence. This allows the\\nnetwork to maintain a memory of what it has seen\\npreviously in the sequence to help it decide what to do with\\nthe current input.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 498}, page_content='• The depth of an RNN arises from the fact that the memory\\nvector is propagated forward and evolved through each\\ninput in the sequence; as a result an RNN network is\\nconsidered as deep as a sequence is long.\\n• Figure 5.2 illustrates the architecture of an RNN and shows\\nhow information flows through the network as it processes a\\nsequence. At each time step, the network in this figure\\nreceives a vector containing two elements as input. The\\nschematic on the left of figure 5.2 (time step=1.0) shows the\\nflow of information in the network when it receives the first\\ninput in the sequence.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 499}, page_content='• This input vector is fed forward into the three neurons in the\\nhidden layer of the network. At the same time these neurons\\nalso receive whatever information is stored in the memory\\nbuffer. Because this is the initial input, the memory buffer will\\nonly contain default initialization values. Each of the neurons\\nin the hidden layer will process the input and generate an\\nactivation.\\n• The schematic in the middle of figure 5.2 (time step=1.5)\\nshows how this activation flows on through the network: the\\nactivation of each neuron is passed to the output layer where\\nit is processed to generate the output of the network, and it is\\nalso stored in the memory buffer (overwriting whatever\\ninformation was stored there).'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 500}, page_content='• The elements of the memory buffer simply store the\\ninformation written to them; they do not transform it in any\\nway. As a result, there are no weights on the edges going\\nfrom the hidden units to the buffer. There are, however,\\nweights on all the other edges in the network, including\\nthose from the memory buffer units to the neurons in the\\nhidden layer. At time step 2, the network receives the next\\ninput from the sequence, and this is passed to the hidden\\nlayer neurons along with the information stored in the\\nbuffer. This time the buffer contains the activations that\\nwere generated by the hidden neurons in response to the\\nfirst input.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 501}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 502}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 503}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 504}, page_content='• The hidden layer generates a vector of activations that is\\npassed to the output layer and is also propagated forward to\\nthe next time step along the horizontal arrows connecting\\nthe hidden states.\\n• Although RNNs can process a sequence of inputs, they\\nstruggle with the problem of vanishing gradients. This is\\nbecause training an RNN to process a sequence of inputs\\nrequires the error to be backpropagated through the entire\\nlength of the sequence.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 505}, page_content='• backpropagating the error through all the hidden layers, which in\\nturn involves repeatedly multiplying the error by the weights on\\nthe connections feeding activations from one hidden layer\\nforward to the next hidden layer. A particular problem with this\\nprocess is that it is the same set of weights that are used on all\\nthe connections between the hidden layers: each horizontal\\narrow represents the same set of connections between the\\nmemory buffer and the hidden layer, and the weights on these\\nconnections are stationary through time (i.e., they don’t change\\nfrom one time step to the next during the processing of a given\\nsequence of inputs).'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 506}, page_content='• Consequently, backpropogating an error through k time\\nsteps involves (among other multiplications) multiplying the\\nerror gradient by the same set of weights k times.\\n• This is equivalent to multiplying each error gradient by a\\nweight raised to the power of k. If this weight is less than 1,\\nthen when it is raised to a power, it diminishes at an\\nexponential rate, and consequently, the error gradient also\\ntends to diminish at an exponential rate with respect to the\\nlength of the sequence—and vanish.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 507}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 508}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 509}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 510}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 511}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 512}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 513}, page_content='LSTM'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 514}, page_content='• Long Short-Term Memory Networks (LSTM) is a deep learning,\\nsequential neural network that allows information to persist.\\n• It is a recurrent neural network (RNN) architecture widely used\\nin Deep Learning. It excels at capturing long-term dependencies,\\nmaking it ideal for sequence prediction tasks.\\n• It is a special type of Recurrent Neural Network which is capable\\nof handling the vanishing gradient problem faced by RNN.\\n• LSTM was designed by Hochreiter and Schmidhuber that\\nresolves the problem caused by traditional rnns and machine\\nlearning algorithms.\\n• Ex: while watching a video we remember the previous scene….\\n• While reading books we remember the previous chapter…'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 515}, page_content='• RNNs work like this: they remember the previous information and\\nuse it for processing the current input.\\n• The shortcoming of RNN is they cannot remember long-term\\ndependencies due to vanishing gradient.\\n• LSTMs are explicitly designed to avoid long-term dependency\\nproblems.\\n• Unlike traditional neural networks, LSTM incorporates feedback\\nconnections, allowing it to process entire sequences of data, not\\njust individual data points.\\n• This makes it highly effective in understanding and predicting\\npatterns in sequential data like time series, text, and speech.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 516}, page_content='• The LSTM network architecture consists of three parts, and each\\npart performs an individual function.\\n• These three parts of an LSTM unit are known as gates. They\\ncontrol the flow of information in and out of the memory cell or\\nlstm cell. The first gate is called Forget gate, the second gate is\\nknown as the Input gate, and the last one is the Output gate.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 517}, page_content='• The first part chooses whether the information coming from the\\nprevious timestamp is to be remembered or is irrelevant and can\\nbe forgotten.\\n• In the second part, the cell tries to learn new information from\\nthe input to this cell.\\n• In the third part, the cell passes the updated information from\\nthe current timestamp to the next timestamp. This one cycle of\\nLSTM is considered a single-time step.\\n• An LSTM unit that consists of these three gates and a memory\\ncell or lstm cell can be considered as a layer of neurons in\\ntraditional feedforward neural network, with each neuron having\\na hidden layer and a current state.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 518}, page_content='• As RNN, an LSTM also has a hidden state where H(t-1) represents\\nthe hidden state of the previous timestamp and Ht is the hidden\\nstate of the current timestamp. In addition to that, LSTM also has a\\ncell state represented by C(t-1) and C(t) for the previous and current\\ntimestamps, respectively.\\n• Here the hidden state is known as Short term memory, and the cell\\nstate is known as Long term memory. Refer to the following image.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 519}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 520}, page_content='• An example to understand how LSTM works. For example we have\\ntwo sentences separated by a full stop.\\n• The first sentence is\\n“Bob is a nice person,”.\\nand the second sentence is\\n“Dan, on the Other hand, is evil”.\\nIt is very clear, in the first sentence, we are talking about Bob, and as\\nsoon as we encounter the full stop(.), we started talking about Dan.\\n• As we move from the first sentence to the second sentence, our\\nnetwork should realize that we are no more talking about Bob. Now\\nour subject is Dan.\\n• Here, the Forget gate of the network allows it to forget about it.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 521}, page_content='Case I\\n“Bob is a nice person,”. “Dan, on the Other hand, is evil”.\\nCase II\\n“Bob knows swimming. He told me over the phone that he had\\nserved the navy for four long years.”\\nCase III\\nBob single-handedly fought the enemy and died for his country.\\nFor his contributions, he was given a brave______.”\\nFor his contributions, he was known as a brave ______.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 522}, page_content='• Forget Gate\\n• In a cell of the LSTM neural network, the first step is to decide\\nwhether we should keep the information from the previous time\\nstep or forget it. Here is the equation for forget gate.\\n• Xt: input to the current timestamp.\\n• Uf: weight associated with the input\\n• Ht-1: The hidden state of the previous timestamp\\n• Wf: It is the weight matrix associated with the hidden state'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 523}, page_content='• Later, a sigmoid function is applied to it. That will make ft a\\nnumber between 0 and 1. This ft is later multiplied with the cell\\nstate of the previous timestamp, as shown below.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 524}, page_content='• Input Gate\\n• The input gate is used to quantify the importance of the new \\ninformation carried by the input. Here is the equation of the \\ninput gate.\\n• Xt: Input at the current timestamp t\\n• Ui: weight matrix of input\\n• Ht-1: A hidden state at the previous timestamp\\n• Wi: Weight matrix of input associated with hidden state\\nAgain we have applied the sigmoid function over it. As a result, the value of I at timestamp t will\\nbe between 0 and 1.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 525}, page_content='• Let’s take another example.\\n• “Bob knows swimming. He told me over the phone that he had\\nserved the navy for four long years.”\\n• So, in both these sentences, we are talking about Bob. However, both\\ngive different kinds of information about Bob. In the first sentence,\\nwe get the information that he knows swimming. Whereas the\\nsecond sentence tells, he uses the phone and served in the navy for\\nfour years.\\n• Now just think about it, based on the context given in the first\\nsentence, which information in the second sentence is critical? First,\\nhe used the phone to tell, or he served in the navy. In this context, it\\ndoesn’t matter whether he used the phone or any other medium of\\ncommunication to pass on the information. The fact that he was in\\nthe navy is important information, and this is something we want our\\nmodel to remember for future computation. This is the task of the\\nInput gate.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 526}, page_content='• New Information\\n• The new information that needed to be passed to the cell state\\nis a function of a hidden state at the previous timestamp t-1 and\\ninput x at timestamp t.\\n• The activation function here is tanh. Due to the tanh function,\\nthe value of new information will be between -1 and 1. If the\\nvalue of Nt is negative, the information is subtracted from the\\ncell state, and if the value is positive, the information is added to\\nthe cell state at the current timestamp.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 527}, page_content='• However, the Nt won’t be added directly to the cell state. Here \\ncomes the updated equation:\\nHere, Ct-1 is the cell state at the current timestamp, and \\nthe others are the values we have calculated previously.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 528}, page_content='• Output Gate\\n• Its value will also lie between 0 and 1 because of this sigmoid \\nfunction. Now to calculate the current hidden state, we will use \\nOt and tanh of the updated cell state. As shown below.\\nIt turns out that the hidden state is a function of Long term memory (Ct) \\nand the current output.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 529}, page_content='• Here the token with the maximum score in the output is the \\nprediction. If you need to take the output of the current \\ntimestamp, just apply the SoftMax activation on hidden state Ht.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 530}, page_content='• “Bob single-handedly fought the enemy and died for his country.\\nFor his contributions, brave______.”\\n• During this task, we have to complete the second sentence.\\nNow, the minute we see the word brave, we know that we are\\ntalking about a person.\\n• In the sentence, only Bob is brave, we can not say the enemy is\\nbrave, or the country is brave. So based on the current\\nexpectation, we have to give a relevant word to fill in the blank.\\n• That word is our output, and this is the function of our Output\\ngate.'),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 531}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 532}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 26.1 (Build 25B78) Quartz PDFContext, AppendMode 1.1', 'creator': '', 'creationdate': \"D:20250225152358Z00'00'\", 'source': '../data/pdf/Deep_learning_book.pdf', 'file_path': '../data/pdf/Deep_learning_book.pdf', 'total_pages': 534, 'format': 'PDF 1.7', 'title': 'Deep Learning - Study Material', 'author': 'Amrita Vishwa Vidyapeetham', 'subject': 'Deep Learning - Study Material', 'keywords': '', 'moddate': \"D:20251118050829Z00'00'\", 'trapped': '', 'modDate': \"D:20251118050829Z00'00'\", 'creationDate': \"D:20250225152358Z00'00'\", 'page': 533}, page_content='')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/pdf\",\n",
    "    glob=\"**/*.pdf\", ## Pattern to match files  \n",
    "    loader_cls= PyMuPDFLoader, ##loader class to use\n",
    "    show_progress=False\n",
    "\n",
    ")\n",
    "\n",
    "pdf_documents=dir_loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf829a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf_documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5d2db2",
   "metadata": {},
   "source": [
    "Now we are able to know how to load the document of any type using document loader "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragenv (3.11.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
