{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2977ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/ragenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7b031fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 PDF files to process\n",
      "\n",
      "Processing: NIPS-2017-attention-is-all-you-need-Paper.pdf\n",
      "  ✓ Loaded 11 pages\n",
      "\n",
      "Processing: Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf\n",
      "  ✓ Loaded 6 pages\n",
      "\n",
      "Total documents loaded: 17\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdf's inside the directory\n",
    "\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(\"/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1\")\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d46d699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser ∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\\nof continuous representations z = (z1,...,z n). Given z, the decoder then generates an output\\nsequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices Kand V. We compute\\nthe matrix of outputs as:\\nAttention(Q,K,V ) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = ∑dk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\\nwhere headi = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈Rdmodel×dk , WK\\ni ∈Rdmodel×dk , WV\\ni ∈Rdmodel×dv\\nand WO ∈Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0,xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 ·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel )\\nPE(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto 10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='the input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k<n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 15], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+ n·d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate= d−0.5\\nmodel ·min(step_num−0.5,step_num·warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup_stepstraining steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps= 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\nResidual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\n7'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [15] 23.75\\nDeep-Att + PosUnk [32] 39.2 1.0 ·1020\\nGNMT + RL [31] 24.6 39.92 2.3 ·1019 1.4 ·1020\\nConvS2S [8] 25.16 40.46 9.6 ·1018 1.5 ·1020\\nMoE [26] 26.03 40.56 2.0 ·1019 1.2 ·1020\\nDeep-Att + PosUnk Ensemble [32] 40.4 8.0 ·1020\\nGNMT + RL Ensemble [31] 26.30 41.16 1.8 ·1020 1.1 ·1021\\nConvS2S Ensemble [8] 26.36 41.29 7.7 ·1019 1.2 ·1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.0 2.3 ·1019\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [30]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α= 0.6 [31]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='References\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n10'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf'}, page_content='Transformer-Based Approach for Detecting\\nLLM-Generated Scientific Text\\nArwa Bader\\nData Science Department\\nPrincess Sumaya University for Technology\\nAmman, Jordan\\nEmail: arw20228003@std.psut.edu.jo\\nBushra Alhijawi\\nData Science Department\\nPrincess Sumaya University for Technology\\nAmman, Jordan\\nEmail: b.alhijawi@psut.edu.jo\\nAbstract—Large Language Models (LLMs), such as ChatGPT\\nand Gemini, have become pivotal in generating human-like\\ntextual content.\\nHowever, their ability to replicate human-authored scientific\\nwriting raises concerns regarding research integrity, particularly\\nin mitigating the risks of fabrication and the dissemination of\\nfalsified data. Ensuring the credibility of scientific publications\\nnecessitates effective detection mechanisms. This study introduces\\nTransformer Catch LLM (TcLLM), a transformer-based deep\\nlearning approach for detecting LLM-authored scientific text.\\nTcLLM is designed to distinguish human-authored from LLM-\\ngenerated scientific content. The AIGTxt dataset is expanded with\\nGemini-generated text to support this task, enhancing diversity\\nin writing styles. Extensive experiments were conducted using\\nTcLLM-BERT and TcLLM-RoBERTa to evaluate classification\\nperformance. The results demonstrate that TcLLM-RoBERTa\\noutperforms TcLLM-BERT, achieving higher accuracy and lower\\nmisclassification rates in identifying LLM-generated scientific\\ntext.\\nKeywords—Large Language Models, ChatGPT, Gemini, Scien-\\ntific content, Transformer, Deep Learning.\\nI. I NTRODUCTION\\nScientific plagiarism is a non-ethical action of using some-\\none else’s ideas, findings, or words without any copyrights or\\nscholarly publications [1]. Hence, this would contradict the\\nfoundational principles and ethics sustained by the scientific\\ncommunity, simultaneously attributing recognition to individ-\\nuals who lack respected contributions [2]. In the scientific\\ncommunity, scientific papers are the core block that holds re-\\nsearchers’ insights, distinguishing masterful researchers from\\nothers.\\nLarge language models (LLMs) are at the forefront of\\nnatural language processing (NLP) advancements, designed to\\nunderstand, generate, and manipulate human language [3], [4].\\nRecently, several LLMs have become available, such as Gen-\\nerative Pre-trained Transformers (GPT) series, Bidirectional\\nEncoder Representations from Transformers (BERT), Text-\\nto-Text Transfer Transformers (T5), and Robustly Optimized\\nBERT Approach (RoBERTa). LLMs are widely employed\\nacross various sectors for generating diverse text formats,\\nwhich has raised concerns about their potential overuse and the\\nimplications for content creation in numerous industries [5].\\nFor instance, Zhang et al. [6] demonstrated that LLMs have\\nbeen effectively used in news summarization. Also, Kreps et\\nal. [7] showed that GPT-2 can generate credible text that may\\ninfluence opinions on foreign policy, thereby posing risks of\\nmedia misinformation. Also, GPT models have been used to\\ngenerate homework exercises, TOEFL writing tasks, graduate\\nrecord examinations writing tasks [8], creative short stories [9],\\nrestaurant reviews [10], the United States medical licensing\\nexam [11], and scientific content [12], [13]. Figure 1 shows\\nan example of an LLM-generated introduction in a research\\npaper published by Elsevier for Zhang et al. [12].\\nFig. 1. Example of using LLMs to generate an introduction.\\nThe manual identification of LLM-generated text by human\\nreviewers presents a significant challenge [14]. Consequently,\\ndeveloping and integrating sophisticated tools are critical to\\neffectively detecting this form of scientific plagiarism.\\nThe aforementioned challenge motivates us to develop a\\nnew LLM-generated scientific text detection method called\\nTransformer catch LLM (TcLLM). TcLLM is a transformer-\\nbased LLM-generated scientific text detection method im-\\nplemented using BERT and RoBERTa. The core objectives\\nof TcLLM are to distinguish human-authored from LLM-\\nauthored scientific text and identify the author of the text. The\\ndevelopment of TcLLM required collecting a new dataset to\\nfine-tune the transformer and evaluate the proposed method.\\nTo summarize the main contributions of the research paper:\\n• A new LLM-generated scientific text dataset, Extended-\\nAIGTxt. Extended-AIGTxt is an extended version of\\nthe AI-Generated Text (AIGTxt) dataset [15], including'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf'}, page_content='Gemini-generated scientific text in addition to human-\\nauthored and ChatGPT-generated scientific text.\\n• Developing a new LLM-generated scientific text using\\ntransformers. The developed method includes fine-tuned\\nBERT and RoBERTa models using AIGTxt-Extended.\\nThe rest of this paper is organized as follows. The next\\nsection reviews the recent related works. Section III describes\\nthe collection process of Extended-AIGTxt dataset. Section\\nIV presents details about Extended-AIGTxt and the proposed\\nmethod. Section V presents the evaluation methodology and\\ngathered results from applying the proposed method. Finally,\\nSection VI concludes the research article and presents potential\\nfuture directions.\\nII. L ITERATURE REVIEW\\nPlagiarism detection is the process of identifying content\\ngenerated or paraphrased from external sources without ci-\\ntations or giving credit to the original sources [29]. LLMs\\nhave been used to generate texts that mimic human writing\\nin different domains, such as academia [14]–[18], [21], [22],\\n[25], [26], news [28], and social media posts [27]. Table I\\nsummarizes the literature review.\\nGao et al. [14] investigated the ability of human reviewers\\nto detect the LLMs-generated abstracts. Also, they compared\\nthe performance of human reviewers with that of AI detection\\ntools, such as Plagiarism Detector, iThenticate, and GPT-2\\nOutput Detector. The study revealed a significant challenge for\\nhuman reviewers in differentiating between human-written and\\nAI-generated content. Weber et al. [16] investigated the ability\\nof 12 publicly open-source tools and two commercial sys-\\ntems (i.e., Turnitin and PlagiarismCheck) to detect ChatGPT-\\ngenerated text. Also, they created a new dataset consisting of\\nhuman-written texts authored by researchers across multiple\\ndomains to simulate undergraduate academic writing alongside\\ncorresponding texts generated using ChatGPT. Elkhatat et\\nal. [17] evaluated the effectiveness of various AI detectors\\nand classifier tools, including OpenAI, Writer, Copyleaks,\\nGPTZero, and CrossPlag, in distinguishing between human-\\nauthored content and AI-generated text. Their dataset com-\\nprised human and ChatGPT-authored text pairs. The human\\ntexts were collected from the introduction sections of lab re-\\nports written by undergraduate chemical engineering students\\nin 2018, while the AI texts were generated by ChatGPT-3.5\\nand ChatGPT-4 on the topic ”Application of Cooling Towers\\nin the Engineering Process”. The study found that the detection\\ntools exhibited higher accuracy in identifying text generated\\nby GPT-3.5 compared to GPT-4. Alamleh et al. [21] collected\\na dataset of 500 responses to 250 computer science questions,\\nencompassing essay-style answers and programming tasks in\\nC and Python, collected from computer science students and\\nGPT-3.0. Each answer was labeled according to its source.\\nThe authors trained various classifiers to detect GPT responses,\\nincluding logistic regression (LR), decision trees (DT), support\\nvector machines (SVM), neural networks (NN), and random\\nforests (RF). An et al. [22] develop a machine-generated text\\ndetection method using a similarity function. Their method cal-\\nculates the similarity scores within machine-generated essays\\nand between human-written and machine-generated essays to\\nestimate the likelihood of an essay being machine-generated.\\nMindner et al. [25] created a corpus comprising 500 articles\\non ten educational topics from various school subjects. They\\nutilized traditional and novel features across eight categories\\n(i.e., perplexity, semantic, list lookup, document, error-based,\\nreadability, AI feedback, and text vector features) to differ-\\nentiate between human-written and AI-generated texts. Also,\\nthey trained three classifiers, including XGBoost, multi-layer\\nperceptrons (MLP), and RF, to detect the AI-generated texts.\\nShijaku et al. [26] developed an XGBoost-based AI-generated\\ntext detection method. They trained and tested the proposed\\nmethod using a dataset containing 252 essays sourced from\\nTOEFL preparation materials, with half of the essays written\\nby humans and the other half generated by ChatGPT. Yu\\net al. [18] employed DistilBERT, BERT, Roberta, BERT-\\nmultilingual, and PubMedBERT to distinguish human-written\\nfrom ChatGPT-generated abstracts. Bhattacharjee et al. [28]\\nemployed ChatGPT to detect the machine-generated text gen-\\nerated by 19 different text generators. ChatGPT (GPT-3.5) and\\nChatGPT (GPT-4) are utilized for the classification task. They\\nfound that while ChatGPT (GPT-3.5) has limitations in detect-\\ning machine-generated text, it reliably identifies human-written\\ncontent. In contrast, GPT-4 tends to misclassify most texts as\\nmachine-generated, suggesting that its detection capabilities\\nmay be affected by updates designed to mitigate misuse.\\nSadiq et al. [27] focused on detecting machine-generated on\\nsocial media platforms. They proposed a FastText-based deep\\nlearning method to classify tweets as human-generated or bot-\\ngenerated. Katib et al. [20] integrated a Tunicate swarm algo-\\nrithm and a long short-term memory recurrent neural network\\n(LSTMRNN) to develop a new machine-generated text called\\nTSA-LSTMRNN. They used the Tunicate swarm algorithm\\nto optimize the parameters of the LSTMRNN model, thus\\nenhancing detection performance. The LSTMRNN detects the\\nmachine-generated text. Hu et al. [24] introduced a robust\\nAI-text detection method using adversarial learning called\\nRADAR. RADAR leverages the paraphraser and a detector\\nof adversarial learning. The paraphraser generates text that\\nmimics human writing to evade detection, while the detector\\naims to accurately identify AI-generated content. The model\\neffectively manages imbalanced data and was tested on four\\ndatasets: news summarization, question answering, creative\\nwriting prompts, and English language testing (TOEFL). Wang\\net al. [19] developed a GPT detection method using a convo-\\nlution neural network (CNN) and a self-attention mechanism\\ncalled, SeqXGPT. Chen et al. [23] introduced statistical-\\nbased deep learning detection of machine-generated text called\\nSTADEE. Their method leverages statistical text features, such\\nas probability, rank, cumulative probability, and information\\nentropy of tokens within the text, with a sequence-based deep\\nclassifier.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf'}, page_content='TABLE I\\nSUMMARY OF LITERATURE REVIEW\\nRef Year Domain Dataset Technique\\n[16] 2023 Academic Texts written by researchers and their corresponding\\nChatGPT text\\n12 publicly open-source tools and two commercial\\nsystems (Turnitin and PlagiarismCheck)\\n[17] 2023 Academic Introduction sections of lab reports and texts gen-\\nerated by chatGPT-3.5 and chatGPT-4 on the topic\\n”Application of Cooling Towers in the Engineering\\nProcess”\\nAI detectors and classifier tools, namely OpenAI,\\nWriter, Copyleaks, GPTZero, and CrossPlag.\\n[14] 2023 Academic 50 human-written abstracts and 50 ChatGPT-\\ngenerated abstracts\\nManual and AI detection tools\\n[18] 2023 Academic 15,395 human-written abstracts and 35,304\\nChatGPT-generated abstracts\\nDistilbert, BERT, Roberta, BERT-multilingual, and\\nPubMedBERT\\n[19] 2023 Multi-domain Corpus SeqXGPT\\n[20] 2023 Multi-domain Text TSA-LSTM-RNN model\\n[21] 2023 Academic 500 responses from both students and GPT for 250\\nquestions\\nLR, DT, SVM, NN, and RF\\n[22] 2023 Academic 12,978 essays written by seventh to tenth graders Statistical approach\\n[23] 2023 Multi-domain HC3, THUCNews, and CPM-CNews Statistical features with deep learning\\n[24] 2023 Multi-domain Corpus Adversarial Learning\\n[25] 2023 Academic Corpus XGBoost, Random Forest, MLP\\n[26] 2023 Academic TOEFL preparation materials XGBoost and SHAP values\\n[27] 2023 Social media TweepFake Deep learning with FastText embeddings\\n[28] 2024 News TuringBench ChatGPT 3.5 and ChatGPT 4\\n[15] 2024 Academic AIGTxt Statistical features with CNN and MLP\\nFig. 2. Sample Reponse of Gemini.\\nIII. E XTENDED -AIGT XT DATASET\\nExtended-AIGTxt is an extension of AIGTxt [15] that\\nincludes human-written text alongside content generated by\\nChatGPT and Gemini. The ultimate goal of extending AIGTxt\\nis to provide the scientific community with a public benchmark\\ndataset to develop machine-generated scientific text detection\\nmethods that can distinguish between content authored by\\nhumans and various large language models (LLMs). The\\ncollection procedure of Extended-AIGTxt is similar to that\\nfollowed when collecting the AIGTxt dataset. Collecting\\nExtended-AIGTxt involves generating the Gemini-written text\\ncorresponding to the human-written text of AIGTxt. Diverse\\nprompts are employed to gather Gemini-generated text, such\\nas ”Rewrite the following paragraphs more professionally and\\nwith an IEEE scientific citation:”. Figure 2 shows a sample\\nGemini response for the mentioned query.\\nThe collected Gemini-generated text is combined with\\nAIGTxt to create Extended-AIGTxt. Extended-AIGTxt con-\\ntains three classes, as shown in Figure 3.\\nFig. 3. Sample of Extended-AIGTxt Dataset.\\n• Human-generated text. Scientific content written by hu-\\nmans and collected from scholarly articles published in\\nacademic journals.\\n• ChatGPT-generated text. The scientific content was gen-\\nerated using ChatGPT 3.5.\\n• Gemini-generated text. The scientific content was gener-\\nated using Google Gemini.\\nTABLE II\\nSTATISTICAL SUMMARY OF EXTENDED -AIGT XT DATASET.\\nMeasure Value\\nTotal number of observations 3000\\nNumber of records per class 1000\\nNumber of classes 3\\nNumber of Topics 10\\nTable II shows a statistical summary of the Extended-\\nAIGTxt dataset. Extended-AIGTxt includes 1000 records per\\nclass distributed across ten distinct topics. Table III presents a\\nstatistical summary per class. It is worth mentioning that these\\nstatistics are computed after applying preprocessing steps,\\nincluding lemmatization and excluding the stopwords and\\ncitations. Gemini class has the highest word count (126,038),'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf'}, page_content='TABLE III\\nSTATISTICAL SUMMARY OF EXTENDED -AIGT XT PER CLASS .\\nMeasure Human ChatGPT Gemini\\nTotal number of words 113,985 117,076 126,038\\nTotal number of unique words 12,980 12,071 12,117\\nAverage paragraph length (word) 113.98 117.08 126.05\\nAverage lexical Diversity 0.7699 0.7932 0.7885\\nindicating a tendency for more verbose or detailed responses\\nthan other classes. The human class includes the highest\\nnumber of unique words (12,980), indicating a richer vocab-\\nulary. ChatGPT class has the highest average lexical diversity\\n(0.7932), indicating more varied vocabulary usage.\\nIV. T RANSFORMER CATCH LLM M ETHOD\\nThe ultimate objective of this research is to detect machine-\\ngenerated scientific textual content. This study proposes a\\nnew LLM-generated scientific text detection method using\\ntransformers called TcLLM. TcLLM employs BERT and\\nRoBERTa transformers to capture long-range dependencies\\nand contextual relationships within scientific text.\\nTcLLM involves two steps: (1) data preparation and (2)\\nfine-tuning the transformer. The data preparation process for\\nTcLLM is summarized below. Table IV shows an example of\\nthe data preparation steps for BERT and RoBERTa.\\n• Tokenization. TcLLM converts the input text into tokens\\n(i.e., words). This step enables the model to understand\\nand process the text by treating each token as a discrete\\nunit. TcLLM employs BERTTokenizer and RobertaTo-\\nkenizer. BERTTokenizer tokenizes the text using Word-\\nPiece tokenization, breaking words into smaller subword\\nunits in the model’s vocabulary.\\n• Adding Special Tokens. The tokenizer adds special tokens\\nto the input text to convey additional information to the\\nmodel, including the structure and context of the input\\ndata. For instance, BERTTokenizer adds ”[CLS]” to mark\\nthe start of a sentence and ”[SEP]” to separate multiple\\nsentences. RobertaTokenizer uses ”<s>” to mark the start\\nof a sentence and ” </s>” to mark the end of a sequence.\\n• Padding. The encoded text is padded or truncated to a\\nsingle, fixed length. Truncation removes tokens from the\\ntext end that exceed a certain maximum length, while\\npadding adds [PAD] tokens to shorter sequences.\\n• Encoding. Once the input text is tokenized, each token\\nis mapped to its corresponding numerical ID in the\\nvocabulary. RobertaTokenizer uses Byte Pair Encoding\\nto convert text into a sequence of numbers suitable for\\nfeeding into the RoBERTa language model.\\n• Attention Masking. The tokenizer generates an attention\\nmask to help the model differentiate between actual input\\nand padding tokens during training and inference.\\nTcLLM passes the prepared input sequence into the BERT\\nand RoBERTa models. These models use a series of trans-\\nformer layers to process the input text, where each layer\\nfocuses on different parts of the input and combines infor-\\nmation to create contextualized word representations. These\\nrepresentations capture complex semantic and syntactic de-\\ntails. BERT and RoBERTa can handle text inputs of varying\\nlengths and complexities, making them highly effective tools\\nfor natural language understanding and processing across a\\nwide range of domains and applications. After fine-tuning,\\nthe BERT or RoBERTa model processes input scientific text\\nthrough its transformer architecture, generating contextualized\\ntext representations passed into a classification layer. This\\nlayer predicts the author of the scientific text, whether it is\\nhuman, ChatGPT, or Gemini, based on the features learned\\nduring training.\\nV. E XPERIMENT DESIGN AND RESULTS\\nDifferent experiments are conducted to evaluate the per-\\nformance of the proposed method. The assessment relies on\\nfour key evaluation metrics: accuracy, precision, recall, and\\nF1-score. Accuracy represents the percentage of correctly\\nclassified texts. Precision measures the proportion of correctly\\nclassified instances among all predicted instances of a given\\nclass. Recall quantifies the proportion of correctly classified\\ninstances relative to the total actual instances of that class.\\nFinally, the F1-score is the harmonic mean of precision and\\nrecall, providing a balanced measure of model performance.\\nSeven experiments were conducted to evaluate the per-\\nformance of the LLM-generated text identification models.\\nThe experiments varied in the number of training epochs\\nand data-splitting strategies to assess the model’s robustness\\nand generalization capabilities. Specifically, three experiments\\n(Exp. 1–3) utilized an 80:20 train-test split with epoch values\\nset at 10, 20, and 30, respectively. Experiments 4 and 6\\nemployed a 5-fold cross-validation scheme with 10 and 20\\nepochs to further examine performance under cross-validation\\nsettings. Experiments 5 and 7 also applied a 10-fold cross-\\nvalidation approach with 10 and 20 epochs, respectively. In\\nall experiments, Adam optimizer has been used. Table V\\nsummarizes the setup of the evaluation experiments.\\nTable VI summarizes the performance metrics obtained\\nfrom TcLLM using two different implementations: BERT\\nand RoBERTa, across seven experimental settings. In general,\\nRoBERTa consistently outperforms BERT in all experiments,\\ndemonstrating superior classification performance.\\nIn experiments using a split of 80:20 train-test (Exp. 1-\\n3), RoBERTa achieves higher accuracy and F1 scores than\\nBERT, with the best performance recorded in Exp. 2, where\\nRoBERTa achieves an accuracy of 0.9367 and an F1 score\\nof 0.9356, compared to BERT’s 0.8283 accuracy and 0.8230\\nF1 score. Similarly, in cross-validation settings, RoBERTa\\noutperforms BERT in 5-fold (Exp. 4, 6) and 10-fold (Exp. 5,\\n7) evaluations. In particular, Exp. 4, which used 5-fold cross-\\nvalidation with 10 epochs, shows a significant performance\\ngap, with RoBERTa achieving an F1 score of 0.9797 compared\\nto BERT’s 0.8797.\\nVI. C ONCLUSION AND FUTURE WORKS\\nLLMs, such as ChatGPT and Gemini, have significantly ad-\\nvanced the generation of scientific text, blurring the distinction'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf'}, page_content='TABLE IV\\nEXAMPLE OF TEXT PREPROCESSING USING BERTT OKENIZER AND ROBERTATOKENIZER\\nInput Text: ”Natural language processing is amazing”\\nPreprocessing Step BERT RoBERTa\\nTokenization [’natural’, ’language’, ’processing’, ’is’, ’amazing’] [’Natural’, ’language’, ’processing’, ’is’, ’amazing’]\\nAdding Special Tokens [’[CLS]’, ’Natural’, ’language’, ’processing’, ’is’,\\n’amazing’, ’[SEP]’]\\n[’<s>’, ’Natural’, ’language’, ’processing’, ’is’,\\n’amazing’, ’</s>’]\\nPadding [’[CLS]’, ’Natural’, ’language’, ’processing’, ’is’,\\n’amazing’, ’[SEP]’, ’[PAD]’, ’[PAD]’, ’[PAD]’\\n[’<s>’, ’Natural’, ’language’, ’processing’, ’is’,\\n’amazing’, ’</s>’, ’<PAD >’, ’<PAD >’, ’<PAD >’\\nEncoding [101, 2300, 2203, 2324, 2003, 2025, 102, 0, 0, 0] [0, 1234, 2345, 3456, 4567, 5678, 2, 1, 1, 1]\\nAttention Masking [1, 1, 1, 1, 1, 1, 1, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\\nTABLE V\\nSUMMARY OF EVALUATION EXPERIMENTS\\nExp. Epochs Data Splitting\\n1 10 80:20\\n2 20 80:20\\n3 30 80:20\\n4 10 5-fold\\n5 10 10-fold\\n6 20 5-fold\\n7 20 10-fold\\nbetween human and machine-authored content. This study pro-\\nposed Transformer catch LLM (TcLLM), a transformer-based\\nmethod utilizing fine-tuned BERT and RoBERTa models to\\ndetect LLM-generated scientific text. Additionally, the AIGTxt\\ndataset is expanded to include text generated by Gemini,\\nenriching the dataset with diverse writing styles from multiple\\nLLMs.\\nMultiple experiments were conducted using both BERT and\\nRoBERTa implementations to evaluate TcLLM. The results\\nindicate that TcLLM-RoBERTa outperforms TcLLM-BERT,\\nachieving higher accuracy and reduced misclassification rates\\nin distinguishing between human- and LLM-generated text.\\nFor future work, we will explore additional LLMs, including\\nClaude-4 and LLaMA 3, using a larger dataset to assess their\\nability to generate scientific content. We will also investi-\\ngate advanced transformers like BigBird for processing long\\ndocuments and Longformer for efficient attention in extended\\nsequences.\\nREFERENCES\\n[1] F. Khaled and M. S. H. Al-Tamimi, “Plagiarism detection methods and\\ntools: An overview,” Iraqi Journal of Science , pp. 2771–2783, 2021.\\n[2] I. Masic, “Plagiarism in scientific research and publications and how to\\nprevent it,” Materia socio-m ´edica, vol. 26, no. 2, p. 141, 2014.\\n[3] Y . Chang, X. Wang, J. Wang, Y . Wu, L. Yang, K. Zhu, H. Chen, X. Yi,\\nC. Wang, Y . Wang et al. , “A survey on evaluation of large language\\nmodels,” ACM Transactions on Intelligent Systems and Technology ,\\nvol. 15, no. 3, pp. 1–45, 2024.\\n[4] H. Zhao, H. Chen, F. Yang, N. Liu, H. Deng, H. Cai, S. Wang, D. Yin,\\nand M. Du, “Explainability for large language models: A survey,” ACM\\nTransactions on Intelligent Systems and Technology , vol. 15, no. 2, pp.\\n1–38, 2024.\\n[5] A. Koubaa, W. Boulila, L. Ghouti, A. Alzahem, and S. Latif, “Exploring\\nchatgpt capabilities and limitations: A survey,” IEEE Access, 2023.\\n[6] T. Zhang, F. Ladhak, E. Durmus, P. Liang, K. McKeown, and T. B.\\nHashimoto, “Benchmarking large language models for news summa-\\nrization,” Transactions of the Association for Computational Linguistics,\\nvol. 12, pp. 39–57, 2024.\\n[7] S. Kreps, R. M. McCain, and M. Brundage, “All the news that’s fit to\\nfabricate: Ai-generated text as a tool of media misinformation,” Journal\\nof Experimental Political Science , vol. 9, no. 1, p. 104–117, 2022.\\n[8] Y . Liu, Z. Zhang, W. Zhang, S. Yue, X. Zhao, X. Cheng, Y . Zhang, and\\nH. Hu, “Argugpt: evaluating, understanding and identifying argumenta-\\ntive essays generated by gpt models,” arXiv preprint arXiv:2304.07666,\\n2023.\\n[9] A. Yuan, A. Coenen, E. Reif, and D. Ippolito, “Wordcraft: Story\\nwriting with large language models,” in 27th International Conference\\non Intelligent User Interfaces , ser. IUI ’22. New York, NY , USA:\\nAssociation for Computing Machinery, 2022, p. 841–852. [Online].\\nAvailable: https://doi.org/10.1145/3490099.3511105\\n[10] S. Mitrovi ´c, D. Andreoletti, and O. Ayoub, “Chatgpt or human? detect\\nand explain. explaining decisions of machine learning model for de-\\ntecting short chatgpt-generated text,” arXiv preprint arXiv:2301.13852 ,\\n2023.\\n[11] T. H. Kung, M. Cheatham, A. Medenilla, C. Sillos, L. De Leon,\\nC. Elepa ˜no, M. Madriaga, R. Aggabao, G. Diaz-Candido, J. Maningo\\net al. , “Performance of chatgpt on usmle: potential for ai-assisted\\nmedical education using large language models,” PLoS digital health ,\\nvol. 2, no. 2, p. e0000198, 2023.\\n[12] M. Zhang, L. Wu, T. Yang, B. Zhu, and Y . Liu, “The three-dimensional\\nporous mesh structure of cu-based metal-organic-framework-aramid\\ncellulose separator enhances the electrochemical performance of lithium\\nmetal anode batteries.” Surfaces and Interfaces , p. 104081, 2024.\\n[13] ChatGPT and A. Zhavoronkov, “Rapamycin in the context of pascal’s\\nwager: generative pre-trained transformer perspective,” Oncoscience,\\nvol. 9, pp. 82–84, 2022.\\n[14] C. A. Gao, F. M. Howard, N. S. Markov, E. C. Dyer, S. Ramesh,\\nY . Luo, and A. T. Pearson, “Comparing scientific abstracts generated by\\nchatgpt to real abstracts with detectors and blinded human reviewers,”\\nNPJ digital medicine , vol. 6, no. 1, p. 75, 2023.\\n[15] B. Alhijawi, R. Jarrar, A. AbuAlRub, and A. Bader, “Deep learning de-\\ntection method for large language models-generated scientific content,”\\nNeural Computing and Applications , 2024.\\n[16] D. Weber-Wulff, A. Anohina-Naumeca, S. Bjelobaba, T. Folt `ynek,\\nJ. Guerrero-Dib, O. Popoola, P. ˇSigut, and L. Waddington, “Testing\\nof detection tools for ai-generated text,” International Journal for\\nEducational Integrity, vol. 19, no. 1, p. 26, 2023.\\n[17] A. M. Elkhatat, K. Elsaid, and S. Almeer, “Evaluating the efficacy\\nof ai content detection tools in differentiating between human and ai-\\ngenerated text,” International Journal for Educational Integrity , vol. 19,\\nno. 1, p. 17, 2023.\\n[18] P. Yu, J. Chen, X. Feng, and Z. Xia, “Cheat: A large-scale dataset for\\ndetecting chatgpt-written abstracts,” arXiv preprint arXiv:2304.12008 ,\\n2023.\\n[19] P. Wang, L. Li, K. Ren, B. Jiang, D. Zhang, and X. Qiu, “Se-\\nqxgpt: Sentence-level ai-generated text detection,” arXiv preprint\\narXiv:2310.08903, 2023.\\n[20] I. Katib, F. Y . Assiri, H. A. Abdushkour, D. Hamed, and M. Ragab,\\n“Differentiating chat generative pretrained transformer from humans:\\nDetecting chatgpt-generated text and human text using machine learn-\\ning,” Mathematics, vol. 11, no. 15, p. 3400, 2023.\\n[21] H. Alamleh, A. A. S. AlQahtani, and A. ElSaid, “Distinguishing\\nhuman-written and chatgpt-generated text using machine learning,” in\\n2023 Systems and Information Engineering Design Symposium (SIEDS),\\n2023, pp. 154–158.\\n[22] R. An, Y . Yang, F. Yang, and S. Wang, “Use prompt to differentiate text'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf'}, page_content='TABLE VI\\nRESULTS OF TCLLM.\\nExperiment Model Accuracy Recall Precision F1 Score\\nExp 1 BERT 0.8317 0.8243 0.8297 0.8225\\nRoBERTa 0.8917 0.8988 0.9060 0.8897\\nExp 2 BERT 0.8283 0.8232 0.8236 0.8230\\nRoBERTa 0.9367 0.9377 0.9353 0.9356\\nExp 3 BERT 0.7983 0.7863 0.8005 0.7800\\nRoBERTa 0.9400 0.9398 0.9385 0.9389\\nExp 4 BERT 0.8812 0.8812 0.8804 0.8797\\nRoBERTa 0.9250 0.9293 0.9291 0.9240\\nExp 5 BERT 0.7934 0.7934 0.8219 0.7935\\nRoBERTa 0.9287 0.9287 0.9368 0.9281\\nExp 6 BERT 0.7934 0.7934 0.8219 0.7935\\nRoBERTa 0.9334 0.9334 0.9355 0.9332\\nExp 7 BERT 0.8018 0.8018 0.82647 0.80414\\nRoBERTa 0.9217 0.9256 0.9249 0.9205\\ngenerated by chatgpt and humans,” Machine Learning with Applications,\\nvol. 14, p. 100497, 2023.\\n[23] Z. Chen and H. Liu, “Stadee: Statistics-based deep detection of machine\\ngenerated text,” in International Conference on Intelligent Computing .\\nSpringer, 2023, pp. 732–743.\\n[24] X. Hu, P.-Y . Chen, and T.-Y . Ho, “Radar: Robust ai-text detection\\nvia adversarial learning,” Advances in Neural Information Processing\\nSystems, vol. 36, pp. 15 077–15 095, 2023.\\n[25] L. Mindner, T. Schlippe, and K. Schaaff, “Classification of human-and\\nai-generated texts: Investigating features for chatgpt,” in International\\nConference on Artificial Intelligence in Education Technology. Springer,\\n2023, pp. 152–170.\\n[26] R. Shijaku and E. Canhasi, “Chatgpt generated text detection,” Pub-\\nlisher: Unpublished, 2023.\\n[27] S. Sadiq, T. Aljrees, and S. Ullah, “Deepfake detection on social\\nmedia: Leveraging deep learning and fasttext embeddings for identifying\\nmachine-generated tweets,” IEEE Access, 2023.\\n[28] A. Bhattacharjee and H. Liu, “Fighting fire with fire: can chatgpt detect\\nai-generated text?” ACM SIGKDD Explorations Newsletter , vol. 25,\\nno. 2, pp. 14–21, 2024.\\n[29] T. Folt `ynek, N. Meuschke, and B. Gipp, “Academic plagiarism detec-\\ntion: a systematic literature review,” ACM Computing Surveys (CSUR) ,\\nvol. 52, no. 6, pp. 1–42, 2019.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af30736e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(all_pdf_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a632011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "SECTION_PATTERNS = {\n",
    "    \"abstract\": r\"\\babstract\\b\",\n",
    "    \"introduction\": r\"\\bintroduction\\b\",\n",
    "    \"methodology\": r\"\\b(methodology|methods|materials and methods)\\b\",\n",
    "    \"results\": r\"\\bresults\\b\",\n",
    "    \"discussion\": r\"\\bdiscussion\\b\",\n",
    "    \"conclusion\": r\"\\b(conclusion|conclusions)\\b\",\n",
    "    \"references\": r\"\\breferences\\b\"\n",
    "}\n",
    "\n",
    "def detect_sections_with_context(text: str, last_section: str) -> str:\n",
    "    text_lower = text.lower()\n",
    "    for section, pattern in SECTION_PATTERNS.items():\n",
    "        if re.search(pattern, text_lower):\n",
    "            return section\n",
    "    return last_section\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2145d42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents_section_aware(documents):\n",
    "    \"\"\"\n",
    "    Chunk research papers for RAG with section-aware metadata\n",
    "    \"\"\"\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,        # ✅ ideal for research papers\n",
    "        chunk_overlap=80,      # ✅ preserves context\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    all_chunks = []\n",
    "\n",
    "    for doc in documents:\n",
    "        # Detect section BEFORE splitting\n",
    "        section = detect_section(doc.page_content)\n",
    "\n",
    "        # Split into chunks\n",
    "        chunks = text_splitter.split_text(doc.page_content)\n",
    "\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            all_chunks.append({\n",
    "                \"page_content\": chunk,\n",
    "                \"metadata\": {\n",
    "                    **doc.metadata,\n",
    "                    \"section\": section,\n",
    "                    \"chunk_id\": idx\n",
    "                }\n",
    "            })\n",
    "\n",
    "    print(f\"✅ Created {len(all_chunks)} RAG-optimized chunks\")\n",
    "\n",
    "    # Preview one chunk\n",
    "    if all_chunks:\n",
    "        print(\"\\n🔍 Example chunk:\")\n",
    "        print(all_chunks[0][\"page_content\"][:300], \"...\")\n",
    "        print(\"Metadata:\", all_chunks[0][\"metadata\"])\n",
    "\n",
    "    return all_chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ffd81fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents_section_aware(documents):\n",
    "    \"\"\"\n",
    "    Correct, production-grade section-aware chunking for research papers\n",
    "    \"\"\"\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=80,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    all_chunks = []\n",
    "    global_chunk_id = 0\n",
    "\n",
    "    for doc in documents:\n",
    "        last_section = doc.metadata.get(\"section\", \"unknown\")\n",
    "\n",
    "        # Split page into chunks\n",
    "        chunks = text_splitter.split_text(doc.page_content)\n",
    "\n",
    "        for chunk in chunks:\n",
    "            # Detect or inherit section\n",
    "            section = detect_sections_with_context(chunk, last_section)\n",
    "            last_section = section\n",
    "\n",
    "            all_chunks.append(\n",
    "                Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\n",
    "                        **doc.metadata,\n",
    "                        \"section\": section,\n",
    "                        \"chunk_id\": global_chunk_id\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "            global_chunk_id += 1\n",
    "\n",
    "    print(f\"✅ Created {len(all_chunks)} RAG-optimized chunks\")\n",
    "\n",
    "    if all_chunks:\n",
    "        print(\"\\n🔍 Example chunk:\")\n",
    "        print(all_chunks[0].page_content[:300], \"...\")\n",
    "        print(\"Metadata:\", all_chunks[0].metadata)\n",
    "\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8dee724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created 150 RAG-optimized chunks\n",
      "\n",
      "🔍 Example chunk:\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗†\n",
      "University of Toronto\n",
      "aid ...\n",
      "Metadata: {'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'abstract', 'chunk_id': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'abstract', 'chunk_id': 0}, page_content='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser ∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'abstract', 'chunk_id': 1}, page_content='lukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 2}, page_content='entirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'introduction', 'chunk_id': 3}, page_content='training for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'introduction', 'chunk_id': 4}, page_content='efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'introduction', 'chunk_id': 5}, page_content='attention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 6}, page_content='implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 7}, page_content='Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 8}, page_content='signiﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 9}, page_content='the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 10}, page_content='translation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 11}, page_content='the number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 12}, page_content='described in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 13}, page_content='End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 14}, page_content='aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\\nof continuous representations z = (z1,...,z n). Given z, the decoder then generates an output'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 15}, page_content='sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 16}, page_content='respectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 17}, page_content='Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 18}, page_content='layers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 19}, page_content='sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 20}, page_content='where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 21}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices Kand V. We compute'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 22}, page_content='the matrix of outputs as:\\nAttention(Q,K,V ) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 23}, page_content='much faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 24}, page_content='extremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 25}, page_content='queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 26}, page_content='4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = ∑dk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 27}, page_content='MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\\nwhere headi = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈Rdmodel×dk , WK\\ni ∈Rdmodel×dk , WV\\ni ∈Rdmodel×dv\\nand WO ∈Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 28}, page_content='is similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 29}, page_content='typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 30}, page_content='all positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 31}, page_content='connected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0,xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 32}, page_content='The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 33}, page_content='our model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 34}, page_content='tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 35}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 ·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 36}, page_content='Recurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel )\\nPE(pos,2i+1) = cos(pos/100002i/dmodel )'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 37}, page_content='PE(pos,2i) = sin(pos/100002i/dmodel )\\nPE(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto 10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of\\nPEpos.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 38}, page_content='PEpos.\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 39}, page_content='tional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 40}, page_content='be parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 41}, page_content='and output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 42}, page_content='computational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 43}, page_content='the input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k<n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 15], increasing the length of the longest paths'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 44}, page_content='between any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+ n·d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 45}, page_content='the approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 46}, page_content='5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 47}, page_content='vocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 48}, page_content='trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate= d−0.5\\nmodel ·min(step_num−0.5,step_num·warmup_steps−1.5) (3)'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 49}, page_content='lrate= d−0.5\\nmodel ·min(step_num−0.5,step_num·warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup_stepstraining steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps= 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\nResidual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 50}, page_content='sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\n7'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 51}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [15] 23.75\\nDeep-Att + PosUnk [32] 39.2 1.0 ·1020\\nGNMT + RL [31] 24.6 39.92 2.3 ·1019 1.4 ·1020\\nConvS2S [8] 25.16 40.46 9.6 ·1018 1.5 ·1020\\nMoE [26] 26.03 40.56 2.0 ·1019 1.2 ·1020\\nDeep-Att + PosUnk Ensemble [32] 40.4 8.0 ·1020'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 52}, page_content='Deep-Att + PosUnk Ensemble [32] 40.4 8.0 ·1020\\nGNMT + RL Ensemble [31] 26.30 41.16 1.8 ·1020 1.1 ·1021\\nConvS2S Ensemble [8] 26.36 41.29 7.7 ·1019 1.2 ·1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.0 2.3 ·1019\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [30]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 53}, page_content='6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 54}, page_content='surpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 55}, page_content='dropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α= 0.6 [31]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 56}, page_content='inference to input length + 50, but terminate early when possible [31].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU 5.\\n6.2 Model Variations'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 57}, page_content='single-precision ﬂoating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 58}, page_content='checkpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 59}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 60}, page_content='(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 61}, page_content='big 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 62}, page_content='sinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 63}, page_content='on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 64}, page_content='to investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'references', 'chunk_id': 65}, page_content='References\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'references', 'chunk_id': 66}, page_content='machine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'references', 'chunk_id': 67}, page_content='machine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'references', 'chunk_id': 68}, page_content='tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'references', 'chunk_id': 69}, page_content='recurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'references', 'chunk_id': 70}, page_content='on Learning Representations (ICLR), 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'references', 'chunk_id': 71}, page_content='[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n10'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 72}, page_content='[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 73}, page_content='summarization. arXiv preprint arXiv:1705.04304, 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 74}, page_content='and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 75}, page_content='networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 76}, page_content='Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 77}, page_content='fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'abstract', 'chunk_id': 78}, page_content='Transformer-Based Approach for Detecting\\nLLM-Generated Scientific Text\\nArwa Bader\\nData Science Department\\nPrincess Sumaya University for Technology\\nAmman, Jordan\\nEmail: arw20228003@std.psut.edu.jo\\nBushra Alhijawi\\nData Science Department\\nPrincess Sumaya University for Technology\\nAmman, Jordan\\nEmail: b.alhijawi@psut.edu.jo\\nAbstract—Large Language Models (LLMs), such as ChatGPT\\nand Gemini, have become pivotal in generating human-like\\ntextual content.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'abstract', 'chunk_id': 79}, page_content='and Gemini, have become pivotal in generating human-like\\ntextual content.\\nHowever, their ability to replicate human-authored scientific\\nwriting raises concerns regarding research integrity, particularly\\nin mitigating the risks of fabrication and the dissemination of\\nfalsified data. Ensuring the credibility of scientific publications\\nnecessitates effective detection mechanisms. This study introduces\\nTransformer Catch LLM (TcLLM), a transformer-based deep'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 80}, page_content='Transformer Catch LLM (TcLLM), a transformer-based deep\\nlearning approach for detecting LLM-authored scientific text.\\nTcLLM is designed to distinguish human-authored from LLM-\\ngenerated scientific content. The AIGTxt dataset is expanded with\\nGemini-generated text to support this task, enhancing diversity\\nin writing styles. Extensive experiments were conducted using\\nTcLLM-BERT and TcLLM-RoBERTa to evaluate classification\\nperformance. The results demonstrate that TcLLM-RoBERTa'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 81}, page_content='performance. The results demonstrate that TcLLM-RoBERTa\\noutperforms TcLLM-BERT, achieving higher accuracy and lower\\nmisclassification rates in identifying LLM-generated scientific\\ntext.\\nKeywords—Large Language Models, ChatGPT, Gemini, Scien-\\ntific content, Transformer, Deep Learning.\\nI. I NTRODUCTION\\nScientific plagiarism is a non-ethical action of using some-\\none else’s ideas, findings, or words without any copyrights or\\nscholarly publications [1]. Hence, this would contradict the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 82}, page_content='scholarly publications [1]. Hence, this would contradict the\\nfoundational principles and ethics sustained by the scientific\\ncommunity, simultaneously attributing recognition to individ-\\nuals who lack respected contributions [2]. In the scientific\\ncommunity, scientific papers are the core block that holds re-\\nsearchers’ insights, distinguishing masterful researchers from\\nothers.\\nLarge language models (LLMs) are at the forefront of\\nnatural language processing (NLP) advancements, designed to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 83}, page_content='natural language processing (NLP) advancements, designed to\\nunderstand, generate, and manipulate human language [3], [4].\\nRecently, several LLMs have become available, such as Gen-\\nerative Pre-trained Transformers (GPT) series, Bidirectional\\nEncoder Representations from Transformers (BERT), Text-\\nto-Text Transfer Transformers (T5), and Robustly Optimized\\nBERT Approach (RoBERTa). LLMs are widely employed\\nacross various sectors for generating diverse text formats,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 84}, page_content='across various sectors for generating diverse text formats,\\nwhich has raised concerns about their potential overuse and the\\nimplications for content creation in numerous industries [5].\\nFor instance, Zhang et al. [6] demonstrated that LLMs have\\nbeen effectively used in news summarization. Also, Kreps et\\nal. [7] showed that GPT-2 can generate credible text that may\\ninfluence opinions on foreign policy, thereby posing risks of\\nmedia misinformation. Also, GPT models have been used to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'introduction', 'chunk_id': 85}, page_content='media misinformation. Also, GPT models have been used to\\ngenerate homework exercises, TOEFL writing tasks, graduate\\nrecord examinations writing tasks [8], creative short stories [9],\\nrestaurant reviews [10], the United States medical licensing\\nexam [11], and scientific content [12], [13]. Figure 1 shows\\nan example of an LLM-generated introduction in a research\\npaper published by Elsevier for Zhang et al. [12].\\nFig. 1. Example of using LLMs to generate an introduction.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'introduction', 'chunk_id': 86}, page_content='Fig. 1. Example of using LLMs to generate an introduction.\\nThe manual identification of LLM-generated text by human\\nreviewers presents a significant challenge [14]. Consequently,\\ndeveloping and integrating sophisticated tools are critical to\\neffectively detecting this form of scientific plagiarism.\\nThe aforementioned challenge motivates us to develop a\\nnew LLM-generated scientific text detection method called\\nTransformer catch LLM (TcLLM). TcLLM is a transformer-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'introduction', 'chunk_id': 87}, page_content='Transformer catch LLM (TcLLM). TcLLM is a transformer-\\nbased LLM-generated scientific text detection method im-\\nplemented using BERT and RoBERTa. The core objectives\\nof TcLLM are to distinguish human-authored from LLM-\\nauthored scientific text and identify the author of the text. The\\ndevelopment of TcLLM required collecting a new dataset to\\nfine-tune the transformer and evaluate the proposed method.\\nTo summarize the main contributions of the research paper:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'introduction', 'chunk_id': 88}, page_content='To summarize the main contributions of the research paper:\\n• A new LLM-generated scientific text dataset, Extended-\\nAIGTxt. Extended-AIGTxt is an extended version of\\nthe AI-Generated Text (AIGTxt) dataset [15], including'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 89}, page_content='Gemini-generated scientific text in addition to human-\\nauthored and ChatGPT-generated scientific text.\\n• Developing a new LLM-generated scientific text using\\ntransformers. The developed method includes fine-tuned\\nBERT and RoBERTa models using AIGTxt-Extended.\\nThe rest of this paper is organized as follows. The next\\nsection reviews the recent related works. Section III describes\\nthe collection process of Extended-AIGTxt dataset. Section\\nIV presents details about Extended-AIGTxt and the proposed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 90}, page_content='IV presents details about Extended-AIGTxt and the proposed\\nmethod. Section V presents the evaluation methodology and\\ngathered results from applying the proposed method. Finally,\\nSection VI concludes the research article and presents potential\\nfuture directions.\\nII. L ITERATURE REVIEW\\nPlagiarism detection is the process of identifying content\\ngenerated or paraphrased from external sources without ci-\\ntations or giving credit to the original sources [29]. LLMs'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 91}, page_content='tations or giving credit to the original sources [29]. LLMs\\nhave been used to generate texts that mimic human writing\\nin different domains, such as academia [14]–[18], [21], [22],\\n[25], [26], news [28], and social media posts [27]. Table I\\nsummarizes the literature review.\\nGao et al. [14] investigated the ability of human reviewers\\nto detect the LLMs-generated abstracts. Also, they compared\\nthe performance of human reviewers with that of AI detection'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 92}, page_content='the performance of human reviewers with that of AI detection\\ntools, such as Plagiarism Detector, iThenticate, and GPT-2\\nOutput Detector. The study revealed a significant challenge for\\nhuman reviewers in differentiating between human-written and\\nAI-generated content. Weber et al. [16] investigated the ability\\nof 12 publicly open-source tools and two commercial sys-\\ntems (i.e., Turnitin and PlagiarismCheck) to detect ChatGPT-\\ngenerated text. Also, they created a new dataset consisting of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 93}, page_content='generated text. Also, they created a new dataset consisting of\\nhuman-written texts authored by researchers across multiple\\ndomains to simulate undergraduate academic writing alongside\\ncorresponding texts generated using ChatGPT. Elkhatat et\\nal. [17] evaluated the effectiveness of various AI detectors\\nand classifier tools, including OpenAI, Writer, Copyleaks,\\nGPTZero, and CrossPlag, in distinguishing between human-\\nauthored content and AI-generated text. Their dataset com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'introduction', 'chunk_id': 94}, page_content='authored content and AI-generated text. Their dataset com-\\nprised human and ChatGPT-authored text pairs. The human\\ntexts were collected from the introduction sections of lab re-\\nports written by undergraduate chemical engineering students\\nin 2018, while the AI texts were generated by ChatGPT-3.5\\nand ChatGPT-4 on the topic ”Application of Cooling Towers\\nin the Engineering Process”. The study found that the detection\\ntools exhibited higher accuracy in identifying text generated'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'introduction', 'chunk_id': 95}, page_content='tools exhibited higher accuracy in identifying text generated\\nby GPT-3.5 compared to GPT-4. Alamleh et al. [21] collected\\na dataset of 500 responses to 250 computer science questions,\\nencompassing essay-style answers and programming tasks in\\nC and Python, collected from computer science students and\\nGPT-3.0. Each answer was labeled according to its source.\\nThe authors trained various classifiers to detect GPT responses,\\nincluding logistic regression (LR), decision trees (DT), support'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'introduction', 'chunk_id': 96}, page_content='including logistic regression (LR), decision trees (DT), support\\nvector machines (SVM), neural networks (NN), and random\\nforests (RF). An et al. [22] develop a machine-generated text\\ndetection method using a similarity function. Their method cal-\\nculates the similarity scores within machine-generated essays\\nand between human-written and machine-generated essays to\\nestimate the likelihood of an essay being machine-generated.\\nMindner et al. [25] created a corpus comprising 500 articles'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'introduction', 'chunk_id': 97}, page_content='Mindner et al. [25] created a corpus comprising 500 articles\\non ten educational topics from various school subjects. They\\nutilized traditional and novel features across eight categories\\n(i.e., perplexity, semantic, list lookup, document, error-based,\\nreadability, AI feedback, and text vector features) to differ-\\nentiate between human-written and AI-generated texts. Also,\\nthey trained three classifiers, including XGBoost, multi-layer\\nperceptrons (MLP), and RF, to detect the AI-generated texts.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'introduction', 'chunk_id': 98}, page_content='perceptrons (MLP), and RF, to detect the AI-generated texts.\\nShijaku et al. [26] developed an XGBoost-based AI-generated\\ntext detection method. They trained and tested the proposed\\nmethod using a dataset containing 252 essays sourced from\\nTOEFL preparation materials, with half of the essays written\\nby humans and the other half generated by ChatGPT. Yu\\net al. [18] employed DistilBERT, BERT, Roberta, BERT-\\nmultilingual, and PubMedBERT to distinguish human-written'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'introduction', 'chunk_id': 99}, page_content='multilingual, and PubMedBERT to distinguish human-written\\nfrom ChatGPT-generated abstracts. Bhattacharjee et al. [28]\\nemployed ChatGPT to detect the machine-generated text gen-\\nerated by 19 different text generators. ChatGPT (GPT-3.5) and\\nChatGPT (GPT-4) are utilized for the classification task. They\\nfound that while ChatGPT (GPT-3.5) has limitations in detect-\\ning machine-generated text, it reliably identifies human-written\\ncontent. In contrast, GPT-4 tends to misclassify most texts as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'introduction', 'chunk_id': 100}, page_content='content. In contrast, GPT-4 tends to misclassify most texts as\\nmachine-generated, suggesting that its detection capabilities\\nmay be affected by updates designed to mitigate misuse.\\nSadiq et al. [27] focused on detecting machine-generated on\\nsocial media platforms. They proposed a FastText-based deep\\nlearning method to classify tweets as human-generated or bot-\\ngenerated. Katib et al. [20] integrated a Tunicate swarm algo-\\nrithm and a long short-term memory recurrent neural network'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'introduction', 'chunk_id': 101}, page_content='rithm and a long short-term memory recurrent neural network\\n(LSTMRNN) to develop a new machine-generated text called\\nTSA-LSTMRNN. They used the Tunicate swarm algorithm\\nto optimize the parameters of the LSTMRNN model, thus\\nenhancing detection performance. The LSTMRNN detects the\\nmachine-generated text. Hu et al. [24] introduced a robust\\nAI-text detection method using adversarial learning called\\nRADAR. RADAR leverages the paraphraser and a detector'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'introduction', 'chunk_id': 102}, page_content='RADAR. RADAR leverages the paraphraser and a detector\\nof adversarial learning. The paraphraser generates text that\\nmimics human writing to evade detection, while the detector\\naims to accurately identify AI-generated content. The model\\neffectively manages imbalanced data and was tested on four\\ndatasets: news summarization, question answering, creative\\nwriting prompts, and English language testing (TOEFL). Wang\\net al. [19] developed a GPT detection method using a convo-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'introduction', 'chunk_id': 103}, page_content='et al. [19] developed a GPT detection method using a convo-\\nlution neural network (CNN) and a self-attention mechanism\\ncalled, SeqXGPT. Chen et al. [23] introduced statistical-\\nbased deep learning detection of machine-generated text called\\nSTADEE. Their method leverages statistical text features, such\\nas probability, rank, cumulative probability, and information\\nentropy of tokens within the text, with a sequence-based deep\\nclassifier.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'introduction', 'chunk_id': 104}, page_content='TABLE I\\nSUMMARY OF LITERATURE REVIEW\\nRef Year Domain Dataset Technique\\n[16] 2023 Academic Texts written by researchers and their corresponding\\nChatGPT text\\n12 publicly open-source tools and two commercial\\nsystems (Turnitin and PlagiarismCheck)\\n[17] 2023 Academic Introduction sections of lab reports and texts gen-\\nerated by chatGPT-3.5 and chatGPT-4 on the topic\\n”Application of Cooling Towers in the Engineering\\nProcess”\\nAI detectors and classifier tools, namely OpenAI,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'introduction', 'chunk_id': 105}, page_content='Process”\\nAI detectors and classifier tools, namely OpenAI,\\nWriter, Copyleaks, GPTZero, and CrossPlag.\\n[14] 2023 Academic 50 human-written abstracts and 50 ChatGPT-\\ngenerated abstracts\\nManual and AI detection tools\\n[18] 2023 Academic 15,395 human-written abstracts and 35,304\\nChatGPT-generated abstracts\\nDistilbert, BERT, Roberta, BERT-multilingual, and\\nPubMedBERT\\n[19] 2023 Multi-domain Corpus SeqXGPT\\n[20] 2023 Multi-domain Text TSA-LSTM-RNN model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'introduction', 'chunk_id': 106}, page_content='[20] 2023 Multi-domain Text TSA-LSTM-RNN model\\n[21] 2023 Academic 500 responses from both students and GPT for 250\\nquestions\\nLR, DT, SVM, NN, and RF\\n[22] 2023 Academic 12,978 essays written by seventh to tenth graders Statistical approach\\n[23] 2023 Multi-domain HC3, THUCNews, and CPM-CNews Statistical features with deep learning\\n[24] 2023 Multi-domain Corpus Adversarial Learning\\n[25] 2023 Academic Corpus XGBoost, Random Forest, MLP'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'introduction', 'chunk_id': 107}, page_content='[25] 2023 Academic Corpus XGBoost, Random Forest, MLP\\n[26] 2023 Academic TOEFL preparation materials XGBoost and SHAP values\\n[27] 2023 Social media TweepFake Deep learning with FastText embeddings\\n[28] 2024 News TuringBench ChatGPT 3.5 and ChatGPT 4\\n[15] 2024 Academic AIGTxt Statistical features with CNN and MLP\\nFig. 2. Sample Reponse of Gemini.\\nIII. E XTENDED -AIGT XT DATASET\\nExtended-AIGTxt is an extension of AIGTxt [15] that\\nincludes human-written text alongside content generated by'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 108}, page_content='includes human-written text alongside content generated by\\nChatGPT and Gemini. The ultimate goal of extending AIGTxt\\nis to provide the scientific community with a public benchmark\\ndataset to develop machine-generated scientific text detection\\nmethods that can distinguish between content authored by\\nhumans and various large language models (LLMs). The\\ncollection procedure of Extended-AIGTxt is similar to that\\nfollowed when collecting the AIGTxt dataset. Collecting'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 109}, page_content='followed when collecting the AIGTxt dataset. Collecting\\nExtended-AIGTxt involves generating the Gemini-written text\\ncorresponding to the human-written text of AIGTxt. Diverse\\nprompts are employed to gather Gemini-generated text, such\\nas ”Rewrite the following paragraphs more professionally and\\nwith an IEEE scientific citation:”. Figure 2 shows a sample\\nGemini response for the mentioned query.\\nThe collected Gemini-generated text is combined with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 110}, page_content='The collected Gemini-generated text is combined with\\nAIGTxt to create Extended-AIGTxt. Extended-AIGTxt con-\\ntains three classes, as shown in Figure 3.\\nFig. 3. Sample of Extended-AIGTxt Dataset.\\n• Human-generated text. Scientific content written by hu-\\nmans and collected from scholarly articles published in\\nacademic journals.\\n• ChatGPT-generated text. The scientific content was gen-\\nerated using ChatGPT 3.5.\\n• Gemini-generated text. The scientific content was gener-\\nated using Google Gemini.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 111}, page_content='ated using Google Gemini.\\nTABLE II\\nSTATISTICAL SUMMARY OF EXTENDED -AIGT XT DATASET.\\nMeasure Value\\nTotal number of observations 3000\\nNumber of records per class 1000\\nNumber of classes 3\\nNumber of Topics 10\\nTable II shows a statistical summary of the Extended-\\nAIGTxt dataset. Extended-AIGTxt includes 1000 records per\\nclass distributed across ten distinct topics. Table III presents a\\nstatistical summary per class. It is worth mentioning that these'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 112}, page_content='statistical summary per class. It is worth mentioning that these\\nstatistics are computed after applying preprocessing steps,\\nincluding lemmatization and excluding the stopwords and\\ncitations. Gemini class has the highest word count (126,038),'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 113}, page_content='TABLE III\\nSTATISTICAL SUMMARY OF EXTENDED -AIGT XT PER CLASS .\\nMeasure Human ChatGPT Gemini\\nTotal number of words 113,985 117,076 126,038\\nTotal number of unique words 12,980 12,071 12,117\\nAverage paragraph length (word) 113.98 117.08 126.05\\nAverage lexical Diversity 0.7699 0.7932 0.7885\\nindicating a tendency for more verbose or detailed responses\\nthan other classes. The human class includes the highest\\nnumber of unique words (12,980), indicating a richer vocab-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 114}, page_content='number of unique words (12,980), indicating a richer vocab-\\nulary. ChatGPT class has the highest average lexical diversity\\n(0.7932), indicating more varied vocabulary usage.\\nIV. T RANSFORMER CATCH LLM M ETHOD\\nThe ultimate objective of this research is to detect machine-\\ngenerated scientific textual content. This study proposes a\\nnew LLM-generated scientific text detection method using\\ntransformers called TcLLM. TcLLM employs BERT and\\nRoBERTa transformers to capture long-range dependencies'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 115}, page_content='RoBERTa transformers to capture long-range dependencies\\nand contextual relationships within scientific text.\\nTcLLM involves two steps: (1) data preparation and (2)\\nfine-tuning the transformer. The data preparation process for\\nTcLLM is summarized below. Table IV shows an example of\\nthe data preparation steps for BERT and RoBERTa.\\n• Tokenization. TcLLM converts the input text into tokens\\n(i.e., words). This step enables the model to understand'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 116}, page_content='(i.e., words). This step enables the model to understand\\nand process the text by treating each token as a discrete\\nunit. TcLLM employs BERTTokenizer and RobertaTo-\\nkenizer. BERTTokenizer tokenizes the text using Word-\\nPiece tokenization, breaking words into smaller subword\\nunits in the model’s vocabulary.\\n• Adding Special Tokens. The tokenizer adds special tokens\\nto the input text to convey additional information to the\\nmodel, including the structure and context of the input'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 117}, page_content='model, including the structure and context of the input\\ndata. For instance, BERTTokenizer adds ”[CLS]” to mark\\nthe start of a sentence and ”[SEP]” to separate multiple\\nsentences. RobertaTokenizer uses ”<s>” to mark the start\\nof a sentence and ” </s>” to mark the end of a sequence.\\n• Padding. The encoded text is padded or truncated to a\\nsingle, fixed length. Truncation removes tokens from the\\ntext end that exceed a certain maximum length, while\\npadding adds [PAD] tokens to shorter sequences.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 118}, page_content='padding adds [PAD] tokens to shorter sequences.\\n• Encoding. Once the input text is tokenized, each token\\nis mapped to its corresponding numerical ID in the\\nvocabulary. RobertaTokenizer uses Byte Pair Encoding\\nto convert text into a sequence of numbers suitable for\\nfeeding into the RoBERTa language model.\\n• Attention Masking. The tokenizer generates an attention\\nmask to help the model differentiate between actual input\\nand padding tokens during training and inference.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 119}, page_content='and padding tokens during training and inference.\\nTcLLM passes the prepared input sequence into the BERT\\nand RoBERTa models. These models use a series of trans-\\nformer layers to process the input text, where each layer\\nfocuses on different parts of the input and combines infor-\\nmation to create contextualized word representations. These\\nrepresentations capture complex semantic and syntactic de-\\ntails. BERT and RoBERTa can handle text inputs of varying'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 120}, page_content='tails. BERT and RoBERTa can handle text inputs of varying\\nlengths and complexities, making them highly effective tools\\nfor natural language understanding and processing across a\\nwide range of domains and applications. After fine-tuning,\\nthe BERT or RoBERTa model processes input scientific text\\nthrough its transformer architecture, generating contextualized\\ntext representations passed into a classification layer. This\\nlayer predicts the author of the scientific text, whether it is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 121}, page_content='layer predicts the author of the scientific text, whether it is\\nhuman, ChatGPT, or Gemini, based on the features learned\\nduring training.\\nV. E XPERIMENT DESIGN AND RESULTS\\nDifferent experiments are conducted to evaluate the per-\\nformance of the proposed method. The assessment relies on\\nfour key evaluation metrics: accuracy, precision, recall, and\\nF1-score. Accuracy represents the percentage of correctly\\nclassified texts. Precision measures the proportion of correctly'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 122}, page_content='classified texts. Precision measures the proportion of correctly\\nclassified instances among all predicted instances of a given\\nclass. Recall quantifies the proportion of correctly classified\\ninstances relative to the total actual instances of that class.\\nFinally, the F1-score is the harmonic mean of precision and\\nrecall, providing a balanced measure of model performance.\\nSeven experiments were conducted to evaluate the per-\\nformance of the LLM-generated text identification models.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 123}, page_content='formance of the LLM-generated text identification models.\\nThe experiments varied in the number of training epochs\\nand data-splitting strategies to assess the model’s robustness\\nand generalization capabilities. Specifically, three experiments\\n(Exp. 1–3) utilized an 80:20 train-test split with epoch values\\nset at 10, 20, and 30, respectively. Experiments 4 and 6\\nemployed a 5-fold cross-validation scheme with 10 and 20\\nepochs to further examine performance under cross-validation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 124}, page_content='epochs to further examine performance under cross-validation\\nsettings. Experiments 5 and 7 also applied a 10-fold cross-\\nvalidation approach with 10 and 20 epochs, respectively. In\\nall experiments, Adam optimizer has been used. Table V\\nsummarizes the setup of the evaluation experiments.\\nTable VI summarizes the performance metrics obtained\\nfrom TcLLM using two different implementations: BERT\\nand RoBERTa, across seven experimental settings. In general,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 125}, page_content='and RoBERTa, across seven experimental settings. In general,\\nRoBERTa consistently outperforms BERT in all experiments,\\ndemonstrating superior classification performance.\\nIn experiments using a split of 80:20 train-test (Exp. 1-\\n3), RoBERTa achieves higher accuracy and F1 scores than\\nBERT, with the best performance recorded in Exp. 2, where\\nRoBERTa achieves an accuracy of 0.9367 and an F1 score\\nof 0.9356, compared to BERT’s 0.8283 accuracy and 0.8230'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 126}, page_content='of 0.9356, compared to BERT’s 0.8283 accuracy and 0.8230\\nF1 score. Similarly, in cross-validation settings, RoBERTa\\noutperforms BERT in 5-fold (Exp. 4, 6) and 10-fold (Exp. 5,\\n7) evaluations. In particular, Exp. 4, which used 5-fold cross-\\nvalidation with 10 epochs, shows a significant performance\\ngap, with RoBERTa achieving an F1 score of 0.9797 compared\\nto BERT’s 0.8797.\\nVI. C ONCLUSION AND FUTURE WORKS\\nLLMs, such as ChatGPT and Gemini, have significantly ad-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 127}, page_content='LLMs, such as ChatGPT and Gemini, have significantly ad-\\nvanced the generation of scientific text, blurring the distinction'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 128}, page_content='TABLE IV\\nEXAMPLE OF TEXT PREPROCESSING USING BERTT OKENIZER AND ROBERTATOKENIZER\\nInput Text: ”Natural language processing is amazing”\\nPreprocessing Step BERT RoBERTa\\nTokenization [’natural’, ’language’, ’processing’, ’is’, ’amazing’] [’Natural’, ’language’, ’processing’, ’is’, ’amazing’]\\nAdding Special Tokens [’[CLS]’, ’Natural’, ’language’, ’processing’, ’is’,\\n’amazing’, ’[SEP]’]\\n[’<s>’, ’Natural’, ’language’, ’processing’, ’is’,\\n’amazing’, ’</s>’]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 129}, page_content='[’<s>’, ’Natural’, ’language’, ’processing’, ’is’,\\n’amazing’, ’</s>’]\\nPadding [’[CLS]’, ’Natural’, ’language’, ’processing’, ’is’,\\n’amazing’, ’[SEP]’, ’[PAD]’, ’[PAD]’, ’[PAD]’\\n[’<s>’, ’Natural’, ’language’, ’processing’, ’is’,\\n’amazing’, ’</s>’, ’<PAD >’, ’<PAD >’, ’<PAD >’\\nEncoding [101, 2300, 2203, 2324, 2003, 2025, 102, 0, 0, 0] [0, 1234, 2345, 3456, 4567, 5678, 2, 1, 1, 1]\\nAttention Masking [1, 1, 1, 1, 1, 1, 1, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\\nTABLE V'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 130}, page_content='TABLE V\\nSUMMARY OF EVALUATION EXPERIMENTS\\nExp. Epochs Data Splitting\\n1 10 80:20\\n2 20 80:20\\n3 30 80:20\\n4 10 5-fold\\n5 10 10-fold\\n6 20 5-fold\\n7 20 10-fold\\nbetween human and machine-authored content. This study pro-\\nposed Transformer catch LLM (TcLLM), a transformer-based\\nmethod utilizing fine-tuned BERT and RoBERTa models to\\ndetect LLM-generated scientific text. Additionally, the AIGTxt\\ndataset is expanded to include text generated by Gemini,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 131}, page_content='dataset is expanded to include text generated by Gemini,\\nenriching the dataset with diverse writing styles from multiple\\nLLMs.\\nMultiple experiments were conducted using both BERT and\\nRoBERTa implementations to evaluate TcLLM. The results\\nindicate that TcLLM-RoBERTa outperforms TcLLM-BERT,\\nachieving higher accuracy and reduced misclassification rates\\nin distinguishing between human- and LLM-generated text.\\nFor future work, we will explore additional LLMs, including'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 132}, page_content='For future work, we will explore additional LLMs, including\\nClaude-4 and LLaMA 3, using a larger dataset to assess their\\nability to generate scientific content. We will also investi-\\ngate advanced transformers like BigBird for processing long\\ndocuments and Longformer for efficient attention in extended\\nsequences.\\nREFERENCES\\n[1] F. Khaled and M. S. H. Al-Tamimi, “Plagiarism detection methods and\\ntools: An overview,” Iraqi Journal of Science , pp. 2771–2783, 2021.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 133}, page_content='tools: An overview,” Iraqi Journal of Science , pp. 2771–2783, 2021.\\n[2] I. Masic, “Plagiarism in scientific research and publications and how to\\nprevent it,” Materia socio-m ´edica, vol. 26, no. 2, p. 141, 2014.\\n[3] Y . Chang, X. Wang, J. Wang, Y . Wu, L. Yang, K. Zhu, H. Chen, X. Yi,\\nC. Wang, Y . Wang et al. , “A survey on evaluation of large language\\nmodels,” ACM Transactions on Intelligent Systems and Technology ,\\nvol. 15, no. 3, pp. 1–45, 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 134}, page_content='vol. 15, no. 3, pp. 1–45, 2024.\\n[4] H. Zhao, H. Chen, F. Yang, N. Liu, H. Deng, H. Cai, S. Wang, D. Yin,\\nand M. Du, “Explainability for large language models: A survey,” ACM\\nTransactions on Intelligent Systems and Technology , vol. 15, no. 2, pp.\\n1–38, 2024.\\n[5] A. Koubaa, W. Boulila, L. Ghouti, A. Alzahem, and S. Latif, “Exploring\\nchatgpt capabilities and limitations: A survey,” IEEE Access, 2023.\\n[6] T. Zhang, F. Ladhak, E. Durmus, P. Liang, K. McKeown, and T. B.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 135}, page_content='[6] T. Zhang, F. Ladhak, E. Durmus, P. Liang, K. McKeown, and T. B.\\nHashimoto, “Benchmarking large language models for news summa-\\nrization,” Transactions of the Association for Computational Linguistics,\\nvol. 12, pp. 39–57, 2024.\\n[7] S. Kreps, R. M. McCain, and M. Brundage, “All the news that’s fit to\\nfabricate: Ai-generated text as a tool of media misinformation,” Journal\\nof Experimental Political Science , vol. 9, no. 1, p. 104–117, 2022.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 136}, page_content='of Experimental Political Science , vol. 9, no. 1, p. 104–117, 2022.\\n[8] Y . Liu, Z. Zhang, W. Zhang, S. Yue, X. Zhao, X. Cheng, Y . Zhang, and\\nH. Hu, “Argugpt: evaluating, understanding and identifying argumenta-\\ntive essays generated by gpt models,” arXiv preprint arXiv:2304.07666,\\n2023.\\n[9] A. Yuan, A. Coenen, E. Reif, and D. Ippolito, “Wordcraft: Story\\nwriting with large language models,” in 27th International Conference\\non Intelligent User Interfaces , ser. IUI ’22. New York, NY , USA:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 137}, page_content='on Intelligent User Interfaces , ser. IUI ’22. New York, NY , USA:\\nAssociation for Computing Machinery, 2022, p. 841–852. [Online].\\nAvailable: https://doi.org/10.1145/3490099.3511105\\n[10] S. Mitrovi ´c, D. Andreoletti, and O. Ayoub, “Chatgpt or human? detect\\nand explain. explaining decisions of machine learning model for de-\\ntecting short chatgpt-generated text,” arXiv preprint arXiv:2301.13852 ,\\n2023.\\n[11] T. H. Kung, M. Cheatham, A. Medenilla, C. Sillos, L. De Leon,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 138}, page_content='2023.\\n[11] T. H. Kung, M. Cheatham, A. Medenilla, C. Sillos, L. De Leon,\\nC. Elepa ˜no, M. Madriaga, R. Aggabao, G. Diaz-Candido, J. Maningo\\net al. , “Performance of chatgpt on usmle: potential for ai-assisted\\nmedical education using large language models,” PLoS digital health ,\\nvol. 2, no. 2, p. e0000198, 2023.\\n[12] M. Zhang, L. Wu, T. Yang, B. Zhu, and Y . Liu, “The three-dimensional\\nporous mesh structure of cu-based metal-organic-framework-aramid'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 139}, page_content='porous mesh structure of cu-based metal-organic-framework-aramid\\ncellulose separator enhances the electrochemical performance of lithium\\nmetal anode batteries.” Surfaces and Interfaces , p. 104081, 2024.\\n[13] ChatGPT and A. Zhavoronkov, “Rapamycin in the context of pascal’s\\nwager: generative pre-trained transformer perspective,” Oncoscience,\\nvol. 9, pp. 82–84, 2022.\\n[14] C. A. Gao, F. M. Howard, N. S. Markov, E. C. Dyer, S. Ramesh,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 140}, page_content='[14] C. A. Gao, F. M. Howard, N. S. Markov, E. C. Dyer, S. Ramesh,\\nY . Luo, and A. T. Pearson, “Comparing scientific abstracts generated by\\nchatgpt to real abstracts with detectors and blinded human reviewers,”\\nNPJ digital medicine , vol. 6, no. 1, p. 75, 2023.\\n[15] B. Alhijawi, R. Jarrar, A. AbuAlRub, and A. Bader, “Deep learning de-\\ntection method for large language models-generated scientific content,”\\nNeural Computing and Applications , 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 141}, page_content='Neural Computing and Applications , 2024.\\n[16] D. Weber-Wulff, A. Anohina-Naumeca, S. Bjelobaba, T. Folt `ynek,\\nJ. Guerrero-Dib, O. Popoola, P. ˇSigut, and L. Waddington, “Testing\\nof detection tools for ai-generated text,” International Journal for\\nEducational Integrity, vol. 19, no. 1, p. 26, 2023.\\n[17] A. M. Elkhatat, K. Elsaid, and S. Almeer, “Evaluating the efficacy\\nof ai content detection tools in differentiating between human and ai-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 142}, page_content='of ai content detection tools in differentiating between human and ai-\\ngenerated text,” International Journal for Educational Integrity , vol. 19,\\nno. 1, p. 17, 2023.\\n[18] P. Yu, J. Chen, X. Feng, and Z. Xia, “Cheat: A large-scale dataset for\\ndetecting chatgpt-written abstracts,” arXiv preprint arXiv:2304.12008 ,\\n2023.\\n[19] P. Wang, L. Li, K. Ren, B. Jiang, D. Zhang, and X. Qiu, “Se-\\nqxgpt: Sentence-level ai-generated text detection,” arXiv preprint\\narXiv:2310.08903, 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 143}, page_content='arXiv:2310.08903, 2023.\\n[20] I. Katib, F. Y . Assiri, H. A. Abdushkour, D. Hamed, and M. Ragab,\\n“Differentiating chat generative pretrained transformer from humans:\\nDetecting chatgpt-generated text and human text using machine learn-\\ning,” Mathematics, vol. 11, no. 15, p. 3400, 2023.\\n[21] H. Alamleh, A. A. S. AlQahtani, and A. ElSaid, “Distinguishing\\nhuman-written and chatgpt-generated text using machine learning,” in\\n2023 Systems and Information Engineering Design Symposium (SIEDS),'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 144}, page_content='2023 Systems and Information Engineering Design Symposium (SIEDS),\\n2023, pp. 154–158.\\n[22] R. An, Y . Yang, F. Yang, and S. Wang, “Use prompt to differentiate text'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 145}, page_content='TABLE VI\\nRESULTS OF TCLLM.\\nExperiment Model Accuracy Recall Precision F1 Score\\nExp 1 BERT 0.8317 0.8243 0.8297 0.8225\\nRoBERTa 0.8917 0.8988 0.9060 0.8897\\nExp 2 BERT 0.8283 0.8232 0.8236 0.8230\\nRoBERTa 0.9367 0.9377 0.9353 0.9356\\nExp 3 BERT 0.7983 0.7863 0.8005 0.7800\\nRoBERTa 0.9400 0.9398 0.9385 0.9389\\nExp 4 BERT 0.8812 0.8812 0.8804 0.8797\\nRoBERTa 0.9250 0.9293 0.9291 0.9240\\nExp 5 BERT 0.7934 0.7934 0.8219 0.7935\\nRoBERTa 0.9287 0.9287 0.9368 0.9281\\nExp 6 BERT 0.7934 0.7934 0.8219 0.7935'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 146}, page_content='RoBERTa 0.9287 0.9287 0.9368 0.9281\\nExp 6 BERT 0.7934 0.7934 0.8219 0.7935\\nRoBERTa 0.9334 0.9334 0.9355 0.9332\\nExp 7 BERT 0.8018 0.8018 0.82647 0.80414\\nRoBERTa 0.9217 0.9256 0.9249 0.9205\\ngenerated by chatgpt and humans,” Machine Learning with Applications,\\nvol. 14, p. 100497, 2023.\\n[23] Z. Chen and H. Liu, “Stadee: Statistics-based deep detection of machine\\ngenerated text,” in International Conference on Intelligent Computing .\\nSpringer, 2023, pp. 732–743.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 147}, page_content='Springer, 2023, pp. 732–743.\\n[24] X. Hu, P.-Y . Chen, and T.-Y . Ho, “Radar: Robust ai-text detection\\nvia adversarial learning,” Advances in Neural Information Processing\\nSystems, vol. 36, pp. 15 077–15 095, 2023.\\n[25] L. Mindner, T. Schlippe, and K. Schaaff, “Classification of human-and\\nai-generated texts: Investigating features for chatgpt,” in International\\nConference on Artificial Intelligence in Education Technology. Springer,\\n2023, pp. 152–170.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 148}, page_content='2023, pp. 152–170.\\n[26] R. Shijaku and E. Canhasi, “Chatgpt generated text detection,” Pub-\\nlisher: Unpublished, 2023.\\n[27] S. Sadiq, T. Aljrees, and S. Ullah, “Deepfake detection on social\\nmedia: Leveraging deep learning and fasttext embeddings for identifying\\nmachine-generated tweets,” IEEE Access, 2023.\\n[28] A. Bhattacharjee and H. Liu, “Fighting fire with fire: can chatgpt detect\\nai-generated text?” ACM SIGKDD Explorations Newsletter , vol. 25,\\nno. 2, pp. 14–21, 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 149}, page_content='no. 2, pp. 14–21, 2024.\\n[29] T. Folt `ynek, N. Meuschke, and B. Gipp, “Academic plagiarism detec-\\ntion: a systematic literature review,” ACM Computing Surveys (CSUR) ,\\nvol. 52, no. 6, pp. 1–42, 2019.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "chunks = split_documents_section_aware(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7834d55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 140}, page_content='[14] C. A. Gao, F. M. Howard, N. S. Markov, E. C. Dyer, S. Ramesh,\\nY . Luo, and A. T. Pearson, “Comparing scientific abstracts generated by\\nchatgpt to real abstracts with detectors and blinded human reviewers,”\\nNPJ digital medicine , vol. 6, no. 1, p. 75, 2023.\\n[15] B. Alhijawi, R. Jarrar, A. AbuAlRub, and A. Bader, “Deep learning de-\\ntection method for large language models-generated scientific content,”\\nNeural Computing and Applications , 2024.')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[140]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef8d3b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e22c9643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'unknown': 55,\n",
       "         'results': 35,\n",
       "         'methodology': 28,\n",
       "         'introduction': 21,\n",
       "         'references': 7,\n",
       "         'abstract': 4})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter([c.metadata[\"section\"] for c in chunks])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f6ab620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "et al. [19] developed a GPT detection method using a convo-\n",
      "lution neural network (CNN) and a self-attention mechanism\n",
      "called, SeqXGPT. Chen et al. [23] introduced statistical-\n",
      "based deep learning detection of machine-generated text called\n",
      "STADEE. Their method leverages statistical text features, su\n",
      "{'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'introduction', 'chunk_id': 103}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "c = random.choice(chunks)\n",
    "print(c.page_content[:300])\n",
    "print(c.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a819944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90346827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        device: str = \"cpu\"\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            print(f\"🔹 Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name, device=self.device)\n",
    "            dim = self.model.get_sentence_embedding_dimension()\n",
    "            print(f\"✅ Model loaded | Embedding dimension: {dim}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        batch_size: int = 32\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts (RAG optimized)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "\n",
    "        print(f\"🔹 Generating embeddings for {len(texts)} chunks...\")\n",
    "\n",
    "        embeddings = self.model.encode(\n",
    "            texts,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=True,\n",
    "            normalize_embeddings=True  # ⭐ VERY IMPORTANT\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Embeddings generated | Shape: {embeddings.shape}\")\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "273b4a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "✅ Model loaded | Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x120cbc4d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2430dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import pickle\n",
    "import numpy as np\n",
    "import faiss\n",
    "from typing import List, Any\n",
    "from langchain_community.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29e65ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings using FAISS\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        index_path: str = \"../data/faiss_store\",\n",
    "        index_name: str = \"pdf_index\"\n",
    "    ):\n",
    "        self.index_path = index_path\n",
    "        self.index_name = index_name\n",
    "\n",
    "        os.makedirs(self.index_path, exist_ok=True)\n",
    "\n",
    "        self.index_file = os.path.join(self.index_path, f\"{self.index_name}.index\")\n",
    "        self.meta_file = os.path.join(self.index_path, f\"{self.index_name}_meta.pkl\")\n",
    "\n",
    "        self.index = None\n",
    "        self.metadatas = []\n",
    "        self.documents = []\n",
    "\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        if os.path.exists(self.index_file) and os.path.exists(self.meta_file):\n",
    "            print(\"🔹 Loading existing FAISS index...\")\n",
    "            self.index = faiss.read_index(self.index_file)\n",
    "            with open(self.meta_file, \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "                self.documents = data[\"documents\"]\n",
    "                self.metadatas = data[\"metadatas\"]\n",
    "            print(f\"✅ Loaded FAISS index with {self.index.ntotal} vectors\")\n",
    "        else:\n",
    "            print(\"🔹 Creating new FAISS index...\")\n",
    "            self.index = None\n",
    "            self.documents = []\n",
    "            self.metadatas = []\n",
    "\n",
    "    def add_documents(\n",
    "        self,\n",
    "        documents: List[Any],\n",
    "        embeddings: np.ndarray\n",
    "    ):\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Documents and embeddings count mismatch\")\n",
    "\n",
    "        # Initialize FAISS index on first insert\n",
    "        if self.index is None:\n",
    "            dim = embeddings.shape[1]\n",
    "            self.index = faiss.IndexFlatIP(dim)  # cosine similarity (normalized embeddings)\n",
    "            print(f\"✅ FAISS IndexFlatIP initialized (dim={dim})\")\n",
    "\n",
    "        print(f\"🔹 Adding {len(documents)} documents to FAISS index\")\n",
    "\n",
    "        self.index.add(embeddings.astype(np.float32))\n",
    "\n",
    "        for doc in documents:\n",
    "            self.documents.append(doc.page_content)\n",
    "            self.metadatas.append(doc.metadata)\n",
    "\n",
    "        print(f\"✅ Total vectors in FAISS index: {self.index.ntotal}\")\n",
    "\n",
    "    def similarity_search(\n",
    "        self,\n",
    "        query_embedding: np.ndarray,\n",
    "        k: int = 5\n",
    "    ):\n",
    "        if self.index is None or self.index.ntotal == 0:\n",
    "            raise ValueError(\"FAISS index is empty\")\n",
    "\n",
    "        query_embedding = query_embedding.astype(np.float32)\n",
    "        scores, indices = self.index.search(query_embedding, k)\n",
    "\n",
    "        results = []\n",
    "        for idx in indices[0]:\n",
    "            results.append({\n",
    "                \"page_content\": self.documents[idx],\n",
    "                \"metadata\": self.metadatas[idx]\n",
    "            })\n",
    "\n",
    "        return results\n",
    "\n",
    "    def save(self):\n",
    "        if self.index is None:\n",
    "            return\n",
    "\n",
    "        faiss.write_index(self.index, self.index_file)\n",
    "        with open(self.meta_file, \"wb\") as f:\n",
    "            pickle.dump(\n",
    "                {\n",
    "                    \"documents\": self.documents,\n",
    "                    \"metadatas\": self.metadatas\n",
    "                },\n",
    "                f\n",
    "            )\n",
    "        print(\"💾 FAISS index saved successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82cbd8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Creating new FAISS index...\n",
      "🔹 Generating embeddings for 150 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:02<00:00,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings generated | Shape: (150, 384)\n",
      "✅ FAISS IndexFlatIP initialized (dim=384)\n",
      "🔹 Adding 150 documents to FAISS index\n",
      "✅ Total vectors in FAISS index: 150\n",
      "💾 FAISS index saved successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x12203d2d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore = VectorStore()\n",
    "\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "vectorstore.add_documents(chunks, embeddings)\n",
    "vectorstore.save()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c095e033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Generating embeddings for 1 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 102.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings generated | Shape: (1, 384)\n",
      "media misinformation. Also, GPT models have been used to\n",
      "generate homework exercises, TOEFL writing tasks, graduate\n",
      "record examinations writing tasks [8], creative short stories [9],\n",
      "restaurant reviews [10], the United States medical licensing\n",
      "exam [11], and scientific content [12], [13]. Figure 1 s\n",
      "{'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'introduction', 'chunk_id': 85}\n",
      "followed when collecting the AIGTxt dataset. Collecting\n",
      "Extended-AIGTxt involves generating the Gemini-written text\n",
      "corresponding to the human-written text of AIGTxt. Diverse\n",
      "prompts are employed to gather Gemini-generated text, such\n",
      "as ”Rewrite the following paragraphs more professionally and\n",
      "with \n",
      "{'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'methodology', 'chunk_id': 109}\n",
      "scholarly publications [1]. Hence, this would contradict the\n",
      "foundational principles and ethics sustained by the scientific\n",
      "community, simultaneously attributing recognition to individ-\n",
      "uals who lack respected contributions [2]. In the scientific\n",
      "community, scientific papers are the core block that \n",
      "{'producer': 'pdfTeX-1.40.26', 'creator': 'TeX', 'creationdate': '2025-11-11T17:41:06+00:00', 'moddate': '2025-11-11T17:41:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'file_type': 'pdf', 'section': 'results', 'chunk_id': 82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What methodology does the paper use?\"\n",
    "\n",
    "query_embedding = embedding_manager.generate_embeddings([query])\n",
    "\n",
    "results = vectorstore.similarity_search(query_embedding, k=3)\n",
    "\n",
    "for r in results:\n",
    "    print(r[\"page_content\"][:300])\n",
    "    print(r[\"metadata\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a57f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from FAISS vector store\"\"\"\n",
    "\n",
    "    def __init__(self, vector_store, embedding_manager):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 5,\n",
    "        score_threshold: float = 0.0\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents using FAISS\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"\\n🔍 Query: {query}\")\n",
    "        print(f\"Top-K: {top_k} | Score threshold: {score_threshold}\")\n",
    "\n",
    "        # 1️⃣ Generate query embedding (normalized)\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])\n",
    "        query_embedding = query_embedding.astype(np.float32)\n",
    "\n",
    "        # 2️⃣ FAISS similarity search\n",
    "        scores, indices = self.vector_store.index.search(query_embedding, top_k)\n",
    "\n",
    "        retrieved_docs = []\n",
    "\n",
    "        for rank, (idx, score) in enumerate(zip(indices[0], scores[0]), start=1):\n",
    "            if idx == -1:\n",
    "                continue\n",
    "\n",
    "            if score < score_threshold:\n",
    "                continue\n",
    "\n",
    "            retrieved_docs.append({\n",
    "                \"content\": self.vector_store.documents[idx],\n",
    "                \"metadata\": self.vector_store.metadatas[idx],\n",
    "                \"similarity_score\": float(score),\n",
    "                \"rank\": rank\n",
    "            })\n",
    "\n",
    "        print(f\"✅ Retrieved {len(retrieved_docs)} documents\")\n",
    "\n",
    "        return retrieved_docs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d1e5c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = RAGRetriever(\n",
    "    vector_store=vectorstore,\n",
    "    embedding_manager=embedding_manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "06eced70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Query: What Is self attention?\n",
      "Top-K: 3 | Score threshold: 0.0\n",
      "🔹 Generating embeddings for 1 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 164.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings generated | Shape: (1, 384)\n",
      "✅ Retrieved 3 documents\n",
      "\n",
      "---\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension\n",
      "Score: 0.6903805732727051\n",
      "Metadata: {'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 12}\n",
      "\n",
      "---\n",
      "the approach we take in our model.\n",
      "As side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\n",
      "from our models and present and discuss examples in the appendix. Not only do individual attention\n",
      "heads clearly learn to perform different tasks, many appear t\n",
      "Score: 0.5774766802787781\n",
      "Metadata: {'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 45}\n",
      "\n",
      "---\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[31, 2, 8].\n",
      "• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
      "and queries come from the same place, in this case, the output of the previous layer in the\n",
      "encoder. Each po\n",
      "Score: 0.542142391204834\n",
      "Metadata: {'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/utkarshupadhyay/Computer Science/Falkomeai/RAG/data/pdf_1/NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf', 'section': 'unknown', 'chunk_id': 29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = retriever.retrieve(\n",
    "    query=\"What Is self attention?\",\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "for r in results:\n",
    "    print(\"\\n---\")\n",
    "    print(r[\"content\"][:300])\n",
    "    print(\"Score:\", r[\"similarity_score\"])\n",
    "    print(\"Metadata:\", r[\"metadata\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "576822ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class LocalHFLLM:\n",
    "    \"\"\"Local HuggingFace LLM for RAG answer generation\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "        device=None,\n",
    "        max_context_tokens=1800\n",
    "    ):\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.max_context_tokens = max_context_tokens\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "        self.model.eval()\n",
    "        print(f\"✅ Loaded LLM: {model_name} on {self.device}\")\n",
    "\n",
    "    def _build_prompt(self, query: str, context: str) -> str:\n",
    "        return f\"\"\"You are an AI assistant answering questions ONLY using the provided context.\n",
    "\n",
    "Rules:\n",
    "- Use ONLY the information in the context.\n",
    "- If the answer is not present, say \"I don't know based on the provided documents.\"\n",
    "- Be concise and factual.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    def generate_response(self, query: str, context: str) -> str:\n",
    "        prompt = self._build_prompt(query, context)\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_context_tokens\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=300,\n",
    "                do_sample=False,\n",
    "                temperature=0.1\n",
    "            )\n",
    "\n",
    "        # Extract only generated answer\n",
    "        generated_tokens = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "        answer = self.tokenizer.decode(\n",
    "            generated_tokens,\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        return answer.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11ae8db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "    \"\"\"End-to-end RAG pipeline\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        retriever: RAGRetriever,\n",
    "        llm: LocalHFLLM,\n",
    "        max_context_chars: int = 4000\n",
    "    ):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.max_context_chars = max_context_chars\n",
    "\n",
    "    def _build_context(self, retrieved_docs):\n",
    "        \"\"\"\n",
    "        Build a single context string from retrieved chunks\n",
    "        \"\"\"\n",
    "        context_parts = []\n",
    "        total_length = 0\n",
    "\n",
    "        for doc in retrieved_docs:\n",
    "            chunk_text = (\n",
    "                f\"[Source: {doc['metadata'].get('source_file', 'unknown')} | \"\n",
    "                f\"Section: {doc['metadata'].get('section', 'unknown')}]\\n\"\n",
    "                f\"{doc['content']}\"\n",
    "            )\n",
    "\n",
    "            if total_length + len(chunk_text) > self.max_context_chars:\n",
    "                break\n",
    "\n",
    "            context_parts.append(chunk_text)\n",
    "            total_length += len(chunk_text)\n",
    "\n",
    "        return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    def answer(self, query: str, top_k: int = 5):\n",
    "        \"\"\"\n",
    "        Generate final answer for a query using RAG\n",
    "        \"\"\"\n",
    "\n",
    "        # 1️⃣ Retrieve relevant chunks\n",
    "        retrieved_docs = self.retriever.retrieve(\n",
    "            query=query,\n",
    "            top_k=top_k\n",
    "        )\n",
    "\n",
    "        if not retrieved_docs:\n",
    "            return \"I couldn't find relevant information in the documents.\"\n",
    "\n",
    "        # 2️⃣ Build context\n",
    "        context = self._build_context(retrieved_docs)\n",
    "\n",
    "        # 3️⃣ Generate answer using LLM\n",
    "        answer = self.llm.generate_response(\n",
    "            query=query,\n",
    "            context=context\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"answer\": answer,\n",
    "            \"sources\": [\n",
    "                {\n",
    "                    \"source_file\": d[\"metadata\"].get(\"source_file\"),\n",
    "                    \"section\": d[\"metadata\"].get(\"section\"),\n",
    "                    \"score\": d[\"similarity_score\"]\n",
    "                }\n",
    "                for d in retrieved_docs\n",
    "            ]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "991b8ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "✅ Model loaded | Embedding dimension: 384\n",
      "🔹 Loading existing FAISS index...\n",
      "✅ Loaded FAISS index with 150 vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded LLM: Qwen/Qwen2.5-1.5B-Instruct on cpu\n"
     ]
    }
   ],
   "source": [
    "# 1. Embeddings\n",
    "embedding_manager = EmbeddingManager()\n",
    "\n",
    "# 2. Vector store (already built & loaded)\n",
    "vectorstore = VectorStore()\n",
    "\n",
    "# 3. Retriever\n",
    "rag_retriever = RAGRetriever(\n",
    "    vector_store=vectorstore,\n",
    "    embedding_manager=embedding_manager\n",
    ")\n",
    "\n",
    "# 4. LLM\n",
    "llm = LocalHFLLM()\n",
    "\n",
    "# 5. RAG Pipeline\n",
    "rag_pipeline = RAGPipeline(\n",
    "    retriever=rag_retriever,\n",
    "    llm=llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c58ef95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Query: What is transformer?\n",
      "Top-K: 3 | Score threshold: 0.0\n",
      "🔹 Generating embeddings for 1 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.82it/s]\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings generated | Shape: (1, 384)\n",
      "✅ Retrieved 3 documents\n",
      "Answer:\n",
      " The Transformer is a neural network architecture designed to process sequential data such as text or speech. It was introduced in the paper \"Attention Is All You Need\" presented at NIPS 2017. The key features of the Transformer include:\n",
      "\n",
      "1. **Self-Attention Mechanism**: Unlike traditional RNNs which use recurrent connections between elements, Transformers use self-attention mechanisms to focus on relevant parts of the input sequence independently. Each position in the sequence can attend to any other position, allowing it to capture long-range dependencies without needing to store past states.\n",
      "\n",
      "2. **Encoder-Decoder Architecture**: The Transformer consists of multiple layers, including self-attention layers and feedforward networks. These layers allow the model to learn representations of sequences through successive transformations.\n",
      "\n",
      "3. **Residual Connections and Layer Normalization**: Residual connections and layer normalization are used to improve training stability and efficiency. They help maintain the gradient flow during backpropagation.\n",
      "\n",
      "4. **Multi-Head Attention**: Instead of a single head of attention, the Transformer employs multiple heads, each focusing on different aspects of the input. This increases the capacity of the model to handle complex relationships within the data.\n",
      "\n",
      "5. **Auto-Regressive Output**: The model generates predictions sequentially, meaning it depends only on its own history up to the current point. This property makes it suitable for tasks like machine translation where the next word should be influenced by the preceding words.\n",
      "\n",
      "6. **Dimensionality**: The model operates with a fixed dimensionality \\(\n",
      "\n",
      "Sources:\n",
      "{'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'section': 'unknown', 'score': 0.451557993888855}\n",
      "{'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'section': 'unknown', 'score': 0.44547104835510254}\n",
      "{'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'section': 'unknown', 'score': 0.4128899574279785}\n"
     ]
    }
   ],
   "source": [
    "response = rag_pipeline.answer(\n",
    "    query=\"What is transformer?\",\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "print(\"Answer:\\n\", response[\"answer\"])\n",
    "print(\"\\nSources:\")\n",
    "for s in response[\"sources\"]:\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c1cb25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Query: What is Attention is all you need ?\n",
      "Top-K: 3 | Score threshold: 0.0\n",
      "🔹 Generating embeddings for 1 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings generated | Shape: (1, 384)\n",
      "✅ Retrieved 3 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " Attention is all you need refers to a paper presented at NIPS 2017 that introduced the concept of self-attention as a fundamental component in neural network architectures. The paper discusses how self-attention can be used to improve the performance of various machine learning tasks by allowing the model to focus on relevant parts of input data simultaneously across multiple positions within sequences. This approach enables more efficient processing of complex data structures like text, images, and other forms of sequential data. The authors also highlight the potential for self-attention to make models more interpretable by revealing which parts of the input contribute most significantly to the output. Additionally, they explore ways to extend this idea beyond traditional language modeling tasks into broader applications such as image captioning and speech recognition. The work aims to provide a new perspective on how deep learning models can better understand and process natural language and visual content.\n",
      "\n",
      "Sources:\n",
      "{'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'section': 'unknown', 'score': 0.5043123364448547}\n",
      "{'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'section': 'unknown', 'score': 0.45623579621315}\n",
      "{'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'section': 'results', 'score': 0.4402937889099121}\n"
     ]
    }
   ],
   "source": [
    "response = rag_pipeline.answer(\n",
    "    query=\"What is Attention is all you need ?\",\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "print(\"Answer:\\n\", response[\"answer\"])\n",
    "print(\"\\nSources:\")\n",
    "for s in response[\"sources\"]:\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a419e36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "91ba95b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Query: What is Plagiarism  detection ?\n",
      "Top-K: 3 | Score threshold: 0.0\n",
      "🔹 Generating embeddings for 1 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings generated | Shape: (1, 384)\n",
      "✅ Retrieved 3 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " Plagiarism detection involves identifying content that has been generated or paraphrased from external sources without proper citations or credit given to the original source. It's a crucial aspect of academic integrity and intellectual property rights, ensuring that authors attribute their work correctly and avoid unintentional copyright infringement. The process typically includes comparing the submitted manuscript against databases of existing literature to find similarities or identical sections. Advanced techniques such as natural language processing and machine learning algorithms can be employed to automate this process more effectively.\n",
      "\n",
      "Sources:\n",
      "{'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'section': 'introduction', 'score': 0.5888799428939819}\n",
      "{'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'section': 'methodology', 'score': 0.570601761341095}\n",
      "{'source_file': 'Transformer_Based_Approach_for_Detecting_LLM_Generated_Scientific_Text4.pdf', 'section': 'methodology', 'score': 0.5602473020553589}\n"
     ]
    }
   ],
   "source": [
    "response = rag_pipeline.answer(\n",
    "    query=\"What is Plagiarism  detection ?\",\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "print(\"Answer:\\n\", response[\"answer\"])\n",
    "print(\"\\nSources:\")\n",
    "for s in response[\"sources\"]:\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745aab72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragenv (3.11.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
